Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Accuracy: 26.82%
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Accuracy: 26.82%
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Accuracy: 26.82%
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Accuracy: 26.82%
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Accuracy: 26.82%
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
**** log-parameter_analysis 运行时间： 2025-01-16 21:47:54 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
**** log-parameter_analysis 运行时间： 2025-01-16 21:48:33 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
初始化模型的准确率：
Accuracy: 26.82%
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.06243327744641907
DataOwner2: noise random: 0.054989255909617854
DataOwner3: noise random: 0.007607090840630016
DataOwner4: noise random: 0.022890580097662674
DataOwner5: noise random: 0.07195974312594888
DataOwner6: noise random: 0.05885538592512627
DataOwner7: noise random: 0.06603582725656529
DataOwner8: noise random: 0.058337104397914
DataOwner9: noise random: 0.0601634506853087
DataOwner10: noise random: 0.08721991535424461
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9984262044307662, 0.9987761276361904, 0.999976569346265, 0.9997876932064483, 0.997901748388904, 0.9985958394269219, 0.9982362565490936, 0.9986235914271477, 0.9985339222502236, 0.9969222546414809]
归一化后的数据质量列表avg_f_list: [0.9492401711889628, 0.9606968558873683, 1.0, 0.9938160877947247, 0.9320691821929448, 0.9547941174110048, 0.9430211695459734, 0.9557027336771151, 0.9527669138421895, 0.9]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3346
DataOwner1的最优x_1 = 0.1158
DataOwner2的最优x_2 = 0.1273
DataOwner3的最优x_3 = 0.1625
DataOwner4的最优x_4 = 0.1574
DataOwner5的最优x_5 = 0.0975
DataOwner6的最优x_6 = 0.1215
DataOwner7的最优x_7 = 0.1094
DataOwner8的最优x_8 = 0.1224
DataOwner9的最优x_9 = 0.1194
DataOwner10的最优x_10 = 0.0592
每个DataOwner应该贡献数据比例 xn_list = [0.11584016358304243, 0.12731003046941367, 0.1625111617628011, 0.15736934839796288, 0.09751165787892696, 0.12147362886365241, 0.10936409767128098, 0.12238205125342426, 0.11943355177036506, 0.059242445418123485]
ModelOwner的最大效用 U(Eta) = 0.5735
DONE
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.334632781236934
DataOwner1的分配到的支付 ： 0.1281
DataOwner2的分配到的支付 ： 0.1425
DataOwner3的分配到的支付 ： 0.1894
DataOwner4的分配到的支付 ： 0.1823
DataOwner5的分配到的支付 ： 0.1059
DataOwner6的分配到的支付 ： 0.1352
DataOwner7的分配到的支付 ： 0.1202
DataOwner8的分配到的支付 ： 0.1363
DataOwner9的分配到的支付 ： 0.1326
DataOwner10的分配到的支付 ： 0.0621
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC2', 'DataOwner7': 'CPC1', 'DataOwner2': 'CPC3', 'DataOwner3': 'CPC10', 'DataOwner4': 'CPC4', 'DataOwner5': 'CPC5', 'DataOwner6': 'CPC6', 'DataOwner8': 'CPC7', 'DataOwner9': 'CPC8', 'DataOwner10': 'CPC9'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC2
DataOwner7 把数据交给 CPC1
DataOwner2 把数据交给 CPC3
DataOwner3 把数据交给 CPC10
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner8 把数据交给 CPC7
DataOwner9 把数据交给 CPC8
DataOwner10 把数据交给 CPC9
DONE
----- literation 1: 模型训练 -----
CPC2调整模型中, 本轮训练的数据量为：818.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 26.82%
Epoch 1/5, Loss: 2.2901
Epoch 2/5, Loss: 2.2867
Epoch 3/5, Loss: 2.2827
Epoch 4/5, Loss: 2.2773
Epoch 5/5, Loss: 2.2700
新模型评估：
Accuracy: 35.19%
Model saved to ../../data/model/mnist_cnn_model
CPC1调整模型中, 本轮训练的数据量为：128.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 35.19%
Epoch 1/5, Loss: 2.2410
Epoch 2/5, Loss: 2.2393
Epoch 3/5, Loss: 2.2374
Epoch 4/5, Loss: 2.2357
Epoch 5/5, Loss: 2.2338
新模型评估：
Accuracy: 35.80%
Model saved to ../../data/model/mnist_cnn_model
CPC3调整模型中, 本轮训练的数据量为：598.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 35.80%
Epoch 1/5, Loss: 2.2711
Epoch 2/5, Loss: 2.2654
Epoch 3/5, Loss: 2.2584
Epoch 4/5, Loss: 2.2500
Epoch 5/5, Loss: 2.2481
新模型评估：
Accuracy: 40.67%
Model saved to ../../data/model/mnist_cnn_model
CPC10调整模型中, 本轮训练的数据量为：764.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 40.67%
Epoch 1/5, Loss: 2.2356
Epoch 2/5, Loss: 2.2253
Epoch 3/5, Loss: 2.2135
Epoch 4/5, Loss: 2.2009
Epoch 5/5, Loss: 2.1868
新模型评估：
Accuracy: 39.95%
CPC4调整模型中, 本轮训练的数据量为：370.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 40.67%
Epoch 1/5, Loss: 2.2215
Epoch 2/5, Loss: 2.2164
Epoch 3/5, Loss: 2.2083
Epoch 4/5, Loss: 2.2031
Epoch 5/5, Loss: 2.1969
新模型评估：
Accuracy: 42.21%
Model saved to ../../data/model/mnist_cnn_model
CPC5调整模型中, 本轮训练的数据量为：573.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 42.21%
Epoch 1/5, Loss: 2.2052
Epoch 2/5, Loss: 2.1951
Epoch 3/5, Loss: 2.1847
Epoch 4/5, Loss: 2.1742
Epoch 5/5, Loss: 2.1627
新模型评估：
Accuracy: 42.97%
Model saved to ../../data/model/mnist_cnn_model
CPC6调整模型中, 本轮训练的数据量为：571.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 42.97%
Epoch 1/5, Loss: 2.1676
Epoch 2/5, Loss: 2.1550
Epoch 3/5, Loss: 2.1433
Epoch 4/5, Loss: 2.1314
Epoch 5/5, Loss: 2.1178
新模型评估：
Accuracy: 46.62%
Model saved to ../../data/model/mnist_cnn_model
CPC7调整模型中, 本轮训练的数据量为：2591.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 46.62%
Epoch 1/5, Loss: 2.0734
Epoch 2/5, Loss: 2.0015
Epoch 3/5, Loss: 1.9179
Epoch 4/5, Loss: 1.8150
Epoch 5/5, Loss: 1.7044
新模型评估：
Accuracy: 59.97%
Model saved to ../../data/model/mnist_cnn_model
CPC8调整模型中, 本轮训练的数据量为：140.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 59.97%
Epoch 1/5, Loss: 1.7086
Epoch 2/5, Loss: 1.5711
Epoch 3/5, Loss: 1.6475
Epoch 4/5, Loss: 1.6114
Epoch 5/5, Loss: 1.6912
新模型评估：
Accuracy: 59.67%
CPC9调整模型中, 本轮训练的数据量为：418.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 59.97%
Epoch 1/5, Loss: 1.6904
Epoch 2/5, Loss: 1.6807
Epoch 3/5, Loss: 1.6615
Epoch 4/5, Loss: 1.6554
Epoch 5/5, Loss: 1.6353
新模型评估：
Accuracy: 62.02%
Model saved to ../../data/model/mnist_cnn_model
DONE
========================= literation: 2 =========================
----- literation 2: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3346
DataOwner1的最优x_1 = 0.1158
DataOwner2的最优x_2 = 0.1273
DataOwner3的最优x_3 = 0.1625
DataOwner4的最优x_4 = 0.1574
DataOwner5的最优x_5 = 0.0975
DataOwner6的最优x_6 = 0.1215
DataOwner7的最优x_7 = 0.1094
DataOwner8的最优x_8 = 0.1224
DataOwner9的最优x_9 = 0.1194
DataOwner10的最优x_10 = 0.0592
每个DataOwner应该贡献数据比例 xn_list = [0.11584016358304243, 0.12731003046941367, 0.1625111617628011, 0.15736934839796288, 0.09751165787892696, 0.12147362886365241, 0.10936409767128098, 0.12238205125342426, 0.11943355177036506, 0.059242445418123485]
ModelOwner的最大效用 U(Eta) = 0.5735
DONE
----- literation 2: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.334632781236934
DataOwner1的分配到的支付 ： 0.1281
DataOwner2的分配到的支付 ： 0.1425
DataOwner3的分配到的支付 ： 0.1894
DataOwner4的分配到的支付 ： 0.1823
DataOwner5的分配到的支付 ： 0.1059
DataOwner6的分配到的支付 ： 0.1352
DataOwner7的分配到的支付 ： 0.1202
DataOwner8的分配到的支付 ： 0.1363
DataOwner9的分配到的支付 ： 0.1326
DataOwner10的分配到的支付 ： 0.0621
DONE
----- literation 2: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC2
DataOwner7 把数据交给 CPC1
DataOwner2 把数据交给 CPC3
DataOwner3 把数据交给 CPC10
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner8 把数据交给 CPC7
DataOwner9 把数据交给 CPC8
DataOwner10 把数据交给 CPC9
DONE
----- literation 2: 模型训练 -----
重新调整fn，进而调整xn、Eta
正在评估DataOwner1的数据质量, 本轮评估的样本数据量为：818.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 62.02%
Epoch 1/5, Loss: 1.6299
Epoch 2/5, Loss: 1.5942
Epoch 3/5, Loss: 1.5626
Epoch 4/5, Loss: 1.5304
Epoch 5/5, Loss: 1.4972
新模型评估：
Accuracy: 64.92%
loss差为：
0.13263187958643985
单位数据loss差为：
0.00016214166208611229
正在评估DataOwner7的数据质量, 本轮评估的样本数据量为：128.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 62.02%
Epoch 1/5, Loss: 1.5747
Epoch 2/5, Loss: 1.5654
Epoch 3/5, Loss: 1.5567
Epoch 4/5, Loss: 1.5490
Epoch 5/5, Loss: 1.5408
新模型评估：
Accuracy: 61.76%
loss差为：
0.03392672538757324
单位数据loss差为：
0.00026505254209041595
正在评估DataOwner2的数据质量, 本轮评估的样本数据量为：598.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 62.02%
Epoch 1/5, Loss: 1.6138
Epoch 2/5, Loss: 1.5840
Epoch 3/5, Loss: 1.5669
Epoch 4/5, Loss: 1.5377
Epoch 5/5, Loss: 1.5095
新模型评估：
Accuracy: 62.53%
loss差为：
0.10427180528640756
单位数据loss差为：
0.00017436756736857452
正在评估DataOwner3的数据质量, 本轮评估的样本数据量为：764.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 62.02%
Epoch 1/5, Loss: 1.6010
Epoch 2/5, Loss: 1.5687
Epoch 3/5, Loss: 1.5379
Epoch 4/5, Loss: 1.5082
Epoch 5/5, Loss: 1.4788
新模型评估：
Accuracy: 63.66%
loss差为：
0.12212305267651868
单位数据loss差为：
0.00015984692758706635
正在评估DataOwner4的数据质量, 本轮评估的样本数据量为：370.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 62.02%
Epoch 1/5, Loss: 1.5337
Epoch 2/5, Loss: 1.5153
Epoch 3/5, Loss: 1.4963
Epoch 4/5, Loss: 1.4825
Epoch 5/5, Loss: 1.4663
新模型评估：
Accuracy: 63.38%
loss差为：
0.0673686067263286
单位数据loss差为：
0.0001820773154765638
正在评估DataOwner5的数据质量, 本轮评估的样本数据量为：573.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 62.02%
Epoch 1/5, Loss: 1.5873
Epoch 2/5, Loss: 1.5626
Epoch 3/5, Loss: 1.5400
Epoch 4/5, Loss: 1.5183
Epoch 5/5, Loss: 1.4974
新模型评估：
Accuracy: 63.43%
loss差为：
0.08990650706821013
单位数据loss差为：
0.00015690489889739987
正在评估DataOwner6的数据质量, 本轮评估的样本数据量为：571.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 62.02%
Epoch 1/5, Loss: 1.6395
Epoch 2/5, Loss: 1.6138
Epoch 3/5, Loss: 1.5897
Epoch 4/5, Loss: 1.5650
Epoch 5/5, Loss: 1.5419
新模型评估：
Accuracy: 65.45%
loss差为：
0.09758251243167448
单位数据loss差为：
0.00017089756993288
正在评估DataOwner8的数据质量, 本轮评估的样本数据量为：2591.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 62.02%
Epoch 1/5, Loss: 1.5622
Epoch 2/5, Loss: 1.4604
Epoch 3/5, Loss: 1.3590
Epoch 4/5, Loss: 1.2621
Epoch 5/5, Loss: 1.1627
新模型评估：
Accuracy: 73.76%
loss差为：
0.3994588735626967
单位数据loss差为：
0.00015417169956105623
正在评估DataOwner9的数据质量, 本轮评估的样本数据量为：140.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 62.02%
Epoch 1/5, Loss: 1.5613
Epoch 2/5, Loss: 1.6712
Epoch 3/5, Loss: 1.5361
Epoch 4/5, Loss: 1.6175
Epoch 5/5, Loss: 1.5694
新模型评估：
Accuracy: 63.68%
loss差为：
-0.008124669392903572
单位数据loss差为：
-5.803335280645409e-05
正在评估DataOwner10的数据质量, 本轮评估的样本数据量为：418.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 62.02%
Epoch 1/5, Loss: 1.5740
Epoch 2/5, Loss: 1.5403
Epoch 3/5, Loss: 1.5210
Epoch 4/5, Loss: 1.5060
Epoch 5/5, Loss: 1.5046
新模型评估：
Accuracy: 63.85%
loss差为：
0.06942813737051834
单位数据loss差为：
0.000166096022417508
经过服务器调节后的真实数据质量：
数据质量列表avg_f_list: [0.00016214166208611229, 0.00017436756736857452, 0.00015984692758706635, 0.0001820773154765638, 0.00015690489889739987, 0.00017089756993288, 0.00026505254209041595, 0.00015417169956105623, -5.803335280645409e-05, 0.000166096022417508]
归一化后的数据质量列表avg_f_list:[0.9681475169204917, 0.9719316206141441, 0.9674372616802317, 0.9743179049520754, 0.9665266590398393, 0.970857603614175, 1.0, 0.965680692261495, 0.9, 0.9693714516059282]
CPC2调整模型中, 本轮训练的数据量为：818.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 62.02%
Epoch 1/5, Loss: 1.6280
Epoch 2/5, Loss: 1.5906
Epoch 3/5, Loss: 1.5593
Epoch 4/5, Loss: 1.5269
Epoch 5/5, Loss: 1.4933
新模型评估：
Accuracy: 64.71%
Model saved to ../../data/model/mnist_cnn_model
CPC1调整模型中, 本轮训练的数据量为：128.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 64.71%
Epoch 1/5, Loss: 1.4318
Epoch 2/5, Loss: 1.4225
Epoch 3/5, Loss: 1.4135
Epoch 4/5, Loss: 1.4055
Epoch 5/5, Loss: 1.3968
新模型评估：
Accuracy: 64.62%
CPC3调整模型中, 本轮训练的数据量为：598.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 64.71%
Epoch 1/5, Loss: 1.4612
Epoch 2/5, Loss: 1.4307
Epoch 3/5, Loss: 1.4162
Epoch 4/5, Loss: 1.3944
Epoch 5/5, Loss: 1.3598
新模型评估：
Accuracy: 66.03%
Model saved to ../../data/model/mnist_cnn_model
CPC10调整模型中, 本轮训练的数据量为：764.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 66.03%
Epoch 1/5, Loss: 1.3385
Epoch 2/5, Loss: 1.3087
Epoch 3/5, Loss: 1.2796
Epoch 4/5, Loss: 1.2521
Epoch 5/5, Loss: 1.2244
新模型评估：
Accuracy: 70.17%
Model saved to ../../data/model/mnist_cnn_model
CPC4调整模型中, 本轮训练的数据量为：370.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 70.17%
Epoch 1/5, Loss: 1.1372
Epoch 2/5, Loss: 1.1184
Epoch 3/5, Loss: 1.1041
Epoch 4/5, Loss: 1.0880
Epoch 5/5, Loss: 1.0735
新模型评估：
Accuracy: 72.07%
Model saved to ../../data/model/mnist_cnn_model
CPC5调整模型中, 本轮训练的数据量为：573.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 72.07%
Epoch 1/5, Loss: 1.1436
Epoch 2/5, Loss: 1.1197
Epoch 3/5, Loss: 1.1013
Epoch 4/5, Loss: 1.0818
Epoch 5/5, Loss: 1.0631
新模型评估：
Accuracy: 73.32%
Model saved to ../../data/model/mnist_cnn_model
CPC6调整模型中, 本轮训练的数据量为：571.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 73.32%
Epoch 1/5, Loss: 1.0731
Epoch 2/5, Loss: 1.0488
Epoch 3/5, Loss: 1.0285
Epoch 4/5, Loss: 1.0070
Epoch 5/5, Loss: 0.9886
新模型评估：
Accuracy: 74.82%
Model saved to ../../data/model/mnist_cnn_model
CPC7调整模型中, 本轮训练的数据量为：2591.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 74.82%
Epoch 1/5, Loss: 0.9291
Epoch 2/5, Loss: 0.8585
Epoch 3/5, Loss: 0.7943
Epoch 4/5, Loss: 0.7429
Epoch 5/5, Loss: 0.6969
新模型评估：
Accuracy: 76.62%
Model saved to ../../data/model/mnist_cnn_model
CPC8调整模型中, 本轮训练的数据量为：140.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 76.62%
Epoch 1/5, Loss: 0.7203
Epoch 2/5, Loss: 0.7209
Epoch 3/5, Loss: 0.6678
Epoch 4/5, Loss: 0.6812
Epoch 5/5, Loss: 0.7740
新模型评估：
Accuracy: 78.52%
Model saved to ../../data/model/mnist_cnn_model
CPC9调整模型中, 本轮训练的数据量为：418.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.52%
Epoch 1/5, Loss: 0.6735
Epoch 2/5, Loss: 0.6646
Epoch 3/5, Loss: 0.6526
Epoch 4/5, Loss: 0.6466
Epoch 5/5, Loss: 0.6330
新模型评估：
Accuracy: 76.71%
DONE
========================= literation: 3 =========================
----- literation 3: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3484
DataOwner1的最优x_1 = 0.1247
DataOwner2的最优x_2 = 0.1284
DataOwner3的最优x_3 = 0.1240
DataOwner4的最优x_4 = 0.1307
DataOwner5的最优x_5 = 0.1231
DataOwner6的最优x_6 = 0.1274
DataOwner7的最优x_7 = 0.1542
DataOwner8的最优x_8 = 0.1222
DataOwner9的最优x_9 = 0.0458
DataOwner10的最优x_10 = 0.1259
每个DataOwner应该贡献数据比例 xn_list = [0.12468285297287045, 0.1284040027468734, 0.12397748412642012, 0.1307189683338194, 0.12306991466579897, 0.12735413817233376, 0.154160281839489, 0.12222349949728369, 0.045774413425866384, 0.1258932073561056]
ModelOwner的最大效用 U(Eta) = 0.5894
Eta开始变化：
eta:0.0
**** log-parameter_analysis 运行时间： 2025-01-16 22:13:53 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
**** log-parameter_analysis 运行时间： 2025-01-16 22:15:55 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
初始化模型的准确率：
Accuracy: 26.82%
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.0632795930733255
DataOwner2: noise random: 0.04503076473182004
DataOwner3: noise random: 0.041237930190075926
DataOwner4: noise random: 0.08227730029454687
DataOwner5: noise random: 0.09925296604154038
DataOwner6: noise random: 0.05915015035531762
DataOwner7: noise random: 0.015312677037508783
DataOwner8: noise random: 0.06284724133808836
DataOwner9: noise random: 0.002469761536688731
DataOwner10: noise random: 0.042656650366639064
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9983798162637667, 0.9991779612945522, 0.9993133871780431, 0.9972548076093326, 0.996016253915969, 0.9985843270555544, 0.9999051171141704, 0.9984014594426085, 0.9999975288579281, 0.9992634863584059]
归一化后的数据质量列表avg_f_list: [0.9593669711902542, 0.9794144444851456, 0.982816015224801, 0.9311094740107072, 0.9, 0.9645037877821547, 0.9976788404442055, 0.9599105955105363, 1.0, 0.9815626272934308]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3483
DataOwner1的最优x_1 = 0.1159
DataOwner2的最优x_2 = 0.1357
DataOwner3的最优x_3 = 0.1388
DataOwner4的最优x_4 = 0.0849
DataOwner5的最优x_5 = 0.0459
DataOwner6的最优x_6 = 0.1211
DataOwner7的最优x_7 = 0.1522
DataOwner8的最优x_8 = 0.1164
DataOwner9的最优x_9 = 0.1542
DataOwner10的最优x_10 = 0.1377
每个DataOwner应该贡献数据比例 xn_list = [0.11588634048351755, 0.1356559105882501, 0.13884096897889717, 0.08487180747731937, 0.0458803464328502, 0.12111932394271689, 0.1522150172100344, 0.11644576867381826, 0.15422643383762558, 0.1376728794129774]
ModelOwner的最大效用 U(Eta) = 0.5893
DONE
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3482618027644884
DataOwner1的分配到的支付 ： 0.1280
DataOwner2的分配到的支付 ： 0.1530
DataOwner3的分配到的支付 ： 0.1572
DataOwner4的分配到的支付 ： 0.0910
DataOwner5的分配到的支付 ： 0.0476
DataOwner6的分配到的支付 ： 0.1345
DataOwner7的分配到的支付 ： 0.1749
DataOwner8的分配到的支付 ： 0.1287
DataOwner9的分配到的支付 ： 0.1776
DataOwner10的分配到的支付 ： 0.1556
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC1', 'DataOwner2': 'CPC2', 'DataOwner3': 'CPC3', 'DataOwner4': 'CPC4', 'DataOwner5': 'CPC5', 'DataOwner6': 'CPC6', 'DataOwner7': 'CPC7', 'DataOwner8': 'CPC8', 'DataOwner9': 'CPC9', 'DataOwner10': 'CPC10'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC2
DataOwner3 把数据交给 CPC3
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC8
DataOwner9 把数据交给 CPC9
DataOwner10 把数据交给 CPC10
DONE
----- literation 1: 模型训练 -----
CPC1调整模型中, 本轮训练的数据量为：1837.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 26.82%
Epoch 1/5, Loss: 2.2939
Epoch 2/5, Loss: 2.2849
Epoch 3/5, Loss: 2.2706
Epoch 4/5, Loss: 2.2483
Epoch 5/5, Loss: 2.2157
新模型评估：
Accuracy: 50.42%
Model saved to ../../data/model/mnist_cnn_model
CPC2调整模型中, 本轮训练的数据量为：307.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 50.42%
Epoch 1/5, Loss: 2.1984
Epoch 2/5, Loss: 2.1911
Epoch 3/5, Loss: 2.1853
Epoch 4/5, Loss: 2.1799
Epoch 5/5, Loss: 2.1713
新模型评估：
Accuracy: 50.05%
CPC3调整模型中, 本轮训练的数据量为：314.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 50.42%
Epoch 1/5, Loss: 2.2024
Epoch 2/5, Loss: 2.1971
Epoch 3/5, Loss: 2.1908
Epoch 4/5, Loss: 2.1847
Epoch 5/5, Loss: 2.1783
新模型评估：
Accuracy: 49.75%
CPC4调整模型中, 本轮训练的数据量为：288.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 50.42%
Epoch 1/5, Loss: 2.1854
Epoch 2/5, Loss: 2.1718
Epoch 3/5, Loss: 2.1704
Epoch 4/5, Loss: 2.1698
Epoch 5/5, Loss: 2.1550
新模型评估：
Accuracy: 52.57%
Model saved to ../../data/model/mnist_cnn_model
CPC5调整模型中, 本轮训练的数据量为：363.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 52.57%
Epoch 1/5, Loss: 2.1732
Epoch 2/5, Loss: 2.1585
Epoch 3/5, Loss: 2.1550
Epoch 4/5, Loss: 2.1446
Epoch 5/5, Loss: 2.1395
新模型评估：
Accuracy: 51.08%
CPC6调整模型中, 本轮训练的数据量为：822.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 52.57%
Epoch 1/5, Loss: 2.1799
Epoch 2/5, Loss: 2.1641
Epoch 3/5, Loss: 2.1482
Epoch 4/5, Loss: 2.1296
Epoch 5/5, Loss: 2.1092
新模型评估：
Accuracy: 54.33%
Model saved to ../../data/model/mnist_cnn_model
CPC7调整模型中, 本轮训练的数据量为：689.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 54.33%
Epoch 1/5, Loss: 2.0644
Epoch 2/5, Loss: 2.0435
Epoch 3/5, Loss: 2.0249
Epoch 4/5, Loss: 2.0035
Epoch 5/5, Loss: 1.9830
新模型评估：
Accuracy: 54.61%
Model saved to ../../data/model/mnist_cnn_model
CPC8调整模型中, 本轮训练的数据量为：1581.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 54.61%
Epoch 1/5, Loss: 1.9699
Epoch 2/5, Loss: 1.9198
Epoch 3/5, Loss: 1.8682
Epoch 4/5, Loss: 1.8095
Epoch 5/5, Loss: 1.7497
新模型评估：
Accuracy: 59.06%
Model saved to ../../data/model/mnist_cnn_model
CPC9调整模型中, 本轮训练的数据量为：349.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 59.06%
Epoch 1/5, Loss: 1.6865
Epoch 2/5, Loss: 1.6619
Epoch 3/5, Loss: 1.6601
Epoch 4/5, Loss: 1.6345
Epoch 5/5, Loss: 1.6234
新模型评估：
Accuracy: 63.24%
Model saved to ../../data/model/mnist_cnn_model
CPC10调整模型中, 本轮训练的数据量为：155.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 63.24%
Epoch 1/5, Loss: 1.5716
Epoch 2/5, Loss: 1.5849
Epoch 3/5, Loss: 1.5739
Epoch 4/5, Loss: 1.5501
Epoch 5/5, Loss: 1.5552
新模型评估：
Accuracy: 62.97%
DONE
========================= literation: 2 =========================
----- literation 2: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3483
DataOwner1的最优x_1 = 0.1159
DataOwner2的最优x_2 = 0.1357
DataOwner3的最优x_3 = 0.1388
DataOwner4的最优x_4 = 0.0849
DataOwner5的最优x_5 = 0.0459
DataOwner6的最优x_6 = 0.1211
DataOwner7的最优x_7 = 0.1522
DataOwner8的最优x_8 = 0.1164
DataOwner9的最优x_9 = 0.1542
DataOwner10的最优x_10 = 0.1377
每个DataOwner应该贡献数据比例 xn_list = [0.11588634048351755, 0.1356559105882501, 0.13884096897889717, 0.08487180747731937, 0.0458803464328502, 0.12111932394271689, 0.1522150172100344, 0.11644576867381826, 0.15422643383762558, 0.1376728794129774]
ModelOwner的最大效用 U(Eta) = 0.5893
DONE
----- literation 2: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3482618027644884
DataOwner1的分配到的支付 ： 0.1280
DataOwner2的分配到的支付 ： 0.1530
DataOwner3的分配到的支付 ： 0.1572
DataOwner4的分配到的支付 ： 0.0910
DataOwner5的分配到的支付 ： 0.0476
DataOwner6的分配到的支付 ： 0.1345
DataOwner7的分配到的支付 ： 0.1749
DataOwner8的分配到的支付 ： 0.1287
DataOwner9的分配到的支付 ： 0.1776
DataOwner10的分配到的支付 ： 0.1556
DONE
----- literation 2: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC2
DataOwner3 把数据交给 CPC3
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC8
DataOwner9 把数据交给 CPC9
DataOwner10 把数据交给 CPC10
DONE
----- literation 2: 模型训练 -----
重新调整fn，进而调整xn、Eta
正在评估DataOwner1的数据质量, 本轮评估的样本数据量为：1837.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 63.24%
Epoch 1/5, Loss: 1.6254
Epoch 2/5, Loss: 1.5526
Epoch 3/5, Loss: 1.4871
Epoch 4/5, Loss: 1.4163
Epoch 5/5, Loss: 1.3453
新模型评估：
Accuracy: 69.53%
loss差为：
0.28014404198219034
单位数据loss差为：
0.00015250083940238996
正在评估DataOwner2的数据质量, 本轮评估的样本数据量为：307.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 63.24%
Epoch 1/5, Loss: 1.6402
Epoch 2/5, Loss: 1.6274
Epoch 3/5, Loss: 1.6112
Epoch 4/5, Loss: 1.6039
Epoch 5/5, Loss: 1.5845
新模型评估：
Accuracy: 64.26%
loss差为：
0.0557025671005249
单位数据loss差为：
0.00018144158664666092
正在评估DataOwner3的数据质量, 本轮评估的样本数据量为：314.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 63.24%
Epoch 1/5, Loss: 1.6740
Epoch 2/5, Loss: 1.6565
Epoch 3/5, Loss: 1.6429
Epoch 4/5, Loss: 1.6289
Epoch 5/5, Loss: 1.6153
新模型评估：
Accuracy: 65.12%
loss差为：
0.05869555473327637
单位数据loss差为：
0.00018692851825884193
正在评估DataOwner4的数据质量, 本轮评估的样本数据量为：288.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 63.24%
Epoch 1/5, Loss: 1.7087
Epoch 2/5, Loss: 1.7082
Epoch 3/5, Loss: 1.6919
Epoch 4/5, Loss: 1.6691
Epoch 5/5, Loss: 1.6632
新模型评估：
Accuracy: 62.53%
loss差为：
0.04546785354614258
单位数据loss差为：
0.00015787449147966172
正在评估DataOwner5的数据质量, 本轮评估的样本数据量为：363.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 63.24%
Epoch 1/5, Loss: 1.6323
Epoch 2/5, Loss: 1.6127
Epoch 3/5, Loss: 1.5997
Epoch 4/5, Loss: 1.5867
Epoch 5/5, Loss: 1.5651
新模型评估：
Accuracy: 63.10%
loss差为：
0.067196786403656
单位数据loss差为：
0.0001851151140596584
正在评估DataOwner6的数据质量, 本轮评估的样本数据量为：822.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 63.24%
Epoch 1/5, Loss: 1.6427
Epoch 2/5, Loss: 1.6119
Epoch 3/5, Loss: 1.5799
Epoch 4/5, Loss: 1.5488
Epoch 5/5, Loss: 1.5196
新模型评估：
Accuracy: 64.70%
loss差为：
0.12308200506063605
单位数据loss差为：
0.00014973479934383946
正在评估DataOwner7的数据质量, 本轮评估的样本数据量为：689.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 63.24%
Epoch 1/5, Loss: 1.6401
Epoch 2/5, Loss: 1.6127
Epoch 3/5, Loss: 1.5856
Epoch 4/5, Loss: 1.5624
Epoch 5/5, Loss: 1.5368
新模型评估：
Accuracy: 63.96%
loss差为：
0.10334283655340037
单位数据loss差为：
0.00014998960312539967
正在评估DataOwner8的数据质量, 本轮评估的样本数据量为：1581.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 63.24%
Epoch 1/5, Loss: 1.6251
Epoch 2/5, Loss: 1.5657
Epoch 3/5, Loss: 1.5059
Epoch 4/5, Loss: 1.4450
Epoch 5/5, Loss: 1.3855
新模型评估：
Accuracy: 68.56%
loss差为：
0.23957962989807124
单位数据loss差为：
0.00015153676780396664
正在评估DataOwner9的数据质量, 本轮评估的样本数据量为：349.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 63.24%
Epoch 1/5, Loss: 1.6217
Epoch 2/5, Loss: 1.6038
Epoch 3/5, Loss: 1.5979
Epoch 4/5, Loss: 1.5784
Epoch 5/5, Loss: 1.5674
新模型评估：
Accuracy: 62.39%
loss差为：
0.05430680513381958
单位数据loss差为：
0.0001556068915009157
正在评估DataOwner10的数据质量, 本轮评估的样本数据量为：155.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 63.24%
Epoch 1/5, Loss: 1.6603
Epoch 2/5, Loss: 1.6464
Epoch 3/5, Loss: 1.6486
Epoch 4/5, Loss: 1.6433
Epoch 5/5, Loss: 1.5951
新模型评估：
Accuracy: 62.90%
loss差为：
0.06519889831542969
单位数据loss差为：
0.00042063805364793346
经过服务器调节后的真实数据质量：
数据质量列表avg_f_list: [0.00015250083940238996, 0.00018144158664666092, 0.00018692851825884193, 0.00015787449147966172, 0.0001851151140596584, 0.00014973479934383946, 0.00014998960312539967, 0.00015153676780396664, 0.0001556068915009157, 0.00042063805364793346]
归一化后的数据质量列表avg_f_list:[0.9010210434960096, 0.9117040998212705, 0.9137295208987235, 0.9030046490791452, 0.9130601290880411, 0.9, 0.900094057113568, 0.9006651704737753, 0.9021675974960732, 1.0]
CPC1调整模型中, 本轮训练的数据量为：1837.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 63.24%
Epoch 1/5, Loss: 1.6259
Epoch 2/5, Loss: 1.5549
Epoch 3/5, Loss: 1.4870
Epoch 4/5, Loss: 1.4169
Epoch 5/5, Loss: 1.3473
新模型评估：
Accuracy: 69.82%
Model saved to ../../data/model/mnist_cnn_model
CPC2调整模型中, 本轮训练的数据量为：307.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 69.82%
Epoch 1/5, Loss: 1.3011
Epoch 2/5, Loss: 1.2877
Epoch 3/5, Loss: 1.2691
Epoch 4/5, Loss: 1.2574
Epoch 5/5, Loss: 1.2363
新模型评估：
Accuracy: 71.63%
Model saved to ../../data/model/mnist_cnn_model
CPC3调整模型中, 本轮训练的数据量为：314.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 71.63%
Epoch 1/5, Loss: 1.2775
Epoch 2/5, Loss: 1.2585
Epoch 3/5, Loss: 1.2437
Epoch 4/5, Loss: 1.2302
Epoch 5/5, Loss: 1.2142
新模型评估：
Accuracy: 73.06%
Model saved to ../../data/model/mnist_cnn_model
CPC4调整模型中, 本轮训练的数据量为：288.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 73.06%
Epoch 1/5, Loss: 1.2780
Epoch 2/5, Loss: 1.2410
Epoch 3/5, Loss: 1.2243
Epoch 4/5, Loss: 1.2062
Epoch 5/5, Loss: 1.2006
新模型评估：
Accuracy: 70.66%
CPC5调整模型中, 本轮训练的数据量为：363.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 73.06%
Epoch 1/5, Loss: 1.1970
Epoch 2/5, Loss: 1.1731
Epoch 3/5, Loss: 1.1647
Epoch 4/5, Loss: 1.1478
Epoch 5/5, Loss: 1.1288
新模型评估：
Accuracy: 72.11%
CPC6调整模型中, 本轮训练的数据量为：822.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 73.06%
Epoch 1/5, Loss: 1.2109
Epoch 2/5, Loss: 1.1803
Epoch 3/5, Loss: 1.1513
Epoch 4/5, Loss: 1.1269
Epoch 5/5, Loss: 1.1012
新模型评估：
Accuracy: 73.58%
Model saved to ../../data/model/mnist_cnn_model
CPC7调整模型中, 本轮训练的数据量为：689.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 73.58%
Epoch 1/5, Loss: 1.1024
Epoch 2/5, Loss: 1.0728
Epoch 3/5, Loss: 1.0491
Epoch 4/5, Loss: 1.0295
Epoch 5/5, Loss: 1.0043
新模型评估：
Accuracy: 74.89%
Model saved to ../../data/model/mnist_cnn_model
CPC8调整模型中, 本轮训练的数据量为：1581.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 74.89%
Epoch 1/5, Loss: 0.9562
Epoch 2/5, Loss: 0.9082
Epoch 3/5, Loss: 0.8670
Epoch 4/5, Loss: 0.8265
Epoch 5/5, Loss: 0.7914
新模型评估：
Accuracy: 75.67%
Model saved to ../../data/model/mnist_cnn_model
CPC9调整模型中, 本轮训练的数据量为：349.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 75.67%
Epoch 1/5, Loss: 0.7694
Epoch 2/5, Loss: 0.7880
Epoch 3/5, Loss: 0.7706
Epoch 4/5, Loss: 0.7506
Epoch 5/5, Loss: 0.7577
新模型评估：
Accuracy: 75.56%
CPC10调整模型中, 本轮训练的数据量为：155.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 75.67%
Epoch 1/5, Loss: 0.7816
Epoch 2/5, Loss: 0.7943
Epoch 3/5, Loss: 0.7680
Epoch 4/5, Loss: 0.7425
Epoch 5/5, Loss: 0.7239
新模型评估：
Accuracy: 75.88%
Model saved to ../../data/model/mnist_cnn_model
DONE
========================= literation: 3 =========================
----- literation 3: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.2839
DataOwner1的最优x_1 = 0.1023
DataOwner2的最优x_2 = 0.1135
DataOwner3的最优x_3 = 0.1156
DataOwner4的最优x_4 = 0.1045
DataOwner5的最优x_5 = 0.1149
DataOwner6的最优x_6 = 0.1012
DataOwner7的最优x_7 = 0.1013
DataOwner8的最优x_8 = 0.1020
DataOwner9的最优x_9 = 0.1036
DataOwner10的最优x_10 = 0.1876
每个DataOwner应该贡献数据比例 xn_list = [0.10234620356980426, 0.11353153471311027, 0.11559010360714898, 0.1044654456868464, 0.1149119017907497, 0.1012476588903034, 0.1013490747488754, 0.10196391360056359, 0.10357355235319035, 0.18759158624393596]
ModelOwner的最大效用 U(Eta) = 0.5177
Eta开始变化：
eta:0.01
new: DataOwner1的最优x_1 = 0.0126
new: DataOwner2的最优x_2 = 0.0140
new: DataOwner3的最优x_3 = 0.0143
new: DataOwner4的最优x_4 = 0.0129
new: DataOwner5的最优x_5 = 0.0142
new: DataOwner6的最优x_6 = 0.0125
new: DataOwner7的最优x_7 = 0.0125
new: DataOwner8的最优x_8 = 0.0126
new: DataOwner9的最优x_9 = 0.0128
new: DataOwner10的最优x_10 = 0.0232
eta:0.02
new: DataOwner1的最优x_1 = 0.0130
new: DataOwner2的最优x_2 = 0.0144
new: DataOwner3的最优x_3 = 0.0147
new: DataOwner4的最优x_4 = 0.0132
new: DataOwner5的最优x_5 = 0.0146
new: DataOwner6的最优x_6 = 0.0128
new: DataOwner7的最优x_7 = 0.0129
new: DataOwner8的最优x_8 = 0.0129
new: DataOwner9的最优x_9 = 0.0131
new: DataOwner10的最优x_10 = 0.0238
eta:0.03
new: DataOwner1的最优x_1 = 0.0133
new: DataOwner2的最优x_2 = 0.0147
new: DataOwner3的最优x_3 = 0.0150
new: DataOwner4的最优x_4 = 0.0136
new: DataOwner5的最优x_5 = 0.0149
new: DataOwner6的最优x_6 = 0.0132
new: DataOwner7的最优x_7 = 0.0132
new: DataOwner8的最优x_8 = 0.0132
new: DataOwner9的最优x_9 = 0.0135
new: DataOwner10的最优x_10 = 0.0244
eta:0.04
new: DataOwner1的最优x_1 = 0.0133
new: DataOwner2的最优x_2 = 0.0148
new: DataOwner3的最优x_3 = 0.0150
new: DataOwner4的最优x_4 = 0.0136
new: DataOwner5的最优x_5 = 0.0150
new: DataOwner6的最优x_6 = 0.0132
new: DataOwner7的最优x_7 = 0.0132
new: DataOwner8的最优x_8 = 0.0133
new: DataOwner9的最优x_9 = 0.0135
new: DataOwner10的最优x_10 = 0.0244
eta:0.05
new: DataOwner1的最优x_1 = 0.0125
new: DataOwner2的最优x_2 = 0.0139
new: DataOwner3的最优x_3 = 0.0141
new: DataOwner4的最优x_4 = 0.0128
new: DataOwner5的最优x_5 = 0.0140
new: DataOwner6的最优x_6 = 0.0124
new: DataOwner7的最优x_7 = 0.0124
new: DataOwner8的最优x_8 = 0.0125
new: DataOwner9的最优x_9 = 0.0127
new: DataOwner10的最优x_10 = 0.0229
eta:0.060000000000000005
new: DataOwner1的最优x_1 = 0.0165
new: DataOwner2的最优x_2 = 0.0183
new: DataOwner3的最优x_3 = 0.0186
new: DataOwner4的最优x_4 = 0.0169
new: DataOwner5的最优x_5 = 0.0185
new: DataOwner6的最优x_6 = 0.0163
new: DataOwner7的最优x_7 = 0.0164
new: DataOwner8的最优x_8 = 0.0164
new: DataOwner9的最优x_9 = 0.0167
new: DataOwner10的最优x_10 = 0.0303
eta:0.06999999999999999
new: DataOwner1的最优x_1 = 0.0132
new: DataOwner2的最优x_2 = 0.0146
new: DataOwner3的最优x_3 = 0.0149
new: DataOwner4的最优x_4 = 0.0134
new: DataOwner5的最优x_5 = 0.0148
new: DataOwner6的最优x_6 = 0.0130
new: DataOwner7的最优x_7 = 0.0130
new: DataOwner8的最优x_8 = 0.0131
new: DataOwner9的最优x_9 = 0.0133
new: DataOwner10的最优x_10 = 0.0241
eta:0.08
new: DataOwner1的最优x_1 = 0.0113
new: DataOwner2的最优x_2 = 0.0125
new: DataOwner3的最优x_3 = 0.0128
new: DataOwner4的最优x_4 = 0.0115
new: DataOwner5的最优x_5 = 0.0127
new: DataOwner6的最优x_6 = 0.0112
new: DataOwner7的最优x_7 = 0.0112
new: DataOwner8的最优x_8 = 0.0113
new: DataOwner9的最优x_9 = 0.0114
new: DataOwner10的最优x_10 = 0.0207
eta:0.09
new: DataOwner1的最优x_1 = 0.0127
new: DataOwner2的最优x_2 = 0.0141
new: DataOwner3的最优x_3 = 0.0144
new: DataOwner4的最优x_4 = 0.0130
new: DataOwner5的最优x_5 = 0.0143
new: DataOwner6的最优x_6 = 0.0126
new: DataOwner7的最优x_7 = 0.0126
new: DataOwner8的最优x_8 = 0.0127
new: DataOwner9的最优x_9 = 0.0129
new: DataOwner10的最优x_10 = 0.0233
eta:0.09999999999999999
new: DataOwner1的最优x_1 = 0.0088
new: DataOwner2的最优x_2 = 0.0097
new: DataOwner3的最优x_3 = 0.0099
new: DataOwner4的最优x_4 = 0.0089
new: DataOwner5的最优x_5 = 0.0098
new: DataOwner6的最优x_6 = 0.0087
new: DataOwner7的最优x_7 = 0.0087
new: DataOwner8的最优x_8 = 0.0087
new: DataOwner9的最优x_9 = 0.0089
new: DataOwner10的最优x_10 = 0.0161
eta:0.11
new: DataOwner1的最优x_1 = 0.0088
new: DataOwner2的最优x_2 = 0.0097
new: DataOwner3的最优x_3 = 0.0099
new: DataOwner4的最优x_4 = 0.0089
new: DataOwner5的最优x_5 = 0.0098
new: DataOwner6的最优x_6 = 0.0087
new: DataOwner7的最优x_7 = 0.0087
new: DataOwner8的最优x_8 = 0.0087
new: DataOwner9的最优x_9 = 0.0089
new: DataOwner10的最优x_10 = 0.0161
eta:0.12
new: DataOwner1的最优x_1 = 0.0127
new: DataOwner2的最优x_2 = 0.0141
new: DataOwner3的最优x_3 = 0.0144
new: DataOwner4的最优x_4 = 0.0130
new: DataOwner5的最优x_5 = 0.0143
new: DataOwner6的最优x_6 = 0.0126
new: DataOwner7的最优x_7 = 0.0126
new: DataOwner8的最优x_8 = 0.0127
new: DataOwner9的最优x_9 = 0.0129
new: DataOwner10的最优x_10 = 0.0233
eta:0.13
new: DataOwner1的最优x_1 = 0.0125
new: DataOwner2的最优x_2 = 0.0139
new: DataOwner3的最优x_3 = 0.0142
new: DataOwner4的最优x_4 = 0.0128
new: DataOwner5的最优x_5 = 0.0141
new: DataOwner6的最优x_6 = 0.0124
new: DataOwner7的最优x_7 = 0.0124
new: DataOwner8的最优x_8 = 0.0125
new: DataOwner9的最优x_9 = 0.0127
new: DataOwner10的最优x_10 = 0.0230
eta:0.14
new: DataOwner1的最优x_1 = 0.0149
new: DataOwner2的最优x_2 = 0.0165
new: DataOwner3的最优x_3 = 0.0168
new: DataOwner4的最优x_4 = 0.0152
new: DataOwner5的最优x_5 = 0.0167
new: DataOwner6的最优x_6 = 0.0147
new: DataOwner7的最优x_7 = 0.0147
new: DataOwner8的最优x_8 = 0.0148
new: DataOwner9的最优x_9 = 0.0150
new: DataOwner10的最优x_10 = 0.0272
eta:0.15000000000000002
new: DataOwner1的最优x_1 = 0.0132
new: DataOwner2的最优x_2 = 0.0146
new: DataOwner3的最优x_3 = 0.0149
new: DataOwner4的最优x_4 = 0.0134
new: DataOwner5的最优x_5 = 0.0148
new: DataOwner6的最优x_6 = 0.0130
new: DataOwner7的最优x_7 = 0.0130
new: DataOwner8的最优x_8 = 0.0131
new: DataOwner9的最优x_9 = 0.0133
new: DataOwner10的最优x_10 = 0.0241
eta:0.16
new: DataOwner1的最优x_1 = 0.0128
new: DataOwner2的最优x_2 = 0.0141
new: DataOwner3的最优x_3 = 0.0144
new: DataOwner4的最优x_4 = 0.0130
new: DataOwner5的最优x_5 = 0.0143
new: DataOwner6的最优x_6 = 0.0126
new: DataOwner7的最优x_7 = 0.0126
new: DataOwner8的最优x_8 = 0.0127
new: DataOwner9的最优x_9 = 0.0129
new: DataOwner10的最优x_10 = 0.0234
eta:0.17
new: DataOwner1的最优x_1 = 0.0164
new: DataOwner2的最优x_2 = 0.0182
new: DataOwner3的最优x_3 = 0.0185
new: DataOwner4的最优x_4 = 0.0167
new: DataOwner5的最优x_5 = 0.0184
new: DataOwner6的最优x_6 = 0.0162
new: DataOwner7的最优x_7 = 0.0162
new: DataOwner8的最优x_8 = 0.0163
new: DataOwner9的最优x_9 = 0.0166
new: DataOwner10的最优x_10 = 0.0301
eta:0.18000000000000002
new: DataOwner1的最优x_1 = 0.0143
new: DataOwner2的最优x_2 = 0.0159
new: DataOwner3的最优x_3 = 0.0162
new: DataOwner4的最优x_4 = 0.0146
new: DataOwner5的最优x_5 = 0.0161
new: DataOwner6的最优x_6 = 0.0142
new: DataOwner7的最优x_7 = 0.0142
new: DataOwner8的最优x_8 = 0.0143
new: DataOwner9的最优x_9 = 0.0145
new: DataOwner10的最优x_10 = 0.0263
eta:0.19
new: DataOwner1的最优x_1 = 0.0183
new: DataOwner2的最优x_2 = 0.0203
new: DataOwner3的最优x_3 = 0.0207
new: DataOwner4的最优x_4 = 0.0187
new: DataOwner5的最优x_5 = 0.0206
new: DataOwner6的最优x_6 = 0.0181
new: DataOwner7的最优x_7 = 0.0181
new: DataOwner8的最优x_8 = 0.0183
new: DataOwner9的最优x_9 = 0.0185
new: DataOwner10的最优x_10 = 0.0336
eta:0.2
new: DataOwner1的最优x_1 = 0.0193
new: DataOwner2的最优x_2 = 0.0214
new: DataOwner3的最优x_3 = 0.0218
new: DataOwner4的最优x_4 = 0.0197
new: DataOwner5的最优x_5 = 0.0217
new: DataOwner6的最优x_6 = 0.0191
new: DataOwner7的最优x_7 = 0.0191
new: DataOwner8的最优x_8 = 0.0192
new: DataOwner9的最优x_9 = 0.0195
new: DataOwner10的最优x_10 = 0.0354
eta:0.21000000000000002
new: DataOwner1的最优x_1 = 0.0167
new: DataOwner2的最优x_2 = 0.0186
new: DataOwner3的最优x_3 = 0.0189
new: DataOwner4的最优x_4 = 0.0171
new: DataOwner5的最优x_5 = 0.0188
new: DataOwner6的最优x_6 = 0.0166
new: DataOwner7的最优x_7 = 0.0166
new: DataOwner8的最优x_8 = 0.0167
new: DataOwner9的最优x_9 = 0.0169
new: DataOwner10的最优x_10 = 0.0307
eta:0.22
new: DataOwner1的最优x_1 = 0.0193
new: DataOwner2的最优x_2 = 0.0214
new: DataOwner3的最优x_3 = 0.0218
new: DataOwner4的最优x_4 = 0.0197
new: DataOwner5的最优x_5 = 0.0217
new: DataOwner6的最优x_6 = 0.0191
new: DataOwner7的最优x_7 = 0.0191
new: DataOwner8的最优x_8 = 0.0192
new: DataOwner9的最优x_9 = 0.0195
new: DataOwner10的最优x_10 = 0.0354
eta:0.23
new: DataOwner1的最优x_1 = 0.0183
new: DataOwner2的最优x_2 = 0.0203
new: DataOwner3的最优x_3 = 0.0207
new: DataOwner4的最优x_4 = 0.0187
new: DataOwner5的最优x_5 = 0.0206
new: DataOwner6的最优x_6 = 0.0181
new: DataOwner7的最优x_7 = 0.0182
new: DataOwner8的最优x_8 = 0.0183
new: DataOwner9的最优x_9 = 0.0186
new: DataOwner10的最优x_10 = 0.0336
eta:0.24000000000000002
new: DataOwner1的最优x_1 = 0.0210
new: DataOwner2的最优x_2 = 0.0233
new: DataOwner3的最优x_3 = 0.0238
new: DataOwner4的最优x_4 = 0.0215
new: DataOwner5的最优x_5 = 0.0236
new: DataOwner6的最优x_6 = 0.0208
new: DataOwner7的最优x_7 = 0.0208
new: DataOwner8的最优x_8 = 0.0210
new: DataOwner9的最优x_9 = 0.0213
new: DataOwner10的最优x_10 = 0.0386
eta:0.25
new: DataOwner1的最优x_1 = 0.0199
new: DataOwner2的最优x_2 = 0.0221
new: DataOwner3的最优x_3 = 0.0225
new: DataOwner4的最优x_4 = 0.0203
new: DataOwner5的最优x_5 = 0.0224
new: DataOwner6的最优x_6 = 0.0197
new: DataOwner7的最优x_7 = 0.0197
new: DataOwner8的最优x_8 = 0.0199
new: DataOwner9的最优x_9 = 0.0202
new: DataOwner10的最优x_10 = 0.0365
eta:0.26
new: DataOwner1的最优x_1 = 0.0228
new: DataOwner2的最优x_2 = 0.0253
new: DataOwner3的最优x_3 = 0.0257
new: DataOwner4的最优x_4 = 0.0233
new: DataOwner5的最优x_5 = 0.0256
new: DataOwner6的最优x_6 = 0.0226
new: DataOwner7的最优x_7 = 0.0226
new: DataOwner8的最优x_8 = 0.0227
new: DataOwner9的最优x_9 = 0.0231
new: DataOwner10的最优x_10 = 0.0418
eta:0.27
new: DataOwner1的最优x_1 = 0.0215
new: DataOwner2的最优x_2 = 0.0239
new: DataOwner3的最优x_3 = 0.0243
new: DataOwner4的最优x_4 = 0.0220
new: DataOwner5的最优x_5 = 0.0242
new: DataOwner6的最优x_6 = 0.0213
new: DataOwner7的最优x_7 = 0.0213
new: DataOwner8的最优x_8 = 0.0214
new: DataOwner9的最优x_9 = 0.0218
new: DataOwner10的最优x_10 = 0.0394
eta:0.28
new: DataOwner1的最优x_1 = 0.0223
new: DataOwner2的最优x_2 = 0.0248
new: DataOwner3的最优x_3 = 0.0252
new: DataOwner4的最优x_4 = 0.0228
new: DataOwner5的最优x_5 = 0.0251
new: DataOwner6的最优x_6 = 0.0221
new: DataOwner7的最优x_7 = 0.0221
new: DataOwner8的最优x_8 = 0.0222
new: DataOwner9的最优x_9 = 0.0226
new: DataOwner10的最优x_10 = 0.0409
eta:0.29000000000000004
new: DataOwner1的最优x_1 = 0.0231
new: DataOwner2的最优x_2 = 0.0256
new: DataOwner3的最优x_3 = 0.0261
new: DataOwner4的最优x_4 = 0.0236
new: DataOwner5的最优x_5 = 0.0260
new: DataOwner6的最优x_6 = 0.0229
new: DataOwner7的最优x_7 = 0.0229
new: DataOwner8的最优x_8 = 0.0230
new: DataOwner9的最优x_9 = 0.0234
new: DataOwner10的最优x_10 = 0.0424
eta:0.3
new: DataOwner1的最优x_1 = 0.0239
new: DataOwner2的最优x_2 = 0.0265
new: DataOwner3的最优x_3 = 0.0270
new: DataOwner4的最优x_4 = 0.0244
new: DataOwner5的最优x_5 = 0.0268
new: DataOwner6的最优x_6 = 0.0237
new: DataOwner7的最优x_7 = 0.0237
new: DataOwner8的最优x_8 = 0.0238
new: DataOwner9的最优x_9 = 0.0242
new: DataOwner10的最优x_10 = 0.0438
eta:0.31
new: DataOwner1的最优x_1 = 0.0247
new: DataOwner2的最优x_2 = 0.0274
new: DataOwner3的最优x_3 = 0.0279
new: DataOwner4的最优x_4 = 0.0252
new: DataOwner5的最优x_5 = 0.0277
new: DataOwner6的最优x_6 = 0.0244
new: DataOwner7的最优x_7 = 0.0245
new: DataOwner8的最优x_8 = 0.0246
new: DataOwner9的最优x_9 = 0.0250
new: DataOwner10的最优x_10 = 0.0453
eta:0.32
new: DataOwner1的最优x_1 = 0.0255
new: DataOwner2的最优x_2 = 0.0283
new: DataOwner3的最优x_3 = 0.0288
new: DataOwner4的最优x_4 = 0.0260
new: DataOwner5的最优x_5 = 0.0286
new: DataOwner6的最优x_6 = 0.0252
new: DataOwner7的最优x_7 = 0.0253
new: DataOwner8的最优x_8 = 0.0254
new: DataOwner9的最优x_9 = 0.0258
new: DataOwner10的最优x_10 = 0.0468
eta:0.33
new: DataOwner1的最优x_1 = 0.0263
new: DataOwner2的最优x_2 = 0.0292
new: DataOwner3的最优x_3 = 0.0297
new: DataOwner4的最优x_4 = 0.0268
new: DataOwner5的最优x_5 = 0.0295
new: DataOwner6的最优x_6 = 0.0260
new: DataOwner7的最优x_7 = 0.0260
new: DataOwner8的最优x_8 = 0.0262
new: DataOwner9的最优x_9 = 0.0266
new: DataOwner10的最优x_10 = 0.0482
eta:0.34
new: DataOwner1的最优x_1 = 0.0271
new: DataOwner2的最优x_2 = 0.0301
new: DataOwner3的最优x_3 = 0.0306
new: DataOwner4的最优x_4 = 0.0277
new: DataOwner5的最优x_5 = 0.0304
new: DataOwner6的最优x_6 = 0.0268
new: DataOwner7的最优x_7 = 0.0268
new: DataOwner8的最优x_8 = 0.0270
new: DataOwner9的最优x_9 = 0.0274
new: DataOwner10的最优x_10 = 0.0497
eta:0.35000000000000003
new: DataOwner1的最优x_1 = 0.0279
new: DataOwner2的最优x_2 = 0.0309
new: DataOwner3的最优x_3 = 0.0315
new: DataOwner4的最优x_4 = 0.0285
new: DataOwner5的最优x_5 = 0.0313
new: DataOwner6的最优x_6 = 0.0276
new: DataOwner7的最优x_7 = 0.0276
new: DataOwner8的最优x_8 = 0.0278
new: DataOwner9的最优x_9 = 0.0282
new: DataOwner10的最优x_10 = 0.0511
eta:0.36000000000000004
new: DataOwner1的最优x_1 = 0.0287
new: DataOwner2的最优x_2 = 0.0318
new: DataOwner3的最优x_3 = 0.0324
new: DataOwner4的最优x_4 = 0.0293
new: DataOwner5的最优x_5 = 0.0322
new: DataOwner6的最优x_6 = 0.0284
new: DataOwner7的最优x_7 = 0.0284
new: DataOwner8的最优x_8 = 0.0286
new: DataOwner9的最优x_9 = 0.0290
new: DataOwner10的最优x_10 = 0.0526
eta:0.37
new: DataOwner1的最优x_1 = 0.0295
new: DataOwner2的最优x_2 = 0.0327
new: DataOwner3的最优x_3 = 0.0333
new: DataOwner4的最优x_4 = 0.0301
new: DataOwner5的最优x_5 = 0.0331
new: DataOwner6的最优x_6 = 0.0292
new: DataOwner7的最优x_7 = 0.0292
new: DataOwner8的最优x_8 = 0.0294
new: DataOwner9的最优x_9 = 0.0298
new: DataOwner10的最优x_10 = 0.0541
eta:0.38
new: DataOwner1的最优x_1 = 0.0303
new: DataOwner2的最优x_2 = 0.0336
new: DataOwner3的最优x_3 = 0.0342
new: DataOwner4的最优x_4 = 0.0309
new: DataOwner5的最优x_5 = 0.0340
new: DataOwner6的最优x_6 = 0.0300
new: DataOwner7的最优x_7 = 0.0300
new: DataOwner8的最优x_8 = 0.0302
new: DataOwner9的最优x_9 = 0.0307
new: DataOwner10的最优x_10 = 0.0555
eta:0.39
new: DataOwner1的最优x_1 = 0.0311
new: DataOwner2的最优x_2 = 0.0345
new: DataOwner3的最优x_3 = 0.0351
new: DataOwner4的最优x_4 = 0.0317
new: DataOwner5的最优x_5 = 0.0349
new: DataOwner6的最优x_6 = 0.0308
new: DataOwner7的最优x_7 = 0.0308
new: DataOwner8的最优x_8 = 0.0310
new: DataOwner9的最优x_9 = 0.0315
new: DataOwner10的最优x_10 = 0.0570
eta:0.4
new: DataOwner1的最优x_1 = 0.0351
new: DataOwner2的最优x_2 = 0.0389
new: DataOwner3的最优x_3 = 0.0396
new: DataOwner4的最优x_4 = 0.0358
new: DataOwner5的最优x_5 = 0.0394
new: DataOwner6的最优x_6 = 0.0347
new: DataOwner7的最优x_7 = 0.0347
new: DataOwner8的最优x_8 = 0.0349
new: DataOwner9的最优x_9 = 0.0355
new: DataOwner10的最优x_10 = 0.0643
eta:0.41000000000000003
new: DataOwner1的最优x_1 = 0.0327
new: DataOwner2的最优x_2 = 0.0363
new: DataOwner3的最优x_3 = 0.0369
new: DataOwner4的最优x_4 = 0.0334
new: DataOwner5的最优x_5 = 0.0367
new: DataOwner6的最优x_6 = 0.0323
new: DataOwner7的最优x_7 = 0.0324
new: DataOwner8的最优x_8 = 0.0326
new: DataOwner9的最优x_9 = 0.0331
new: DataOwner10的最优x_10 = 0.0599
eta:0.42000000000000004
new: DataOwner1的最优x_1 = 0.0405
new: DataOwner2的最优x_2 = 0.0449
new: DataOwner3的最优x_3 = 0.0458
new: DataOwner4的最优x_4 = 0.0413
new: DataOwner5的最优x_5 = 0.0455
new: DataOwner6的最优x_6 = 0.0401
new: DataOwner7的最优x_7 = 0.0401
new: DataOwner8的最优x_8 = 0.0404
new: DataOwner9的最优x_9 = 0.0410
new: DataOwner10的最优x_10 = 0.0743
eta:0.43
new: DataOwner1的最优x_1 = 0.0343
new: DataOwner2的最优x_2 = 0.0380
new: DataOwner3的最优x_3 = 0.0387
new: DataOwner4的最优x_4 = 0.0350
new: DataOwner5的最优x_5 = 0.0385
new: DataOwner6的最优x_6 = 0.0339
new: DataOwner7的最优x_7 = 0.0339
new: DataOwner8的最优x_8 = 0.0341
new: DataOwner9的最优x_9 = 0.0347
new: DataOwner10的最优x_10 = 0.0628
eta:0.44
new: DataOwner1的最优x_1 = 0.0351
new: DataOwner2的最优x_2 = 0.0389
new: DataOwner3的最优x_3 = 0.0396
new: DataOwner4的最优x_4 = 0.0358
new: DataOwner5的最优x_5 = 0.0394
new: DataOwner6的最优x_6 = 0.0347
new: DataOwner7的最优x_7 = 0.0347
new: DataOwner8的最优x_8 = 0.0349
new: DataOwner9的最优x_9 = 0.0355
new: DataOwner10的最优x_10 = 0.0643
eta:0.45
new: DataOwner1的最优x_1 = 0.0359
new: DataOwner2的最优x_2 = 0.0398
new: DataOwner3的最优x_3 = 0.0405
new: DataOwner4的最优x_4 = 0.0366
new: DataOwner5的最优x_5 = 0.0403
new: DataOwner6的最优x_6 = 0.0355
new: DataOwner7的最优x_7 = 0.0355
new: DataOwner8的最优x_8 = 0.0357
new: DataOwner9的最优x_9 = 0.0363
new: DataOwner10的最优x_10 = 0.0657
eta:0.46
new: DataOwner1的最优x_1 = 0.0367
new: DataOwner2的最优x_2 = 0.0407
new: DataOwner3的最优x_3 = 0.0414
new: DataOwner4的最优x_4 = 0.0374
new: DataOwner5的最优x_5 = 0.0412
new: DataOwner6的最优x_6 = 0.0363
new: DataOwner7的最优x_7 = 0.0363
new: DataOwner8的最优x_8 = 0.0365
new: DataOwner9的最优x_9 = 0.0371
new: DataOwner10的最优x_10 = 0.0672
eta:0.47000000000000003
new: DataOwner1的最优x_1 = 0.0375
new: DataOwner2的最优x_2 = 0.0416
new: DataOwner3的最优x_3 = 0.0423
new: DataOwner4的最优x_4 = 0.0382
new: DataOwner5的最优x_5 = 0.0421
new: DataOwner6的最优x_6 = 0.0371
new: DataOwner7的最优x_7 = 0.0371
new: DataOwner8的最优x_8 = 0.0373
new: DataOwner9的最优x_9 = 0.0379
new: DataOwner10的最优x_10 = 0.0687
eta:0.48000000000000004
new: DataOwner1的最优x_1 = 0.0383
new: DataOwner2的最优x_2 = 0.0424
new: DataOwner3的最优x_3 = 0.0432
new: DataOwner4的最优x_4 = 0.0391
new: DataOwner5的最优x_5 = 0.0430
new: DataOwner6的最优x_6 = 0.0379
new: DataOwner7的最优x_7 = 0.0379
new: DataOwner8的最优x_8 = 0.0381
new: DataOwner9的最优x_9 = 0.0387
new: DataOwner10的最优x_10 = 0.0701
eta:0.49
new: DataOwner1的最优x_1 = 0.0391
new: DataOwner2的最优x_2 = 0.0433
new: DataOwner3的最优x_3 = 0.0441
new: DataOwner4的最优x_4 = 0.0399
new: DataOwner5的最优x_5 = 0.0439
new: DataOwner6的最优x_6 = 0.0386
new: DataOwner7的最优x_7 = 0.0387
new: DataOwner8的最优x_8 = 0.0389
new: DataOwner9的最优x_9 = 0.0395
new: DataOwner10的最优x_10 = 0.0716
eta:0.5
new: DataOwner1的最优x_1 = 0.0530
new: DataOwner2的最优x_2 = 0.0588
new: DataOwner3的最优x_3 = 0.0599
new: DataOwner4的最优x_4 = 0.0541
new: DataOwner5的最优x_5 = 0.0596
new: DataOwner6的最优x_6 = 0.0525
new: DataOwner7的最优x_7 = 0.0525
new: DataOwner8的最优x_8 = 0.0529
new: DataOwner9的最优x_9 = 0.0537
new: DataOwner10的最优x_10 = 0.0972
eta:0.51
new: DataOwner1的最优x_1 = 0.0407
new: DataOwner2的最优x_2 = 0.0451
new: DataOwner3的最优x_3 = 0.0459
new: DataOwner4的最优x_4 = 0.0415
new: DataOwner5的最优x_5 = 0.0456
new: DataOwner6的最优x_6 = 0.0402
new: DataOwner7的最优x_7 = 0.0403
new: DataOwner8的最优x_8 = 0.0405
new: DataOwner9的最优x_9 = 0.0411
new: DataOwner10的最优x_10 = 0.0745
eta:0.52
new: DataOwner1的最优x_1 = 0.0456
new: DataOwner2的最优x_2 = 0.0506
new: DataOwner3的最优x_3 = 0.0515
new: DataOwner4的最优x_4 = 0.0465
new: DataOwner5的最优x_5 = 0.0512
new: DataOwner6的最优x_6 = 0.0451
new: DataOwner7的最优x_7 = 0.0452
new: DataOwner8的最优x_8 = 0.0454
new: DataOwner9的最优x_9 = 0.0461
new: DataOwner10的最优x_10 = 0.0836
eta:0.53
new: DataOwner1的最优x_1 = 0.0465
new: DataOwner2的最优x_2 = 0.0516
new: DataOwner3的最优x_3 = 0.0525
new: DataOwner4的最优x_4 = 0.0474
new: DataOwner5的最优x_5 = 0.0522
new: DataOwner6的最优x_6 = 0.0460
new: DataOwner7的最优x_7 = 0.0460
new: DataOwner8的最优x_8 = 0.0463
new: DataOwner9的最优x_9 = 0.0470
new: DataOwner10的最优x_10 = 0.0852
eta:0.54
new: DataOwner1的最优x_1 = 0.0473
new: DataOwner2的最优x_2 = 0.0525
new: DataOwner3的最优x_3 = 0.0535
new: DataOwner4的最优x_4 = 0.0483
new: DataOwner5的最优x_5 = 0.0532
new: DataOwner6的最优x_6 = 0.0468
new: DataOwner7的最优x_7 = 0.0469
new: DataOwner8的最优x_8 = 0.0472
new: DataOwner9的最优x_9 = 0.0479
new: DataOwner10的最优x_10 = 0.0868
eta:0.55
new: DataOwner1的最优x_1 = 0.0530
new: DataOwner2的最优x_2 = 0.0588
new: DataOwner3的最优x_3 = 0.0599
new: DataOwner4的最优x_4 = 0.0541
new: DataOwner5的最优x_5 = 0.0596
new: DataOwner6的最优x_6 = 0.0525
new: DataOwner7的最优x_7 = 0.0525
new: DataOwner8的最优x_8 = 0.0529
new: DataOwner9的最优x_9 = 0.0537
new: DataOwner10的最优x_10 = 0.0972
eta:0.56
new: DataOwner1的最优x_1 = 0.0446
new: DataOwner2的最优x_2 = 0.0495
new: DataOwner3的最优x_3 = 0.0504
new: DataOwner4的最优x_4 = 0.0456
new: DataOwner5的最优x_5 = 0.0501
new: DataOwner6的最优x_6 = 0.0442
new: DataOwner7的最优x_7 = 0.0442
new: DataOwner8的最优x_8 = 0.0445
new: DataOwner9的最优x_9 = 0.0452
new: DataOwner10的最优x_10 = 0.0818
eta:0.5700000000000001
new: DataOwner1的最优x_1 = 0.0454
new: DataOwner2的最优x_2 = 0.0504
new: DataOwner3的最优x_3 = 0.0513
new: DataOwner4的最优x_4 = 0.0464
new: DataOwner5的最优x_5 = 0.0510
new: DataOwner6的最优x_6 = 0.0449
new: DataOwner7的最优x_7 = 0.0450
new: DataOwner8的最优x_8 = 0.0453
new: DataOwner9的最优x_9 = 0.0460
new: DataOwner10的最优x_10 = 0.0833
eta:0.5800000000000001
new: DataOwner1的最优x_1 = 0.0462
new: DataOwner2的最优x_2 = 0.0513
new: DataOwner3的最优x_3 = 0.0522
new: DataOwner4的最优x_4 = 0.0472
new: DataOwner5的最优x_5 = 0.0519
new: DataOwner6的最优x_6 = 0.0457
new: DataOwner7的最优x_7 = 0.0458
new: DataOwner8的最优x_8 = 0.0461
new: DataOwner9的最优x_9 = 0.0468
new: DataOwner10的最优x_10 = 0.0847
eta:0.59
new: DataOwner1的最优x_1 = 0.0470
new: DataOwner2的最优x_2 = 0.0522
new: DataOwner3的最优x_3 = 0.0531
new: DataOwner4的最优x_4 = 0.0480
new: DataOwner5的最优x_5 = 0.0528
new: DataOwner6的最优x_6 = 0.0465
new: DataOwner7的最优x_7 = 0.0466
new: DataOwner8的最优x_8 = 0.0469
new: DataOwner9的最优x_9 = 0.0476
new: DataOwner10的最优x_10 = 0.0862
eta:0.6
new: DataOwner1的最优x_1 = 0.0526
new: DataOwner2的最优x_2 = 0.0584
new: DataOwner3的最优x_3 = 0.0594
new: DataOwner4的最优x_4 = 0.0537
new: DataOwner5的最优x_5 = 0.0591
new: DataOwner6的最优x_6 = 0.0520
new: DataOwner7的最优x_7 = 0.0521
new: DataOwner8的最优x_8 = 0.0524
new: DataOwner9的最优x_9 = 0.0532
new: DataOwner10的最优x_10 = 0.0964
eta:0.61
new: DataOwner1的最优x_1 = 0.0486
new: DataOwner2的最优x_2 = 0.0539
new: DataOwner3的最优x_3 = 0.0549
new: DataOwner4的最优x_4 = 0.0496
new: DataOwner5的最优x_5 = 0.0546
new: DataOwner6的最优x_6 = 0.0481
new: DataOwner7的最优x_7 = 0.0482
new: DataOwner8的最优x_8 = 0.0484
new: DataOwner9的最优x_9 = 0.0492
new: DataOwner10的最优x_10 = 0.0891
eta:0.62
new: DataOwner1的最优x_1 = 0.0494
new: DataOwner2的最优x_2 = 0.0548
new: DataOwner3的最优x_3 = 0.0558
new: DataOwner4的最优x_4 = 0.0504
new: DataOwner5的最优x_5 = 0.0555
new: DataOwner6的最优x_6 = 0.0489
new: DataOwner7的最优x_7 = 0.0489
new: DataOwner8的最优x_8 = 0.0492
new: DataOwner9的最优x_9 = 0.0500
new: DataOwner10的最优x_10 = 0.0906
eta:0.63
new: DataOwner1的最优x_1 = 0.0502
new: DataOwner2的最优x_2 = 0.0557
new: DataOwner3的最优x_3 = 0.0567
new: DataOwner4的最优x_4 = 0.0513
new: DataOwner5的最优x_5 = 0.0564
new: DataOwner6的最优x_6 = 0.0497
new: DataOwner7的最优x_7 = 0.0497
new: DataOwner8的最优x_8 = 0.0500
new: DataOwner9的最优x_9 = 0.0508
new: DataOwner10的最优x_10 = 0.0920
eta:0.64
new: DataOwner1的最优x_1 = 0.0510
new: DataOwner2的最优x_2 = 0.0566
new: DataOwner3的最优x_3 = 0.0576
new: DataOwner4的最优x_4 = 0.0521
new: DataOwner5的最优x_5 = 0.0573
new: DataOwner6的最优x_6 = 0.0505
new: DataOwner7的最优x_7 = 0.0505
new: DataOwner8的最优x_8 = 0.0508
new: DataOwner9的最优x_9 = 0.0516
new: DataOwner10的最优x_10 = 0.0935
eta:0.65
new: DataOwner1的最优x_1 = 0.0518
new: DataOwner2的最优x_2 = 0.0575
new: DataOwner3的最优x_3 = 0.0585
new: DataOwner4的最优x_4 = 0.0529
new: DataOwner5的最优x_5 = 0.0582
new: DataOwner6的最优x_6 = 0.0513
new: DataOwner7的最优x_7 = 0.0513
new: DataOwner8的最优x_8 = 0.0516
new: DataOwner9的最优x_9 = 0.0524
new: DataOwner10的最优x_10 = 0.0950
eta:0.66
new: DataOwner1的最优x_1 = 0.0526
new: DataOwner2的最优x_2 = 0.0584
new: DataOwner3的最优x_3 = 0.0594
new: DataOwner4的最优x_4 = 0.0537
new: DataOwner5的最优x_5 = 0.0591
new: DataOwner6的最优x_6 = 0.0520
new: DataOwner7的最优x_7 = 0.0521
new: DataOwner8的最优x_8 = 0.0524
new: DataOwner9的最优x_9 = 0.0532
new: DataOwner10的最优x_10 = 0.0964
eta:0.67
new: DataOwner1的最优x_1 = 0.0534
new: DataOwner2的最优x_2 = 0.0592
new: DataOwner3的最优x_3 = 0.0603
new: DataOwner4的最优x_4 = 0.0545
new: DataOwner5的最优x_5 = 0.0600
new: DataOwner6的最优x_6 = 0.0528
new: DataOwner7的最优x_7 = 0.0529
new: DataOwner8的最优x_8 = 0.0532
new: DataOwner9的最优x_9 = 0.0540
new: DataOwner10的最优x_10 = 0.0979
eta:0.68
new: DataOwner1的最优x_1 = 0.0542
new: DataOwner2的最优x_2 = 0.0601
new: DataOwner3的最优x_3 = 0.0612
new: DataOwner4的最优x_4 = 0.0553
new: DataOwner5的最优x_5 = 0.0609
new: DataOwner6的最优x_6 = 0.0536
new: DataOwner7的最优x_7 = 0.0537
new: DataOwner8的最优x_8 = 0.0540
new: DataOwner9的最优x_9 = 0.0549
new: DataOwner10的最优x_10 = 0.0994
eta:0.6900000000000001
new: DataOwner1的最优x_1 = 0.0550
new: DataOwner2的最优x_2 = 0.0610
new: DataOwner3的最优x_3 = 0.0621
new: DataOwner4的最优x_4 = 0.0561
new: DataOwner5的最优x_5 = 0.0618
new: DataOwner6的最优x_6 = 0.0544
new: DataOwner7的最优x_7 = 0.0545
new: DataOwner8的最优x_8 = 0.0548
new: DataOwner9的最优x_9 = 0.0557
new: DataOwner10的最优x_10 = 0.1008
eta:0.7000000000000001
new: DataOwner1的最优x_1 = 0.0558
new: DataOwner2的最优x_2 = 0.0619
new: DataOwner3的最优x_3 = 0.0630
new: DataOwner4的最优x_4 = 0.0570
new: DataOwner5的最优x_5 = 0.0626
new: DataOwner6的最优x_6 = 0.0552
new: DataOwner7的最优x_7 = 0.0553
new: DataOwner8的最优x_8 = 0.0556
new: DataOwner9的最优x_9 = 0.0565
new: DataOwner10的最优x_10 = 0.1023
eta:0.7100000000000001
new: DataOwner1的最优x_1 = 0.0566
new: DataOwner2的最优x_2 = 0.0628
new: DataOwner3的最优x_3 = 0.0639
new: DataOwner4的最优x_4 = 0.0578
new: DataOwner5的最优x_5 = 0.0635
new: DataOwner6的最优x_6 = 0.0560
new: DataOwner7的最优x_7 = 0.0560
new: DataOwner8的最优x_8 = 0.0564
new: DataOwner9的最优x_9 = 0.0573
new: DataOwner10的最优x_10 = 0.1037
eta:0.72
new: DataOwner1的最优x_1 = 0.0574
new: DataOwner2的最优x_2 = 0.0637
new: DataOwner3的最优x_3 = 0.0648
new: DataOwner4的最优x_4 = 0.0586
new: DataOwner5的最优x_5 = 0.0644
new: DataOwner6的最优x_6 = 0.0568
new: DataOwner7的最优x_7 = 0.0568
new: DataOwner8的最优x_8 = 0.0572
new: DataOwner9的最优x_9 = 0.0581
new: DataOwner10的最优x_10 = 0.1052
eta:0.73
new: DataOwner1的最优x_1 = 0.0582
new: DataOwner2的最优x_2 = 0.0646
new: DataOwner3的最优x_3 = 0.0657
new: DataOwner4的最优x_4 = 0.0594
new: DataOwner5的最优x_5 = 0.0653
new: DataOwner6的最优x_6 = 0.0576
new: DataOwner7的最优x_7 = 0.0576
new: DataOwner8的最优x_8 = 0.0580
new: DataOwner9的最优x_9 = 0.0589
new: DataOwner10的最优x_10 = 0.1067
eta:0.74
new: DataOwner1的最优x_1 = 0.0590
new: DataOwner2的最优x_2 = 0.0654
new: DataOwner3的最优x_3 = 0.0666
new: DataOwner4的最优x_4 = 0.0602
new: DataOwner5的最优x_5 = 0.0662
new: DataOwner6的最优x_6 = 0.0584
new: DataOwner7的最优x_7 = 0.0584
new: DataOwner8的最优x_8 = 0.0588
new: DataOwner9的最优x_9 = 0.0597
new: DataOwner10的最优x_10 = 0.1081
eta:0.75
new: DataOwner1的最优x_1 = 0.0598
new: DataOwner2的最优x_2 = 0.0663
new: DataOwner3的最优x_3 = 0.0675
new: DataOwner4的最优x_4 = 0.0610
new: DataOwner5的最优x_5 = 0.0671
new: DataOwner6的最优x_6 = 0.0591
new: DataOwner7的最优x_7 = 0.0592
new: DataOwner8的最优x_8 = 0.0596
new: DataOwner9的最优x_9 = 0.0605
new: DataOwner10的最优x_10 = 0.1096
eta:0.76
new: DataOwner1的最优x_1 = 0.0606
new: DataOwner2的最优x_2 = 0.0672
new: DataOwner3的最优x_3 = 0.0684
new: DataOwner4的最优x_4 = 0.0618
new: DataOwner5的最优x_5 = 0.0680
new: DataOwner6的最优x_6 = 0.0599
new: DataOwner7的最优x_7 = 0.0600
new: DataOwner8的最优x_8 = 0.0604
new: DataOwner9的最优x_9 = 0.0613
new: DataOwner10的最优x_10 = 0.1110
eta:0.77
new: DataOwner1的最优x_1 = 0.0614
new: DataOwner2的最优x_2 = 0.0681
new: DataOwner3的最优x_3 = 0.0693
new: DataOwner4的最优x_4 = 0.0626
new: DataOwner5的最优x_5 = 0.0689
new: DataOwner6的最优x_6 = 0.0607
new: DataOwner7的最优x_7 = 0.0608
new: DataOwner8的最优x_8 = 0.0611
new: DataOwner9的最优x_9 = 0.0621
new: DataOwner10的最优x_10 = 0.1125
eta:0.78
new: DataOwner1的最优x_1 = 0.0622
new: DataOwner2的最优x_2 = 0.0690
new: DataOwner3的最优x_3 = 0.0702
new: DataOwner4的最优x_4 = 0.0635
new: DataOwner5的最优x_5 = 0.0698
new: DataOwner6的最优x_6 = 0.0615
new: DataOwner7的最优x_7 = 0.0616
new: DataOwner8的最优x_8 = 0.0619
new: DataOwner9的最优x_9 = 0.0629
new: DataOwner10的最优x_10 = 0.1140
eta:0.79
new: DataOwner1的最优x_1 = 0.0630
new: DataOwner2的最优x_2 = 0.0699
new: DataOwner3的最优x_3 = 0.0711
new: DataOwner4的最优x_4 = 0.0643
new: DataOwner5的最优x_5 = 0.0707
new: DataOwner6的最优x_6 = 0.0623
new: DataOwner7的最优x_7 = 0.0624
new: DataOwner8的最优x_8 = 0.0627
new: DataOwner9的最优x_9 = 0.0637
new: DataOwner10的最优x_10 = 0.1154
eta:0.8
new: DataOwner1的最优x_1 = 0.0638
new: DataOwner2的最优x_2 = 0.0707
new: DataOwner3的最优x_3 = 0.0720
new: DataOwner4的最优x_4 = 0.0651
new: DataOwner5的最优x_5 = 0.0716
new: DataOwner6的最优x_6 = 0.0631
new: DataOwner7的最优x_7 = 0.0631
new: DataOwner8的最优x_8 = 0.0635
new: DataOwner9的最优x_9 = 0.0645
new: DataOwner10的最优x_10 = 0.1169
eta:0.81
new: DataOwner1的最优x_1 = 0.0646
new: DataOwner2的最优x_2 = 0.0716
new: DataOwner3的最优x_3 = 0.0729
new: DataOwner4的最优x_4 = 0.0659
new: DataOwner5的最优x_5 = 0.0725
new: DataOwner6的最优x_6 = 0.0639
new: DataOwner7的最优x_7 = 0.0639
new: DataOwner8的最优x_8 = 0.0643
new: DataOwner9的最优x_9 = 0.0653
new: DataOwner10的最优x_10 = 0.1183
eta:0.8200000000000001
new: DataOwner1的最优x_1 = 0.0654
new: DataOwner2的最优x_2 = 0.0725
new: DataOwner3的最优x_3 = 0.0738
new: DataOwner4的最优x_4 = 0.0667
new: DataOwner5的最优x_5 = 0.0734
new: DataOwner6的最优x_6 = 0.0647
new: DataOwner7的最优x_7 = 0.0647
new: DataOwner8的最优x_8 = 0.0651
new: DataOwner9的最优x_9 = 0.0661
new: DataOwner10的最优x_10 = 0.1198
eta:0.8300000000000001
new: DataOwner1的最优x_1 = 0.0662
new: DataOwner2的最优x_2 = 0.0734
new: DataOwner3的最优x_3 = 0.0747
new: DataOwner4的最优x_4 = 0.0675
new: DataOwner5的最优x_5 = 0.0743
new: DataOwner6的最优x_6 = 0.0655
new: DataOwner7的最优x_7 = 0.0655
new: DataOwner8的最优x_8 = 0.0659
new: DataOwner9的最优x_9 = 0.0670
new: DataOwner10的最优x_10 = 0.1213
eta:0.8400000000000001
new: DataOwner1的最优x_1 = 0.0670
new: DataOwner2的最优x_2 = 0.0743
new: DataOwner3的最优x_3 = 0.0756
new: DataOwner4的最优x_4 = 0.0683
new: DataOwner5的最优x_5 = 0.0752
new: DataOwner6的最优x_6 = 0.0662
new: DataOwner7的最优x_7 = 0.0663
new: DataOwner8的最优x_8 = 0.0667
new: DataOwner9的最优x_9 = 0.0678
new: DataOwner10的最优x_10 = 0.1227
eta:0.85
new: DataOwner1的最优x_1 = 0.0678
new: DataOwner2的最优x_2 = 0.0752
new: DataOwner3的最优x_3 = 0.0765
new: DataOwner4的最优x_4 = 0.0692
new: DataOwner5的最优x_5 = 0.0761
new: DataOwner6的最优x_6 = 0.0670
new: DataOwner7的最优x_7 = 0.0671
new: DataOwner8的最优x_8 = 0.0675
new: DataOwner9的最优x_9 = 0.0686
new: DataOwner10的最优x_10 = 0.1242
eta:0.86
new: DataOwner1的最优x_1 = 0.0686
new: DataOwner2的最优x_2 = 0.0760
new: DataOwner3的最优x_3 = 0.0774
new: DataOwner4的最优x_4 = 0.0700
new: DataOwner5的最优x_5 = 0.0770
new: DataOwner6的最优x_6 = 0.0678
new: DataOwner7的最优x_7 = 0.0679
new: DataOwner8的最优x_8 = 0.0683
new: DataOwner9的最优x_9 = 0.0694
new: DataOwner10的最优x_10 = 0.1257
eta:0.87
new: DataOwner1的最优x_1 = 0.0694
new: DataOwner2的最优x_2 = 0.0769
new: DataOwner3的最优x_3 = 0.0783
new: DataOwner4的最优x_4 = 0.0708
new: DataOwner5的最优x_5 = 0.0779
new: DataOwner6的最优x_6 = 0.0686
new: DataOwner7的最优x_7 = 0.0687
new: DataOwner8的最优x_8 = 0.0691
new: DataOwner9的最优x_9 = 0.0702
new: DataOwner10的最优x_10 = 0.1271
eta:0.88
new: DataOwner1的最优x_1 = 0.0701
new: DataOwner2的最优x_2 = 0.0778
new: DataOwner3的最优x_3 = 0.0792
new: DataOwner4的最优x_4 = 0.0716
new: DataOwner5的最优x_5 = 0.0788
new: DataOwner6的最优x_6 = 0.0694
new: DataOwner7的最优x_7 = 0.0695
new: DataOwner8的最优x_8 = 0.0699
new: DataOwner9的最优x_9 = 0.0710
new: DataOwner10的最优x_10 = 0.1286
eta:0.89
new: DataOwner1的最优x_1 = 0.0709
new: DataOwner2的最优x_2 = 0.0787
new: DataOwner3的最优x_3 = 0.0801
new: DataOwner4的最优x_4 = 0.0724
new: DataOwner5的最优x_5 = 0.0797
new: DataOwner6的最优x_6 = 0.0702
new: DataOwner7的最优x_7 = 0.0703
new: DataOwner8的最优x_8 = 0.0707
new: DataOwner9的最优x_9 = 0.0718
new: DataOwner10的最优x_10 = 0.1300
eta:0.9
new: DataOwner1的最优x_1 = 0.0717
new: DataOwner2的最优x_2 = 0.0796
new: DataOwner3的最优x_3 = 0.0810
new: DataOwner4的最优x_4 = 0.0732
new: DataOwner5的最优x_5 = 0.0805
new: DataOwner6的最优x_6 = 0.0710
new: DataOwner7的最优x_7 = 0.0710
new: DataOwner8的最优x_8 = 0.0715
new: DataOwner9的最优x_9 = 0.0726
new: DataOwner10的最优x_10 = 0.1315
eta:0.91
new: DataOwner1的最优x_1 = 0.0725
new: DataOwner2的最优x_2 = 0.0805
new: DataOwner3的最优x_3 = 0.0819
new: DataOwner4的最优x_4 = 0.0740
new: DataOwner5的最优x_5 = 0.0814
new: DataOwner6的最优x_6 = 0.0718
new: DataOwner7的最优x_7 = 0.0718
new: DataOwner8的最优x_8 = 0.0723
new: DataOwner9的最优x_9 = 0.0734
new: DataOwner10的最优x_10 = 0.1330
eta:0.92
new: DataOwner1的最优x_1 = 0.0733
new: DataOwner2的最优x_2 = 0.0814
new: DataOwner3的最优x_3 = 0.0828
new: DataOwner4的最优x_4 = 0.0749
new: DataOwner5的最优x_5 = 0.0823
new: DataOwner6的最优x_6 = 0.0725
new: DataOwner7的最优x_7 = 0.0726
new: DataOwner8的最优x_8 = 0.0731
new: DataOwner9的最优x_9 = 0.0742
new: DataOwner10的最优x_10 = 0.1344
eta:0.93
new: DataOwner1的最优x_1 = 0.0741
new: DataOwner2的最优x_2 = 0.0822
new: DataOwner3的最优x_3 = 0.0837
new: DataOwner4的最优x_4 = 0.0757
new: DataOwner5的最优x_5 = 0.0832
new: DataOwner6的最优x_6 = 0.0733
new: DataOwner7的最优x_7 = 0.0734
new: DataOwner8的最优x_8 = 0.0739
new: DataOwner9的最优x_9 = 0.0750
new: DataOwner10的最优x_10 = 0.1359
eta:0.9400000000000001
new: DataOwner1的最优x_1 = 0.0749
new: DataOwner2的最优x_2 = 0.0831
new: DataOwner3的最优x_3 = 0.0846
new: DataOwner4的最优x_4 = 0.0765
new: DataOwner5的最优x_5 = 0.0841
new: DataOwner6的最优x_6 = 0.0741
new: DataOwner7的最优x_7 = 0.0742
new: DataOwner8的最优x_8 = 0.0747
new: DataOwner9的最优x_9 = 0.0758
new: DataOwner10的最优x_10 = 0.1373
eta:0.9500000000000001
new: DataOwner1的最优x_1 = 0.0757
new: DataOwner2的最优x_2 = 0.0840
new: DataOwner3的最优x_3 = 0.0855
new: DataOwner4的最优x_4 = 0.0773
new: DataOwner5的最优x_5 = 0.0850
new: DataOwner6的最优x_6 = 0.0749
new: DataOwner7的最优x_7 = 0.0750
new: DataOwner8的最优x_8 = 0.0754
new: DataOwner9的最优x_9 = 0.0766
new: DataOwner10的最优x_10 = 0.1388
eta:0.9600000000000001
new: DataOwner1的最优x_1 = 0.0765
new: DataOwner2的最优x_2 = 0.0849
new: DataOwner3的最优x_3 = 0.0864
new: DataOwner4的最优x_4 = 0.0781
new: DataOwner5的最优x_5 = 0.0859
new: DataOwner6的最优x_6 = 0.0757
new: DataOwner7的最优x_7 = 0.0758
new: DataOwner8的最优x_8 = 0.0762
new: DataOwner9的最优x_9 = 0.0774
new: DataOwner10的最优x_10 = 0.1403
eta:0.97
new: DataOwner1的最优x_1 = 0.0773
new: DataOwner2的最优x_2 = 0.0858
new: DataOwner3的最优x_3 = 0.0873
new: DataOwner4的最优x_4 = 0.0789
new: DataOwner5的最优x_5 = 0.0868
new: DataOwner6的最优x_6 = 0.0765
new: DataOwner7的最优x_7 = 0.0766
new: DataOwner8的最优x_8 = 0.0770
new: DataOwner9的最优x_9 = 0.0782
new: DataOwner10的最优x_10 = 0.1417
eta:0.98
new: DataOwner1的最优x_1 = 0.0781
new: DataOwner2的最优x_2 = 0.0867
new: DataOwner3的最优x_3 = 0.0882
new: DataOwner4的最优x_4 = 0.0797
new: DataOwner5的最优x_5 = 0.0877
new: DataOwner6的最优x_6 = 0.0773
new: DataOwner7的最优x_7 = 0.0774
new: DataOwner8的最优x_8 = 0.0778
new: DataOwner9的最优x_9 = 0.0791
new: DataOwner10的最优x_10 = 0.1432
eta:0.99
new: DataOwner1的最优x_1 = 0.0789
new: DataOwner2的最优x_2 = 0.0875
new: DataOwner3的最优x_3 = 0.0891
new: DataOwner4的最优x_4 = 0.0805
new: DataOwner5的最优x_5 = 0.0886
new: DataOwner6的最优x_6 = 0.0781
new: DataOwner7的最优x_7 = 0.0781
new: DataOwner8的最优x_8 = 0.0786
new: DataOwner9的最优x_9 = 0.0799
new: DataOwner10的最优x_10 = 0.1446
eta:1.0
new: DataOwner1的最优x_1 = 0.0797
new: DataOwner2的最优x_2 = 0.0884
new: DataOwner3的最优x_3 = 0.0900
new: DataOwner4的最优x_4 = 0.0814
new: DataOwner5的最优x_5 = 0.0895
new: DataOwner6的最优x_6 = 0.0789
new: DataOwner7的最优x_7 = 0.0789
new: DataOwner8的最优x_8 = 0.0794
new: DataOwner9的最优x_9 = 0.0807
new: DataOwner10的最优x_10 = 0.1461
DONE
----- literation 3: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.2839333904312875
DataOwner1的分配到的支付 ： 0.1088
DataOwner2的分配到的支付 ： 0.1289
DataOwner3的分配到的支付 ： 0.1322
DataOwner4的分配到的支付 ： 0.0983
DataOwner5的分配到的支付 ： 0.1094
DataOwner6的分配到的支付 ： 0.1136
DataOwner7的分配到的支付 ： 0.1428
DataOwner8的分配到的支付 ： 0.1093
DataOwner9的分配到的支付 ： 0.1450
DataOwner10的分配到的支付 ： 0.1955
DONE
----- literation 3: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC2
DataOwner3 把数据交给 CPC3
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC8
DataOwner9 把数据交给 CPC9
DataOwner10 把数据交给 CPC10
DONE
----- literation 3: 模型训练 -----
CPC1调整模型中, 本轮训练的数据量为：1837.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 75.88%
Epoch 1/5, Loss: 0.7682
Epoch 2/5, Loss: 0.7336
Epoch 3/5, Loss: 0.7038
Epoch 4/5, Loss: 0.6740
Epoch 5/5, Loss: 0.6487
新模型评估：
Accuracy: 76.99%
Model saved to ../../data/model/mnist_cnn_model
CPC2调整模型中, 本轮训练的数据量为：307.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 76.99%
Epoch 1/5, Loss: 0.6290
Epoch 2/5, Loss: 0.6237
Epoch 3/5, Loss: 0.6170
Epoch 4/5, Loss: 0.6156
Epoch 5/5, Loss: 0.5982
新模型评估：
Accuracy: 78.18%
Model saved to ../../data/model/mnist_cnn_model
CPC3调整模型中, 本轮训练的数据量为：314.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.18%
Epoch 1/5, Loss: 0.6428
Epoch 2/5, Loss: 0.6269
Epoch 3/5, Loss: 0.6149
Epoch 4/5, Loss: 0.6080
Epoch 5/5, Loss: 0.6023
新模型评估：
Accuracy: 75.74%
CPC4调整模型中, 本轮训练的数据量为：354.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.18%
Epoch 1/5, Loss: 0.6019
Epoch 2/5, Loss: 0.5859
Epoch 3/5, Loss: 0.5875
Epoch 4/5, Loss: 0.5759
Epoch 5/5, Loss: 0.5716
新模型评估：
Accuracy: 76.37%
CPC5调整模型中, 本轮训练的数据量为：910.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.18%
Epoch 1/5, Loss: 0.6485
Epoch 2/5, Loss: 0.6260
Epoch 3/5, Loss: 0.6168
Epoch 4/5, Loss: 0.5924
Epoch 5/5, Loss: 0.5780
新模型评估：
Accuracy: 77.05%
CPC6调整模型中, 本轮训练的数据量为：822.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.18%
Epoch 1/5, Loss: 0.6450
Epoch 2/5, Loss: 0.6277
Epoch 3/5, Loss: 0.6185
Epoch 4/5, Loss: 0.6093
Epoch 5/5, Loss: 0.5973
新模型评估：
Accuracy: 76.53%
CPC7调整模型中, 本轮训练的数据量为：689.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.18%
Epoch 1/5, Loss: 0.6397
Epoch 2/5, Loss: 0.6286
Epoch 3/5, Loss: 0.6109
Epoch 4/5, Loss: 0.6020
Epoch 5/5, Loss: 0.5970
新模型评估：
Accuracy: 76.07%
CPC8调整模型中, 本轮训练的数据量为：1581.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.18%
Epoch 1/5, Loss: 0.6127
Epoch 2/5, Loss: 0.5881
Epoch 3/5, Loss: 0.5679
Epoch 4/5, Loss: 0.5533
Epoch 5/5, Loss: 0.5354
新模型评估：
Accuracy: 76.01%
CPC9调整模型中, 本轮训练的数据量为：349.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.18%
Epoch 1/5, Loss: 0.6430
Epoch 2/5, Loss: 0.6043
Epoch 3/5, Loss: 0.5870
Epoch 4/5, Loss: 0.5974
Epoch 5/5, Loss: 0.5688
新模型评估：
Accuracy: 74.54%
CPC10调整模型中, 本轮训练的数据量为：212.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.18%
Epoch 1/5, Loss: 0.6673
Epoch 2/5, Loss: 0.5600
Epoch 3/5, Loss: 0.5867
Epoch 4/5, Loss: 0.5962
Epoch 5/5, Loss: 0.5745
新模型评估：
Accuracy: 76.77%
DONE
最终的列表：
[0.2965308180541614, 0.2941111185845482, 0.2913385068654928, 0.2818670779526195, 0.2534199943362924, 0.3332212849127076, 0.24818873946998898, 0.19559513084091534, 0.21800263806699263, 0.11648918283380015, 0.10648918283380014, 0.1885105996930746, 0.17410923818146434, 0.21642095578132658, 0.16808675799180056, 0.1490193483495381, 0.22069704941492194, 0.16509331821107945, 0.24289170202936605, 0.2537249405004975, 0.18824666302407717, 0.2337249405005086, 0.20306459096730764, 0.2511644189345281, 0.21740425688093978, 0.2680514743503707, 0.23127862182855063, 0.2380452238659514, 0.24470012703400357, 0.2512448097617808, 0.257680721318236, 0.2640092825751938, 0.2702318867434917, 0.27634990008698085, 0.2823646626126242, 0.28827748874956954, 0.29408966794513014, 0.29980246539398403, 0.3054171225403406, 0.37206016804674646, 0.3163568667590112, 0.4529515830079368, 0.3269183800450551, 0.3320601680467489, 0.3371107983906784, 0.34207136209365635, 0.3469429307320206, 0.3517265568047016, 0.3564232743246104, 0.591209092859339, 0.3655600293716357, 0.443784474349711, 0.4491171140816069, 0.4543562907997343, 0.5412090928593389, 0.3869500080877195, 0.39098659039930805, 0.39494480227022866, 0.39882551397498267, 0.4838906449471322, 0.40635784622313176, 0.41001113648700993, 0.4135902666483008, 0.41709603797945294, 0.4205292388394398, 0.42389064494713213, 0.4271810196485034, 0.43040111418158233, 0.43355166792788735, 0.43663340866130296, 0.4396470527282864, 0.44259330560601495, 0.44547286147438236, 0.44828640410070864, 0.4510346067307984, 0.4537181323591908, 0.45633763392781557, 0.4588937545977898, 0.46138712780173785, 0.46381837757928235, 0.46618811869441945, 0.46849695683536097, 0.47074548875452904, 0.47293430255066027, 0.4750639776797231, 0.47713508524937, 0.4791481881234857, 0.4811038410906442, 0.4830025910231891, 0.4848449770123816, 0.48663153053119823, 0.48836277556902796, 0.4900392287733045, 0.49166139958590793, 0.49322979037658554, 0.4947448965736475, 0.4962072067892074, 0.4976172029477155, 0.4989753604035203, 0.5002821480615223]
**** log-parameter_analysis 运行时间： 2025-01-16 22:26:05 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
初始化模型的准确率：
Accuracy: 26.82%
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.04151349600731256
DataOwner2: noise random: 0.046612983030370085
DataOwner3: noise random: 0.017411176263931672
DataOwner4: noise random: 0.03890017389637121
DataOwner5: noise random: 0.0497559243724908
DataOwner6: noise random: 0.035827308349533585
DataOwner7: noise random: 0.07952148308702177
DataOwner8: noise random: 0.05386855861964357
DataOwner9: noise random: 0.05328004969862751
DataOwner10: noise random: 0.09272632123950135
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9993034924313705, 0.9991162695788327, 0.9998775473031273, 0.9993880384901934, 0.998996536795742, 0.9994803941561424, 0.9974332535322054, 0.9988305012152284, 0.9988532625513901, 0.9965250591615107]
归一化后的数据质量列表avg_f_list: [0.98287675160933, 0.9772921575815783, 1.0, 0.9853986414789282, 0.9737206972800659, 0.988153480930932, 0.9270901590797793, 0.968768089739045, 0.9694470283422599, 0.9]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3502
DataOwner1的最优x_1 = 0.1375
DataOwner2的最优x_2 = 0.1322
DataOwner3的最优x_3 = 0.1530
DataOwner4的最优x_4 = 0.1399
DataOwner5的最优x_5 = 0.1288
DataOwner6的最优x_6 = 0.1424
DataOwner7的最优x_7 = 0.0784
DataOwner8的最优x_8 = 0.1239
DataOwner9的最优x_9 = 0.1246
DataOwner10的最优x_10 = 0.0439
每个DataOwner应该贡献数据比例 xn_list = [0.137534867745608, 0.13224468474016188, 0.15297362465600614, 0.1398817678393709, 0.12879302088048886, 0.14241606407869858, 0.07836296778214286, 0.12391614189817246, 0.12459098913363321, 0.04387605961272437]
ModelOwner的最大效用 U(Eta) = 0.5916
DONE
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.350225181027784
DataOwner1的分配到的支付 ： 0.1554
DataOwner2的分配到的支付 ： 0.1486
DataOwner3的分配到的支付 ： 0.1759
DataOwner4的分配到的支付 ： 0.1585
DataOwner5的分配到的支付 ： 0.1442
DataOwner6的分配到的支付 ： 0.1618
DataOwner7的分配到的支付 ： 0.0835
DataOwner8的分配到的支付 ： 0.1380
DataOwner9的分配到的支付 ： 0.1389
DataOwner10的分配到的支付 ： 0.0454
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC1', 'DataOwner2': 'CPC2', 'DataOwner3': 'CPC3', 'DataOwner4': 'CPC4', 'DataOwner5': 'CPC5', 'DataOwner6': 'CPC6', 'DataOwner7': 'CPC7', 'DataOwner8': 'CPC8', 'DataOwner9': 'CPC9', 'DataOwner10': 'CPC10'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC2
DataOwner3 把数据交给 CPC3
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC8
DataOwner9 把数据交给 CPC9
DataOwner10 把数据交给 CPC10
DONE
----- literation 1: 模型训练 -----
CPC1调整模型中, 本轮训练的数据量为：527.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 26.82%
Epoch 1/5, Loss: 2.2929
Epoch 2/5, Loss: 2.2877
Epoch 3/5, Loss: 2.2884
Epoch 4/5, Loss: 2.2836
Epoch 5/5, Loss: 2.2862
新模型评估：
Accuracy: 32.43%
Model saved to ../../data/model/mnist_cnn_model
CPC2调整模型中, 本轮训练的数据量为：168.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 32.43%
Epoch 1/5, Loss: 2.2873
Epoch 2/5, Loss: 2.2833
Epoch 3/5, Loss: 2.2782
Epoch 4/5, Loss: 2.2796
Epoch 5/5, Loss: 2.2780
新模型评估：
Accuracy: 33.06%
Model saved to ../../data/model/mnist_cnn_model
CPC3调整模型中, 本轮训练的数据量为：1952.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 33.06%
Epoch 1/5, Loss: 2.2721
Epoch 2/5, Loss: 2.2540
Epoch 3/5, Loss: 2.2265
Epoch 4/5, Loss: 2.1894
Epoch 5/5, Loss: 2.1357
新模型评估：
Accuracy: 50.57%
Model saved to ../../data/model/mnist_cnn_model
CPC4调整模型中, 本轮训练的数据量为：2142.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 50.57%
Epoch 1/5, Loss: 2.0872
Epoch 2/5, Loss: 2.0334
Epoch 3/5, Loss: 1.9603
Epoch 4/5, Loss: 1.8863
Epoch 5/5, Loss: 1.7998
新模型评估：
Accuracy: 60.41%
Model saved to ../../data/model/mnist_cnn_model
CPC5调整模型中, 本轮训练的数据量为：657.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 60.41%
Epoch 1/5, Loss: 1.7351
Epoch 2/5, Loss: 1.7072
Epoch 3/5, Loss: 1.6802
Epoch 4/5, Loss: 1.6538
Epoch 5/5, Loss: 1.6278
新模型评估：
Accuracy: 61.90%
Model saved to ../../data/model/mnist_cnn_model
CPC6调整模型中, 本轮训练的数据量为：545.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 61.90%
Epoch 1/5, Loss: 1.6304
Epoch 2/5, Loss: 1.6179
Epoch 3/5, Loss: 1.5827
Epoch 4/5, Loss: 1.5735
Epoch 5/5, Loss: 1.5396
新模型评估：
Accuracy: 66.08%
Model saved to ../../data/model/mnist_cnn_model
CPC7调整模型中, 本轮训练的数据量为：900.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 66.08%
Epoch 1/5, Loss: 1.5416
Epoch 2/5, Loss: 1.5035
Epoch 3/5, Loss: 1.4608
Epoch 4/5, Loss: 1.4679
Epoch 5/5, Loss: 1.4410
新模型评估：
Accuracy: 68.61%
Model saved to ../../data/model/mnist_cnn_model
CPC8调整模型中, 本轮训练的数据量为：158.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 68.61%
Epoch 1/5, Loss: 1.3626
Epoch 2/5, Loss: 1.3430
Epoch 3/5, Loss: 1.2929
Epoch 4/5, Loss: 1.3177
Epoch 5/5, Loss: 1.2947
新模型评估：
Accuracy: 68.12%
CPC9调整模型中, 本轮训练的数据量为：158.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 68.61%
Epoch 1/5, Loss: 1.3961
Epoch 2/5, Loss: 1.3819
Epoch 3/5, Loss: 1.3210
Epoch 4/5, Loss: 1.3636
Epoch 5/5, Loss: 1.3165
新模型评估：
Accuracy: 68.99%
Model saved to ../../data/model/mnist_cnn_model
CPC10调整模型中, 本轮训练的数据量为：168.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 68.99%
Epoch 1/5, Loss: 1.3605
Epoch 2/5, Loss: 1.3471
Epoch 3/5, Loss: 1.3327
Epoch 4/5, Loss: 1.3331
Epoch 5/5, Loss: 1.3271
新模型评估：
Accuracy: 68.73%
DONE
========================= literation: 2 =========================
----- literation 2: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3502
DataOwner1的最优x_1 = 0.1375
DataOwner2的最优x_2 = 0.1322
DataOwner3的最优x_3 = 0.1530
DataOwner4的最优x_4 = 0.1399
DataOwner5的最优x_5 = 0.1288
DataOwner6的最优x_6 = 0.1424
DataOwner7的最优x_7 = 0.0784
DataOwner8的最优x_8 = 0.1239
DataOwner9的最优x_9 = 0.1246
DataOwner10的最优x_10 = 0.0439
每个DataOwner应该贡献数据比例 xn_list = [0.137534867745608, 0.13224468474016188, 0.15297362465600614, 0.1398817678393709, 0.12879302088048886, 0.14241606407869858, 0.07836296778214286, 0.12391614189817246, 0.12459098913363321, 0.04387605961272437]
ModelOwner的最大效用 U(Eta) = 0.5916
DONE
----- literation 2: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.350225181027784
DataOwner1的分配到的支付 ： 0.1554
DataOwner2的分配到的支付 ： 0.1486
DataOwner3的分配到的支付 ： 0.1759
DataOwner4的分配到的支付 ： 0.1585
DataOwner5的分配到的支付 ： 0.1442
DataOwner6的分配到的支付 ： 0.1618
DataOwner7的分配到的支付 ： 0.0835
DataOwner8的分配到的支付 ： 0.1380
DataOwner9的分配到的支付 ： 0.1389
DataOwner10的分配到的支付 ： 0.0454
DONE
----- literation 2: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC2
DataOwner3 把数据交给 CPC3
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC8
DataOwner9 把数据交给 CPC9
DataOwner10 把数据交给 CPC10
DONE
----- literation 2: 模型训练 -----
重新调整fn，进而调整xn、Eta
正在评估DataOwner1的数据质量, 本轮评估的样本数据量为：527.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 68.99%
Epoch 1/5, Loss: 1.3326
Epoch 2/5, Loss: 1.2919
Epoch 3/5, Loss: 1.3019
Epoch 4/5, Loss: 1.2728
Epoch 5/5, Loss: 1.2489
新模型评估：
Accuracy: 70.96%
loss差为：
0.08372253841824007
单位数据loss差为：
0.00015886629680880469
正在评估DataOwner2的数据质量, 本轮评估的样本数据量为：168.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 68.99%
Epoch 1/5, Loss: 1.3102
Epoch 2/5, Loss: 1.3167
Epoch 3/5, Loss: 1.3092
Epoch 4/5, Loss: 1.2989
Epoch 5/5, Loss: 1.2653
新模型评估：
Accuracy: 68.16%
loss差为：
0.04494853814442945
单位数据loss差为：
0.00026755082228827054
正在评估DataOwner3的数据质量, 本轮评估的样本数据量为：1952.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 68.99%
Epoch 1/5, Loss: 1.2801
Epoch 2/5, Loss: 1.2070
Epoch 3/5, Loss: 1.1440
Epoch 4/5, Loss: 1.0710
Epoch 5/5, Loss: 1.0084
新模型评估：
Accuracy: 74.28%
loss差为：
0.2716945236729036
单位数据loss差为：
0.00013918776827505307
正在评估DataOwner4的数据质量, 本轮评估的样本数据量为：2142.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 68.99%
Epoch 1/5, Loss: 1.3128
Epoch 2/5, Loss: 1.2341
Epoch 3/5, Loss: 1.1610
Epoch 4/5, Loss: 1.0899
Epoch 5/5, Loss: 1.0240
新模型评估：
Accuracy: 75.17%
loss差为：
0.2887916354572071
单位数据loss差为：
0.00013482335922371946
正在评估DataOwner5的数据质量, 本轮评估的样本数据量为：657.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 68.99%
Epoch 1/5, Loss: 1.3092
Epoch 2/5, Loss: 1.2944
Epoch 3/5, Loss: 1.2571
Epoch 4/5, Loss: 1.2204
Epoch 5/5, Loss: 1.2227
新模型评估：
Accuracy: 70.99%
loss差为：
0.08650871840390284
单位数据loss差为：
0.00013167232633775166
正在评估DataOwner6的数据质量, 本轮评估的样本数据量为：545.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 68.99%
Epoch 1/5, Loss: 1.3121
Epoch 2/5, Loss: 1.2784
Epoch 3/5, Loss: 1.2663
Epoch 4/5, Loss: 1.2467
Epoch 5/5, Loss: 1.2238
新模型评估：
Accuracy: 69.86%
loss差为：
0.08823635843065047
单位数据loss差为：
0.00016190157510211097
正在评估DataOwner7的数据质量, 本轮评估的样本数据量为：900.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 68.99%
Epoch 1/5, Loss: 1.3363
Epoch 2/5, Loss: 1.2659
Epoch 3/5, Loss: 1.2441
Epoch 4/5, Loss: 1.2402
Epoch 5/5, Loss: 1.2246
新模型评估：
Accuracy: 70.95%
loss差为：
0.11170395215352391
单位数据loss差为：
0.00012411550239280433
正在评估DataOwner8的数据质量, 本轮评估的样本数据量为：158.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 68.99%
Epoch 1/5, Loss: 1.3612
Epoch 2/5, Loss: 1.3370
Epoch 3/5, Loss: 1.3670
Epoch 4/5, Loss: 1.3504
Epoch 5/5, Loss: 1.3066
新模型评估：
Accuracy: 69.92%
loss差为：
0.054616451263427734
单位数据loss差为：
0.00034567374217359326
正在评估DataOwner9的数据质量, 本轮评估的样本数据量为：158.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 68.99%
Epoch 1/5, Loss: 1.2993
Epoch 2/5, Loss: 1.3248
Epoch 3/5, Loss: 1.3409
Epoch 4/5, Loss: 1.2884
Epoch 5/5, Loss: 1.2854
新模型评估：
Accuracy: 69.04%
loss差为：
0.013891180356343513
单位数据loss差为：
8.791886301483236e-05
正在评估DataOwner10的数据质量, 本轮评估的样本数据量为：168.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 68.99%
Epoch 1/5, Loss: 1.3128
Epoch 2/5, Loss: 1.2965
Epoch 3/5, Loss: 1.2921
Epoch 4/5, Loss: 1.2750
Epoch 5/5, Loss: 1.2946
新模型评估：
Accuracy: 67.89%
loss差为：
0.018131693204243904
单位数据loss差为：
0.00010792674526335657
经过服务器调节后的真实数据质量：
数据质量列表avg_f_list: [0.00015886629680880469, 0.00026755082228827054, 0.00013918776827505307, 0.00013482335922371946, 0.00013167232633775166, 0.00016190157510211097, 0.00012411550239280433, 0.00034567374217359326, 8.791886301483236e-05, 0.00010792674526335657]
归一化后的数据质量列表avg_f_list:[0.9275251564686282, 0.9696910024980734, 0.9198905663503046, 0.9181973262201555, 0.916974834178006, 0.9287027397226145, 0.9140430472145116, 1.0, 0.9, 0.9077623679962236]
CPC1调整模型中, 本轮训练的数据量为：527.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 68.99%
Epoch 1/5, Loss: 1.3577
Epoch 2/5, Loss: 1.3192
Epoch 3/5, Loss: 1.2915
Epoch 4/5, Loss: 1.2610
Epoch 5/5, Loss: 1.2602
新模型评估：
Accuracy: 70.62%
Model saved to ../../data/model/mnist_cnn_model
CPC2调整模型中, 本轮训练的数据量为：168.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
**** log-parameter_analysis 运行时间： 2025-01-16 22:48:53 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
初始化模型的准确率：
Accuracy: 26.82%
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.04801723896813356
DataOwner2: noise random: 0.013828695794205382
DataOwner3: noise random: 0.006348097989480406
DataOwner4: noise random: 0.07557704579440738
DataOwner5: noise random: 0.024660978075070852
DataOwner6: noise random: 0.06827969205510441
DataOwner7: noise random: 0.03074177039705124
DataOwner8: noise random: 0.008345320849753524
DataOwner9: noise random: 0.057462311342606265
DataOwner10: noise random: 0.07192308523565658
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9990658090206056, 0.9999228321745915, 0.9999836761596198, 0.9976889748375081, 0.9997541291500119, 0.9981136563152806, 0.9996176509290464, 0.9999718053776145, 0.998663897434256, 0.997901697864157]
归一化后的数据质量列表avg_f_list: [0.9600005835108195, 0.9973485008945624, 1.0, 0.9, 0.9899966497863599, 0.9185070481147284, 0.9840491123159966, 0.9994826872721572, 0.9424858166661415, 0.9092701836443409]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3409
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1565
DataOwner3的最优x_3 = 0.1588
DataOwner4的最优x_4 = 0.0532
DataOwner5的最优x_5 = 0.1502
DataOwner6的最优x_6 = 0.0765
DataOwner7的最优x_7 = 0.1449
DataOwner8的最优x_8 = 0.1583
DataOwner9的最优x_9 = 0.1038
DataOwner10的最优x_10 = 0.0651
每个DataOwner应该贡献数据比例 xn_list = [0.12208055180731643, 0.156545163868101, 0.15878353465075998, 0.05320610334353326, 0.15020099597345887, 0.07646147022172775, 0.14491672602361286, 0.15834886257933178, 0.10384944730279615, 0.06509809931648429]
ModelOwner的最大效用 U(Eta) = 0.5807
DONE
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3409100954141866
DataOwner1的分配到的支付 ： 0.1358
DataOwner2的分配到的支付 ： 0.1810
DataOwner3的分配到的支付 ： 0.1840
DataOwner4的分配到的支付 ： 0.0555
DataOwner5的分配到的支付 ： 0.1724
DataOwner6的分配到的支付 ： 0.0814
DataOwner7的分配到的支付 ： 0.1653
DataOwner8的分配到的支付 ： 0.1834
DataOwner9的分配到的支付 ： 0.1134
DataOwner10的分配到的支付 ： 0.0686
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC1', 'DataOwner2': 'CPC2', 'DataOwner3': 'CPC3', 'DataOwner4': 'CPC4', 'DataOwner5': 'CPC5', 'DataOwner6': 'CPC6', 'DataOwner7': 'CPC7', 'DataOwner8': 'CPC10', 'DataOwner9': 'CPC8', 'DataOwner10': 'CPC9'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC2
DataOwner3 把数据交给 CPC3
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC10
DataOwner9 把数据交给 CPC8
DataOwner10 把数据交给 CPC9
DONE
----- literation 1: 模型训练 -----
CPC1调整模型中, 本轮训练的数据量为：575.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 26.82%
Epoch 1/5, Loss: 2.2912
Epoch 2/5, Loss: 2.2888
Epoch 3/5, Loss: 2.2860
Epoch 4/5, Loss: 2.2829
Epoch 5/5, Loss: 2.2791
新模型评估：
Accuracy: 33.84%
Model saved to ../../data/model/mnist_cnn_model
CPC2调整模型中, 本轮训练的数据量为：1473.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 33.84%
Epoch 1/5, Loss: 2.2735
Epoch 2/5, Loss: 2.2768
Epoch 3/5, Loss: 2.2422
Epoch 4/5, Loss: 2.2264
Epoch 5/5, Loss: 2.2093
新模型评估：
Accuracy: 45.02%
Model saved to ../../data/model/mnist_cnn_model
CPC3调整模型中, 本轮训练的数据量为：1494.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 45.02%
Epoch 1/5, Loss: 2.1851
Epoch 2/5, Loss: 2.1580
Epoch 3/5, Loss: 2.1277
Epoch 4/5, Loss: 2.0916
Epoch 5/5, Loss: 2.0425
新模型评估：
Accuracy: 51.12%
Model saved to ../../data/model/mnist_cnn_model
CPC4调整模型中, 本轮训练的数据量为：563.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 51.12%
Epoch 1/5, Loss: 2.0437
Epoch 2/5, Loss: 2.0247
Epoch 3/5, Loss: 2.0108
Epoch 4/5, Loss: 1.9913
Epoch 5/5, Loss: 1.9766
新模型评估：
Accuracy: 52.23%
Model saved to ../../data/model/mnist_cnn_model
CPC5调整模型中, 本轮训练的数据量为：176.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 52.23%
Epoch 1/5, Loss: 1.9645
Epoch 2/5, Loss: 1.9649
Epoch 3/5, Loss: 1.9519
Epoch 4/5, Loss: 1.9394
Epoch 5/5, Loss: 1.9302
新模型评估：
Accuracy: 53.30%
Model saved to ../../data/model/mnist_cnn_model
CPC6调整模型中, 本轮训练的数据量为：179.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 53.30%
Epoch 1/5, Loss: 1.9247
Epoch 2/5, Loss: 1.9131
Epoch 3/5, Loss: 1.9012
Epoch 4/5, Loss: 1.9007
Epoch 5/5, Loss: 1.8927
新模型评估：
Accuracy: 54.09%
Model saved to ../../data/model/mnist_cnn_model
CPC7调整模型中, 本轮训练的数据量为：1875.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 54.09%
Epoch 1/5, Loss: 1.8612
Epoch 2/5, Loss: 1.7958
Epoch 3/5, Loss: 1.7269
Epoch 4/5, Loss: 1.6486
Epoch 5/5, Loss: 1.5726
新模型评估：
Accuracy: 61.77%
Model saved to ../../data/model/mnist_cnn_model
CPC10调整模型中, 本轮训练的数据量为：372.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 61.77%
Epoch 1/5, Loss: 1.5527
Epoch 2/5, Loss: 1.5388
Epoch 3/5, Loss: 1.5216
Epoch 4/5, Loss: 1.5012
Epoch 5/5, Loss: 1.4857
新模型评估：
Accuracy: 63.91%
Model saved to ../../data/model/mnist_cnn_model
CPC8调整模型中, 本轮训练的数据量为：488.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 63.91%
Epoch 1/5, Loss: 1.5155
Epoch 2/5, Loss: 1.4852
Epoch 3/5, Loss: 1.4658
Epoch 4/5, Loss: 1.4453
Epoch 5/5, Loss: 1.4206
新模型评估：
Accuracy: 64.39%
Model saved to ../../data/model/mnist_cnn_model
CPC9调整模型中, 本轮训练的数据量为：153.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 64.39%
Epoch 1/5, Loss: 1.3611
Epoch 2/5, Loss: 1.3746
Epoch 3/5, Loss: 1.3459
Epoch 4/5, Loss: 1.3085
Epoch 5/5, Loss: 1.3196
新模型评估：
Accuracy: 65.63%
Model saved to ../../data/model/mnist_cnn_model
DONE
========================= literation: 2 =========================
----- literation 2: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3409
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1565
DataOwner3的最优x_3 = 0.1588
DataOwner4的最优x_4 = 0.0532
DataOwner5的最优x_5 = 0.1502
DataOwner6的最优x_6 = 0.0765
DataOwner7的最优x_7 = 0.1449
DataOwner8的最优x_8 = 0.1583
DataOwner9的最优x_9 = 0.1038
DataOwner10的最优x_10 = 0.0651
每个DataOwner应该贡献数据比例 xn_list = [0.12208055180731643, 0.156545163868101, 0.15878353465075998, 0.05320610334353326, 0.15020099597345887, 0.07646147022172775, 0.14491672602361286, 0.15834886257933178, 0.10384944730279615, 0.06509809931648429]
ModelOwner的最大效用 U(Eta) = 0.5807
DONE
----- literation 2: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3409100954141866
DataOwner1的分配到的支付 ： 0.1358
DataOwner2的分配到的支付 ： 0.1810
DataOwner3的分配到的支付 ： 0.1840
DataOwner4的分配到的支付 ： 0.0555
DataOwner5的分配到的支付 ： 0.1724
DataOwner6的分配到的支付 ： 0.0814
DataOwner7的分配到的支付 ： 0.1653
DataOwner8的分配到的支付 ： 0.1834
DataOwner9的分配到的支付 ： 0.1134
DataOwner10的分配到的支付 ： 0.0686
DONE
----- literation 2: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC2
DataOwner3 把数据交给 CPC3
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC10
DataOwner9 把数据交给 CPC8
DataOwner10 把数据交给 CPC9
DONE
----- literation 2: 模型训练 -----
重新调整fn，进而调整xn、Eta
正在评估DataOwner1的数据质量, 本轮评估的样本数据量为：575.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 65.63%
Epoch 1/5, Loss: 1.3276
Epoch 2/5, Loss: 1.3024
Epoch 3/5, Loss: 1.2802
Epoch 4/5, Loss: 1.2583
Epoch 5/5, Loss: 1.2370
新模型评估：
Accuracy: 68.55%
loss差为：
0.09060584174262143
单位数据loss差为：
0.00015757537694368945
正在评估DataOwner2的数据质量, 本轮评估的样本数据量为：1473.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 65.63%
Epoch 1/5, Loss: 1.3186
Epoch 2/5, Loss: 1.2970
Epoch 3/5, Loss: 1.2780
Epoch 4/5, Loss: 1.2066
Epoch 5/5, Loss: 1.1644
新模型评估：
Accuracy: 72.00%
loss差为：
0.15425249064962054
单位数据loss差为：
0.00010471995291895488
正在评估DataOwner3的数据质量, 本轮评估的样本数据量为：1494.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 65.63%
Epoch 1/5, Loss: 1.3278
Epoch 2/5, Loss: 1.2730
Epoch 3/5, Loss: 1.2125
Epoch 4/5, Loss: 1.1596
Epoch 5/5, Loss: 1.1159
新模型评估：
Accuracy: 71.10%
loss差为：
0.21189370254675555
单位数据loss差为：
0.00014182978751456194
正在评估DataOwner4的数据质量, 本轮评估的样本数据量为：563.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 65.63%
Epoch 1/5, Loss: 1.4256
Epoch 2/5, Loss: 1.4002
Epoch 3/5, Loss: 1.3748
Epoch 4/5, Loss: 1.3558
Epoch 5/5, Loss: 1.3358
新模型评估：
Accuracy: 67.90%
loss差为：
0.08988640043470597
单位数据loss差为：
0.00015965612865844755
正在评估DataOwner5的数据质量, 本轮评估的样本数据量为：176.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 65.63%
Epoch 1/5, Loss: 1.3800
Epoch 2/5, Loss: 1.3669
Epoch 3/5, Loss: 1.3529
Epoch 4/5, Loss: 1.3474
Epoch 5/5, Loss: 1.3455
新模型评估：
Accuracy: 67.83%
loss差为：
0.034468015034993416
单位数据loss差为：
0.00019584099451700805
正在评估DataOwner6的数据质量, 本轮评估的样本数据量为：179.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 65.63%
Epoch 1/5, Loss: 1.4484
Epoch 2/5, Loss: 1.4253
Epoch 3/5, Loss: 1.4166
Epoch 4/5, Loss: 1.4094
Epoch 5/5, Loss: 1.3986
新模型评估：
Accuracy: 66.09%
loss差为：
0.049703796704610115
单位数据loss差为：
0.00027767484192519617
正在评估DataOwner7的数据质量, 本轮评估的样本数据量为：1875.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 65.63%
Epoch 1/5, Loss: 1.3431
Epoch 2/5, Loss: 1.2712
Epoch 3/5, Loss: 1.2086
Epoch 4/5, Loss: 1.1468
Epoch 5/5, Loss: 1.0836
新模型评估：
Accuracy: 74.73%
loss差为：
0.25942529042561846
单位数据loss差为：
0.00013836015489366319
正在评估DataOwner8的数据质量, 本轮评估的样本数据量为：372.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 65.63%
Epoch 1/5, Loss: 1.3454
Epoch 2/5, Loss: 1.3270
Epoch 3/5, Loss: 1.3075
Epoch 4/5, Loss: 1.2851
Epoch 5/5, Loss: 1.2695
新模型评估：
Accuracy: 66.54%
loss差为：
0.07583886384963989
单位数据loss差为：
0.00020386791357430078
正在评估DataOwner9的数据质量, 本轮评估的样本数据量为：488.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 65.63%
Epoch 1/5, Loss: 1.3955
Epoch 2/5, Loss: 1.3819
Epoch 3/5, Loss: 1.3611
Epoch 4/5, Loss: 1.3433
Epoch 5/5, Loss: 1.3209
新模型评估：
Accuracy: 68.35%
loss差为：
0.07459424436092377
单位数据loss差为：
0.00015285705811664705
正在评估DataOwner10的数据质量, 本轮评估的样本数据量为：153.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 65.63%
Epoch 1/5, Loss: 1.3561
Epoch 2/5, Loss: 1.3415
Epoch 3/5, Loss: 1.3783
Epoch 4/5, Loss: 1.3576
Epoch 5/5, Loss: 1.3417
新模型评估：
Accuracy: 66.91%
loss差为：
0.014415979385375977
单位数据loss差为：
9.422208748611749e-05
经过服务器调节后的真实数据质量：
数据质量列表avg_f_list: [0.00015757537694368945, 0.00010471995291895488, 0.00014182978751456194, 0.00015965612865844755, 0.00019584099451700805, 0.00027767484192519617, 0.00013836015489366319, 0.00020386791357430078, 0.00015285705811664705, 9.422208748611749e-05]
归一化后的数据质量列表avg_f_list:[0.9345338447772451, 0.9057223809285042, 0.9259509322571955, 0.9356680614430674, 0.955392412799469, 1.0, 0.9240596373395981, 0.9597678821576892, 0.9319618916651378, 0.9]
CPC1调整模型中, 本轮训练的数据量为：575.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 65.63%
Epoch 1/5, Loss: 1.3275
Epoch 2/5, Loss: 1.3034
Epoch 3/5, Loss: 1.2813
Epoch 4/5, Loss: 1.2591
Epoch 5/5, Loss: 1.2382
新模型评估：
Accuracy: 68.61%
Model saved to ../../data/model/mnist_cnn_model
CPC2调整模型中, 本轮训练的数据量为：1473.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 68.61%
Epoch 1/5, Loss: 1.2725
Epoch 2/5, Loss: 1.2163
Epoch 3/5, Loss: 1.1345
Epoch 4/5, Loss: 1.1070
Epoch 5/5, Loss: 1.0665
新模型评估：
Accuracy: 72.18%
Model saved to ../../data/model/mnist_cnn_model
CPC3调整模型中, 本轮训练的数据量为：1494.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 72.18%
Epoch 1/5, Loss: 1.0169
Epoch 2/5, Loss: 0.9693
Epoch 3/5, Loss: 0.9217
Epoch 4/5, Loss: 0.8875
Epoch 5/5, Loss: 0.8441
新模型评估：
Accuracy: 74.98%
Model saved to ../../data/model/mnist_cnn_model
CPC4调整模型中, 本轮训练的数据量为：563.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 74.98%
Epoch 1/5, Loss: 0.9194
Epoch 2/5, Loss: 0.8943
Epoch 3/5, Loss: 0.8804
Epoch 4/5, Loss: 0.8652
Epoch 5/5, Loss: 0.8497
新模型评估：
Accuracy: 74.57%
CPC5调整模型中, 本轮训练的数据量为：176.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 74.98%
Epoch 1/5, Loss: 0.8500
Epoch 2/5, Loss: 0.8329
Epoch 3/5, Loss: 0.8298
Epoch 4/5, Loss: 0.8097
Epoch 5/5, Loss: 0.8084
新模型评估：
Accuracy: 76.70%
Model saved to ../../data/model/mnist_cnn_model
CPC6调整模型中, 本轮训练的数据量为：179.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 76.70%
Epoch 1/5, Loss: 0.8739
Epoch 2/5, Loss: 0.8692
Epoch 3/5, Loss: 0.8583
Epoch 4/5, Loss: 0.8371
Epoch 5/5, Loss: 0.8379
新模型评估：
Accuracy: 74.93%
CPC7调整模型中, 本轮训练的数据量为：1875.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 76.70%
Epoch 1/5, Loss: 0.8129
Epoch 2/5, Loss: 0.7779
Epoch 3/5, Loss: 0.7379
Epoch 4/5, Loss: 0.7047
Epoch 5/5, Loss: 0.6765
新模型评估：
Accuracy: 77.85%
Model saved to ../../data/model/mnist_cnn_model
CPC10调整模型中, 本轮训练的数据量为：372.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.85%
Epoch 1/5, Loss: 0.6299
Epoch 2/5, Loss: 0.6095
Epoch 3/5, Loss: 0.6017
Epoch 4/5, Loss: 0.5937
Epoch 5/5, Loss: 0.5825
新模型评估：
Accuracy: 75.98%
CPC8调整模型中, 本轮训练的数据量为：488.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.85%
Epoch 1/5, Loss: 0.7327
Epoch 2/5, Loss: 0.7161
Epoch 3/5, Loss: 0.7134
Epoch 4/5, Loss: 0.6957
Epoch 5/5, Loss: 0.6907
新模型评估：
Accuracy: 77.38%
CPC9调整模型中, 本轮训练的数据量为：153.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.85%
Epoch 1/5, Loss: 0.6532
Epoch 2/5, Loss: 0.6175
Epoch 3/5, Loss: 0.5831
Epoch 4/5, Loss: 0.6010
Epoch 5/5, Loss: 0.5761
新模型评估：
Accuracy: 75.29%
DONE
========================= literation: 3 =========================
----- literation 3: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3136
DataOwner1的最优x_1 = 0.1162
DataOwner2的最优x_2 = 0.0848
DataOwner3的最优x_3 = 0.1073
DataOwner4的最优x_4 = 0.1174
DataOwner5的最优x_5 = 0.1365
DataOwner6的最优x_6 = 0.1740
DataOwner7的最优x_7 = 0.1053
DataOwner8的最优x_8 = 0.1405
DataOwner9的最优x_9 = 0.1136
DataOwner10的最优x_10 = 0.0781
每个DataOwner应该贡献数据比例 xn_list = [0.11620255377803676, 0.08482707488724157, 0.10728327525586422, 0.11735540341075558, 0.13648503595001474, 0.17396867979660227, 0.10527056275068633, 0.14050251885918308, 0.11356623983823431, 0.07808734539011171]
ModelOwner的最大效用 U(Eta) = 0.5498
Eta开始变化：
eta:0.01
new: DataOwner1的最优x_1 = 0.0128
new: DataOwner2的最优x_2 = 0.0093
new: DataOwner3的最优x_3 = 0.0118
new: DataOwner4的最优x_4 = 0.0129
new: DataOwner5的最优x_5 = 0.0150
new: DataOwner6的最优x_6 = 0.0191
new: DataOwner7的最优x_7 = 0.0116
new: DataOwner8的最优x_8 = 0.0154
new: DataOwner9的最优x_9 = 0.0125
new: DataOwner10的最优x_10 = 0.0086
eta:0.02
new: DataOwner1的最优x_1 = 0.0144
new: DataOwner2的最优x_2 = 0.0105
new: DataOwner3的最优x_3 = 0.0133
new: DataOwner4的最优x_4 = 0.0145
new: DataOwner5的最优x_5 = 0.0169
new: DataOwner6的最优x_6 = 0.0216
new: DataOwner7的最优x_7 = 0.0130
new: DataOwner8的最优x_8 = 0.0174
new: DataOwner9的最优x_9 = 0.0141
new: DataOwner10的最优x_10 = 0.0097
eta:0.03
new: DataOwner1的最优x_1 = 0.0122
new: DataOwner2的最优x_2 = 0.0089
new: DataOwner3的最优x_3 = 0.0113
new: DataOwner4的最优x_4 = 0.0123
new: DataOwner5的最优x_5 = 0.0143
new: DataOwner6的最优x_6 = 0.0183
new: DataOwner7的最优x_7 = 0.0110
new: DataOwner8的最优x_8 = 0.0147
new: DataOwner9的最优x_9 = 0.0119
new: DataOwner10的最优x_10 = 0.0082
eta:0.04
new: DataOwner1的最优x_1 = 0.0134
new: DataOwner2的最优x_2 = 0.0098
new: DataOwner3的最优x_3 = 0.0124
new: DataOwner4的最优x_4 = 0.0136
new: DataOwner5的最优x_5 = 0.0158
new: DataOwner6的最优x_6 = 0.0201
new: DataOwner7的最优x_7 = 0.0122
new: DataOwner8的最优x_8 = 0.0162
new: DataOwner9的最优x_9 = 0.0131
new: DataOwner10的最优x_10 = 0.0090
eta:0.05
new: DataOwner1的最优x_1 = 0.0139
new: DataOwner2的最优x_2 = 0.0101
new: DataOwner3的最优x_3 = 0.0128
new: DataOwner4的最优x_4 = 0.0140
new: DataOwner5的最优x_5 = 0.0163
new: DataOwner6的最优x_6 = 0.0208
new: DataOwner7的最优x_7 = 0.0126
new: DataOwner8的最优x_8 = 0.0168
new: DataOwner9的最优x_9 = 0.0136
new: DataOwner10的最优x_10 = 0.0093
eta:0.060000000000000005
new: DataOwner1的最优x_1 = 0.0125
new: DataOwner2的最优x_2 = 0.0091
new: DataOwner3的最优x_3 = 0.0116
new: DataOwner4的最优x_4 = 0.0126
new: DataOwner5的最优x_5 = 0.0147
new: DataOwner6的最优x_6 = 0.0187
new: DataOwner7的最优x_7 = 0.0113
new: DataOwner8的最优x_8 = 0.0151
new: DataOwner9的最优x_9 = 0.0122
new: DataOwner10的最优x_10 = 0.0084
eta:0.06999999999999999
new: DataOwner1的最优x_1 = 0.0161
new: DataOwner2的最优x_2 = 0.0117
new: DataOwner3的最优x_3 = 0.0148
new: DataOwner4的最优x_4 = 0.0162
new: DataOwner5的最优x_5 = 0.0189
new: DataOwner6的最优x_6 = 0.0240
new: DataOwner7的最优x_7 = 0.0146
new: DataOwner8的最优x_8 = 0.0194
new: DataOwner9的最优x_9 = 0.0157
new: DataOwner10的最优x_10 = 0.0108
eta:0.08
new: DataOwner1的最优x_1 = 0.0138
new: DataOwner2的最优x_2 = 0.0101
new: DataOwner3的最优x_3 = 0.0127
new: DataOwner4的最优x_4 = 0.0139
new: DataOwner5的最优x_5 = 0.0162
new: DataOwner6的最优x_6 = 0.0206
new: DataOwner7的最优x_7 = 0.0125
new: DataOwner8的最优x_8 = 0.0167
new: DataOwner9的最优x_9 = 0.0135
new: DataOwner10的最优x_10 = 0.0093
eta:0.09
new: DataOwner1的最优x_1 = 0.0141
new: DataOwner2的最优x_2 = 0.0103
new: DataOwner3的最优x_3 = 0.0130
new: DataOwner4的最优x_4 = 0.0142
new: DataOwner5的最优x_5 = 0.0166
new: DataOwner6的最优x_6 = 0.0211
new: DataOwner7的最优x_7 = 0.0128
new: DataOwner8的最优x_8 = 0.0171
new: DataOwner9的最优x_9 = 0.0138
new: DataOwner10的最优x_10 = 0.0095
eta:0.09999999999999999
new: DataOwner1的最优x_1 = 0.0097
new: DataOwner2的最优x_2 = 0.0071
new: DataOwner3的最优x_3 = 0.0090
new: DataOwner4的最优x_4 = 0.0098
new: DataOwner5的最优x_5 = 0.0114
new: DataOwner6的最优x_6 = 0.0146
new: DataOwner7的最优x_7 = 0.0088
new: DataOwner8的最优x_8 = 0.0118
new: DataOwner9的最优x_9 = 0.0095
new: DataOwner10的最优x_10 = 0.0065
eta:0.11
new: DataOwner1的最优x_1 = 0.0097
new: DataOwner2的最优x_2 = 0.0071
new: DataOwner3的最优x_3 = 0.0090
new: DataOwner4的最优x_4 = 0.0098
new: DataOwner5的最优x_5 = 0.0114
new: DataOwner6的最优x_6 = 0.0146
new: DataOwner7的最优x_7 = 0.0088
new: DataOwner8的最优x_8 = 0.0118
new: DataOwner9的最优x_9 = 0.0095
new: DataOwner10的最优x_10 = 0.0065
eta:0.12
new: DataOwner1的最优x_1 = 0.0141
new: DataOwner2的最优x_2 = 0.0103
new: DataOwner3的最优x_3 = 0.0130
new: DataOwner4的最优x_4 = 0.0143
new: DataOwner5的最优x_5 = 0.0166
new: DataOwner6的最优x_6 = 0.0212
new: DataOwner7的最优x_7 = 0.0128
new: DataOwner8的最优x_8 = 0.0171
new: DataOwner9的最优x_9 = 0.0138
new: DataOwner10的最优x_10 = 0.0095
eta:0.13
new: DataOwner1的最优x_1 = 0.0127
new: DataOwner2的最优x_2 = 0.0092
new: DataOwner3的最优x_3 = 0.0117
new: DataOwner4的最优x_4 = 0.0128
new: DataOwner5的最优x_5 = 0.0149
new: DataOwner6的最优x_6 = 0.0189
new: DataOwner7的最优x_7 = 0.0115
new: DataOwner8的最优x_8 = 0.0153
new: DataOwner9的最优x_9 = 0.0124
new: DataOwner10的最优x_10 = 0.0085
eta:0.14
new: DataOwner1的最优x_1 = 0.0136
new: DataOwner2的最优x_2 = 0.0099
new: DataOwner3的最优x_3 = 0.0126
new: DataOwner4的最优x_4 = 0.0138
new: DataOwner5的最优x_5 = 0.0160
new: DataOwner6的最优x_6 = 0.0204
new: DataOwner7的最优x_7 = 0.0123
new: DataOwner8的最优x_8 = 0.0165
new: DataOwner9的最优x_9 = 0.0133
new: DataOwner10的最优x_10 = 0.0092
eta:0.15000000000000002
new: DataOwner1的最优x_1 = 0.0161
new: DataOwner2的最优x_2 = 0.0117
new: DataOwner3的最优x_3 = 0.0148
new: DataOwner4的最优x_4 = 0.0162
new: DataOwner5的最优x_5 = 0.0189
new: DataOwner6的最优x_6 = 0.0240
new: DataOwner7的最优x_7 = 0.0145
new: DataOwner8的最优x_8 = 0.0194
new: DataOwner9的最优x_9 = 0.0157
new: DataOwner10的最优x_10 = 0.0108
eta:0.16
new: DataOwner1的最优x_1 = 0.0142
new: DataOwner2的最优x_2 = 0.0103
new: DataOwner3的最优x_3 = 0.0131
new: DataOwner4的最优x_4 = 0.0143
new: DataOwner5的最优x_5 = 0.0166
new: DataOwner6的最优x_6 = 0.0212
new: DataOwner7的最优x_7 = 0.0128
new: DataOwner8的最优x_8 = 0.0171
new: DataOwner9的最优x_9 = 0.0138
new: DataOwner10的最优x_10 = 0.0095
eta:0.17
new: DataOwner1的最优x_1 = 0.0165
new: DataOwner2的最优x_2 = 0.0121
new: DataOwner3的最优x_3 = 0.0153
new: DataOwner4的最优x_4 = 0.0167
new: DataOwner5的最优x_5 = 0.0194
new: DataOwner6的最优x_6 = 0.0248
new: DataOwner7的最优x_7 = 0.0150
new: DataOwner8的最优x_8 = 0.0200
new: DataOwner9的最优x_9 = 0.0162
new: DataOwner10的最优x_10 = 0.0111
eta:0.18000000000000002
new: DataOwner1的最优x_1 = 0.0193
new: DataOwner2的最优x_2 = 0.0141
new: DataOwner3的最优x_3 = 0.0178
new: DataOwner4的最优x_4 = 0.0195
new: DataOwner5的最优x_5 = 0.0226
new: DataOwner6的最优x_6 = 0.0288
new: DataOwner7的最优x_7 = 0.0175
new: DataOwner8的最优x_8 = 0.0233
new: DataOwner9的最优x_9 = 0.0188
new: DataOwner10的最优x_10 = 0.0129
eta:0.19
new: DataOwner1的最优x_1 = 0.0203
new: DataOwner2的最优x_2 = 0.0148
new: DataOwner3的最优x_3 = 0.0188
new: DataOwner4的最优x_4 = 0.0205
new: DataOwner5的最优x_5 = 0.0239
new: DataOwner6的最优x_6 = 0.0304
new: DataOwner7的最优x_7 = 0.0184
new: DataOwner8的最优x_8 = 0.0246
new: DataOwner9的最优x_9 = 0.0199
new: DataOwner10的最优x_10 = 0.0137
eta:0.2
new: DataOwner1的最优x_1 = 0.0195
new: DataOwner2的最优x_2 = 0.0142
new: DataOwner3的最优x_3 = 0.0180
new: DataOwner4的最优x_4 = 0.0197
new: DataOwner5的最优x_5 = 0.0229
new: DataOwner6的最优x_6 = 0.0291
new: DataOwner7的最优x_7 = 0.0176
new: DataOwner8的最优x_8 = 0.0235
new: DataOwner9的最优x_9 = 0.0190
new: DataOwner10的最优x_10 = 0.0131
eta:0.21000000000000002
new: DataOwner1的最优x_1 = 0.0186
new: DataOwner2的最优x_2 = 0.0136
new: DataOwner3的最优x_3 = 0.0172
new: DataOwner4的最优x_4 = 0.0188
new: DataOwner5的最优x_5 = 0.0218
new: DataOwner6的最优x_6 = 0.0278
new: DataOwner7的最优x_7 = 0.0168
new: DataOwner8的最优x_8 = 0.0225
new: DataOwner9的最优x_9 = 0.0182
new: DataOwner10的最优x_10 = 0.0125
eta:0.22
new: DataOwner1的最优x_1 = 0.0195
new: DataOwner2的最优x_2 = 0.0142
new: DataOwner3的最优x_3 = 0.0180
new: DataOwner4的最优x_4 = 0.0197
new: DataOwner5的最优x_5 = 0.0229
new: DataOwner6的最优x_6 = 0.0291
new: DataOwner7的最优x_7 = 0.0176
new: DataOwner8的最优x_8 = 0.0235
new: DataOwner9的最优x_9 = 0.0190
new: DataOwner10的最优x_10 = 0.0131
eta:0.23
new: DataOwner1的最优x_1 = 0.0203
new: DataOwner2的最优x_2 = 0.0149
new: DataOwner3的最优x_3 = 0.0188
new: DataOwner4的最优x_4 = 0.0205
new: DataOwner5的最优x_5 = 0.0239
new: DataOwner6的最优x_6 = 0.0305
new: DataOwner7的最优x_7 = 0.0184
new: DataOwner8的最优x_8 = 0.0246
new: DataOwner9的最优x_9 = 0.0199
new: DataOwner10的最优x_10 = 0.0137
eta:0.24000000000000002
new: DataOwner1的最优x_1 = 0.0212
new: DataOwner2的最优x_2 = 0.0155
new: DataOwner3的最优x_3 = 0.0196
new: DataOwner4的最优x_4 = 0.0214
new: DataOwner5的最优x_5 = 0.0249
new: DataOwner6的最优x_6 = 0.0318
new: DataOwner7的最优x_7 = 0.0192
new: DataOwner8的最优x_8 = 0.0257
new: DataOwner9的最优x_9 = 0.0207
new: DataOwner10的最优x_10 = 0.0143
eta:0.25
new: DataOwner1的最优x_1 = 0.0243
new: DataOwner2的最优x_2 = 0.0178
new: DataOwner3的最优x_3 = 0.0225
new: DataOwner4的最优x_4 = 0.0246
new: DataOwner5的最优x_5 = 0.0286
new: DataOwner6的最优x_6 = 0.0364
new: DataOwner7的最优x_7 = 0.0220
new: DataOwner8的最优x_8 = 0.0294
new: DataOwner9的最优x_9 = 0.0238
new: DataOwner10的最优x_10 = 0.0163
eta:0.26
new: DataOwner1的最优x_1 = 0.0230
new: DataOwner2的最优x_2 = 0.0168
new: DataOwner3的最优x_3 = 0.0212
new: DataOwner4的最优x_4 = 0.0232
new: DataOwner5的最优x_5 = 0.0270
new: DataOwner6的最优x_6 = 0.0344
new: DataOwner7的最优x_7 = 0.0208
new: DataOwner8的最优x_8 = 0.0278
new: DataOwner9的最优x_9 = 0.0225
new: DataOwner10的最优x_10 = 0.0155
eta:0.27
new: DataOwner1的最优x_1 = 0.0239
new: DataOwner2的最优x_2 = 0.0174
new: DataOwner3的最优x_3 = 0.0221
new: DataOwner4的最优x_4 = 0.0241
new: DataOwner5的最优x_5 = 0.0281
new: DataOwner6的最优x_6 = 0.0358
new: DataOwner7的最优x_7 = 0.0216
new: DataOwner8的最优x_8 = 0.0289
new: DataOwner9的最优x_9 = 0.0233
new: DataOwner10的最优x_10 = 0.0161
eta:0.28
new: DataOwner1的最优x_1 = 0.0248
new: DataOwner2的最优x_2 = 0.0181
new: DataOwner3的最优x_3 = 0.0229
new: DataOwner4的最优x_4 = 0.0250
new: DataOwner5的最优x_5 = 0.0291
new: DataOwner6的最优x_6 = 0.0371
new: DataOwner7的最优x_7 = 0.0224
new: DataOwner8的最优x_8 = 0.0299
new: DataOwner9的最优x_9 = 0.0242
new: DataOwner10的最优x_10 = 0.0166
eta:0.29000000000000004
new: DataOwner1的最优x_1 = 0.0257
new: DataOwner2的最优x_2 = 0.0187
new: DataOwner3的最优x_3 = 0.0237
new: DataOwner4的最优x_4 = 0.0259
new: DataOwner5的最优x_5 = 0.0301
new: DataOwner6的最优x_6 = 0.0384
new: DataOwner7的最优x_7 = 0.0232
new: DataOwner8的最优x_8 = 0.0310
new: DataOwner9的最优x_9 = 0.0251
new: DataOwner10的最优x_10 = 0.0172
eta:0.3
new: DataOwner1的最优x_1 = 0.0265
new: DataOwner2的最优x_2 = 0.0194
new: DataOwner3的最优x_3 = 0.0245
new: DataOwner4的最优x_4 = 0.0268
new: DataOwner5的最优x_5 = 0.0312
new: DataOwner6的最优x_6 = 0.0397
new: DataOwner7的最优x_7 = 0.0240
new: DataOwner8的最优x_8 = 0.0321
new: DataOwner9的最优x_9 = 0.0259
new: DataOwner10的最优x_10 = 0.0178
eta:0.31
new: DataOwner1的最优x_1 = 0.0274
new: DataOwner2的最优x_2 = 0.0200
new: DataOwner3的最优x_3 = 0.0253
new: DataOwner4的最优x_4 = 0.0277
new: DataOwner5的最优x_5 = 0.0322
new: DataOwner6的最优x_6 = 0.0411
new: DataOwner7的最优x_7 = 0.0248
new: DataOwner8的最优x_8 = 0.0332
new: DataOwner9的最优x_9 = 0.0268
new: DataOwner10的最优x_10 = 0.0184
eta:0.32
new: DataOwner1的最优x_1 = 0.0311
new: DataOwner2的最优x_2 = 0.0227
new: DataOwner3的最优x_3 = 0.0287
new: DataOwner4的最优x_4 = 0.0314
new: DataOwner5的最优x_5 = 0.0366
new: DataOwner6的最优x_6 = 0.0466
new: DataOwner7的最优x_7 = 0.0282
new: DataOwner8的最优x_8 = 0.0377
new: DataOwner9的最优x_9 = 0.0304
new: DataOwner10的最优x_10 = 0.0209
eta:0.33
new: DataOwner1的最优x_1 = 0.0292
new: DataOwner2的最优x_2 = 0.0213
new: DataOwner3的最优x_3 = 0.0270
new: DataOwner4的最优x_4 = 0.0295
new: DataOwner5的最优x_5 = 0.0343
new: DataOwner6的最优x_6 = 0.0437
new: DataOwner7的最优x_7 = 0.0264
new: DataOwner8的最优x_8 = 0.0353
new: DataOwner9的最优x_9 = 0.0285
new: DataOwner10的最优x_10 = 0.0196
eta:0.34
new: DataOwner1的最优x_1 = 0.0400
new: DataOwner2的最优x_2 = 0.0292
new: DataOwner3的最优x_3 = 0.0370
new: DataOwner4的最优x_4 = 0.0404
new: DataOwner5的最优x_5 = 0.0470
new: DataOwner6的最优x_6 = 0.0599
new: DataOwner7的最优x_7 = 0.0363
new: DataOwner8的最优x_8 = 0.0484
new: DataOwner9的最优x_9 = 0.0391
new: DataOwner10的最优x_10 = 0.0269
eta:0.35000000000000003
new: DataOwner1的最优x_1 = 0.0310
new: DataOwner2的最优x_2 = 0.0226
new: DataOwner3的最优x_3 = 0.0286
new: DataOwner4的最优x_4 = 0.0313
new: DataOwner5的最优x_5 = 0.0364
new: DataOwner6的最优x_6 = 0.0464
new: DataOwner7的最优x_7 = 0.0280
new: DataOwner8的最优x_8 = 0.0374
new: DataOwner9的最优x_9 = 0.0303
new: DataOwner10的最优x_10 = 0.0208
eta:0.36000000000000004
new: DataOwner1的最优x_1 = 0.0318
new: DataOwner2的最优x_2 = 0.0232
new: DataOwner3的最优x_3 = 0.0294
new: DataOwner4的最优x_4 = 0.0322
new: DataOwner5的最优x_5 = 0.0374
new: DataOwner6的最优x_6 = 0.0477
new: DataOwner7的最优x_7 = 0.0289
new: DataOwner8的最优x_8 = 0.0385
new: DataOwner9的最优x_9 = 0.0311
new: DataOwner10的最优x_10 = 0.0214
eta:0.37
new: DataOwner1的最优x_1 = 0.0327
new: DataOwner2的最优x_2 = 0.0239
new: DataOwner3的最优x_3 = 0.0302
new: DataOwner4的最优x_4 = 0.0331
new: DataOwner5的最优x_5 = 0.0384
new: DataOwner6的最优x_6 = 0.0490
new: DataOwner7的最优x_7 = 0.0297
new: DataOwner8的最优x_8 = 0.0396
new: DataOwner9的最优x_9 = 0.0320
new: DataOwner10的最优x_10 = 0.0220
eta:0.38
new: DataOwner1的最优x_1 = 0.0370
new: DataOwner2的最优x_2 = 0.0270
new: DataOwner3的最优x_3 = 0.0341
new: DataOwner4的最优x_4 = 0.0373
new: DataOwner5的最优x_5 = 0.0434
new: DataOwner6的最优x_6 = 0.0554
new: DataOwner7的最优x_7 = 0.0335
new: DataOwner8的最优x_8 = 0.0447
new: DataOwner9的最优x_9 = 0.0361
new: DataOwner10的最优x_10 = 0.0248
eta:0.39
new: DataOwner1的最优x_1 = 0.0345
new: DataOwner2的最优x_2 = 0.0252
new: DataOwner3的最优x_3 = 0.0319
new: DataOwner4的最优x_4 = 0.0348
new: DataOwner5的最优x_5 = 0.0405
new: DataOwner6的最优x_6 = 0.0517
new: DataOwner7的最优x_7 = 0.0313
new: DataOwner8的最优x_8 = 0.0417
new: DataOwner9的最优x_9 = 0.0337
new: DataOwner10的最优x_10 = 0.0232
eta:0.4
new: DataOwner1的最优x_1 = 0.0354
new: DataOwner2的最优x_2 = 0.0258
new: DataOwner3的最优x_3 = 0.0327
new: DataOwner4的最优x_4 = 0.0357
new: DataOwner5的最优x_5 = 0.0416
new: DataOwner6的最优x_6 = 0.0530
new: DataOwner7的最优x_7 = 0.0321
new: DataOwner8的最优x_8 = 0.0428
new: DataOwner9的最优x_9 = 0.0346
new: DataOwner10的最优x_10 = 0.0238
eta:0.41000000000000003
new: DataOwner1的最优x_1 = 0.0399
new: DataOwner2的最优x_2 = 0.0291
new: DataOwner3的最优x_3 = 0.0368
new: DataOwner4的最优x_4 = 0.0403
new: DataOwner5的最优x_5 = 0.0469
new: DataOwner6的最优x_6 = 0.0597
new: DataOwner7的最优x_7 = 0.0361
new: DataOwner8的最优x_8 = 0.0482
new: DataOwner9的最优x_9 = 0.0390
new: DataOwner10的最优x_10 = 0.0268
eta:0.42000000000000004
new: DataOwner1的最优x_1 = 0.0372
new: DataOwner2的最优x_2 = 0.0271
new: DataOwner3的最优x_3 = 0.0343
new: DataOwner4的最优x_4 = 0.0375
new: DataOwner5的最优x_5 = 0.0436
new: DataOwner6的最优x_6 = 0.0556
new: DataOwner7的最优x_7 = 0.0337
new: DataOwner8的最优x_8 = 0.0449
new: DataOwner9的最优x_9 = 0.0363
new: DataOwner10的最优x_10 = 0.0250
eta:0.43
new: DataOwner1的最优x_1 = 0.0380
new: DataOwner2的最优x_2 = 0.0278
new: DataOwner3的最优x_3 = 0.0351
new: DataOwner4的最优x_4 = 0.0384
new: DataOwner5的最优x_5 = 0.0447
new: DataOwner6的最优x_6 = 0.0569
new: DataOwner7的最优x_7 = 0.0345
new: DataOwner8的最优x_8 = 0.0460
new: DataOwner9的最优x_9 = 0.0372
new: DataOwner10的最优x_10 = 0.0256
eta:0.44
new: DataOwner1的最优x_1 = 0.0389
new: DataOwner2的最优x_2 = 0.0284
new: DataOwner3的最优x_3 = 0.0359
new: DataOwner4的最优x_4 = 0.0393
new: DataOwner5的最优x_5 = 0.0457
new: DataOwner6的最优x_6 = 0.0583
new: DataOwner7的最优x_7 = 0.0353
new: DataOwner8的最优x_8 = 0.0471
new: DataOwner9的最优x_9 = 0.0380
new: DataOwner10的最优x_10 = 0.0262
eta:0.45
new: DataOwner1的最优x_1 = 0.0398
new: DataOwner2的最优x_2 = 0.0291
new: DataOwner3的最优x_3 = 0.0368
new: DataOwner4的最优x_4 = 0.0402
new: DataOwner5的最优x_5 = 0.0468
new: DataOwner6的最优x_6 = 0.0596
new: DataOwner7的最优x_7 = 0.0361
new: DataOwner8的最优x_8 = 0.0481
new: DataOwner9的最优x_9 = 0.0389
new: DataOwner10的最优x_10 = 0.0268
eta:0.46
new: DataOwner1的最优x_1 = 0.0407
new: DataOwner2的最优x_2 = 0.0297
new: DataOwner3的最优x_3 = 0.0376
new: DataOwner4的最优x_4 = 0.0411
new: DataOwner5的最优x_5 = 0.0478
new: DataOwner6的最优x_6 = 0.0609
new: DataOwner7的最优x_7 = 0.0369
new: DataOwner8的最优x_8 = 0.0492
new: DataOwner9的最优x_9 = 0.0398
new: DataOwner10的最优x_10 = 0.0273
eta:0.47000000000000003
new: DataOwner1的最优x_1 = 0.0553
new: DataOwner2的最优x_2 = 0.0404
new: DataOwner3的最优x_3 = 0.0511
new: DataOwner4的最优x_4 = 0.0559
new: DataOwner5的最优x_5 = 0.0650
new: DataOwner6的最优x_6 = 0.0828
new: DataOwner7的最优x_7 = 0.0501
new: DataOwner8的最优x_8 = 0.0669
new: DataOwner9的最优x_9 = 0.0541
new: DataOwner10的最优x_10 = 0.0372
eta:0.48000000000000004
new: DataOwner1的最优x_1 = 0.0425
new: DataOwner2的最优x_2 = 0.0310
new: DataOwner3的最优x_3 = 0.0392
new: DataOwner4的最优x_4 = 0.0429
new: DataOwner5的最优x_5 = 0.0499
new: DataOwner6的最优x_6 = 0.0636
new: DataOwner7的最优x_7 = 0.0385
new: DataOwner8的最优x_8 = 0.0513
new: DataOwner9的最优x_9 = 0.0415
new: DataOwner10的最优x_10 = 0.0285
eta:0.49
new: DataOwner1的最优x_1 = 0.0477
new: DataOwner2的最优x_2 = 0.0348
new: DataOwner3的最优x_3 = 0.0440
new: DataOwner4的最优x_4 = 0.0482
new: DataOwner5的最优x_5 = 0.0560
new: DataOwner6的最优x_6 = 0.0714
new: DataOwner7的最优x_7 = 0.0432
new: DataOwner8的最优x_8 = 0.0577
new: DataOwner9的最优x_9 = 0.0466
new: DataOwner10的最优x_10 = 0.0320
eta:0.5
new: DataOwner1的最优x_1 = 0.0487
new: DataOwner2的最优x_2 = 0.0355
new: DataOwner3的最优x_3 = 0.0449
new: DataOwner4的最优x_4 = 0.0491
new: DataOwner5的最优x_5 = 0.0571
new: DataOwner6的最优x_6 = 0.0728
new: DataOwner7的最优x_7 = 0.0441
new: DataOwner8的最优x_8 = 0.0588
new: DataOwner9的最优x_9 = 0.0476
new: DataOwner10的最优x_10 = 0.0327
eta:0.51
new: DataOwner1的最优x_1 = 0.0496
new: DataOwner2的最优x_2 = 0.0362
new: DataOwner3的最优x_3 = 0.0458
new: DataOwner4的最优x_4 = 0.0501
new: DataOwner5的最优x_5 = 0.0583
new: DataOwner6的最优x_6 = 0.0743
new: DataOwner7的最优x_7 = 0.0450
new: DataOwner8的最优x_8 = 0.0600
new: DataOwner9的最优x_9 = 0.0485
new: DataOwner10的最优x_10 = 0.0333
eta:0.52
new: DataOwner1的最优x_1 = 0.0506
new: DataOwner2的最优x_2 = 0.0369
new: DataOwner3的最优x_3 = 0.0467
new: DataOwner4的最优x_4 = 0.0511
new: DataOwner5的最优x_5 = 0.0594
new: DataOwner6的最优x_6 = 0.0758
new: DataOwner7的最优x_7 = 0.0458
new: DataOwner8的最优x_8 = 0.0612
new: DataOwner9的最优x_9 = 0.0495
new: DataOwner10的最优x_10 = 0.0340
eta:0.53
new: DataOwner1的最优x_1 = 0.0516
new: DataOwner2的最优x_2 = 0.0376
new: DataOwner3的最优x_3 = 0.0476
new: DataOwner4的最优x_4 = 0.0521
new: DataOwner5的最优x_5 = 0.0606
new: DataOwner6的最优x_6 = 0.0772
new: DataOwner7的最优x_7 = 0.0467
new: DataOwner8的最优x_8 = 0.0624
new: DataOwner9的最优x_9 = 0.0504
new: DataOwner10的最优x_10 = 0.0347
eta:0.54
new: DataOwner1的最优x_1 = 0.0478
new: DataOwner2的最优x_2 = 0.0349
new: DataOwner3的最优x_3 = 0.0441
new: DataOwner4的最优x_4 = 0.0482
new: DataOwner5的最优x_5 = 0.0561
new: DataOwner6的最优x_6 = 0.0715
new: DataOwner7的最优x_7 = 0.0433
new: DataOwner8的最优x_8 = 0.0578
new: DataOwner9的最优x_9 = 0.0467
new: DataOwner10的最优x_10 = 0.0321
eta:0.55
new: DataOwner1的最优x_1 = 0.0487
new: DataOwner2的最优x_2 = 0.0355
new: DataOwner3的最优x_3 = 0.0449
new: DataOwner4的最优x_4 = 0.0491
new: DataOwner5的最优x_5 = 0.0571
new: DataOwner6的最优x_6 = 0.0728
new: DataOwner7的最优x_7 = 0.0441
new: DataOwner8的最优x_8 = 0.0588
new: DataOwner9的最优x_9 = 0.0476
new: DataOwner10的最优x_10 = 0.0327
eta:0.56
new: DataOwner1的最优x_1 = 0.0545
new: DataOwner2的最优x_2 = 0.0398
new: DataOwner3的最优x_3 = 0.0503
new: DataOwner4的最优x_4 = 0.0550
new: DataOwner5的最优x_5 = 0.0640
new: DataOwner6的最优x_6 = 0.0816
new: DataOwner7的最优x_7 = 0.0494
new: DataOwner8的最优x_8 = 0.0659
new: DataOwner9的最优x_9 = 0.0533
new: DataOwner10的最优x_10 = 0.0366
eta:0.5700000000000001
new: DataOwner1的最优x_1 = 0.0504
new: DataOwner2的最优x_2 = 0.0368
new: DataOwner3的最优x_3 = 0.0466
new: DataOwner4的最优x_4 = 0.0509
new: DataOwner5的最优x_5 = 0.0592
new: DataOwner6的最优x_6 = 0.0755
new: DataOwner7的最优x_7 = 0.0457
new: DataOwner8的最优x_8 = 0.0610
new: DataOwner9的最优x_9 = 0.0493
new: DataOwner10的最优x_10 = 0.0339
eta:0.5800000000000001
new: DataOwner1的最优x_1 = 0.0564
new: DataOwner2的最优x_2 = 0.0412
new: DataOwner3的最优x_3 = 0.0521
new: DataOwner4的最优x_4 = 0.0570
new: DataOwner5的最优x_5 = 0.0663
new: DataOwner6的最优x_6 = 0.0845
new: DataOwner7的最优x_7 = 0.0511
new: DataOwner8的最优x_8 = 0.0682
new: DataOwner9的最优x_9 = 0.0552
new: DataOwner10的最优x_10 = 0.0379
eta:0.59
new: DataOwner1的最优x_1 = 0.0522
new: DataOwner2的最优x_2 = 0.0381
new: DataOwner3的最优x_3 = 0.0482
new: DataOwner4的最优x_4 = 0.0527
new: DataOwner5的最优x_5 = 0.0613
new: DataOwner6的最优x_6 = 0.0781
new: DataOwner7的最优x_7 = 0.0473
new: DataOwner8的最优x_8 = 0.0631
new: DataOwner9的最优x_9 = 0.0510
new: DataOwner10的最优x_10 = 0.0351
eta:0.6
new: DataOwner1的最优x_1 = 0.0531
new: DataOwner2的最优x_2 = 0.0387
new: DataOwner3的最优x_3 = 0.0490
new: DataOwner4的最优x_4 = 0.0536
new: DataOwner5的最优x_5 = 0.0623
new: DataOwner6的最优x_6 = 0.0795
new: DataOwner7的最优x_7 = 0.0481
new: DataOwner8的最优x_8 = 0.0642
new: DataOwner9的最优x_9 = 0.0519
new: DataOwner10的最优x_10 = 0.0357
eta:0.61
new: DataOwner1的最优x_1 = 0.0653
new: DataOwner2的最优x_2 = 0.0477
new: DataOwner3的最优x_3 = 0.0603
new: DataOwner4的最优x_4 = 0.0659
new: DataOwner5的最优x_5 = 0.0767
new: DataOwner6的最优x_6 = 0.0978
new: DataOwner7的最优x_7 = 0.0592
new: DataOwner8的最优x_8 = 0.0789
new: DataOwner9的最优x_9 = 0.0638
new: DataOwner10的最优x_10 = 0.0439
eta:0.62
new: DataOwner1的最优x_1 = 0.0548
new: DataOwner2的最优x_2 = 0.0400
new: DataOwner3的最优x_3 = 0.0506
new: DataOwner4的最优x_4 = 0.0554
new: DataOwner5的最优x_5 = 0.0644
new: DataOwner6的最优x_6 = 0.0821
new: DataOwner7的最优x_7 = 0.0497
new: DataOwner8的最优x_8 = 0.0663
new: DataOwner9的最优x_9 = 0.0536
new: DataOwner10的最优x_10 = 0.0369
eta:0.63
new: DataOwner1的最优x_1 = 0.0557
new: DataOwner2的最优x_2 = 0.0407
new: DataOwner3的最优x_3 = 0.0515
new: DataOwner4的最优x_4 = 0.0563
new: DataOwner5的最优x_5 = 0.0655
new: DataOwner6的最优x_6 = 0.0834
new: DataOwner7的最优x_7 = 0.0505
new: DataOwner8的最优x_8 = 0.0674
new: DataOwner9的最优x_9 = 0.0545
new: DataOwner10的最优x_10 = 0.0375
eta:0.64
new: DataOwner1的最优x_1 = 0.0566
new: DataOwner2的最优x_2 = 0.0413
new: DataOwner3的最优x_3 = 0.0523
new: DataOwner4的最优x_4 = 0.0572
new: DataOwner5的最优x_5 = 0.0665
new: DataOwner6的最优x_6 = 0.0848
new: DataOwner7的最优x_7 = 0.0513
new: DataOwner8的最优x_8 = 0.0685
new: DataOwner9的最优x_9 = 0.0553
new: DataOwner10的最优x_10 = 0.0380
eta:0.65
new: DataOwner1的最优x_1 = 0.0575
new: DataOwner2的最优x_2 = 0.0420
new: DataOwner3的最优x_3 = 0.0531
new: DataOwner4的最优x_4 = 0.0581
new: DataOwner5的最优x_5 = 0.0675
new: DataOwner6的最优x_6 = 0.0861
new: DataOwner7的最优x_7 = 0.0521
new: DataOwner8的最优x_8 = 0.0695
new: DataOwner9的最优x_9 = 0.0562
new: DataOwner10的最优x_10 = 0.0386
eta:0.66
new: DataOwner1的最优x_1 = 0.0584
new: DataOwner2的最优x_2 = 0.0426
new: DataOwner3的最优x_3 = 0.0539
new: DataOwner4的最优x_4 = 0.0590
new: DataOwner5的最优x_5 = 0.0686
new: DataOwner6的最优x_6 = 0.0874
new: DataOwner7的最优x_7 = 0.0529
new: DataOwner8的最优x_8 = 0.0706
new: DataOwner9的最优x_9 = 0.0571
new: DataOwner10的最优x_10 = 0.0392
eta:0.67
new: DataOwner1的最优x_1 = 0.0652
new: DataOwner2的最优x_2 = 0.0476
new: DataOwner3的最优x_3 = 0.0602
new: DataOwner4的最优x_4 = 0.0658
new: DataOwner5的最优x_5 = 0.0766
new: DataOwner6的最优x_6 = 0.0976
new: DataOwner7的最优x_7 = 0.0591
new: DataOwner8的最优x_8 = 0.0788
new: DataOwner9的最优x_9 = 0.0637
new: DataOwner10的最优x_10 = 0.0438
eta:0.68
new: DataOwner1的最优x_1 = 0.0662
new: DataOwner2的最优x_2 = 0.0483
new: DataOwner3的最优x_3 = 0.0611
new: DataOwner4的最优x_4 = 0.0668
new: DataOwner5的最优x_5 = 0.0777
new: DataOwner6的最优x_6 = 0.0991
new: DataOwner7的最优x_7 = 0.0599
new: DataOwner8的最优x_8 = 0.0800
new: DataOwner9的最优x_9 = 0.0647
new: DataOwner10的最优x_10 = 0.0445
eta:0.6900000000000001
new: DataOwner1的最优x_1 = 0.0610
new: DataOwner2的最优x_2 = 0.0446
new: DataOwner3的最优x_3 = 0.0564
new: DataOwner4的最优x_4 = 0.0616
new: DataOwner5的最优x_5 = 0.0717
new: DataOwner6的最优x_6 = 0.0914
new: DataOwner7的最优x_7 = 0.0553
new: DataOwner8的最优x_8 = 0.0738
new: DataOwner9的最优x_9 = 0.0597
new: DataOwner10的最优x_10 = 0.0410
eta:0.7000000000000001
new: DataOwner1的最优x_1 = 0.0619
new: DataOwner2的最优x_2 = 0.0452
new: DataOwner3的最优x_3 = 0.0572
new: DataOwner4的最优x_4 = 0.0625
new: DataOwner5的最优x_5 = 0.0727
new: DataOwner6的最优x_6 = 0.0927
new: DataOwner7的最优x_7 = 0.0561
new: DataOwner8的最优x_8 = 0.0749
new: DataOwner9的最优x_9 = 0.0605
new: DataOwner10的最优x_10 = 0.0416
eta:0.7100000000000001
new: DataOwner1的最优x_1 = 0.0628
new: DataOwner2的最优x_2 = 0.0458
new: DataOwner3的最优x_3 = 0.0580
new: DataOwner4的最优x_4 = 0.0634
new: DataOwner5的最优x_5 = 0.0738
new: DataOwner6的最优x_6 = 0.0940
new: DataOwner7的最优x_7 = 0.0569
new: DataOwner8的最优x_8 = 0.0759
new: DataOwner9的最优x_9 = 0.0614
new: DataOwner10的最优x_10 = 0.0422
eta:0.72
new: DataOwner1的最优x_1 = 0.0637
new: DataOwner2的最优x_2 = 0.0465
new: DataOwner3的最优x_3 = 0.0588
new: DataOwner4的最优x_4 = 0.0643
new: DataOwner5的最优x_5 = 0.0748
new: DataOwner6的最优x_6 = 0.0954
new: DataOwner7的最优x_7 = 0.0577
new: DataOwner8的最优x_8 = 0.0770
new: DataOwner9的最优x_9 = 0.0622
new: DataOwner10的最优x_10 = 0.0428
eta:0.73
new: DataOwner1的最优x_1 = 0.0646
new: DataOwner2的最优x_2 = 0.0471
new: DataOwner3的最优x_3 = 0.0596
new: DataOwner4的最优x_4 = 0.0652
new: DataOwner5的最优x_5 = 0.0758
new: DataOwner6的最优x_6 = 0.0967
new: DataOwner7的最优x_7 = 0.0585
new: DataOwner8的最优x_8 = 0.0781
new: DataOwner9的最优x_9 = 0.0631
new: DataOwner10的最优x_10 = 0.0434
eta:0.74
new: DataOwner1的最优x_1 = 0.0655
new: DataOwner2的最优x_2 = 0.0478
new: DataOwner3的最优x_3 = 0.0604
new: DataOwner4的最优x_4 = 0.0661
new: DataOwner5的最优x_5 = 0.0769
new: DataOwner6的最优x_6 = 0.0980
new: DataOwner7的最优x_7 = 0.0593
new: DataOwner8的最优x_8 = 0.0792
new: DataOwner9的最优x_9 = 0.0640
new: DataOwner10的最优x_10 = 0.0440
eta:0.75
new: DataOwner1的最优x_1 = 0.0663
new: DataOwner2的最优x_2 = 0.0484
new: DataOwner3的最优x_3 = 0.0613
new: DataOwner4的最优x_4 = 0.0670
new: DataOwner5的最优x_5 = 0.0779
new: DataOwner6的最优x_6 = 0.0993
new: DataOwner7的最优x_7 = 0.0601
new: DataOwner8的最优x_8 = 0.0802
new: DataOwner9的最优x_9 = 0.0648
new: DataOwner10的最优x_10 = 0.0446
eta:0.76
new: DataOwner1的最优x_1 = 0.0672
new: DataOwner2的最优x_2 = 0.0491
new: DataOwner3的最优x_3 = 0.0621
new: DataOwner4的最优x_4 = 0.0679
new: DataOwner5的最优x_5 = 0.0790
new: DataOwner6的最优x_6 = 0.1007
new: DataOwner7的最优x_7 = 0.0609
new: DataOwner8的最优x_8 = 0.0813
new: DataOwner9的最优x_9 = 0.0657
new: DataOwner10的最优x_10 = 0.0452
eta:0.77
new: DataOwner1的最优x_1 = 0.0681
new: DataOwner2的最优x_2 = 0.0497
new: DataOwner3的最优x_3 = 0.0629
new: DataOwner4的最优x_4 = 0.0688
new: DataOwner5的最优x_5 = 0.0800
new: DataOwner6的最优x_6 = 0.1020
new: DataOwner7的最优x_7 = 0.0617
new: DataOwner8的最优x_8 = 0.0824
new: DataOwner9的最优x_9 = 0.0666
new: DataOwner10的最优x_10 = 0.0458
eta:0.78
new: DataOwner1的最优x_1 = 0.0690
new: DataOwner2的最优x_2 = 0.0504
new: DataOwner3的最优x_3 = 0.0637
new: DataOwner4的最优x_4 = 0.0697
new: DataOwner5的最优x_5 = 0.0810
new: DataOwner6的最优x_6 = 0.1033
new: DataOwner7的最优x_7 = 0.0625
new: DataOwner8的最优x_8 = 0.0834
new: DataOwner9的最优x_9 = 0.0674
new: DataOwner10的最优x_10 = 0.0464
eta:0.79
new: DataOwner1的最优x_1 = 0.0699
new: DataOwner2的最优x_2 = 0.0510
new: DataOwner3的最优x_3 = 0.0645
new: DataOwner4的最优x_4 = 0.0706
new: DataOwner5的最优x_5 = 0.0821
new: DataOwner6的最优x_6 = 0.1046
new: DataOwner7的最优x_7 = 0.0633
new: DataOwner8的最优x_8 = 0.0845
new: DataOwner9的最优x_9 = 0.0683
new: DataOwner10的最优x_10 = 0.0470
eta:0.8
new: DataOwner1的最优x_1 = 0.0708
new: DataOwner2的最优x_2 = 0.0517
new: DataOwner3的最优x_3 = 0.0653
new: DataOwner4的最优x_4 = 0.0715
new: DataOwner5的最优x_5 = 0.0831
new: DataOwner6的最优x_6 = 0.1060
new: DataOwner7的最优x_7 = 0.0641
new: DataOwner8的最优x_8 = 0.0856
new: DataOwner9的最优x_9 = 0.0692
new: DataOwner10的最优x_10 = 0.0476
eta:0.81
new: DataOwner1的最优x_1 = 0.0717
new: DataOwner2的最优x_2 = 0.0523
new: DataOwner3的最优x_3 = 0.0662
new: DataOwner4的最优x_4 = 0.0724
new: DataOwner5的最优x_5 = 0.0842
new: DataOwner6的最优x_6 = 0.1073
new: DataOwner7的最优x_7 = 0.0649
new: DataOwner8的最优x_8 = 0.0866
new: DataOwner9的最优x_9 = 0.0700
new: DataOwner10的最优x_10 = 0.0482
eta:0.8200000000000001
new: DataOwner1的最优x_1 = 0.0725
new: DataOwner2的最优x_2 = 0.0530
new: DataOwner3的最优x_3 = 0.0670
new: DataOwner4的最优x_4 = 0.0733
new: DataOwner5的最优x_5 = 0.0852
new: DataOwner6的最优x_6 = 0.1086
new: DataOwner7的最优x_7 = 0.0657
new: DataOwner8的最优x_8 = 0.0877
new: DataOwner9的最优x_9 = 0.0709
new: DataOwner10的最优x_10 = 0.0487
eta:0.8300000000000001
new: DataOwner1的最优x_1 = 0.0734
new: DataOwner2的最优x_2 = 0.0536
new: DataOwner3的最优x_3 = 0.0678
new: DataOwner4的最优x_4 = 0.0742
new: DataOwner5的最优x_5 = 0.0862
new: DataOwner6的最优x_6 = 0.1099
new: DataOwner7的最优x_7 = 0.0665
new: DataOwner8的最优x_8 = 0.0888
new: DataOwner9的最优x_9 = 0.0718
new: DataOwner10的最优x_10 = 0.0493
eta:0.8400000000000001
new: DataOwner1的最优x_1 = 0.0743
new: DataOwner2的最优x_2 = 0.0542
new: DataOwner3的最优x_3 = 0.0686
new: DataOwner4的最优x_4 = 0.0750
new: DataOwner5的最优x_5 = 0.0873
new: DataOwner6的最优x_6 = 0.1112
new: DataOwner7的最优x_7 = 0.0673
new: DataOwner8的最优x_8 = 0.0898
new: DataOwner9的最优x_9 = 0.0726
new: DataOwner10的最优x_10 = 0.0499
eta:0.85
new: DataOwner1的最优x_1 = 0.0752
new: DataOwner2的最优x_2 = 0.0549
new: DataOwner3的最优x_3 = 0.0694
new: DataOwner4的最优x_4 = 0.0759
new: DataOwner5的最优x_5 = 0.0883
new: DataOwner6的最优x_6 = 0.1126
new: DataOwner7的最优x_7 = 0.0681
new: DataOwner8的最优x_8 = 0.0909
new: DataOwner9的最优x_9 = 0.0735
new: DataOwner10的最优x_10 = 0.0505
eta:0.86
new: DataOwner1的最优x_1 = 0.0761
new: DataOwner2的最优x_2 = 0.0555
new: DataOwner3的最优x_3 = 0.0702
new: DataOwner4的最优x_4 = 0.0768
new: DataOwner5的最优x_5 = 0.0894
new: DataOwner6的最优x_6 = 0.1139
new: DataOwner7的最优x_7 = 0.0689
new: DataOwner8的最优x_8 = 0.0920
new: DataOwner9的最优x_9 = 0.0744
new: DataOwner10的最优x_10 = 0.0511
eta:0.87
new: DataOwner1的最优x_1 = 0.0770
new: DataOwner2的最优x_2 = 0.0562
new: DataOwner3的最优x_3 = 0.0711
new: DataOwner4的最优x_4 = 0.0777
new: DataOwner5的最优x_5 = 0.0904
new: DataOwner6的最优x_6 = 0.1152
new: DataOwner7的最优x_7 = 0.0697
new: DataOwner8的最优x_8 = 0.0931
new: DataOwner9的最优x_9 = 0.0752
new: DataOwner10的最优x_10 = 0.0517
eta:0.88
new: DataOwner1的最优x_1 = 0.0778
new: DataOwner2的最优x_2 = 0.0568
new: DataOwner3的最优x_3 = 0.0719
new: DataOwner4的最优x_4 = 0.0786
new: DataOwner5的最优x_5 = 0.0914
new: DataOwner6的最优x_6 = 0.1165
new: DataOwner7的最优x_7 = 0.0705
new: DataOwner8的最优x_8 = 0.0941
new: DataOwner9的最优x_9 = 0.0761
new: DataOwner10的最优x_10 = 0.0523
eta:0.89
new: DataOwner1的最优x_1 = 0.0787
new: DataOwner2的最优x_2 = 0.0575
new: DataOwner3的最优x_3 = 0.0727
new: DataOwner4的最优x_4 = 0.0795
new: DataOwner5的最优x_5 = 0.0925
new: DataOwner6的最优x_6 = 0.1179
new: DataOwner7的最优x_7 = 0.0713
new: DataOwner8的最优x_8 = 0.0952
new: DataOwner9的最优x_9 = 0.0769
new: DataOwner10的最优x_10 = 0.0529
eta:0.9
new: DataOwner1的最优x_1 = 0.0796
new: DataOwner2的最优x_2 = 0.0581
new: DataOwner3的最优x_3 = 0.0735
new: DataOwner4的最优x_4 = 0.0804
new: DataOwner5的最优x_5 = 0.0935
new: DataOwner6的最优x_6 = 0.1192
new: DataOwner7的最优x_7 = 0.0721
new: DataOwner8的最优x_8 = 0.0963
new: DataOwner9的最优x_9 = 0.0778
new: DataOwner10的最优x_10 = 0.0535
eta:0.91
new: DataOwner1的最优x_1 = 0.0805
new: DataOwner2的最优x_2 = 0.0588
new: DataOwner3的最优x_3 = 0.0743
new: DataOwner4的最优x_4 = 0.0813
new: DataOwner5的最优x_5 = 0.0946
new: DataOwner6的最优x_6 = 0.1205
new: DataOwner7的最优x_7 = 0.0729
new: DataOwner8的最优x_8 = 0.0973
new: DataOwner9的最优x_9 = 0.0787
new: DataOwner10的最优x_10 = 0.0541
eta:0.92
new: DataOwner1的最优x_1 = 0.0814
new: DataOwner2的最优x_2 = 0.0594
new: DataOwner3的最优x_3 = 0.0751
new: DataOwner4的最优x_4 = 0.0822
new: DataOwner5的最优x_5 = 0.0956
new: DataOwner6的最优x_6 = 0.1218
new: DataOwner7的最优x_7 = 0.0737
new: DataOwner8的最优x_8 = 0.0984
new: DataOwner9的最优x_9 = 0.0795
new: DataOwner10的最优x_10 = 0.0547
eta:0.93
new: DataOwner1的最优x_1 = 0.0823
new: DataOwner2的最优x_2 = 0.0601
new: DataOwner3的最优x_3 = 0.0760
new: DataOwner4的最优x_4 = 0.0831
new: DataOwner5的最优x_5 = 0.0966
new: DataOwner6的最优x_6 = 0.1232
new: DataOwner7的最优x_7 = 0.0745
new: DataOwner8的最优x_8 = 0.0995
new: DataOwner9的最优x_9 = 0.0804
new: DataOwner10的最优x_10 = 0.0553
eta:0.9400000000000001
new: DataOwner1的最优x_1 = 0.0832
new: DataOwner2的最优x_2 = 0.0607
new: DataOwner3的最优x_3 = 0.0768
new: DataOwner4的最优x_4 = 0.0840
new: DataOwner5的最优x_5 = 0.0977
new: DataOwner6的最优x_6 = 0.1245
new: DataOwner7的最优x_7 = 0.0753
new: DataOwner8的最优x_8 = 0.1005
new: DataOwner9的最优x_9 = 0.0813
new: DataOwner10的最优x_10 = 0.0559
eta:0.9500000000000001
new: DataOwner1的最优x_1 = 0.0840
new: DataOwner2的最优x_2 = 0.0613
new: DataOwner3的最优x_3 = 0.0776
new: DataOwner4的最优x_4 = 0.0849
new: DataOwner5的最优x_5 = 0.0987
new: DataOwner6的最优x_6 = 0.1258
new: DataOwner7的最优x_7 = 0.0761
new: DataOwner8的最优x_8 = 0.1016
new: DataOwner9的最优x_9 = 0.0821
new: DataOwner10的最优x_10 = 0.0565
eta:0.9600000000000001
new: DataOwner1的最优x_1 = 0.0849
new: DataOwner2的最优x_2 = 0.0620
new: DataOwner3的最优x_3 = 0.0784
new: DataOwner4的最优x_4 = 0.0858
new: DataOwner5的最优x_5 = 0.0997
new: DataOwner6的最优x_6 = 0.1271
new: DataOwner7的最优x_7 = 0.0769
new: DataOwner8的最优x_8 = 0.1027
new: DataOwner9的最优x_9 = 0.0830
new: DataOwner10的最优x_10 = 0.0571
eta:0.97
new: DataOwner1的最优x_1 = 0.0858
new: DataOwner2的最优x_2 = 0.0626
new: DataOwner3的最优x_3 = 0.0792
new: DataOwner4的最优x_4 = 0.0867
new: DataOwner5的最优x_5 = 0.1008
new: DataOwner6的最优x_6 = 0.1285
new: DataOwner7的最优x_7 = 0.0777
new: DataOwner8的最优x_8 = 0.1038
new: DataOwner9的最优x_9 = 0.0839
new: DataOwner10的最优x_10 = 0.0577
eta:0.98
new: DataOwner1的最优x_1 = 0.0867
new: DataOwner2的最优x_2 = 0.0633
new: DataOwner3的最优x_3 = 0.0800
new: DataOwner4的最优x_4 = 0.0876
new: DataOwner5的最优x_5 = 0.1018
new: DataOwner6的最优x_6 = 0.1298
new: DataOwner7的最优x_7 = 0.0785
new: DataOwner8的最优x_8 = 0.1048
new: DataOwner9的最优x_9 = 0.0847
new: DataOwner10的最优x_10 = 0.0583
eta:0.99
new: DataOwner1的最优x_1 = 0.0876
new: DataOwner2的最优x_2 = 0.0639
new: DataOwner3的最优x_3 = 0.0809
new: DataOwner4的最优x_4 = 0.0884
new: DataOwner5的最优x_5 = 0.1029
new: DataOwner6的最优x_6 = 0.1311
new: DataOwner7的最优x_7 = 0.0793
new: DataOwner8的最优x_8 = 0.1059
new: DataOwner9的最优x_9 = 0.0856
new: DataOwner10的最优x_10 = 0.0589
eta:1.0
new: DataOwner1的最优x_1 = 0.0885
new: DataOwner2的最优x_2 = 0.0646
new: DataOwner3的最优x_3 = 0.0817
new: DataOwner4的最优x_4 = 0.0893
new: DataOwner5的最优x_5 = 0.1039
new: DataOwner6的最优x_6 = 0.1324
new: DataOwner7的最优x_7 = 0.0801
new: DataOwner8的最优x_8 = 0.1070
new: DataOwner9的最优x_9 = 0.0865
new: DataOwner10的最优x_10 = 0.0594
eta:1.01
new: DataOwner1的最优x_1 = 0.0893
new: DataOwner2的最优x_2 = 0.0652
new: DataOwner3的最优x_3 = 0.0825
new: DataOwner4的最优x_4 = 0.0902
new: DataOwner5的最优x_5 = 0.1049
new: DataOwner6的最优x_6 = 0.1338
new: DataOwner7的最优x_7 = 0.0809
new: DataOwner8的最优x_8 = 0.1080
new: DataOwner9的最优x_9 = 0.0873
new: DataOwner10的最优x_10 = 0.0600
eta:1.02
new: DataOwner1的最优x_1 = 0.0902
new: DataOwner2的最优x_2 = 0.0659
new: DataOwner3的最优x_3 = 0.0833
new: DataOwner4的最优x_4 = 0.0911
new: DataOwner5的最优x_5 = 0.1060
new: DataOwner6的最优x_6 = 0.1351
new: DataOwner7的最优x_7 = 0.0817
new: DataOwner8的最优x_8 = 0.1091
new: DataOwner9的最优x_9 = 0.0882
new: DataOwner10的最优x_10 = 0.0606
eta:1.03
new: DataOwner1的最优x_1 = 0.0911
new: DataOwner2的最优x_2 = 0.0665
new: DataOwner3的最优x_3 = 0.0841
new: DataOwner4的最优x_4 = 0.0920
new: DataOwner5的最优x_5 = 0.1070
new: DataOwner6的最优x_6 = 0.1364
new: DataOwner7的最优x_7 = 0.0825
new: DataOwner8的最优x_8 = 0.1102
new: DataOwner9的最优x_9 = 0.0890
new: DataOwner10的最优x_10 = 0.0612
eta:1.04
new: DataOwner1的最优x_1 = 0.0920
new: DataOwner2的最优x_2 = 0.0672
new: DataOwner3的最优x_3 = 0.0849
new: DataOwner4的最优x_4 = 0.0929
new: DataOwner5的最优x_5 = 0.1081
new: DataOwner6的最优x_6 = 0.1377
new: DataOwner7的最优x_7 = 0.0833
new: DataOwner8的最优x_8 = 0.1112
new: DataOwner9的最优x_9 = 0.0899
new: DataOwner10的最优x_10 = 0.0618
eta:1.05
new: DataOwner1的最优x_1 = 0.0929
new: DataOwner2的最优x_2 = 0.0678
new: DataOwner3的最优x_3 = 0.0858
new: DataOwner4的最优x_4 = 0.0938
new: DataOwner5的最优x_5 = 0.1091
new: DataOwner6的最优x_6 = 0.1391
new: DataOwner7的最优x_7 = 0.0841
new: DataOwner8的最优x_8 = 0.1123
new: DataOwner9的最优x_9 = 0.0908
new: DataOwner10的最优x_10 = 0.0624
eta:1.06
new: DataOwner1的最优x_1 = 0.0938
new: DataOwner2的最优x_2 = 0.0685
new: DataOwner3的最优x_3 = 0.0866
new: DataOwner4的最优x_4 = 0.0947
new: DataOwner5的最优x_5 = 0.1101
new: DataOwner6的最优x_6 = 0.1404
new: DataOwner7的最优x_7 = 0.0849
new: DataOwner8的最优x_8 = 0.1134
new: DataOwner9的最优x_9 = 0.0916
new: DataOwner10的最优x_10 = 0.0630
eta:1.07
new: DataOwner1的最优x_1 = 0.0947
new: DataOwner2的最优x_2 = 0.0691
new: DataOwner3的最优x_3 = 0.0874
new: DataOwner4的最优x_4 = 0.0956
new: DataOwner5的最优x_5 = 0.1112
new: DataOwner6的最优x_6 = 0.1417
new: DataOwner7的最优x_7 = 0.0858
new: DataOwner8的最优x_8 = 0.1144
new: DataOwner9的最优x_9 = 0.0925
new: DataOwner10的最优x_10 = 0.0636
eta:1.08
new: DataOwner1的最优x_1 = 0.0955
new: DataOwner2的最优x_2 = 0.0697
new: DataOwner3的最优x_3 = 0.0882
new: DataOwner4的最优x_4 = 0.0965
new: DataOwner5的最优x_5 = 0.1122
new: DataOwner6的最优x_6 = 0.1430
new: DataOwner7的最优x_7 = 0.0866
new: DataOwner8的最优x_8 = 0.1155
new: DataOwner9的最优x_9 = 0.0934
new: DataOwner10的最优x_10 = 0.0642
eta:1.09
new: DataOwner1的最优x_1 = 0.0964
new: DataOwner2的最优x_2 = 0.0704
new: DataOwner3的最优x_3 = 0.0890
new: DataOwner4的最优x_4 = 0.0974
new: DataOwner5的最优x_5 = 0.1133
new: DataOwner6的最优x_6 = 0.1444
new: DataOwner7的最优x_7 = 0.0874
new: DataOwner8的最优x_8 = 0.1166
new: DataOwner9的最优x_9 = 0.0942
new: DataOwner10的最优x_10 = 0.0648
eta:1.1
new: DataOwner1的最优x_1 = 0.0973
new: DataOwner2的最优x_2 = 0.0710
new: DataOwner3的最优x_3 = 0.0898
new: DataOwner4的最优x_4 = 0.0983
new: DataOwner5的最优x_5 = 0.1143
new: DataOwner6的最优x_6 = 0.1457
new: DataOwner7的最优x_7 = 0.0882
new: DataOwner8的最优x_8 = 0.1177
new: DataOwner9的最优x_9 = 0.0951
new: DataOwner10的最优x_10 = 0.0654
eta:1.11
new: DataOwner1的最优x_1 = 0.0982
new: DataOwner2的最优x_2 = 0.0717
new: DataOwner3的最优x_3 = 0.0907
new: DataOwner4的最优x_4 = 0.0992
new: DataOwner5的最优x_5 = 0.1153
new: DataOwner6的最优x_6 = 0.1470
new: DataOwner7的最优x_7 = 0.0890
new: DataOwner8的最优x_8 = 0.1187
new: DataOwner9的最优x_9 = 0.0960
new: DataOwner10的最优x_10 = 0.0660
eta:1.12
new: DataOwner1的最优x_1 = 0.0991
new: DataOwner2的最优x_2 = 0.0723
new: DataOwner3的最优x_3 = 0.0915
new: DataOwner4的最优x_4 = 0.1001
new: DataOwner5的最优x_5 = 0.1164
new: DataOwner6的最优x_6 = 0.1483
new: DataOwner7的最优x_7 = 0.0898
new: DataOwner8的最优x_8 = 0.1198
new: DataOwner9的最优x_9 = 0.0968
new: DataOwner10的最优x_10 = 0.0666
eta:1.1300000000000001
new: DataOwner1的最优x_1 = 0.1000
new: DataOwner2的最优x_2 = 0.0730
new: DataOwner3的最优x_3 = 0.0923
new: DataOwner4的最优x_4 = 0.1010
new: DataOwner5的最优x_5 = 0.1174
new: DataOwner6的最优x_6 = 0.1497
new: DataOwner7的最优x_7 = 0.0906
new: DataOwner8的最优x_8 = 0.1209
new: DataOwner9的最优x_9 = 0.0977
new: DataOwner10的最优x_10 = 0.0672
eta:1.1400000000000001
new: DataOwner1的最优x_1 = 0.1008
new: DataOwner2的最优x_2 = 0.0736
new: DataOwner3的最优x_3 = 0.0931
new: DataOwner4的最优x_4 = 0.1018
new: DataOwner5的最优x_5 = 0.1184
new: DataOwner6的最优x_6 = 0.1510
new: DataOwner7的最优x_7 = 0.0914
new: DataOwner8的最优x_8 = 0.1219
new: DataOwner9的最优x_9 = 0.0986
new: DataOwner10的最优x_10 = 0.0678
eta:1.1500000000000001
new: DataOwner1的最优x_1 = 0.1017
new: DataOwner2的最优x_2 = 0.0743
new: DataOwner3的最优x_3 = 0.0939
new: DataOwner4的最优x_4 = 0.1027
new: DataOwner5的最优x_5 = 0.1195
new: DataOwner6的最优x_6 = 0.1523
new: DataOwner7的最优x_7 = 0.0922
new: DataOwner8的最优x_8 = 0.1230
new: DataOwner9的最优x_9 = 0.0994
new: DataOwner10的最优x_10 = 0.0684
eta:1.1600000000000001
new: DataOwner1的最优x_1 = 0.1026
new: DataOwner2的最优x_2 = 0.0749
new: DataOwner3的最优x_3 = 0.0947
new: DataOwner4的最优x_4 = 0.1036
new: DataOwner5的最优x_5 = 0.1205
new: DataOwner6的最优x_6 = 0.1536
new: DataOwner7的最优x_7 = 0.0930
new: DataOwner8的最优x_8 = 0.1241
new: DataOwner9的最优x_9 = 0.1003
new: DataOwner10的最优x_10 = 0.0690
eta:1.17
new: DataOwner1的最优x_1 = 0.1035
new: DataOwner2的最优x_2 = 0.0756
new: DataOwner3的最优x_3 = 0.0956
new: DataOwner4的最优x_4 = 0.1045
new: DataOwner5的最优x_5 = 0.1216
new: DataOwner6的最优x_6 = 0.1550
new: DataOwner7的最优x_7 = 0.0938
new: DataOwner8的最优x_8 = 0.1251
new: DataOwner9的最优x_9 = 0.1012
new: DataOwner10的最优x_10 = 0.0696
eta:1.18
new: DataOwner1的最优x_1 = 0.1044
new: DataOwner2的最优x_2 = 0.0762
new: DataOwner3的最优x_3 = 0.0964
new: DataOwner4的最优x_4 = 0.1054
new: DataOwner5的最优x_5 = 0.1226
new: DataOwner6的最优x_6 = 0.1563
new: DataOwner7的最优x_7 = 0.0946
new: DataOwner8的最优x_8 = 0.1262
new: DataOwner9的最优x_9 = 0.1020
new: DataOwner10的最优x_10 = 0.0701
eta:1.19
new: DataOwner1的最优x_1 = 0.1053
new: DataOwner2的最优x_2 = 0.0768
new: DataOwner3的最优x_3 = 0.0972
new: DataOwner4的最优x_4 = 0.1063
new: DataOwner5的最优x_5 = 0.1236
new: DataOwner6的最优x_6 = 0.1576
new: DataOwner7的最优x_7 = 0.0954
new: DataOwner8的最优x_8 = 0.1273
new: DataOwner9的最优x_9 = 0.1029
new: DataOwner10的最优x_10 = 0.0707
eta:1.2
new: DataOwner1的最优x_1 = 0.1062
new: DataOwner2的最优x_2 = 0.0775
new: DataOwner3的最优x_3 = 0.0980
new: DataOwner4的最优x_4 = 0.1072
new: DataOwner5的最优x_5 = 0.1247
new: DataOwner6的最优x_6 = 0.1589
new: DataOwner7的最优x_7 = 0.0962
new: DataOwner8的最优x_8 = 0.1284
new: DataOwner9的最优x_9 = 0.1037
new: DataOwner10的最优x_10 = 0.0713
eta:1.21
new: DataOwner1的最优x_1 = 0.1070
new: DataOwner2的最优x_2 = 0.0781
new: DataOwner3的最优x_3 = 0.0988
new: DataOwner4的最优x_4 = 0.1081
new: DataOwner5的最优x_5 = 0.1257
new: DataOwner6的最优x_6 = 0.1603
new: DataOwner7的最优x_7 = 0.0970
new: DataOwner8的最优x_8 = 0.1294
new: DataOwner9的最优x_9 = 0.1046
new: DataOwner10的最优x_10 = 0.0719
eta:1.22
new: DataOwner1的最优x_1 = 0.1079
new: DataOwner2的最优x_2 = 0.0788
new: DataOwner3的最优x_3 = 0.0996
new: DataOwner4的最优x_4 = 0.1090
new: DataOwner5的最优x_5 = 0.1268
new: DataOwner6的最优x_6 = 0.1616
new: DataOwner7的最优x_7 = 0.0978
new: DataOwner8的最优x_8 = 0.1305
new: DataOwner9的最优x_9 = 0.1055
new: DataOwner10的最优x_10 = 0.0725
eta:1.23
new: DataOwner1的最优x_1 = 0.1088
new: DataOwner2的最优x_2 = 0.0794
new: DataOwner3的最优x_3 = 0.1005
new: DataOwner4的最优x_4 = 0.1099
new: DataOwner5的最优x_5 = 0.1278
new: DataOwner6的最优x_6 = 0.1629
new: DataOwner7的最优x_7 = 0.0986
new: DataOwner8的最优x_8 = 0.1316
new: DataOwner9的最优x_9 = 0.1063
new: DataOwner10的最优x_10 = 0.0731
eta:1.24
new: DataOwner1的最优x_1 = 0.1097
new: DataOwner2的最优x_2 = 0.0801
new: DataOwner3的最优x_3 = 0.1013
new: DataOwner4的最优x_4 = 0.1108
new: DataOwner5的最优x_5 = 0.1288
new: DataOwner6的最优x_6 = 0.1642
new: DataOwner7的最优x_7 = 0.0994
new: DataOwner8的最优x_8 = 0.1326
new: DataOwner9的最优x_9 = 0.1072
new: DataOwner10的最优x_10 = 0.0737
eta:1.25
new: DataOwner1的最优x_1 = 0.1106
new: DataOwner2的最优x_2 = 0.0807
new: DataOwner3的最优x_3 = 0.1021
new: DataOwner4的最优x_4 = 0.1117
new: DataOwner5的最优x_5 = 0.1299
new: DataOwner6的最优x_6 = 0.1655
new: DataOwner7的最优x_7 = 0.1002
new: DataOwner8的最优x_8 = 0.1337
new: DataOwner9的最优x_9 = 0.1081
new: DataOwner10的最优x_10 = 0.0743
eta:1.26
new: DataOwner1的最优x_1 = 0.1115
new: DataOwner2的最优x_2 = 0.0814
new: DataOwner3的最优x_3 = 0.1029
new: DataOwner4的最优x_4 = 0.1126
new: DataOwner5的最优x_5 = 0.1309
new: DataOwner6的最优x_6 = 0.1669
new: DataOwner7的最优x_7 = 0.1010
new: DataOwner8的最优x_8 = 0.1348
new: DataOwner9的最优x_9 = 0.1089
new: DataOwner10的最优x_10 = 0.0749
eta:1.27
new: DataOwner1的最优x_1 = 0.1123
new: DataOwner2的最优x_2 = 0.0820
new: DataOwner3的最优x_3 = 0.1037
new: DataOwner4的最优x_4 = 0.1135
new: DataOwner5的最优x_5 = 0.1320
new: DataOwner6的最优x_6 = 0.1682
new: DataOwner7的最优x_7 = 0.1018
new: DataOwner8的最优x_8 = 0.1358
new: DataOwner9的最优x_9 = 0.1098
new: DataOwner10的最优x_10 = 0.0755
eta:1.28
new: DataOwner1的最优x_1 = 0.1132
new: DataOwner2的最优x_2 = 0.0827
new: DataOwner3的最优x_3 = 0.1045
new: DataOwner4的最优x_4 = 0.1144
new: DataOwner5的最优x_5 = 0.1330
new: DataOwner6的最优x_6 = 0.1695
new: DataOwner7的最优x_7 = 0.1026
new: DataOwner8的最优x_8 = 0.1369
new: DataOwner9的最优x_9 = 0.1107
new: DataOwner10的最优x_10 = 0.0761
eta:1.29
new: DataOwner1的最优x_1 = 0.1141
new: DataOwner2的最优x_2 = 0.0833
new: DataOwner3的最优x_3 = 0.1054
new: DataOwner4的最优x_4 = 0.1152
new: DataOwner5的最优x_5 = 0.1340
new: DataOwner6的最优x_6 = 0.1708
new: DataOwner7的最优x_7 = 0.1034
new: DataOwner8的最优x_8 = 0.1380
new: DataOwner9的最优x_9 = 0.1115
new: DataOwner10的最优x_10 = 0.0767
eta:1.3
new: DataOwner1的最优x_1 = 0.1150
new: DataOwner2的最优x_2 = 0.0840
new: DataOwner3的最优x_3 = 0.1062
new: DataOwner4的最优x_4 = 0.1161
new: DataOwner5的最优x_5 = 0.1351
new: DataOwner6的最优x_6 = 0.1722
new: DataOwner7的最优x_7 = 0.1042
new: DataOwner8的最优x_8 = 0.1391
new: DataOwner9的最优x_9 = 0.1124
new: DataOwner10的最优x_10 = 0.0773
eta:1.31
new: DataOwner1的最优x_1 = 0.1159
new: DataOwner2的最优x_2 = 0.0846
new: DataOwner3的最优x_3 = 0.1070
new: DataOwner4的最优x_4 = 0.1170
new: DataOwner5的最优x_5 = 0.1361
new: DataOwner6的最优x_6 = 0.1735
new: DataOwner7的最优x_7 = 0.1050
new: DataOwner8的最优x_8 = 0.1401
new: DataOwner9的最优x_9 = 0.1133
new: DataOwner10的最优x_10 = 0.0779
eta:1.32
new: DataOwner1的最优x_1 = 0.1168
new: DataOwner2的最优x_2 = 0.0852
new: DataOwner3的最优x_3 = 0.1078
new: DataOwner4的最优x_4 = 0.1179
new: DataOwner5的最优x_5 = 0.1372
new: DataOwner6的最优x_6 = 0.1748
new: DataOwner7的最优x_7 = 0.1058
new: DataOwner8的最优x_8 = 0.1412
new: DataOwner9的最优x_9 = 0.1141
new: DataOwner10的最优x_10 = 0.0785
eta:1.33
new: DataOwner1的最优x_1 = 0.1177
new: DataOwner2的最优x_2 = 0.0859
new: DataOwner3的最优x_3 = 0.1086
new: DataOwner4的最优x_4 = 0.1188
new: DataOwner5的最优x_5 = 0.1382
new: DataOwner6的最优x_6 = 0.1761
new: DataOwner7的最优x_7 = 0.1066
new: DataOwner8的最优x_8 = 0.1423
new: DataOwner9的最优x_9 = 0.1150
new: DataOwner10的最优x_10 = 0.0791
eta:1.34
new: DataOwner1的最优x_1 = 0.1185
new: DataOwner2的最优x_2 = 0.0865
new: DataOwner3的最优x_3 = 0.1094
new: DataOwner4的最优x_4 = 0.1197
new: DataOwner5的最优x_5 = 0.1392
new: DataOwner6的最优x_6 = 0.1775
new: DataOwner7的最优x_7 = 0.1074
new: DataOwner8的最优x_8 = 0.1433
new: DataOwner9的最优x_9 = 0.1159
new: DataOwner10的最优x_10 = 0.0797
eta:1.35
new: DataOwner1的最优x_1 = 0.1194
new: DataOwner2的最优x_2 = 0.0872
new: DataOwner3的最优x_3 = 0.1103
new: DataOwner4的最优x_4 = 0.1206
new: DataOwner5的最优x_5 = 0.1403
new: DataOwner6的最优x_6 = 0.1788
new: DataOwner7的最优x_7 = 0.1082
new: DataOwner8的最优x_8 = 0.1444
new: DataOwner9的最优x_9 = 0.1167
new: DataOwner10的最优x_10 = 0.0803
eta:1.36
new: DataOwner1的最优x_1 = 0.1203
new: DataOwner2的最优x_2 = 0.0878
new: DataOwner3的最优x_3 = 0.1111
new: DataOwner4的最优x_4 = 0.1215
new: DataOwner5的最优x_5 = 0.1413
new: DataOwner6的最优x_6 = 0.1801
new: DataOwner7的最优x_7 = 0.1090
new: DataOwner8的最优x_8 = 0.1455
new: DataOwner9的最优x_9 = 0.1176
new: DataOwner10的最优x_10 = 0.0808
eta:1.37
new: DataOwner1的最优x_1 = 0.1212
new: DataOwner2的最优x_2 = 0.0885
new: DataOwner3的最优x_3 = 0.1119
new: DataOwner4的最优x_4 = 0.1224
new: DataOwner5的最优x_5 = 0.1423
new: DataOwner6的最优x_6 = 0.1814
new: DataOwner7的最优x_7 = 0.1098
new: DataOwner8的最优x_8 = 0.1465
new: DataOwner9的最优x_9 = 0.1184
new: DataOwner10的最优x_10 = 0.0814
eta:1.3800000000000001
new: DataOwner1的最优x_1 = 0.1221
new: DataOwner2的最优x_2 = 0.0891
new: DataOwner3的最优x_3 = 0.1127
new: DataOwner4的最优x_4 = 0.1233
new: DataOwner5的最优x_5 = 0.1434
new: DataOwner6的最优x_6 = 0.1828
new: DataOwner7的最优x_7 = 0.1106
new: DataOwner8的最优x_8 = 0.1476
new: DataOwner9的最优x_9 = 0.1193
new: DataOwner10的最优x_10 = 0.0820
eta:1.3900000000000001
new: DataOwner1的最优x_1 = 0.1230
new: DataOwner2的最优x_2 = 0.0898
new: DataOwner3的最优x_3 = 0.1135
new: DataOwner4的最优x_4 = 0.1242
new: DataOwner5的最优x_5 = 0.1444
new: DataOwner6的最优x_6 = 0.1841
new: DataOwner7的最优x_7 = 0.1114
new: DataOwner8的最优x_8 = 0.1487
new: DataOwner9的最优x_9 = 0.1202
new: DataOwner10的最优x_10 = 0.0826
eta:1.4000000000000001
new: DataOwner1的最优x_1 = 0.1238
new: DataOwner2的最优x_2 = 0.0904
new: DataOwner3的最优x_3 = 0.1143
new: DataOwner4的最优x_4 = 0.1251
new: DataOwner5的最优x_5 = 0.1455
new: DataOwner6的最优x_6 = 0.1854
new: DataOwner7的最优x_7 = 0.1122
new: DataOwner8的最优x_8 = 0.1497
new: DataOwner9的最优x_9 = 0.1210
new: DataOwner10的最优x_10 = 0.0832
eta:1.4100000000000001
new: DataOwner1的最优x_1 = 0.1247
new: DataOwner2的最优x_2 = 0.0911
new: DataOwner3的最优x_3 = 0.1152
new: DataOwner4的最优x_4 = 0.1260
new: DataOwner5的最优x_5 = 0.1465
new: DataOwner6的最优x_6 = 0.1867
new: DataOwner7的最优x_7 = 0.1130
new: DataOwner8的最优x_8 = 0.1508
new: DataOwner9的最优x_9 = 0.1219
new: DataOwner10的最优x_10 = 0.0838
eta:1.42
new: DataOwner1的最优x_1 = 0.1256
new: DataOwner2的最优x_2 = 0.0917
new: DataOwner3的最优x_3 = 0.1160
new: DataOwner4的最优x_4 = 0.1269
new: DataOwner5的最优x_5 = 0.1475
new: DataOwner6的最优x_6 = 0.1881
new: DataOwner7的最优x_7 = 0.1138
new: DataOwner8的最优x_8 = 0.1519
new: DataOwner9的最优x_9 = 0.1228
new: DataOwner10的最优x_10 = 0.0844
eta:1.43
new: DataOwner1的最优x_1 = 0.1265
new: DataOwner2的最优x_2 = 0.0923
new: DataOwner3的最优x_3 = 0.1168
new: DataOwner4的最优x_4 = 0.1278
new: DataOwner5的最优x_5 = 0.1486
new: DataOwner6的最优x_6 = 0.1894
new: DataOwner7的最优x_7 = 0.1146
new: DataOwner8的最优x_8 = 0.1530
new: DataOwner9的最优x_9 = 0.1236
new: DataOwner10的最优x_10 = 0.0850
eta:1.44
new: DataOwner1的最优x_1 = 0.1274
new: DataOwner2的最优x_2 = 0.0930
new: DataOwner3的最优x_3 = 0.1176
new: DataOwner4的最优x_4 = 0.1286
new: DataOwner5的最优x_5 = 0.1496
new: DataOwner6的最优x_6 = 0.1907
new: DataOwner7的最优x_7 = 0.1154
new: DataOwner8的最优x_8 = 0.1540
new: DataOwner9的最优x_9 = 0.1245
new: DataOwner10的最优x_10 = 0.0856
eta:1.45
new: DataOwner1的最优x_1 = 0.1283
new: DataOwner2的最优x_2 = 0.0936
new: DataOwner3的最优x_3 = 0.1184
new: DataOwner4的最优x_4 = 0.1295
new: DataOwner5的最优x_5 = 0.1507
new: DataOwner6的最优x_6 = 0.1920
new: DataOwner7的最优x_7 = 0.1162
new: DataOwner8的最优x_8 = 0.1551
new: DataOwner9的最优x_9 = 0.1254
new: DataOwner10的最优x_10 = 0.0862
eta:1.46
new: DataOwner1的最优x_1 = 0.1292
new: DataOwner2的最优x_2 = 0.0943
new: DataOwner3的最优x_3 = 0.1192
new: DataOwner4的最优x_4 = 0.1304
new: DataOwner5的最优x_5 = 0.1517
new: DataOwner6的最优x_6 = 0.1934
new: DataOwner7的最优x_7 = 0.1170
new: DataOwner8的最优x_8 = 0.1562
new: DataOwner9的最优x_9 = 0.1262
new: DataOwner10的最优x_10 = 0.0868
eta:1.47
new: DataOwner1的最优x_1 = 0.1300
new: DataOwner2的最优x_2 = 0.0949
new: DataOwner3的最优x_3 = 0.1201
new: DataOwner4的最优x_4 = 0.1313
new: DataOwner5的最优x_5 = 0.1527
new: DataOwner6的最优x_6 = 0.1947
new: DataOwner7的最优x_7 = 0.1178
new: DataOwner8的最优x_8 = 0.1572
new: DataOwner9的最优x_9 = 0.1271
new: DataOwner10的最优x_10 = 0.0874
eta:1.48
new: DataOwner1的最优x_1 = 0.1309
new: DataOwner2的最优x_2 = 0.0956
new: DataOwner3的最优x_3 = 0.1209
new: DataOwner4的最优x_4 = 0.1322
new: DataOwner5的最优x_5 = 0.1538
new: DataOwner6的最优x_6 = 0.1960
new: DataOwner7的最优x_7 = 0.1186
new: DataOwner8的最优x_8 = 0.1583
new: DataOwner9的最优x_9 = 0.1280
new: DataOwner10的最优x_10 = 0.0880
eta:1.49
new: DataOwner1的最优x_1 = 0.1318
new: DataOwner2的最优x_2 = 0.0962
new: DataOwner3的最优x_3 = 0.1217
new: DataOwner4的最优x_4 = 0.1331
new: DataOwner5的最优x_5 = 0.1548
new: DataOwner6的最优x_6 = 0.1973
new: DataOwner7的最优x_7 = 0.1194
new: DataOwner8的最优x_8 = 0.1594
new: DataOwner9的最优x_9 = 0.1288
new: DataOwner10的最优x_10 = 0.0886
eta:1.5
new: DataOwner1的最优x_1 = 0.1327
new: DataOwner2的最优x_2 = 0.0969
new: DataOwner3的最优x_3 = 0.1225
new: DataOwner4的最优x_4 = 0.1340
new: DataOwner5的最优x_5 = 0.1559
new: DataOwner6的最优x_6 = 0.1987
new: DataOwner7的最优x_7 = 0.1202
new: DataOwner8的最优x_8 = 0.1604
new: DataOwner9的最优x_9 = 0.1297
new: DataOwner10的最优x_10 = 0.0892
eta:1.51
new: DataOwner1的最优x_1 = 0.1336
new: DataOwner2的最优x_2 = 0.0975
new: DataOwner3的最优x_3 = 0.1233
new: DataOwner4的最优x_4 = 0.1349
new: DataOwner5的最优x_5 = 0.1569
new: DataOwner6的最优x_6 = 0.2000
new: DataOwner7的最优x_7 = 0.1210
new: DataOwner8的最优x_8 = 0.1615
new: DataOwner9的最优x_9 = 0.1305
new: DataOwner10的最优x_10 = 0.0898
eta:1.52
new: DataOwner1的最优x_1 = 0.1345
new: DataOwner2的最优x_2 = 0.0982
new: DataOwner3的最优x_3 = 0.1241
new: DataOwner4的最优x_4 = 0.1358
new: DataOwner5的最优x_5 = 0.1579
new: DataOwner6的最优x_6 = 0.2013
new: DataOwner7的最优x_7 = 0.1218
new: DataOwner8的最优x_8 = 0.1626
new: DataOwner9的最优x_9 = 0.1314
new: DataOwner10的最优x_10 = 0.0904
eta:1.53
new: DataOwner1的最优x_1 = 0.1353
new: DataOwner2的最优x_2 = 0.0988
new: DataOwner3的最优x_3 = 0.1250
new: DataOwner4的最优x_4 = 0.1367
new: DataOwner5的最优x_5 = 0.1590
new: DataOwner6的最优x_6 = 0.2026
new: DataOwner7的最优x_7 = 0.1226
new: DataOwner8的最优x_8 = 0.1637
new: DataOwner9的最优x_9 = 0.1323
new: DataOwner10的最优x_10 = 0.0910
eta:1.54
new: DataOwner1的最优x_1 = 0.1362
new: DataOwner2的最优x_2 = 0.0994
new: DataOwner3的最优x_3 = 0.1258
new: DataOwner4的最优x_4 = 0.1376
new: DataOwner5的最优x_5 = 0.1600
new: DataOwner6的最优x_6 = 0.2040
new: DataOwner7的最优x_7 = 0.1234
new: DataOwner8的最优x_8 = 0.1647
new: DataOwner9的最优x_9 = 0.1331
new: DataOwner10的最优x_10 = 0.0915
eta:1.55
new: DataOwner1的最优x_1 = 0.1371
new: DataOwner2的最优x_2 = 0.1001
new: DataOwner3的最优x_3 = 0.1266
new: DataOwner4的最优x_4 = 0.1385
new: DataOwner5的最优x_5 = 0.1610
new: DataOwner6的最优x_6 = 0.2053
new: DataOwner7的最优x_7 = 0.1242
new: DataOwner8的最优x_8 = 0.1658
new: DataOwner9的最优x_9 = 0.1340
new: DataOwner10的最优x_10 = 0.0921
eta:1.56
new: DataOwner1的最优x_1 = 0.1380
new: DataOwner2的最优x_2 = 0.1007
new: DataOwner3的最优x_3 = 0.1274
new: DataOwner4的最优x_4 = 0.1394
new: DataOwner5的最优x_5 = 0.1621
new: DataOwner6的最优x_6 = 0.2066
new: DataOwner7的最优x_7 = 0.1250
new: DataOwner8的最优x_8 = 0.1669
new: DataOwner9的最优x_9 = 0.1349
new: DataOwner10的最优x_10 = 0.0927
eta:1.57
new: DataOwner1的最优x_1 = 0.1389
new: DataOwner2的最优x_2 = 0.1014
new: DataOwner3的最优x_3 = 0.1282
new: DataOwner4的最优x_4 = 0.1403
new: DataOwner5的最优x_5 = 0.1631
new: DataOwner6的最优x_6 = 0.2079
new: DataOwner7的最优x_7 = 0.1258
new: DataOwner8的最优x_8 = 0.1679
new: DataOwner9的最优x_9 = 0.1357
new: DataOwner10的最优x_10 = 0.0933
eta:1.58
new: DataOwner1的最优x_1 = 0.1398
new: DataOwner2的最优x_2 = 0.1020
new: DataOwner3的最优x_3 = 0.1290
new: DataOwner4的最优x_4 = 0.1412
new: DataOwner5的最优x_5 = 0.1642
new: DataOwner6的最优x_6 = 0.2093
new: DataOwner7的最优x_7 = 0.1266
new: DataOwner8的最优x_8 = 0.1690
new: DataOwner9的最优x_9 = 0.1366
new: DataOwner10的最优x_10 = 0.0939
eta:1.59
new: DataOwner1的最优x_1 = 0.1407
new: DataOwner2的最优x_2 = 0.1027
new: DataOwner3的最优x_3 = 0.1299
new: DataOwner4的最优x_4 = 0.1421
new: DataOwner5的最优x_5 = 0.1652
new: DataOwner6的最优x_6 = 0.2106
new: DataOwner7的最优x_7 = 0.1274
new: DataOwner8的最优x_8 = 0.1701
new: DataOwner9的最优x_9 = 0.1375
new: DataOwner10的最优x_10 = 0.0945
eta:1.6
new: DataOwner1的最优x_1 = 0.1415
new: DataOwner2的最优x_2 = 0.1033
new: DataOwner3的最优x_3 = 0.1307
new: DataOwner4的最优x_4 = 0.1429
new: DataOwner5的最优x_5 = 0.1662
new: DataOwner6的最优x_6 = 0.2119
new: DataOwner7的最优x_7 = 0.1282
new: DataOwner8的最优x_8 = 0.1711
new: DataOwner9的最优x_9 = 0.1383
new: DataOwner10的最优x_10 = 0.0951
eta:1.61
new: DataOwner1的最优x_1 = 0.1424
new: DataOwner2的最优x_2 = 0.1040
new: DataOwner3的最优x_3 = 0.1315
new: DataOwner4的最优x_4 = 0.1438
new: DataOwner5的最优x_5 = 0.1673
new: DataOwner6的最优x_6 = 0.2132
new: DataOwner7的最优x_7 = 0.1290
new: DataOwner8的最优x_8 = 0.1722
new: DataOwner9的最优x_9 = 0.1392
new: DataOwner10的最优x_10 = 0.0957
eta:1.62
new: DataOwner1的最优x_1 = 0.1433
new: DataOwner2的最优x_2 = 0.1046
new: DataOwner3的最优x_3 = 0.1323
new: DataOwner4的最优x_4 = 0.1447
new: DataOwner5的最优x_5 = 0.1683
new: DataOwner6的最优x_6 = 0.2146
new: DataOwner7的最优x_7 = 0.1298
new: DataOwner8的最优x_8 = 0.1733
new: DataOwner9的最优x_9 = 0.1401
new: DataOwner10的最优x_10 = 0.0963
eta:1.6300000000000001
new: DataOwner1的最优x_1 = 0.1442
new: DataOwner2的最优x_2 = 0.1053
new: DataOwner3的最优x_3 = 0.1331
new: DataOwner4的最优x_4 = 0.1456
new: DataOwner5的最优x_5 = 0.1694
new: DataOwner6的最优x_6 = 0.2159
new: DataOwner7的最优x_7 = 0.1306
new: DataOwner8的最优x_8 = 0.1743
new: DataOwner9的最优x_9 = 0.1409
new: DataOwner10的最优x_10 = 0.0969
eta:1.6400000000000001
new: DataOwner1的最优x_1 = 0.1451
new: DataOwner2的最优x_2 = 0.1059
new: DataOwner3的最优x_3 = 0.1339
new: DataOwner4的最优x_4 = 0.1465
new: DataOwner5的最优x_5 = 0.1704
new: DataOwner6的最优x_6 = 0.2172
new: DataOwner7的最优x_7 = 0.1314
new: DataOwner8的最优x_8 = 0.1754
new: DataOwner9的最优x_9 = 0.1418
new: DataOwner10的最优x_10 = 0.0975
eta:1.6500000000000001
new: DataOwner1的最优x_1 = 0.1460
new: DataOwner2的最优x_2 = 0.1066
new: DataOwner3的最优x_3 = 0.1348
new: DataOwner4的最优x_4 = 0.1474
new: DataOwner5的最优x_5 = 0.1714
new: DataOwner6的最优x_6 = 0.2185
new: DataOwner7的最优x_7 = 0.1322
new: DataOwner8的最优x_8 = 0.1765
new: DataOwner9的最优x_9 = 0.1427
new: DataOwner10的最优x_10 = 0.0981
eta:1.6600000000000001
new: DataOwner1的最优x_1 = 0.1468
new: DataOwner2的最优x_2 = 0.1072
new: DataOwner3的最优x_3 = 0.1356
new: DataOwner4的最优x_4 = 0.1483
new: DataOwner5的最优x_5 = 0.1725
new: DataOwner6的最优x_6 = 0.2198
new: DataOwner7的最优x_7 = 0.1330
new: DataOwner8的最优x_8 = 0.1776
new: DataOwner9的最优x_9 = 0.1435
new: DataOwner10的最优x_10 = 0.0987
eta:1.6700000000000002
new: DataOwner1的最优x_1 = 0.1477
new: DataOwner2的最优x_2 = 0.1078
new: DataOwner3的最优x_3 = 0.1364
new: DataOwner4的最优x_4 = 0.1492
new: DataOwner5的最优x_5 = 0.1735
new: DataOwner6的最优x_6 = 0.2212
new: DataOwner7的最优x_7 = 0.1338
new: DataOwner8的最优x_8 = 0.1786
new: DataOwner9的最优x_9 = 0.1444
new: DataOwner10的最优x_10 = 0.0993
eta:1.68
new: DataOwner1的最优x_1 = 0.1486
new: DataOwner2的最优x_2 = 0.1085
new: DataOwner3的最优x_3 = 0.1372
new: DataOwner4的最优x_4 = 0.1501
new: DataOwner5的最优x_5 = 0.1746
new: DataOwner6的最优x_6 = 0.2225
new: DataOwner7的最优x_7 = 0.1346
new: DataOwner8的最优x_8 = 0.1797
new: DataOwner9的最优x_9 = 0.1452
new: DataOwner10的最优x_10 = 0.0999
eta:1.69
new: DataOwner1的最优x_1 = 0.1495
new: DataOwner2的最优x_2 = 0.1091
new: DataOwner3的最优x_3 = 0.1380
new: DataOwner4的最优x_4 = 0.1510
new: DataOwner5的最优x_5 = 0.1756
new: DataOwner6的最优x_6 = 0.2238
new: DataOwner7的最优x_7 = 0.1354
new: DataOwner8的最优x_8 = 0.1808
new: DataOwner9的最优x_9 = 0.1461
new: DataOwner10的最优x_10 = 0.1005
eta:1.7
new: DataOwner1的最优x_1 = 0.1504
new: DataOwner2的最优x_2 = 0.1098
new: DataOwner3的最优x_3 = 0.1388
new: DataOwner4的最优x_4 = 0.1519
new: DataOwner5的最优x_5 = 0.1766
new: DataOwner6的最优x_6 = 0.2251
new: DataOwner7的最优x_7 = 0.1362
new: DataOwner8的最优x_8 = 0.1818
new: DataOwner9的最优x_9 = 0.1470
new: DataOwner10的最优x_10 = 0.1011
eta:1.71
new: DataOwner1的最优x_1 = 0.1513
new: DataOwner2的最优x_2 = 0.1104
new: DataOwner3的最优x_3 = 0.1397
new: DataOwner4的最优x_4 = 0.1528
new: DataOwner5的最优x_5 = 0.1777
new: DataOwner6的最优x_6 = 0.2265
new: DataOwner7的最优x_7 = 0.1370
new: DataOwner8的最优x_8 = 0.1829
new: DataOwner9的最优x_9 = 0.1478
new: DataOwner10的最优x_10 = 0.1017
eta:1.72
new: DataOwner1的最优x_1 = 0.1522
new: DataOwner2的最优x_2 = 0.1111
new: DataOwner3的最优x_3 = 0.1405
new: DataOwner4的最优x_4 = 0.1537
new: DataOwner5的最优x_5 = 0.1787
new: DataOwner6的最优x_6 = 0.2278
new: DataOwner7的最优x_7 = 0.1378
new: DataOwner8的最优x_8 = 0.1840
new: DataOwner9的最优x_9 = 0.1487
new: DataOwner10的最优x_10 = 0.1022
eta:1.73
new: DataOwner1的最优x_1 = 0.1530
new: DataOwner2的最优x_2 = 0.1117
new: DataOwner3的最优x_3 = 0.1413
new: DataOwner4的最优x_4 = 0.1546
new: DataOwner5的最优x_5 = 0.1798
new: DataOwner6的最优x_6 = 0.2291
new: DataOwner7的最优x_7 = 0.1386
new: DataOwner8的最优x_8 = 0.1850
new: DataOwner9的最优x_9 = 0.1496
new: DataOwner10的最优x_10 = 0.1028
eta:1.74
new: DataOwner1的最优x_1 = 0.1539
new: DataOwner2的最优x_2 = 0.1124
new: DataOwner3的最优x_3 = 0.1421
new: DataOwner4的最优x_4 = 0.1555
new: DataOwner5的最优x_5 = 0.1808
new: DataOwner6的最优x_6 = 0.2304
new: DataOwner7的最优x_7 = 0.1394
new: DataOwner8的最优x_8 = 0.1861
new: DataOwner9的最优x_9 = 0.1504
new: DataOwner10的最优x_10 = 0.1034
eta:1.75
new: DataOwner1的最优x_1 = 0.1548
new: DataOwner2的最优x_2 = 0.1130
new: DataOwner3的最优x_3 = 0.1429
new: DataOwner4的最优x_4 = 0.1563
new: DataOwner5的最优x_5 = 0.1818
new: DataOwner6的最优x_6 = 0.2318
new: DataOwner7的最优x_7 = 0.1402
new: DataOwner8的最优x_8 = 0.1872
new: DataOwner9的最优x_9 = 0.1513
new: DataOwner10的最优x_10 = 0.1040
eta:1.76
new: DataOwner1的最优x_1 = 0.1557
new: DataOwner2的最优x_2 = 0.1137
new: DataOwner3的最优x_3 = 0.1437
new: DataOwner4的最优x_4 = 0.1572
new: DataOwner5的最优x_5 = 0.1829
new: DataOwner6的最优x_6 = 0.2331
new: DataOwner7的最优x_7 = 0.1410
new: DataOwner8的最优x_8 = 0.1883
new: DataOwner9的最优x_9 = 0.1522
new: DataOwner10的最优x_10 = 0.1046
eta:1.77
new: DataOwner1的最优x_1 = 0.1566
new: DataOwner2的最优x_2 = 0.1143
new: DataOwner3的最优x_3 = 0.1446
new: DataOwner4的最优x_4 = 0.1581
new: DataOwner5的最优x_5 = 0.1839
new: DataOwner6的最优x_6 = 0.2344
new: DataOwner7的最优x_7 = 0.1418
new: DataOwner8的最优x_8 = 0.1893
new: DataOwner9的最优x_9 = 0.1530
new: DataOwner10的最优x_10 = 0.1052
eta:1.78
new: DataOwner1的最优x_1 = 0.1575
new: DataOwner2的最优x_2 = 0.1149
new: DataOwner3的最优x_3 = 0.1454
new: DataOwner4的最优x_4 = 0.1590
new: DataOwner5的最优x_5 = 0.1849
new: DataOwner6的最优x_6 = 0.2357
new: DataOwner7的最优x_7 = 0.1426
new: DataOwner8的最优x_8 = 0.1904
new: DataOwner9的最优x_9 = 0.1539
new: DataOwner10的最优x_10 = 0.1058
eta:1.79
new: DataOwner1的最优x_1 = 0.1583
new: DataOwner2的最优x_2 = 0.1156
new: DataOwner3的最优x_3 = 0.1462
new: DataOwner4的最优x_4 = 0.1599
new: DataOwner5的最优x_5 = 0.1860
new: DataOwner6的最优x_6 = 0.2371
new: DataOwner7的最优x_7 = 0.1435
new: DataOwner8的最优x_8 = 0.1915
new: DataOwner9的最优x_9 = 0.1548
new: DataOwner10的最优x_10 = 0.1064
eta:1.8
new: DataOwner1的最优x_1 = 0.1592
new: DataOwner2的最优x_2 = 0.1162
new: DataOwner3的最优x_3 = 0.1470
new: DataOwner4的最优x_4 = 0.1608
new: DataOwner5的最优x_5 = 0.1870
new: DataOwner6的最优x_6 = 0.2384
new: DataOwner7的最优x_7 = 0.1443
new: DataOwner8的最优x_8 = 0.1925
new: DataOwner9的最优x_9 = 0.1556
new: DataOwner10的最优x_10 = 0.1070
eta:1.81
new: DataOwner1的最优x_1 = 0.1601
new: DataOwner2的最优x_2 = 0.1169
new: DataOwner3的最优x_3 = 0.1478
new: DataOwner4的最优x_4 = 0.1617
new: DataOwner5的最优x_5 = 0.1881
new: DataOwner6的最优x_6 = 0.2397
new: DataOwner7的最优x_7 = 0.1451
new: DataOwner8的最优x_8 = 0.1936
new: DataOwner9的最优x_9 = 0.1565
new: DataOwner10的最优x_10 = 0.1076
eta:1.82
new: DataOwner1的最优x_1 = 0.1610
new: DataOwner2的最优x_2 = 0.1175
new: DataOwner3的最优x_3 = 0.1486
new: DataOwner4的最优x_4 = 0.1626
new: DataOwner5的最优x_5 = 0.1891
new: DataOwner6的最优x_6 = 0.2410
new: DataOwner7的最优x_7 = 0.1459
new: DataOwner8的最优x_8 = 0.1947
new: DataOwner9的最优x_9 = 0.1573
new: DataOwner10的最优x_10 = 0.1082
eta:1.83
new: DataOwner1的最优x_1 = 0.1619
new: DataOwner2的最优x_2 = 0.1182
new: DataOwner3的最优x_3 = 0.1495
new: DataOwner4的最优x_4 = 0.1635
new: DataOwner5的最优x_5 = 0.1901
new: DataOwner6的最优x_6 = 0.2424
new: DataOwner7的最优x_7 = 0.1467
new: DataOwner8的最优x_8 = 0.1957
new: DataOwner9的最优x_9 = 0.1582
new: DataOwner10的最优x_10 = 0.1088
eta:1.84
new: DataOwner1的最优x_1 = 0.1628
new: DataOwner2的最优x_2 = 0.1188
new: DataOwner3的最优x_3 = 0.1503
new: DataOwner4的最优x_4 = 0.1644
new: DataOwner5的最优x_5 = 0.1912
new: DataOwner6的最优x_6 = 0.2437
new: DataOwner7的最优x_7 = 0.1475
new: DataOwner8的最优x_8 = 0.1968
new: DataOwner9的最优x_9 = 0.1591
new: DataOwner10的最优x_10 = 0.1094
eta:1.85
new: DataOwner1的最优x_1 = 0.1637
new: DataOwner2的最优x_2 = 0.1195
new: DataOwner3的最优x_3 = 0.1511
new: DataOwner4的最优x_4 = 0.1653
new: DataOwner5的最优x_5 = 0.1922
new: DataOwner6的最优x_6 = 0.2450
new: DataOwner7的最优x_7 = 0.1483
new: DataOwner8的最优x_8 = 0.1979
new: DataOwner9的最优x_9 = 0.1599
new: DataOwner10的最优x_10 = 0.1100
eta:1.86
new: DataOwner1的最优x_1 = 0.1645
new: DataOwner2的最优x_2 = 0.1201
new: DataOwner3的最优x_3 = 0.1519
new: DataOwner4的最优x_4 = 0.1662
new: DataOwner5的最优x_5 = 0.1933
new: DataOwner6的最优x_6 = 0.2463
new: DataOwner7的最优x_7 = 0.1491
new: DataOwner8的最优x_8 = 0.1989
new: DataOwner9的最优x_9 = 0.1608
new: DataOwner10的最优x_10 = 0.1106
eta:1.87
new: DataOwner1的最优x_1 = 0.1654
new: DataOwner2的最优x_2 = 0.1208
new: DataOwner3的最优x_3 = 0.1527
new: DataOwner4的最优x_4 = 0.1671
new: DataOwner5的最优x_5 = 0.1943
new: DataOwner6的最优x_6 = 0.2477
new: DataOwner7的最优x_7 = 0.1499
new: DataOwner8的最优x_8 = 0.2000
new: DataOwner9的最优x_9 = 0.1617
new: DataOwner10的最优x_10 = 0.1112
eta:1.8800000000000001
new: DataOwner1的最优x_1 = 0.1663
new: DataOwner2的最优x_2 = 0.1214
new: DataOwner3的最优x_3 = 0.1535
new: DataOwner4的最优x_4 = 0.1680
new: DataOwner5的最优x_5 = 0.1953
new: DataOwner6的最优x_6 = 0.2490
new: DataOwner7的最优x_7 = 0.1507
new: DataOwner8的最优x_8 = 0.2011
new: DataOwner9的最优x_9 = 0.1625
new: DataOwner10的最优x_10 = 0.1118
eta:1.8900000000000001
new: DataOwner1的最优x_1 = 0.1672
new: DataOwner2的最优x_2 = 0.1221
new: DataOwner3的最优x_3 = 0.1544
new: DataOwner4的最优x_4 = 0.1689
new: DataOwner5的最优x_5 = 0.1964
new: DataOwner6的最优x_6 = 0.2503
new: DataOwner7的最优x_7 = 0.1515
new: DataOwner8的最优x_8 = 0.2022
new: DataOwner9的最优x_9 = 0.1634
new: DataOwner10的最优x_10 = 0.1124
eta:1.9000000000000001
new: DataOwner1的最优x_1 = 0.1681
new: DataOwner2的最优x_2 = 0.1227
new: DataOwner3的最优x_3 = 0.1552
new: DataOwner4的最优x_4 = 0.1697
new: DataOwner5的最优x_5 = 0.1974
new: DataOwner6的最优x_6 = 0.2516
new: DataOwner7的最优x_7 = 0.1523
new: DataOwner8的最优x_8 = 0.2032
new: DataOwner9的最优x_9 = 0.1643
new: DataOwner10的最优x_10 = 0.1129
eta:1.9100000000000001
new: DataOwner1的最优x_1 = 0.1690
new: DataOwner2的最优x_2 = 0.1233
new: DataOwner3的最优x_3 = 0.1560
new: DataOwner4的最优x_4 = 0.1706
new: DataOwner5的最优x_5 = 0.1985
new: DataOwner6的最优x_6 = 0.2530
new: DataOwner7的最优x_7 = 0.1531
new: DataOwner8的最优x_8 = 0.2043
new: DataOwner9的最优x_9 = 0.1651
new: DataOwner10的最优x_10 = 0.1135
eta:1.9200000000000002
new: DataOwner1的最优x_1 = 0.1698
new: DataOwner2的最优x_2 = 0.1240
new: DataOwner3的最优x_3 = 0.1568
new: DataOwner4的最优x_4 = 0.1715
new: DataOwner5的最优x_5 = 0.1995
new: DataOwner6的最优x_6 = 0.2543
new: DataOwner7的最优x_7 = 0.1539
new: DataOwner8的最优x_8 = 0.2054
new: DataOwner9的最优x_9 = 0.1660
new: DataOwner10的最优x_10 = 0.1141
eta:1.93
new: DataOwner1的最优x_1 = 0.1707
new: DataOwner2的最优x_2 = 0.1246
new: DataOwner3的最优x_3 = 0.1576
new: DataOwner4的最优x_4 = 0.1724
new: DataOwner5的最优x_5 = 0.2005
new: DataOwner6的最优x_6 = 0.2556
new: DataOwner7的最优x_7 = 0.1547
new: DataOwner8的最优x_8 = 0.2064
new: DataOwner9的最优x_9 = 0.1669
new: DataOwner10的最优x_10 = 0.1147
eta:1.94
new: DataOwner1的最优x_1 = 0.1716
new: DataOwner2的最优x_2 = 0.1253
new: DataOwner3的最优x_3 = 0.1584
new: DataOwner4的最优x_4 = 0.1733
new: DataOwner5的最优x_5 = 0.2016
new: DataOwner6的最优x_6 = 0.2569
new: DataOwner7的最优x_7 = 0.1555
new: DataOwner8的最优x_8 = 0.2075
new: DataOwner9的最优x_9 = 0.1677
new: DataOwner10的最优x_10 = 0.1153
eta:1.95
new: DataOwner1的最优x_1 = 0.1725
new: DataOwner2的最优x_2 = 0.1259
new: DataOwner3的最优x_3 = 0.1593
new: DataOwner4的最优x_4 = 0.1742
new: DataOwner5的最优x_5 = 0.2026
new: DataOwner6的最优x_6 = 0.2583
new: DataOwner7的最优x_7 = 0.1563
new: DataOwner8的最优x_8 = 0.2086
new: DataOwner9的最优x_9 = 0.1686
new: DataOwner10的最优x_10 = 0.1159
eta:1.96
new: DataOwner1的最优x_1 = 0.1734
new: DataOwner2的最优x_2 = 0.1266
new: DataOwner3的最优x_3 = 0.1601
new: DataOwner4的最优x_4 = 0.1751
new: DataOwner5的最优x_5 = 0.2037
new: DataOwner6的最优x_6 = 0.2596
new: DataOwner7的最优x_7 = 0.1571
new: DataOwner8的最优x_8 = 0.2096
new: DataOwner9的最优x_9 = 0.1695
new: DataOwner10的最优x_10 = 0.1165
eta:1.97
new: DataOwner1的最优x_1 = 0.1743
new: DataOwner2的最优x_2 = 0.1272
new: DataOwner3的最优x_3 = 0.1609
new: DataOwner4的最优x_4 = 0.1760
new: DataOwner5的最优x_5 = 0.2047
new: DataOwner6的最优x_6 = 0.2609
new: DataOwner7的最优x_7 = 0.1579
new: DataOwner8的最优x_8 = 0.2107
new: DataOwner9的最优x_9 = 0.1703
new: DataOwner10的最优x_10 = 0.1171
eta:1.98
new: DataOwner1的最优x_1 = 0.1752
new: DataOwner2的最优x_2 = 0.1279
new: DataOwner3的最优x_3 = 0.1617
new: DataOwner4的最优x_4 = 0.1769
new: DataOwner5的最优x_5 = 0.2057
new: DataOwner6的最优x_6 = 0.2622
new: DataOwner7的最优x_7 = 0.1587
new: DataOwner8的最优x_8 = 0.2118
new: DataOwner9的最优x_9 = 0.1712
new: DataOwner10的最优x_10 = 0.1177
eta:1.99
new: DataOwner1的最优x_1 = 0.1760
new: DataOwner2的最优x_2 = 0.1285
new: DataOwner3的最优x_3 = 0.1625
new: DataOwner4的最优x_4 = 0.1778
new: DataOwner5的最优x_5 = 0.2068
new: DataOwner6的最优x_6 = 0.2636
new: DataOwner7的最优x_7 = 0.1595
new: DataOwner8的最优x_8 = 0.2129
new: DataOwner9的最优x_9 = 0.1720
new: DataOwner10的最优x_10 = 0.1183
eta:2.0
new: DataOwner1的最优x_1 = 0.1769
new: DataOwner2的最优x_2 = 0.1292
new: DataOwner3的最优x_3 = 0.1633
new: DataOwner4的最优x_4 = 0.1787
new: DataOwner5的最优x_5 = 0.2078
new: DataOwner6的最优x_6 = 0.2649
new: DataOwner7的最优x_7 = 0.1603
new: DataOwner8的最优x_8 = 0.2139
new: DataOwner9的最优x_9 = 0.1729
new: DataOwner10的最优x_10 = 0.1189
eta:2.01
new: DataOwner1的最优x_1 = 0.1778
new: DataOwner2的最优x_2 = 0.1298
new: DataOwner3的最优x_3 = 0.1642
new: DataOwner4的最优x_4 = 0.1796
new: DataOwner5的最优x_5 = 0.2088
new: DataOwner6的最优x_6 = 0.2662
new: DataOwner7的最优x_7 = 0.1611
new: DataOwner8的最优x_8 = 0.2150
new: DataOwner9的最优x_9 = 0.1738
new: DataOwner10的最优x_10 = 0.1195
eta:2.02
new: DataOwner1的最优x_1 = 0.1787
new: DataOwner2的最优x_2 = 0.1304
new: DataOwner3的最优x_3 = 0.1650
new: DataOwner4的最优x_4 = 0.1805
new: DataOwner5的最优x_5 = 0.2099
new: DataOwner6的最优x_6 = 0.2675
new: DataOwner7的最优x_7 = 0.1619
new: DataOwner8的最优x_8 = 0.2161
new: DataOwner9的最优x_9 = 0.1746
new: DataOwner10的最优x_10 = 0.1201
eta:2.03
new: DataOwner1的最优x_1 = 0.1796
new: DataOwner2的最优x_2 = 0.1311
new: DataOwner3的最优x_3 = 0.1658
new: DataOwner4的最优x_4 = 0.1814
new: DataOwner5的最优x_5 = 0.2109
new: DataOwner6的最优x_6 = 0.2689
new: DataOwner7的最优x_7 = 0.1627
new: DataOwner8的最优x_8 = 0.2171
new: DataOwner9的最优x_9 = 0.1755
new: DataOwner10的最优x_10 = 0.1207
eta:2.04
new: DataOwner1的最优x_1 = 0.1805
new: DataOwner2的最优x_2 = 0.1317
new: DataOwner3的最优x_3 = 0.1666
new: DataOwner4的最优x_4 = 0.1823
new: DataOwner5的最优x_5 = 0.2120
new: DataOwner6的最优x_6 = 0.2702
new: DataOwner7的最优x_7 = 0.1635
new: DataOwner8的最优x_8 = 0.2182
new: DataOwner9的最优x_9 = 0.1764
new: DataOwner10的最优x_10 = 0.1213
eta:2.05
new: DataOwner1的最优x_1 = 0.1813
new: DataOwner2的最优x_2 = 0.1324
new: DataOwner3的最优x_3 = 0.1674
new: DataOwner4的最优x_4 = 0.1831
new: DataOwner5的最优x_5 = 0.2130
new: DataOwner6的最优x_6 = 0.2715
new: DataOwner7的最优x_7 = 0.1643
new: DataOwner8的最优x_8 = 0.2193
new: DataOwner9的最优x_9 = 0.1772
new: DataOwner10的最优x_10 = 0.1219
eta:2.0599999999999996
new: DataOwner1的最优x_1 = 0.1822
new: DataOwner2的最优x_2 = 0.1330
new: DataOwner3的最优x_3 = 0.1682
new: DataOwner4的最优x_4 = 0.1840
new: DataOwner5的最优x_5 = 0.2140
new: DataOwner6的最优x_6 = 0.2728
new: DataOwner7的最优x_7 = 0.1651
new: DataOwner8的最优x_8 = 0.2203
new: DataOwner9的最优x_9 = 0.1781
new: DataOwner10的最优x_10 = 0.1225
eta:2.07
new: DataOwner1的最优x_1 = 0.1831
new: DataOwner2的最优x_2 = 0.1337
new: DataOwner3的最优x_3 = 0.1691
new: DataOwner4的最优x_4 = 0.1849
new: DataOwner5的最优x_5 = 0.2151
new: DataOwner6的最优x_6 = 0.2741
new: DataOwner7的最优x_7 = 0.1659
new: DataOwner8的最优x_8 = 0.2214
new: DataOwner9的最优x_9 = 0.1790
new: DataOwner10的最优x_10 = 0.1231
eta:2.0799999999999996
new: DataOwner1的最优x_1 = 0.1840
new: DataOwner2的最优x_2 = 0.1343
new: DataOwner3的最优x_3 = 0.1699
new: DataOwner4的最优x_4 = 0.1858
new: DataOwner5的最优x_5 = 0.2161
new: DataOwner6的最优x_6 = 0.2755
new: DataOwner7的最优x_7 = 0.1667
new: DataOwner8的最优x_8 = 0.2225
new: DataOwner9的最优x_9 = 0.1798
new: DataOwner10的最优x_10 = 0.1236
eta:2.09
new: DataOwner1的最优x_1 = 0.1849
new: DataOwner2的最优x_2 = 0.1350
new: DataOwner3的最优x_3 = 0.1707
new: DataOwner4的最优x_4 = 0.1867
new: DataOwner5的最优x_5 = 0.2172
new: DataOwner6的最优x_6 = 0.2768
new: DataOwner7的最优x_7 = 0.1675
new: DataOwner8的最优x_8 = 0.2235
new: DataOwner9的最优x_9 = 0.1807
new: DataOwner10的最优x_10 = 0.1242
eta:2.0999999999999996
new: DataOwner1的最优x_1 = 0.1858
new: DataOwner2的最优x_2 = 0.1356
new: DataOwner3的最优x_3 = 0.1715
new: DataOwner4的最优x_4 = 0.1876
new: DataOwner5的最优x_5 = 0.2182
new: DataOwner6的最优x_6 = 0.2781
new: DataOwner7的最优x_7 = 0.1683
new: DataOwner8的最优x_8 = 0.2246
new: DataOwner9的最优x_9 = 0.1816
new: DataOwner10的最优x_10 = 0.1248
eta:2.11
new: DataOwner1的最优x_1 = 0.1867
new: DataOwner2的最优x_2 = 0.1363
new: DataOwner3的最优x_3 = 0.1723
new: DataOwner4的最优x_4 = 0.1885
new: DataOwner5的最优x_5 = 0.2192
new: DataOwner6的最优x_6 = 0.2794
new: DataOwner7的最优x_7 = 0.1691
new: DataOwner8的最优x_8 = 0.2257
new: DataOwner9的最优x_9 = 0.1824
new: DataOwner10的最优x_10 = 0.1254
eta:2.1199999999999997
new: DataOwner1的最优x_1 = 0.1875
new: DataOwner2的最优x_2 = 0.1369
new: DataOwner3的最优x_3 = 0.1731
new: DataOwner4的最优x_4 = 0.1894
new: DataOwner5的最优x_5 = 0.2203
new: DataOwner6的最优x_6 = 0.2808
new: DataOwner7的最优x_7 = 0.1699
new: DataOwner8的最优x_8 = 0.2268
new: DataOwner9的最优x_9 = 0.1833
new: DataOwner10的最优x_10 = 0.1260
eta:2.13
new: DataOwner1的最优x_1 = 0.1884
new: DataOwner2的最优x_2 = 0.1375
new: DataOwner3的最优x_3 = 0.1740
new: DataOwner4的最优x_4 = 0.1903
new: DataOwner5的最优x_5 = 0.2213
new: DataOwner6的最优x_6 = 0.2821
new: DataOwner7的最优x_7 = 0.1707
new: DataOwner8的最优x_8 = 0.2278
new: DataOwner9的最优x_9 = 0.1842
new: DataOwner10的最优x_10 = 0.1266
eta:2.1399999999999997
new: DataOwner1的最优x_1 = 0.1893
new: DataOwner2的最优x_2 = 0.1382
new: DataOwner3的最优x_3 = 0.1748
new: DataOwner4的最优x_4 = 0.1912
new: DataOwner5的最优x_5 = 0.2224
new: DataOwner6的最优x_6 = 0.2834
new: DataOwner7的最优x_7 = 0.1715
new: DataOwner8的最优x_8 = 0.2289
new: DataOwner9的最优x_9 = 0.1850
new: DataOwner10的最优x_10 = 0.1272
eta:2.15
new: DataOwner1的最优x_1 = 0.1902
new: DataOwner2的最优x_2 = 0.1388
new: DataOwner3的最优x_3 = 0.1756
new: DataOwner4的最优x_4 = 0.1921
new: DataOwner5的最优x_5 = 0.2234
new: DataOwner6的最优x_6 = 0.2847
new: DataOwner7的最优x_7 = 0.1723
new: DataOwner8的最优x_8 = 0.2300
new: DataOwner9的最优x_9 = 0.1859
new: DataOwner10的最优x_10 = 0.1278
eta:2.1599999999999997
new: DataOwner1的最优x_1 = 0.1911
new: DataOwner2的最优x_2 = 0.1395
new: DataOwner3的最优x_3 = 0.1764
new: DataOwner4的最优x_4 = 0.1930
new: DataOwner5的最优x_5 = 0.2244
new: DataOwner6的最优x_6 = 0.2861
new: DataOwner7的最优x_7 = 0.1731
new: DataOwner8的最优x_8 = 0.2310
new: DataOwner9的最优x_9 = 0.1867
new: DataOwner10的最优x_10 = 0.1284
eta:2.17
new: DataOwner1的最优x_1 = 0.1920
new: DataOwner2的最优x_2 = 0.1401
new: DataOwner3的最优x_3 = 0.1772
new: DataOwner4的最优x_4 = 0.1939
new: DataOwner5的最优x_5 = 0.2255
new: DataOwner6的最优x_6 = 0.2874
new: DataOwner7的最优x_7 = 0.1739
new: DataOwner8的最优x_8 = 0.2321
new: DataOwner9的最优x_9 = 0.1876
new: DataOwner10的最优x_10 = 0.1290
eta:2.1799999999999997
new: DataOwner1的最优x_1 = 0.1928
new: DataOwner2的最优x_2 = 0.1408
new: DataOwner3的最优x_3 = 0.1780
new: DataOwner4的最优x_4 = 0.1948
new: DataOwner5的最优x_5 = 0.2265
new: DataOwner6的最优x_6 = 0.2887
new: DataOwner7的最优x_7 = 0.1747
new: DataOwner8的最优x_8 = 0.2332
new: DataOwner9的最优x_9 = 0.1885
new: DataOwner10的最优x_10 = 0.1296
eta:2.19
new: DataOwner1的最优x_1 = 0.1937
new: DataOwner2的最优x_2 = 0.1414
new: DataOwner3的最优x_3 = 0.1789
new: DataOwner4的最优x_4 = 0.1957
new: DataOwner5的最优x_5 = 0.2275
new: DataOwner6的最优x_6 = 0.2900
new: DataOwner7的最优x_7 = 0.1755
new: DataOwner8的最优x_8 = 0.2342
new: DataOwner9的最优x_9 = 0.1893
new: DataOwner10的最优x_10 = 0.1302
eta:2.1999999999999997
new: DataOwner1的最优x_1 = 0.1946
new: DataOwner2的最优x_2 = 0.1421
new: DataOwner3的最优x_3 = 0.1797
new: DataOwner4的最优x_4 = 0.1965
new: DataOwner5的最优x_5 = 0.2286
new: DataOwner6的最优x_6 = 0.2914
new: DataOwner7的最优x_7 = 0.1763
new: DataOwner8的最优x_8 = 0.2353
new: DataOwner9的最优x_9 = 0.1902
new: DataOwner10的最优x_10 = 0.1308
eta:2.21
new: DataOwner1的最优x_1 = 0.1955
new: DataOwner2的最优x_2 = 0.1427
new: DataOwner3的最优x_3 = 0.1805
new: DataOwner4的最优x_4 = 0.1974
new: DataOwner5的最优x_5 = 0.2296
new: DataOwner6的最优x_6 = 0.2927
new: DataOwner7的最优x_7 = 0.1771
new: DataOwner8的最优x_8 = 0.2364
new: DataOwner9的最优x_9 = 0.1911
new: DataOwner10的最优x_10 = 0.1314
eta:2.2199999999999998
new: DataOwner1的最优x_1 = 0.1964
new: DataOwner2的最优x_2 = 0.1434
new: DataOwner3的最优x_3 = 0.1813
new: DataOwner4的最优x_4 = 0.1983
new: DataOwner5的最优x_5 = 0.2307
new: DataOwner6的最优x_6 = 0.2940
new: DataOwner7的最优x_7 = 0.1779
new: DataOwner8的最优x_8 = 0.2375
new: DataOwner9的最优x_9 = 0.1919
new: DataOwner10的最优x_10 = 0.1320
eta:2.23
new: DataOwner1的最优x_1 = 0.1973
new: DataOwner2的最优x_2 = 0.1440
new: DataOwner3的最优x_3 = 0.1821
new: DataOwner4的最优x_4 = 0.1992
new: DataOwner5的最优x_5 = 0.2317
new: DataOwner6的最优x_6 = 0.2953
new: DataOwner7的最优x_7 = 0.1787
new: DataOwner8的最优x_8 = 0.2385
new: DataOwner9的最优x_9 = 0.1928
new: DataOwner10的最优x_10 = 0.1326
eta:2.2399999999999998
new: DataOwner1的最优x_1 = 0.1982
new: DataOwner2的最优x_2 = 0.1447
new: DataOwner3的最优x_3 = 0.1829
new: DataOwner4的最优x_4 = 0.2001
new: DataOwner5的最优x_5 = 0.2327
new: DataOwner6的最优x_6 = 0.2967
new: DataOwner7的最优x_7 = 0.1795
new: DataOwner8的最优x_8 = 0.2396
new: DataOwner9的最优x_9 = 0.1937
new: DataOwner10的最优x_10 = 0.1332
eta:2.25
new: DataOwner1的最优x_1 = 0.1990
new: DataOwner2的最优x_2 = 0.1453
new: DataOwner3的最优x_3 = 0.1838
new: DataOwner4的最优x_4 = 0.2010
new: DataOwner5的最优x_5 = 0.2338
new: DataOwner6的最优x_6 = 0.2980
new: DataOwner7的最优x_7 = 0.1803
new: DataOwner8的最优x_8 = 0.2407
new: DataOwner9的最优x_9 = 0.1945
new: DataOwner10的最优x_10 = 0.1338
eta:2.26
new: DataOwner1的最优x_1 = 0.1999
new: DataOwner2的最优x_2 = 0.1459
new: DataOwner3的最优x_3 = 0.1846
new: DataOwner4的最优x_4 = 0.2019
new: DataOwner5的最优x_5 = 0.2348
new: DataOwner6的最优x_6 = 0.2993
new: DataOwner7的最优x_7 = 0.1811
new: DataOwner8的最优x_8 = 0.2417
new: DataOwner9的最优x_9 = 0.1954
new: DataOwner10的最优x_10 = 0.1343
eta:2.27
new: DataOwner1的最优x_1 = 0.2008
new: DataOwner2的最优x_2 = 0.1466
new: DataOwner3的最优x_3 = 0.1854
new: DataOwner4的最优x_4 = 0.2028
new: DataOwner5的最优x_5 = 0.2359
new: DataOwner6的最优x_6 = 0.3006
new: DataOwner7的最优x_7 = 0.1819
new: DataOwner8的最优x_8 = 0.2428
new: DataOwner9的最优x_9 = 0.1963
new: DataOwner10的最优x_10 = 0.1349
eta:2.28
new: DataOwner1的最优x_1 = 0.2017
new: DataOwner2的最优x_2 = 0.1472
new: DataOwner3的最优x_3 = 0.1862
new: DataOwner4的最优x_4 = 0.2037
new: DataOwner5的最优x_5 = 0.2369
new: DataOwner6的最优x_6 = 0.3020
new: DataOwner7的最优x_7 = 0.1827
new: DataOwner8的最优x_8 = 0.2439
new: DataOwner9的最优x_9 = 0.1971
new: DataOwner10的最优x_10 = 0.1355
eta:2.29
new: DataOwner1的最优x_1 = 0.2026
new: DataOwner2的最优x_2 = 0.1479
new: DataOwner3的最优x_3 = 0.1870
new: DataOwner4的最优x_4 = 0.2046
new: DataOwner5的最优x_5 = 0.2379
new: DataOwner6的最优x_6 = 0.3033
new: DataOwner7的最优x_7 = 0.1835
new: DataOwner8的最优x_8 = 0.2449
new: DataOwner9的最优x_9 = 0.1980
new: DataOwner10的最优x_10 = 0.1361
eta:2.3
new: DataOwner1的最优x_1 = 0.2035
new: DataOwner2的最优x_2 = 0.1485
new: DataOwner3的最优x_3 = 0.1878
new: DataOwner4的最优x_4 = 0.2055
new: DataOwner5的最优x_5 = 0.2390
new: DataOwner6的最优x_6 = 0.3046
new: DataOwner7的最优x_7 = 0.1843
new: DataOwner8的最优x_8 = 0.2460
new: DataOwner9的最优x_9 = 0.1988
new: DataOwner10的最优x_10 = 0.1367
eta:2.31
new: DataOwner1的最优x_1 = 0.2043
new: DataOwner2的最优x_2 = 0.1492
new: DataOwner3的最优x_3 = 0.1887
new: DataOwner4的最优x_4 = 0.2064
new: DataOwner5的最优x_5 = 0.2400
new: DataOwner6的最优x_6 = 0.3059
new: DataOwner7的最优x_7 = 0.1851
new: DataOwner8的最优x_8 = 0.2471
new: DataOwner9的最优x_9 = 0.1997
new: DataOwner10的最优x_10 = 0.1373
eta:2.32
new: DataOwner1的最优x_1 = 0.2052
new: DataOwner2的最优x_2 = 0.1498
new: DataOwner3的最优x_3 = 0.1895
new: DataOwner4的最优x_4 = 0.2073
new: DataOwner5的最优x_5 = 0.2411
new: DataOwner6的最优x_6 = 0.3073
new: DataOwner7的最优x_7 = 0.1859
new: DataOwner8的最优x_8 = 0.2482
new: DataOwner9的最优x_9 = 0.2006
new: DataOwner10的最优x_10 = 0.1379
eta:2.3299999999999996
new: DataOwner1的最优x_1 = 0.2061
new: DataOwner2的最优x_2 = 0.1505
new: DataOwner3的最优x_3 = 0.1903
new: DataOwner4的最优x_4 = 0.2082
new: DataOwner5的最优x_5 = 0.2421
new: DataOwner6的最优x_6 = 0.3086
new: DataOwner7的最优x_7 = 0.1867
new: DataOwner8的最优x_8 = 0.2492
new: DataOwner9的最优x_9 = 0.2014
new: DataOwner10的最优x_10 = 0.1385
eta:2.34
new: DataOwner1的最优x_1 = 0.2070
new: DataOwner2的最优x_2 = 0.1511
new: DataOwner3的最优x_3 = 0.1911
new: DataOwner4的最优x_4 = 0.2091
new: DataOwner5的最优x_5 = 0.2431
new: DataOwner6的最优x_6 = 0.3099
new: DataOwner7的最优x_7 = 0.1875
new: DataOwner8的最优x_8 = 0.2503
new: DataOwner9的最优x_9 = 0.2023
new: DataOwner10的最优x_10 = 0.1391
eta:2.3499999999999996
new: DataOwner1的最优x_1 = 0.2079
new: DataOwner2的最优x_2 = 0.1518
new: DataOwner3的最优x_3 = 0.1919
new: DataOwner4的最优x_4 = 0.2099
new: DataOwner5的最优x_5 = 0.2442
new: DataOwner6的最优x_6 = 0.3112
new: DataOwner7的最优x_7 = 0.1883
new: DataOwner8的最优x_8 = 0.2514
new: DataOwner9的最优x_9 = 0.2032
new: DataOwner10的最优x_10 = 0.1397
eta:2.36
new: DataOwner1的最优x_1 = 0.2088
new: DataOwner2的最优x_2 = 0.1524
new: DataOwner3的最优x_3 = 0.1927
new: DataOwner4的最优x_4 = 0.2108
new: DataOwner5的最优x_5 = 0.2452
new: DataOwner6的最优x_6 = 0.3126
new: DataOwner7的最优x_7 = 0.1891
new: DataOwner8的最优x_8 = 0.2524
new: DataOwner9的最优x_9 = 0.2040
new: DataOwner10的最优x_10 = 0.1403
eta:2.3699999999999997
new: DataOwner1的最优x_1 = 0.2097
new: DataOwner2的最优x_2 = 0.1530
new: DataOwner3的最优x_3 = 0.1936
new: DataOwner4的最优x_4 = 0.2117
new: DataOwner5的最优x_5 = 0.2463
new: DataOwner6的最优x_6 = 0.3139
new: DataOwner7的最优x_7 = 0.1899
new: DataOwner8的最优x_8 = 0.2535
new: DataOwner9的最优x_9 = 0.2049
new: DataOwner10的最优x_10 = 0.1409
eta:2.38
new: DataOwner1的最优x_1 = 0.2105
new: DataOwner2的最优x_2 = 0.1537
new: DataOwner3的最优x_3 = 0.1944
new: DataOwner4的最优x_4 = 0.2126
new: DataOwner5的最优x_5 = 0.2473
new: DataOwner6的最优x_6 = 0.3152
new: DataOwner7的最优x_7 = 0.1907
new: DataOwner8的最优x_8 = 0.2546
new: DataOwner9的最优x_9 = 0.2058
new: DataOwner10的最优x_10 = 0.1415
eta:2.3899999999999997
new: DataOwner1的最优x_1 = 0.2114
new: DataOwner2的最优x_2 = 0.1543
new: DataOwner3的最优x_3 = 0.1952
new: DataOwner4的最优x_4 = 0.2135
new: DataOwner5的最优x_5 = 0.2483
new: DataOwner6的最优x_6 = 0.3165
new: DataOwner7的最优x_7 = 0.1915
new: DataOwner8的最优x_8 = 0.2556
new: DataOwner9的最优x_9 = 0.2066
new: DataOwner10的最优x_10 = 0.1421
eta:2.4
new: DataOwner1的最优x_1 = 0.2123
new: DataOwner2的最优x_2 = 0.1550
new: DataOwner3的最优x_3 = 0.1960
new: DataOwner4的最优x_4 = 0.2144
new: DataOwner5的最优x_5 = 0.2494
new: DataOwner6的最优x_6 = 0.3179
new: DataOwner7的最优x_7 = 0.1923
new: DataOwner8的最优x_8 = 0.2567
new: DataOwner9的最优x_9 = 0.2075
new: DataOwner10的最优x_10 = 0.1427
eta:2.4099999999999997
new: DataOwner1的最优x_1 = 0.2132
new: DataOwner2的最优x_2 = 0.1556
new: DataOwner3的最优x_3 = 0.1968
new: DataOwner4的最优x_4 = 0.2153
new: DataOwner5的最优x_5 = 0.2504
new: DataOwner6的最优x_6 = 0.3192
new: DataOwner7的最优x_7 = 0.1931
new: DataOwner8的最优x_8 = 0.2578
new: DataOwner9的最优x_9 = 0.2084
new: DataOwner10的最优x_10 = 0.1433
eta:2.42
new: DataOwner1的最优x_1 = 0.2141
new: DataOwner2的最优x_2 = 0.1563
new: DataOwner3的最优x_3 = 0.1976
new: DataOwner4的最优x_4 = 0.2162
new: DataOwner5的最优x_5 = 0.2514
new: DataOwner6的最优x_6 = 0.3205
new: DataOwner7的最优x_7 = 0.1939
new: DataOwner8的最优x_8 = 0.2588
new: DataOwner9的最优x_9 = 0.2092
new: DataOwner10的最优x_10 = 0.1439
eta:2.4299999999999997
new: DataOwner1的最优x_1 = 0.2150
new: DataOwner2的最优x_2 = 0.1569
new: DataOwner3的最优x_3 = 0.1985
new: DataOwner4的最优x_4 = 0.2171
new: DataOwner5的最优x_5 = 0.2525
new: DataOwner6的最优x_6 = 0.3218
new: DataOwner7的最优x_7 = 0.1947
new: DataOwner8的最优x_8 = 0.2599
new: DataOwner9的最优x_9 = 0.2101
new: DataOwner10的最优x_10 = 0.1445
eta:2.44
new: DataOwner1的最优x_1 = 0.2158
new: DataOwner2的最优x_2 = 0.1576
new: DataOwner3的最优x_3 = 0.1993
new: DataOwner4的最优x_4 = 0.2180
new: DataOwner5的最优x_5 = 0.2535
new: DataOwner6的最优x_6 = 0.3232
new: DataOwner7的最优x_7 = 0.1955
new: DataOwner8的最优x_8 = 0.2610
new: DataOwner9的最优x_9 = 0.2110
new: DataOwner10的最优x_10 = 0.1450
eta:2.4499999999999997
new: DataOwner1的最优x_1 = 0.2167
new: DataOwner2的最优x_2 = 0.1582
new: DataOwner3的最优x_3 = 0.2001
new: DataOwner4的最优x_4 = 0.2189
new: DataOwner5的最优x_5 = 0.2546
new: DataOwner6的最优x_6 = 0.3245
new: DataOwner7的最优x_7 = 0.1963
new: DataOwner8的最优x_8 = 0.2621
new: DataOwner9的最优x_9 = 0.2118
new: DataOwner10的最优x_10 = 0.1456
eta:2.46
new: DataOwner1的最优x_1 = 0.2176
new: DataOwner2的最优x_2 = 0.1589
new: DataOwner3的最优x_3 = 0.2009
new: DataOwner4的最优x_4 = 0.2198
new: DataOwner5的最优x_5 = 0.2556
new: DataOwner6的最优x_6 = 0.3258
new: DataOwner7的最优x_7 = 0.1971
new: DataOwner8的最优x_8 = 0.2631
new: DataOwner9的最优x_9 = 0.2127
new: DataOwner10的最优x_10 = 0.1462
eta:2.4699999999999998
new: DataOwner1的最优x_1 = 0.2185
new: DataOwner2的最优x_2 = 0.1595
new: DataOwner3的最优x_3 = 0.2017
new: DataOwner4的最优x_4 = 0.2207
new: DataOwner5的最优x_5 = 0.2566
new: DataOwner6的最优x_6 = 0.3271
new: DataOwner7的最优x_7 = 0.1979
new: DataOwner8的最优x_8 = 0.2642
new: DataOwner9的最优x_9 = 0.2135
new: DataOwner10的最优x_10 = 0.1468
eta:2.48
new: DataOwner1的最优x_1 = 0.2194
new: DataOwner2的最优x_2 = 0.1602
new: DataOwner3的最优x_3 = 0.2025
new: DataOwner4的最优x_4 = 0.2216
new: DataOwner5的最优x_5 = 0.2577
new: DataOwner6的最优x_6 = 0.3284
new: DataOwner7的最优x_7 = 0.1987
new: DataOwner8的最优x_8 = 0.2653
new: DataOwner9的最优x_9 = 0.2144
new: DataOwner10的最优x_10 = 0.1474
eta:2.4899999999999998
new: DataOwner1的最优x_1 = 0.2203
new: DataOwner2的最优x_2 = 0.1608
new: DataOwner3的最优x_3 = 0.2034
new: DataOwner4的最优x_4 = 0.2225
new: DataOwner5的最优x_5 = 0.2587
new: DataOwner6的最优x_6 = 0.3298
new: DataOwner7的最优x_7 = 0.1995
new: DataOwner8的最优x_8 = 0.2663
new: DataOwner9的最优x_9 = 0.2153
new: DataOwner10的最优x_10 = 0.1480
eta:2.5
new: DataOwner1的最优x_1 = 0.2212
new: DataOwner2的最优x_2 = 0.1614
new: DataOwner3的最优x_3 = 0.2042
new: DataOwner4的最优x_4 = 0.2234
new: DataOwner5的最优x_5 = 0.2598
new: DataOwner6的最优x_6 = 0.3311
new: DataOwner7的最优x_7 = 0.2004
new: DataOwner8的最优x_8 = 0.2674
new: DataOwner9的最优x_9 = 0.2161
new: DataOwner10的最优x_10 = 0.1486
eta:2.51
new: DataOwner1的最优x_1 = 0.2220
new: DataOwner2的最优x_2 = 0.1621
new: DataOwner3的最优x_3 = 0.2050
new: DataOwner4的最优x_4 = 0.2242
new: DataOwner5的最优x_5 = 0.2608
new: DataOwner6的最优x_6 = 0.3324
new: DataOwner7的最优x_7 = 0.2012
new: DataOwner8的最优x_8 = 0.2685
new: DataOwner9的最优x_9 = 0.2170
new: DataOwner10的最优x_10 = 0.1492
eta:2.52
new: DataOwner1的最优x_1 = 0.2229
new: DataOwner2的最优x_2 = 0.1627
new: DataOwner3的最优x_3 = 0.2058
new: DataOwner4的最优x_4 = 0.2251
new: DataOwner5的最优x_5 = 0.2618
new: DataOwner6的最优x_6 = 0.3337
new: DataOwner7的最优x_7 = 0.2020
new: DataOwner8的最优x_8 = 0.2695
new: DataOwner9的最优x_9 = 0.2179
new: DataOwner10的最优x_10 = 0.1498
eta:2.53
new: DataOwner1的最优x_1 = 0.2238
new: DataOwner2的最优x_2 = 0.1634
new: DataOwner3的最优x_3 = 0.2066
new: DataOwner4的最优x_4 = 0.2260
new: DataOwner5的最优x_5 = 0.2629
new: DataOwner6的最优x_6 = 0.3351
new: DataOwner7的最优x_7 = 0.2028
new: DataOwner8的最优x_8 = 0.2706
new: DataOwner9的最优x_9 = 0.2187
new: DataOwner10的最优x_10 = 0.1504
eta:2.54
new: DataOwner1的最优x_1 = 0.2247
new: DataOwner2的最优x_2 = 0.1640
new: DataOwner3的最优x_3 = 0.2074
new: DataOwner4的最优x_4 = 0.2269
new: DataOwner5的最优x_5 = 0.2639
new: DataOwner6的最优x_6 = 0.3364
new: DataOwner7的最优x_7 = 0.2036
new: DataOwner8的最优x_8 = 0.2717
new: DataOwner9的最优x_9 = 0.2196
new: DataOwner10的最优x_10 = 0.1510
eta:2.55
new: DataOwner1的最优x_1 = 0.2256
new: DataOwner2的最优x_2 = 0.1647
new: DataOwner3的最优x_3 = 0.2083
new: DataOwner4的最优x_4 = 0.2278
new: DataOwner5的最优x_5 = 0.2650
new: DataOwner6的最优x_6 = 0.3377
new: DataOwner7的最优x_7 = 0.2044
new: DataOwner8的最优x_8 = 0.2728
new: DataOwner9的最优x_9 = 0.2205
new: DataOwner10的最优x_10 = 0.1516
eta:2.56
new: DataOwner1的最优x_1 = 0.2265
new: DataOwner2的最优x_2 = 0.1653
new: DataOwner3的最优x_3 = 0.2091
new: DataOwner4的最优x_4 = 0.2287
new: DataOwner5的最优x_5 = 0.2660
new: DataOwner6的最优x_6 = 0.3390
new: DataOwner7的最优x_7 = 0.2052
new: DataOwner8的最优x_8 = 0.2738
new: DataOwner9的最优x_9 = 0.2213
new: DataOwner10的最优x_10 = 0.1522
eta:2.57
new: DataOwner1的最优x_1 = 0.2273
new: DataOwner2的最优x_2 = 0.1660
new: DataOwner3的最优x_3 = 0.2099
new: DataOwner4的最优x_4 = 0.2296
new: DataOwner5的最优x_5 = 0.2670
new: DataOwner6的最优x_6 = 0.3404
new: DataOwner7的最优x_7 = 0.2060
new: DataOwner8的最优x_8 = 0.2749
new: DataOwner9的最优x_9 = 0.2222
new: DataOwner10的最优x_10 = 0.1528
eta:2.5799999999999996
new: DataOwner1的最优x_1 = 0.2282
new: DataOwner2的最优x_2 = 0.1666
new: DataOwner3的最优x_3 = 0.2107
new: DataOwner4的最优x_4 = 0.2305
new: DataOwner5的最优x_5 = 0.2681
new: DataOwner6的最优x_6 = 0.3417
new: DataOwner7的最优x_7 = 0.2068
new: DataOwner8的最优x_8 = 0.2760
new: DataOwner9的最优x_9 = 0.2231
new: DataOwner10的最优x_10 = 0.1534
eta:2.59
new: DataOwner1的最优x_1 = 0.2291
new: DataOwner2的最优x_2 = 0.1673
new: DataOwner3的最优x_3 = 0.2115
new: DataOwner4的最优x_4 = 0.2314
new: DataOwner5的最优x_5 = 0.2691
new: DataOwner6的最优x_6 = 0.3430
new: DataOwner7的最优x_7 = 0.2076
new: DataOwner8的最优x_8 = 0.2770
new: DataOwner9的最优x_9 = 0.2239
new: DataOwner10的最优x_10 = 0.1540
eta:2.5999999999999996
new: DataOwner1的最优x_1 = 0.2300
new: DataOwner2的最优x_2 = 0.1679
new: DataOwner3的最优x_3 = 0.2123
new: DataOwner4的最优x_4 = 0.2323
new: DataOwner5的最优x_5 = 0.2701
new: DataOwner6的最优x_6 = 0.3443
new: DataOwner7的最优x_7 = 0.2084
new: DataOwner8的最优x_8 = 0.2781
new: DataOwner9的最优x_9 = 0.2248
new: DataOwner10的最优x_10 = 0.1546
eta:2.61
new: DataOwner1的最优x_1 = 0.2309
new: DataOwner2的最优x_2 = 0.1685
new: DataOwner3的最优x_3 = 0.2132
new: DataOwner4的最优x_4 = 0.2332
new: DataOwner5的最优x_5 = 0.2712
new: DataOwner6的最优x_6 = 0.3457
new: DataOwner7的最优x_7 = 0.2092
new: DataOwner8的最优x_8 = 0.2792
new: DataOwner9的最优x_9 = 0.2256
new: DataOwner10的最优x_10 = 0.1552
eta:2.6199999999999997
new: DataOwner1的最优x_1 = 0.2318
new: DataOwner2的最优x_2 = 0.1692
new: DataOwner3的最优x_3 = 0.2140
new: DataOwner4的最优x_4 = 0.2341
new: DataOwner5的最优x_5 = 0.2722
new: DataOwner6的最优x_6 = 0.3470
new: DataOwner7的最优x_7 = 0.2100
new: DataOwner8的最优x_8 = 0.2802
new: DataOwner9的最优x_9 = 0.2265
new: DataOwner10的最优x_10 = 0.1557
eta:2.63
new: DataOwner1的最优x_1 = 0.2327
new: DataOwner2的最优x_2 = 0.1698
new: DataOwner3的最优x_3 = 0.2148
new: DataOwner4的最优x_4 = 0.2350
new: DataOwner5的最优x_5 = 0.2733
new: DataOwner6的最优x_6 = 0.3483
new: DataOwner7的最优x_7 = 0.2108
new: DataOwner8的最优x_8 = 0.2813
new: DataOwner9的最优x_9 = 0.2274
new: DataOwner10的最优x_10 = 0.1563
eta:2.6399999999999997
new: DataOwner1的最优x_1 = 0.2335
new: DataOwner2的最优x_2 = 0.1705
new: DataOwner3的最优x_3 = 0.2156
new: DataOwner4的最优x_4 = 0.2359
new: DataOwner5的最优x_5 = 0.2743
new: DataOwner6的最优x_6 = 0.3496
new: DataOwner7的最优x_7 = 0.2116
new: DataOwner8的最优x_8 = 0.2824
new: DataOwner9的最优x_9 = 0.2282
new: DataOwner10的最优x_10 = 0.1569
eta:2.65
new: DataOwner1的最优x_1 = 0.2344
new: DataOwner2的最优x_2 = 0.1711
new: DataOwner3的最优x_3 = 0.2164
new: DataOwner4的最优x_4 = 0.2368
new: DataOwner5的最优x_5 = 0.2753
new: DataOwner6的最优x_6 = 0.3510
new: DataOwner7的最优x_7 = 0.2124
new: DataOwner8的最优x_8 = 0.2834
new: DataOwner9的最优x_9 = 0.2291
new: DataOwner10的最优x_10 = 0.1575
eta:2.6599999999999997
new: DataOwner1的最优x_1 = 0.2353
new: DataOwner2的最优x_2 = 0.1718
new: DataOwner3的最优x_3 = 0.2172
new: DataOwner4的最优x_4 = 0.2376
new: DataOwner5的最优x_5 = 0.2764
new: DataOwner6的最优x_6 = 0.3523
new: DataOwner7的最优x_7 = 0.2132
new: DataOwner8的最优x_8 = 0.2845
new: DataOwner9的最优x_9 = 0.2300
new: DataOwner10的最优x_10 = 0.1581
eta:2.67
new: DataOwner1的最优x_1 = 0.2362
new: DataOwner2的最优x_2 = 0.1724
new: DataOwner3的最优x_3 = 0.2181
new: DataOwner4的最优x_4 = 0.2385
new: DataOwner5的最优x_5 = 0.2774
new: DataOwner6的最优x_6 = 0.3536
new: DataOwner7的最优x_7 = 0.2140
new: DataOwner8的最优x_8 = 0.2856
new: DataOwner9的最优x_9 = 0.2308
new: DataOwner10的最优x_10 = 0.1587
eta:2.6799999999999997
new: DataOwner1的最优x_1 = 0.2371
new: DataOwner2的最优x_2 = 0.1731
new: DataOwner3的最优x_3 = 0.2189
new: DataOwner4的最优x_4 = 0.2394
new: DataOwner5的最优x_5 = 0.2785
new: DataOwner6的最优x_6 = 0.3549
new: DataOwner7的最优x_7 = 0.2148
new: DataOwner8的最优x_8 = 0.2867
new: DataOwner9的最优x_9 = 0.2317
new: DataOwner10的最优x_10 = 0.1593
eta:2.69
new: DataOwner1的最优x_1 = 0.2380
new: DataOwner2的最优x_2 = 0.1737
new: DataOwner3的最优x_3 = 0.2197
new: DataOwner4的最优x_4 = 0.2403
new: DataOwner5的最优x_5 = 0.2795
new: DataOwner6的最优x_6 = 0.3563
new: DataOwner7的最优x_7 = 0.2156
new: DataOwner8的最优x_8 = 0.2877
new: DataOwner9的最优x_9 = 0.2326
new: DataOwner10的最优x_10 = 0.1599
eta:2.6999999999999997
new: DataOwner1的最优x_1 = 0.2388
new: DataOwner2的最优x_2 = 0.1744
new: DataOwner3的最优x_3 = 0.2205
new: DataOwner4的最优x_4 = 0.2412
new: DataOwner5的最优x_5 = 0.2805
new: DataOwner6的最优x_6 = 0.3576
new: DataOwner7的最优x_7 = 0.2164
new: DataOwner8的最优x_8 = 0.2888
new: DataOwner9的最优x_9 = 0.2334
new: DataOwner10的最优x_10 = 0.1605
eta:2.71
new: DataOwner1的最优x_1 = 0.2397
new: DataOwner2的最优x_2 = 0.1750
new: DataOwner3的最优x_3 = 0.2213
new: DataOwner4的最优x_4 = 0.2421
new: DataOwner5的最优x_5 = 0.2816
new: DataOwner6的最优x_6 = 0.3589
new: DataOwner7的最优x_7 = 0.2172
new: DataOwner8的最优x_8 = 0.2899
new: DataOwner9的最优x_9 = 0.2343
new: DataOwner10的最优x_10 = 0.1611
eta:2.7199999999999998
new: DataOwner1的最优x_1 = 0.2406
new: DataOwner2的最优x_2 = 0.1756
new: DataOwner3的最优x_3 = 0.2221
new: DataOwner4的最优x_4 = 0.2430
new: DataOwner5的最优x_5 = 0.2826
new: DataOwner6的最优x_6 = 0.3602
new: DataOwner7的最优x_7 = 0.2180
new: DataOwner8的最优x_8 = 0.2909
new: DataOwner9的最优x_9 = 0.2352
new: DataOwner10的最优x_10 = 0.1617
eta:2.73
new: DataOwner1的最优x_1 = 0.2415
new: DataOwner2的最优x_2 = 0.1763
new: DataOwner3的最优x_3 = 0.2230
new: DataOwner4的最优x_4 = 0.2439
new: DataOwner5的最优x_5 = 0.2837
new: DataOwner6的最优x_6 = 0.3616
new: DataOwner7的最优x_7 = 0.2188
new: DataOwner8的最优x_8 = 0.2920
new: DataOwner9的最优x_9 = 0.2360
new: DataOwner10的最优x_10 = 0.1623
eta:2.7399999999999998
new: DataOwner1的最优x_1 = 0.2424
new: DataOwner2的最优x_2 = 0.1769
new: DataOwner3的最优x_3 = 0.2238
new: DataOwner4的最优x_4 = 0.2448
new: DataOwner5的最优x_5 = 0.2847
new: DataOwner6的最优x_6 = 0.3629
new: DataOwner7的最优x_7 = 0.2196
new: DataOwner8的最优x_8 = 0.2931
new: DataOwner9的最优x_9 = 0.2369
new: DataOwner10的最优x_10 = 0.1629
eta:2.75
new: DataOwner1的最优x_1 = 0.2433
new: DataOwner2的最优x_2 = 0.1776
new: DataOwner3的最优x_3 = 0.2246
new: DataOwner4的最优x_4 = 0.2457
new: DataOwner5的最优x_5 = 0.2857
new: DataOwner6的最优x_6 = 0.3642
new: DataOwner7的最优x_7 = 0.2204
new: DataOwner8的最优x_8 = 0.2941
new: DataOwner9的最优x_9 = 0.2378
new: DataOwner10的最优x_10 = 0.1635
eta:2.76
new: DataOwner1的最优x_1 = 0.2442
new: DataOwner2的最优x_2 = 0.1782
new: DataOwner3的最优x_3 = 0.2254
new: DataOwner4的最优x_4 = 0.2466
new: DataOwner5的最优x_5 = 0.2868
new: DataOwner6的最优x_6 = 0.3655
new: DataOwner7的最优x_7 = 0.2212
new: DataOwner8的最优x_8 = 0.2952
new: DataOwner9的最优x_9 = 0.2386
new: DataOwner10的最优x_10 = 0.1641
eta:2.77
new: DataOwner1的最优x_1 = 0.2450
new: DataOwner2的最优x_2 = 0.1789
new: DataOwner3的最优x_3 = 0.2262
new: DataOwner4的最优x_4 = 0.2475
new: DataOwner5的最优x_5 = 0.2878
new: DataOwner6的最优x_6 = 0.3669
new: DataOwner7的最优x_7 = 0.2220
new: DataOwner8的最优x_8 = 0.2963
new: DataOwner9的最优x_9 = 0.2395
new: DataOwner10的最优x_10 = 0.1647
eta:2.78
new: DataOwner1的最优x_1 = 0.2459
new: DataOwner2的最优x_2 = 0.1795
new: DataOwner3的最优x_3 = 0.2270
new: DataOwner4的最优x_4 = 0.2484
new: DataOwner5的最优x_5 = 0.2889
new: DataOwner6的最优x_6 = 0.3682
new: DataOwner7的最优x_7 = 0.2228
new: DataOwner8的最优x_8 = 0.2974
new: DataOwner9的最优x_9 = 0.2403
new: DataOwner10的最优x_10 = 0.1653
eta:2.79
new: DataOwner1的最优x_1 = 0.2468
new: DataOwner2的最优x_2 = 0.1802
new: DataOwner3的最优x_3 = 0.2279
new: DataOwner4的最优x_4 = 0.2493
new: DataOwner5的最优x_5 = 0.2899
new: DataOwner6的最优x_6 = 0.3695
new: DataOwner7的最优x_7 = 0.2236
new: DataOwner8的最优x_8 = 0.2984
new: DataOwner9的最优x_9 = 0.2412
new: DataOwner10的最优x_10 = 0.1659
eta:2.8
new: DataOwner1的最优x_1 = 0.2477
new: DataOwner2的最优x_2 = 0.1808
new: DataOwner3的最优x_3 = 0.2287
new: DataOwner4的最优x_4 = 0.2502
new: DataOwner5的最优x_5 = 0.2909
new: DataOwner6的最优x_6 = 0.3708
new: DataOwner7的最优x_7 = 0.2244
new: DataOwner8的最优x_8 = 0.2995
new: DataOwner9的最优x_9 = 0.2421
new: DataOwner10的最优x_10 = 0.1664
eta:2.81
new: DataOwner1的最优x_1 = 0.2486
new: DataOwner2的最优x_2 = 0.1815
new: DataOwner3的最优x_3 = 0.2295
new: DataOwner4的最优x_4 = 0.2510
new: DataOwner5的最优x_5 = 0.2920
new: DataOwner6的最优x_6 = 0.3722
new: DataOwner7的最优x_7 = 0.2252
new: DataOwner8的最优x_8 = 0.3006
new: DataOwner9的最优x_9 = 0.2429
new: DataOwner10的最优x_10 = 0.1670
eta:2.82
new: DataOwner1的最优x_1 = 0.2495
new: DataOwner2的最优x_2 = 0.1821
new: DataOwner3的最优x_3 = 0.2303
new: DataOwner4的最优x_4 = 0.2519
new: DataOwner5的最优x_5 = 0.2930
new: DataOwner6的最优x_6 = 0.3735
new: DataOwner7的最优x_7 = 0.2260
new: DataOwner8的最优x_8 = 0.3016
new: DataOwner9的最优x_9 = 0.2438
new: DataOwner10的最优x_10 = 0.1676
eta:2.8299999999999996
new: DataOwner1的最优x_1 = 0.2503
new: DataOwner2的最优x_2 = 0.1828
new: DataOwner3的最优x_3 = 0.2311
new: DataOwner4的最优x_4 = 0.2528
new: DataOwner5的最优x_5 = 0.2940
new: DataOwner6的最优x_6 = 0.3748
new: DataOwner7的最优x_7 = 0.2268
new: DataOwner8的最优x_8 = 0.3027
new: DataOwner9的最优x_9 = 0.2447
new: DataOwner10的最优x_10 = 0.1682
eta:2.84
new: DataOwner1的最优x_1 = 0.2512
new: DataOwner2的最优x_2 = 0.1834
new: DataOwner3的最优x_3 = 0.2319
new: DataOwner4的最优x_4 = 0.2537
new: DataOwner5的最优x_5 = 0.2951
new: DataOwner6的最优x_6 = 0.3761
new: DataOwner7的最优x_7 = 0.2276
new: DataOwner8的最优x_8 = 0.3038
new: DataOwner9的最优x_9 = 0.2455
new: DataOwner10的最优x_10 = 0.1688
eta:2.8499999999999996
new: DataOwner1的最优x_1 = 0.2521
new: DataOwner2的最优x_2 = 0.1840
new: DataOwner3的最优x_3 = 0.2328
new: DataOwner4的最优x_4 = 0.2546
new: DataOwner5的最优x_5 = 0.2961
new: DataOwner6的最优x_6 = 0.3775
new: DataOwner7的最优x_7 = 0.2284
new: DataOwner8的最优x_8 = 0.3048
new: DataOwner9的最优x_9 = 0.2464
new: DataOwner10的最优x_10 = 0.1694
eta:2.86
new: DataOwner1的最优x_1 = 0.2530
new: DataOwner2的最优x_2 = 0.1847
new: DataOwner3的最优x_3 = 0.2336
new: DataOwner4的最优x_4 = 0.2555
new: DataOwner5的最优x_5 = 0.2972
new: DataOwner6的最优x_6 = 0.3788
new: DataOwner7的最优x_7 = 0.2292
new: DataOwner8的最优x_8 = 0.3059
new: DataOwner9的最优x_9 = 0.2473
new: DataOwner10的最优x_10 = 0.1700
eta:2.8699999999999997
new: DataOwner1的最优x_1 = 0.2539
new: DataOwner2的最优x_2 = 0.1853
new: DataOwner3的最优x_3 = 0.2344
new: DataOwner4的最优x_4 = 0.2564
new: DataOwner5的最优x_5 = 0.2982
new: DataOwner6的最优x_6 = 0.3801
new: DataOwner7的最优x_7 = 0.2300
new: DataOwner8的最优x_8 = 0.3070
new: DataOwner9的最优x_9 = 0.2481
new: DataOwner10的最优x_10 = 0.1706
eta:2.88
new: DataOwner1的最优x_1 = 0.2548
new: DataOwner2的最优x_2 = 0.1860
new: DataOwner3的最优x_3 = 0.2352
new: DataOwner4的最优x_4 = 0.2573
new: DataOwner5的最优x_5 = 0.2992
new: DataOwner6的最优x_6 = 0.3814
new: DataOwner7的最优x_7 = 0.2308
new: DataOwner8的最优x_8 = 0.3080
new: DataOwner9的最优x_9 = 0.2490
new: DataOwner10的最优x_10 = 0.1712
eta:2.8899999999999997
new: DataOwner1的最优x_1 = 0.2557
new: DataOwner2的最优x_2 = 0.1866
new: DataOwner3的最优x_3 = 0.2360
new: DataOwner4的最优x_4 = 0.2582
new: DataOwner5的最优x_5 = 0.3003
new: DataOwner6的最优x_6 = 0.3827
new: DataOwner7的最优x_7 = 0.2316
new: DataOwner8的最优x_8 = 0.3091
new: DataOwner9的最优x_9 = 0.2499
new: DataOwner10的最优x_10 = 0.1718
eta:2.9
new: DataOwner1的最优x_1 = 0.2565
new: DataOwner2的最优x_2 = 0.1873
new: DataOwner3的最优x_3 = 0.2369
new: DataOwner4的最优x_4 = 0.2591
new: DataOwner5的最优x_5 = 0.3013
new: DataOwner6的最优x_6 = 0.3841
new: DataOwner7的最优x_7 = 0.2324
new: DataOwner8的最优x_8 = 0.3102
new: DataOwner9的最优x_9 = 0.2507
new: DataOwner10的最优x_10 = 0.1724
eta:2.9099999999999997
new: DataOwner1的最优x_1 = 0.2574
new: DataOwner2的最优x_2 = 0.1879
new: DataOwner3的最优x_3 = 0.2377
new: DataOwner4的最优x_4 = 0.2600
new: DataOwner5的最优x_5 = 0.3024
new: DataOwner6的最优x_6 = 0.3854
new: DataOwner7的最优x_7 = 0.2332
new: DataOwner8的最优x_8 = 0.3113
new: DataOwner9的最优x_9 = 0.2516
new: DataOwner10的最优x_10 = 0.1730
eta:2.92
new: DataOwner1的最优x_1 = 0.2583
new: DataOwner2的最优x_2 = 0.1886
new: DataOwner3的最优x_3 = 0.2385
new: DataOwner4的最优x_4 = 0.2609
new: DataOwner5的最优x_5 = 0.3034
new: DataOwner6的最优x_6 = 0.3867
new: DataOwner7的最优x_7 = 0.2340
new: DataOwner8的最优x_8 = 0.3123
new: DataOwner9的最优x_9 = 0.2525
new: DataOwner10的最优x_10 = 0.1736
eta:2.9299999999999997
new: DataOwner1的最优x_1 = 0.2592
new: DataOwner2的最优x_2 = 0.1892
new: DataOwner3的最优x_3 = 0.2393
new: DataOwner4的最优x_4 = 0.2618
new: DataOwner5的最优x_5 = 0.3044
new: DataOwner6的最优x_6 = 0.3880
new: DataOwner7的最优x_7 = 0.2348
new: DataOwner8的最优x_8 = 0.3134
new: DataOwner9的最优x_9 = 0.2533
new: DataOwner10的最优x_10 = 0.1742
eta:2.94
new: DataOwner1的最优x_1 = 0.2601
new: DataOwner2的最优x_2 = 0.1899
new: DataOwner3的最优x_3 = 0.2401
new: DataOwner4的最优x_4 = 0.2627
new: DataOwner5的最优x_5 = 0.3055
new: DataOwner6的最优x_6 = 0.3894
new: DataOwner7的最优x_7 = 0.2356
new: DataOwner8的最优x_8 = 0.3145
new: DataOwner9的最优x_9 = 0.2542
new: DataOwner10的最优x_10 = 0.1748
eta:2.9499999999999997
new: DataOwner1的最优x_1 = 0.2610
new: DataOwner2的最优x_2 = 0.1905
new: DataOwner3的最优x_3 = 0.2409
new: DataOwner4的最优x_4 = 0.2636
new: DataOwner5的最优x_5 = 0.3065
new: DataOwner6的最优x_6 = 0.3907
new: DataOwner7的最优x_7 = 0.2364
new: DataOwner8的最优x_8 = 0.3155
new: DataOwner9的最优x_9 = 0.2550
new: DataOwner10的最优x_10 = 0.1754
eta:2.96
new: DataOwner1的最优x_1 = 0.2618
new: DataOwner2的最优x_2 = 0.1911
new: DataOwner3的最优x_3 = 0.2418
new: DataOwner4的最优x_4 = 0.2644
new: DataOwner5的最优x_5 = 0.3076
new: DataOwner6的最优x_6 = 0.3920
new: DataOwner7的最优x_7 = 0.2372
new: DataOwner8的最优x_8 = 0.3166
new: DataOwner9的最优x_9 = 0.2559
new: DataOwner10的最优x_10 = 0.1760
eta:2.9699999999999998
new: DataOwner1的最优x_1 = 0.2627
new: DataOwner2的最优x_2 = 0.1918
new: DataOwner3的最优x_3 = 0.2426
new: DataOwner4的最优x_4 = 0.2653
new: DataOwner5的最优x_5 = 0.3086
new: DataOwner6的最优x_6 = 0.3933
new: DataOwner7的最优x_7 = 0.2380
new: DataOwner8的最优x_8 = 0.3177
new: DataOwner9的最优x_9 = 0.2568
new: DataOwner10的最优x_10 = 0.1766
eta:2.98
new: DataOwner1的最优x_1 = 0.2636
new: DataOwner2的最优x_2 = 0.1924
new: DataOwner3的最优x_3 = 0.2434
new: DataOwner4的最优x_4 = 0.2662
new: DataOwner5的最优x_5 = 0.3096
new: DataOwner6的最优x_6 = 0.3947
new: DataOwner7的最优x_7 = 0.2388
new: DataOwner8的最优x_8 = 0.3187
new: DataOwner9的最优x_9 = 0.2576
new: DataOwner10的最优x_10 = 0.1771
eta:2.9899999999999998
new: DataOwner1的最优x_1 = 0.2645
new: DataOwner2的最优x_2 = 0.1931
new: DataOwner3的最优x_3 = 0.2442
new: DataOwner4的最优x_4 = 0.2671
new: DataOwner5的最优x_5 = 0.3107
new: DataOwner6的最优x_6 = 0.3960
new: DataOwner7的最优x_7 = 0.2396
new: DataOwner8的最优x_8 = 0.3198
new: DataOwner9的最优x_9 = 0.2585
new: DataOwner10的最优x_10 = 0.1777
eta:3.0
new: DataOwner1的最优x_1 = 0.2654
new: DataOwner2的最优x_2 = 0.1937
new: DataOwner3的最优x_3 = 0.2450
new: DataOwner4的最优x_4 = 0.2680
new: DataOwner5的最优x_5 = 0.3117
new: DataOwner6的最优x_6 = 0.3973
new: DataOwner7的最优x_7 = 0.2404
new: DataOwner8的最优x_8 = 0.3209
new: DataOwner9的最优x_9 = 0.2594
new: DataOwner10的最优x_10 = 0.1783
DONE
----- literation 3: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.313579138606405
DataOwner1的分配到的支付 ： 0.1160
DataOwner2的分配到的支付 ： 0.1441
DataOwner3的分配到的支付 ： 0.1495
DataOwner4的分配到的支付 ： 0.1116
DataOwner5的分配到的支付 ： 0.1459
DataOwner6的分配到的支付 ： 0.1768
DataOwner7的分配到的支付 ： 0.1361
DataOwner8的分配到的支付 ： 0.1545
DataOwner9的分配到的支付 ： 0.1076
DataOwner10的分配到的支付 ： 0.0714
DONE
----- literation 3: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC2
DataOwner3 把数据交给 CPC3
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC10
DataOwner9 把数据交给 CPC8
DataOwner10 把数据交给 CPC9
DONE
----- literation 3: 模型训练 -----
CPC1调整模型中, 本轮训练的数据量为：575.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.85%
Epoch 1/5, Loss: 0.6261
Epoch 2/5, Loss: 0.6121
Epoch 3/5, Loss: 0.5994
Epoch 4/5, Loss: 0.5898
Epoch 5/5, Loss: 0.5807
新模型评估：
Accuracy: 76.83%
CPC2调整模型中, 本轮训练的数据量为：1473.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.85%
Epoch 1/5, Loss: 0.6399
Epoch 2/5, Loss: 0.6235
Epoch 3/5, Loss: 0.6431
Epoch 4/5, Loss: 0.5781
Epoch 5/5, Loss: 0.5977
新模型评估：
Accuracy: 77.25%
CPC3调整模型中, 本轮训练的数据量为：1494.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.85%
Epoch 1/5, Loss: 0.6442
Epoch 2/5, Loss: 0.6151
Epoch 3/5, Loss: 0.5973
Epoch 4/5, Loss: 0.5851
Epoch 5/5, Loss: 0.5668
新模型评估：
Accuracy: 77.31%
CPC4调整模型中, 本轮训练的数据量为：1242.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.85%
Epoch 1/5, Loss: 0.6595
Epoch 2/5, Loss: 0.6326
Epoch 3/5, Loss: 0.6083
Epoch 4/5, Loss: 0.5952
Epoch 5/5, Loss: 0.5816
新模型评估：
Accuracy: 75.72%
CPC5调整模型中, 本轮训练的数据量为：176.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.85%
Epoch 1/5, Loss: 0.5433
Epoch 2/5, Loss: 0.5395
Epoch 3/5, Loss: 0.5246
Epoch 4/5, Loss: 0.5196
Epoch 5/5, Loss: 0.5159
新模型评估：
Accuracy: 75.37%
CPC6调整模型中, 本轮训练的数据量为：409.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.85%
Epoch 1/5, Loss: 0.6606
Epoch 2/5, Loss: 0.6423
Epoch 3/5, Loss: 0.6436
Epoch 4/5, Loss: 0.6336
Epoch 5/5, Loss: 0.6463
新模型评估：
Accuracy: 77.26%
CPC7调整模型中, 本轮训练的数据量为：1875.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.85%
Epoch 1/5, Loss: 0.6506
Epoch 2/5, Loss: 0.6177
Epoch 3/5, Loss: 0.5954
Epoch 4/5, Loss: 0.5762
Epoch 5/5, Loss: 0.5484
新模型评估：
Accuracy: 76.67%
CPC10调整模型中, 本轮训练的数据量为：372.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.85%
Epoch 1/5, Loss: 0.6821
Epoch 2/5, Loss: 0.6701
Epoch 3/5, Loss: 0.6606
Epoch 4/5, Loss: 0.6554
Epoch 5/5, Loss: 0.6479
新模型评估：
Accuracy: 77.33%
CPC8调整模型中, 本轮训练的数据量为：534.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.85%
Epoch 1/5, Loss: 0.6523
Epoch 2/5, Loss: 0.6303
Epoch 3/5, Loss: 0.6170
Epoch 4/5, Loss: 0.6141
Epoch 5/5, Loss: 0.6048
新模型评估：
Accuracy: 76.94%
CPC9调整模型中, 本轮训练的数据量为：183.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.85%
Epoch 1/5, Loss: 0.6356
Epoch 2/5, Loss: 0.6311
Epoch 3/5, Loss: 0.6212
Epoch 4/5, Loss: 0.6151
Epoch 5/5, Loss: 0.6081
新模型评估：
Accuracy: 76.27%
DONE
最终的列表：
[0.27678035080685637, 0.3014760054596696, 0.24480106144959393, 0.2611868060124596, 0.26054945922303613, 0.22164014196785037, 0.28599081178770003, 0.22864294143720176, 0.2252331402568338, 0.12166571028734295, 0.11166571028734294, 0.19575228966864383, 0.15450607653969717, 0.165106685872902, 0.20587755438844263, 0.15627224113236207, 0.19590916172139253, 0.241354203186488, 0.2528041353132157, 0.22526790468196162, 0.19742877396670427, 0.20526790468198178, 0.21298064292354804, 0.22056876711162623, 0.27117067960420294, 0.23537810167590933, 0.2426026866284292, 0.2497094086432512, 0.2566998697693824, 0.26357563961264513, 0.27033825620503343, 0.32953909519652475, 0.28353002892082363, 0.46780408915867117, 0.2962868919906408, 0.30250576510864985, 0.30862009535054, 0.37455513907518545, 0.32054045799758646, 0.3263490925372391, 0.39545393356021685, 0.3376695902024146, 0.34318391149291333, 0.3486025486022383, 0.3539266745561858, 0.3591574409522002, 0.5887136447301864, 0.36934339731004434, 0.44632563745549614, 0.45221320020083156, 0.45800043434717475, 0.46368859907161875, 0.46927893000503396, 0.3977741407229052, 0.4022132002008315, 0.4854749351166523, 0.4108435182750616, 0.49580474785770434, 0.419149667224323, 0.42318345618266917, 0.5993409260078756, 0.43101688584328135, 0.4348182516567981, 0.43854384450826644, 0.4421944918774138, 0.4457710077733653, 0.5379115893994405, 0.5421683140487429, 0.45606371052482053, 0.45935158090505157, 0.462569197569877, 0.46571729945388185, 0.46879661391945837, 0.47180785697005834, 0.47475173349996136, 0.4776289375062013, 0.4804401523166095, 0.48318605080646604, 0.48586729561252096, 0.48848453934395364, 0.49103842476620985, 0.49352958501908173, 0.49595864379758026, 0.49832621553953316, 0.5006329056091493, 0.5028793104738635, 0.5050660177451368, 0.5071936070145847, 0.5092626486846114, 0.5112737054656747, 0.5132273318621733, 0.5151240744641375, 0.5169644720951482, 0.5187490559587448, 0.5204783497817383, 0.5221528699538122, 0.5237731256638269, 0.5253396190332618, 0.5268528452424033, 0.5283132926768794, 0.5297214430134689, 0.5310777713800217, 0.5323827464604893, 0.5336368305874557, 0.5348404799102524, 0.5359941444535805, 0.5370982682419345, 0.5381532894225929, 0.5391596403486558, 0.5401177476843793, 0.5410280325164125, 0.5418909104366523, 0.5427067916444059, 0.5434760810379138, 0.5441991783051745, 0.5448764780132338, 0.5455083696949399, 0.5460952379345982, 0.5466374624510206, 0.5471354181796169, 0.5475894753522299, 0.5479999995754998, 0.5483673519072461, 0.5486918889332428, 0.5489739628379433, 0.5492139214788625, 0.54941210845646, 0.5495688631831563, 0.549684520951816, 0.5497594130010119, 0.5497938665810556, 0.5497882050174141, 0.5497427477732655, 0.5496578105107914, 0.5495337051512148, 0.5493707399336103, 0.549169219472871, 0.548929444816024, 0.5486517134978806, 0.5483363195954327, 0.5479835537814957, 0.5475937033756832, 0.5471670523985097, 0.5467038816182628, 0.5462044685976031, 0.5456690877655244, 0.545098010451569, 0.5444915048407273, 0.5438498362098463, 0.5431732668730564, 0.5424620561383506, 0.5417164604709668, 0.5409367335105622, 0.5401231260925567, 0.5392758862983891, 0.5383952594916788, 0.5374814883635068, 0.5365348129678686, 0.5355554707333952, 0.5345436966322432, 0.5334997229553384, 0.5324237796102413, 0.531316094023605, 0.5301768912064602, 0.5290063936845566, 0.5278048220307148, 0.5265723939343181, 0.5253093250846266, 0.5240158289749672, 0.5226921167909333, 0.5213383975168444, 0.5199548780154595, 0.5185417629939861, 0.5170992550636859, 0.5156275547625206, 0.5141268605816933, 0.5125973690279797, 0.5110392744950729, 0.5094527695885214, 0.5078380448679527, 0.5061952890073984, 0.5045246887879651, 0.5028264291376701, 0.5011006931339568, 0.4993476620887858, 0.497567515458438, 0.49576043096284694, 0.4939265846296468, 0.49206615067969484, 0.4901793016950624, 0.4882662085736298, 0.4863270404973259, 0.48436196522144637, 0.48237114860252017, 0.48035475509500114, 0.47831294753600373, 0.476245887211159, 0.47415373388315096, 0.4720366457800116, 0.46989477967075377, 0.4677282908294442, 0.46553733308795797, 0.4633220587871505, 0.4610826189470192, 0.4588191631277727, 0.45653183956917154, 0.454220794963232, 0.4518861748410221, 0.4495281233245434, 0.4471467832267373, 0.4447422960590073, 0.442314802047036, 0.43986444014522874, 0.43739134805398905, 0.43489566223314347, 0.43237751791716095, 0.42983704913016263, 0.42727438870006784, 0.42468966827288934, 0.4220830183265991, 0.4194545681922226, 0.4168044460314606, 0.4141327789224927, 0.4114396927979844, 0.4087253125003385, 0.4059897617825525, 0.403233163319904, 0.40045563872788836, 0.39765730856813786, 0.3948382923632665, 0.39199870861159036, 0.3891386747940948, 0.3862583073882444, 0.38335772187996664, 0.3804370327729658, 0.37749635360177347, 0.3745357970761112, 0.3715554744210814, 0.3685554967271738, 0.365535973622948, 0.3624970139536505, 0.35943872565784085, 0.3563612157774423, 0.3532645904841365, 0.3501489550066026, 0.34701441380574494, 0.34386107041637803, 0.34068902754251296, 0.3374983870560917, 0.33428924998489595, 0.33106171654649375, 0.32781588608247736, 0.32455185735651915, 0.32126972802655906, 0.31796959514194834, 0.3146515549507618, 0.3113157029126925, 0.3079621337600744, 0.30459094142379195, 0.3012022191231374, 0.2977960593282467, 0.2943725537818813, 0.29093179350418374, 0.2874738688007894, 0.2839988692696953, 0.28050688380991984, 0.2769980006275632, 0.2734723072435412, 0.26992989050059, 0.266370836570216, 0.2627952309595569, 0.2592031585184911, 0.25559470344577173, 0.25196994929484573, 0.24832897898591622, 0.24467187480241215, 0.24099871840640263, 0.23730959084072456, 0.23360457253686207, 0.22988374331777983, 0.22614718240884768, 0.2223949684409341, 0.21862717945655463, 0.2148438929161216, 0.2110451857035689, 0.20723113413229877, 0.20340181395029866, 0.19955730034638242, 0.19569766795520271, 0.19182299086268406, 0.18793334261164052, 0.1840287962070617, 0.18010942412104658, 0.17617529829819167, 0.1722264901608912, 0.1682630706178081, 0.16428511005030888, 0.16029267835330474, 0.15628584490759545, 0.1522646785945585]
**** log-parameter_analysis 运行时间： 2025-01-16 23:10:02 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
初始化模型的准确率：
Accuracy: 26.82%
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.05333042959028392
DataOwner2: noise random: 0.002465562356408857
DataOwner3: noise random: 0.06340355900931897
DataOwner4: noise random: 0.00916433623490257
DataOwner5: noise random: 0.05201252528923949
DataOwner6: noise random: 0.0862004153168488
DataOwner7: noise random: 0.07312518015964657
DataOwner8: noise random: 0.09617793152653337
DataOwner9: noise random: 0.06266964457055588
DataOwner10: noise random: 0.00945153699197724
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9988420936485858, 0.9999975407367466, 0.9983729131363964, 0.9999661269385938, 0.9989071757293772, 0.9969912957476931, 0.9978354893214628, 0.996256705335168, 0.9984134690677692, 0.9999637468649726]
归一化后的数据质量列表avg_f_list: [0.9691125921318742, 1.0, 0.9565704601794401, 0.9991602464481704, 0.9708523661075991, 0.9196370685600095, 0.9422040484761397, 0.9, 0.9576546012072875, 0.999096622327735]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3430
DataOwner1的最优x_1 = 0.1295
DataOwner2的最优x_2 = 0.1575
DataOwner3的最优x_3 = 0.1171
DataOwner4的最优x_4 = 0.1568
DataOwner5的最优x_5 = 0.1312
DataOwner6的最优x_6 = 0.0760
DataOwner7的最优x_7 = 0.1019
DataOwner8的最优x_8 = 0.0512
DataOwner9的最优x_9 = 0.1182
DataOwner10的最优x_10 = 0.1568
每个DataOwner应该贡献数据比例 xn_list = [0.12954339642177673, 0.15751670008586452, 0.1170526248600555, 0.1568052098457323, 0.13122202901239088, 0.07595342855446488, 0.1018651780240644, 0.05116388819165711, 0.11815990280091734, 0.156751197068638]
ModelOwner的最大效用 U(Eta) = 0.5831
DONE
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3429880115694797
DataOwner1的分配到的支付 ： 0.1453
DataOwner2的分配到的支付 ： 0.1822
DataOwner3的分配到的支付 ： 0.1295
DataOwner4的分配到的支付 ： 0.1813
DataOwner5的分配到的支付 ： 0.1474
DataOwner6的分配到的支付 ： 0.0808
DataOwner7的分配到的支付 ： 0.1110
DataOwner8的分配到的支付 ： 0.0533
DataOwner9的分配到的支付 ： 0.1309
DataOwner10的分配到的支付 ： 0.1812
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC1', 'DataOwner2': 'CPC2', 'DataOwner3': 'CPC3', 'DataOwner4': 'CPC4', 'DataOwner5': 'CPC5', 'DataOwner6': 'CPC6', 'DataOwner7': 'CPC7', 'DataOwner8': 'CPC8', 'DataOwner9': 'CPC9', 'DataOwner10': 'CPC10'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC2
DataOwner3 把数据交给 CPC3
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC8
DataOwner9 把数据交给 CPC9
DataOwner10 把数据交给 CPC10
DONE
----- literation 1: 模型训练 -----
CPC1调整模型中, 本轮训练的数据量为：190.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 26.82%
Epoch 1/5, Loss: 2.3145
Epoch 2/5, Loss: 2.3138
Epoch 3/5, Loss: 2.3127
Epoch 4/5, Loss: 2.3116
Epoch 5/5, Loss: 2.3108
新模型评估：
Accuracy: 28.34%
Model saved to ../../data/model/mnist_cnn_model
CPC2调整模型中, 本轮训练的数据量为：1613.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 28.34%
Epoch 1/5, Loss: 2.2904
Epoch 2/5, Loss: 2.2829
Epoch 3/5, Loss: 2.2690
Epoch 4/5, Loss: 2.2496
Epoch 5/5, Loss: 2.2240
新模型评估：
Accuracy: 46.55%
Model saved to ../../data/model/mnist_cnn_model
CPC3调整模型中, 本轮训练的数据量为：171.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 46.55%
Epoch 1/5, Loss: 2.2083
Epoch 2/5, Loss: 2.2073
Epoch 3/5, Loss: 2.1993
Epoch 4/5, Loss: 2.1961
Epoch 5/5, Loss: 2.1913
新模型评估：
Accuracy: 46.96%
Model saved to ../../data/model/mnist_cnn_model
CPC4调整模型中, 本轮训练的数据量为：229.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 46.96%
Epoch 1/5, Loss: 2.2022
Epoch 2/5, Loss: 2.1928
Epoch 3/5, Loss: 2.1969
Epoch 4/5, Loss: 2.1857
Epoch 5/5, Loss: 2.1835
新模型评估：
Accuracy: 47.96%
Model saved to ../../data/model/mnist_cnn_model
CPC5调整模型中, 本轮训练的数据量为：1536.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 47.96%
Epoch 1/5, Loss: 2.1588
Epoch 2/5, Loss: 2.1281
Epoch 3/5, Loss: 2.0930
Epoch 4/5, Loss: 2.0526
Epoch 5/5, Loss: 2.0055
新模型评估：
Accuracy: 48.51%
Model saved to ../../data/model/mnist_cnn_model
CPC6调整模型中, 本轮训练的数据量为：555.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 48.51%
Epoch 1/5, Loss: 1.9669
Epoch 2/5, Loss: 1.9446
Epoch 3/5, Loss: 1.9296
Epoch 4/5, Loss: 1.9101
Epoch 5/5, Loss: 1.8874
新模型评估：
Accuracy: 51.15%
Model saved to ../../data/model/mnist_cnn_model
CPC7调整模型中, 本轮训练的数据量为：596.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 51.15%
Epoch 1/5, Loss: 1.8651
Epoch 2/5, Loss: 1.8496
Epoch 3/5, Loss: 1.8246
Epoch 4/5, Loss: 1.7939
Epoch 5/5, Loss: 1.7698
新模型评估：
Accuracy: 51.78%
Model saved to ../../data/model/mnist_cnn_model
CPC8调整模型中, 本轮训练的数据量为：673.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 51.78%
Epoch 1/5, Loss: 1.8330
Epoch 2/5, Loss: 1.8025
Epoch 3/5, Loss: 1.7815
Epoch 4/5, Loss: 1.7545
Epoch 5/5, Loss: 1.7275
新模型评估：
Accuracy: 57.81%
Model saved to ../../data/model/mnist_cnn_model
CPC9调整模型中, 本轮训练的数据量为：691.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 57.81%
Epoch 1/5, Loss: 1.6731
Epoch 2/5, Loss: 1.6446
Epoch 3/5, Loss: 1.6168
Epoch 4/5, Loss: 1.5869
Epoch 5/5, Loss: 1.5603
新模型评估：
Accuracy: 61.46%
Model saved to ../../data/model/mnist_cnn_model
CPC10调整模型中, 本轮训练的数据量为：229.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 61.46%
Epoch 1/5, Loss: 1.5799
Epoch 2/5, Loss: 1.5650
Epoch 3/5, Loss: 1.5500
Epoch 4/5, Loss: 1.5480
Epoch 5/5, Loss: 1.5137
新模型评估：
Accuracy: 64.29%
Model saved to ../../data/model/mnist_cnn_model
DONE
========================= literation: 2 =========================
----- literation 2: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3430
DataOwner1的最优x_1 = 0.1295
DataOwner2的最优x_2 = 0.1575
DataOwner3的最优x_3 = 0.1171
DataOwner4的最优x_4 = 0.1568
DataOwner5的最优x_5 = 0.1312
DataOwner6的最优x_6 = 0.0760
DataOwner7的最优x_7 = 0.1019
DataOwner8的最优x_8 = 0.0512
DataOwner9的最优x_9 = 0.1182
DataOwner10的最优x_10 = 0.1568
每个DataOwner应该贡献数据比例 xn_list = [0.12954339642177673, 0.15751670008586452, 0.1170526248600555, 0.1568052098457323, 0.13122202901239088, 0.07595342855446488, 0.1018651780240644, 0.05116388819165711, 0.11815990280091734, 0.156751197068638]
ModelOwner的最大效用 U(Eta) = 0.5831
DONE
----- literation 2: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3429880115694797
DataOwner1的分配到的支付 ： 0.1453
DataOwner2的分配到的支付 ： 0.1822
DataOwner3的分配到的支付 ： 0.1295
DataOwner4的分配到的支付 ： 0.1813
DataOwner5的分配到的支付 ： 0.1474
DataOwner6的分配到的支付 ： 0.0808
DataOwner7的分配到的支付 ： 0.1110
DataOwner8的分配到的支付 ： 0.0533
DataOwner9的分配到的支付 ： 0.1309
DataOwner10的分配到的支付 ： 0.1812
DONE
----- literation 2: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC2
DataOwner3 把数据交给 CPC3
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC8
DataOwner9 把数据交给 CPC9
DataOwner10 把数据交给 CPC10
DONE
----- literation 2: 模型训练 -----
重新调整fn，进而调整xn、Eta
正在评估DataOwner1的数据质量, 本轮评估的样本数据量为：190.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 64.29%
Epoch 1/5, Loss: 1.5494
Epoch 2/5, Loss: 1.5360
Epoch 3/5, Loss: 1.5264
Epoch 4/5, Loss: 1.5163
Epoch 5/5, Loss: 1.5055
新模型评估：
Accuracy: 64.47%
loss差为：
0.04397960503896092
单位数据loss差为：
0.00023147160546821538
正在评估DataOwner2的数据质量, 本轮评估的样本数据量为：1613.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 64.29%
Epoch 1/5, Loss: 1.4776
Epoch 2/5, Loss: 1.4088
Epoch 3/5, Loss: 1.3549
Epoch 4/5, Loss: 1.2975
Epoch 5/5, Loss: 1.2385
新模型评估：
Accuracy: 70.20%
loss差为：
0.2391393138812139
单位数据loss差为：
0.00014825747915760316
正在评估DataOwner3的数据质量, 本轮评估的样本数据量为：171.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 64.29%
Epoch 1/5, Loss: 1.4877
Epoch 2/5, Loss: 1.4864
Epoch 3/5, Loss: 1.4687
Epoch 4/5, Loss: 1.4386
Epoch 5/5, Loss: 1.4537
新模型评估：
Accuracy: 65.52%
loss差为：
0.03403902053833008
单位数据loss差为：
0.0001990585996393572
正在评估DataOwner4的数据质量, 本轮评估的样本数据量为：229.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 64.29%
Epoch 1/5, Loss: 1.5171
Epoch 2/5, Loss: 1.5096
Epoch 3/5, Loss: 1.4935
Epoch 4/5, Loss: 1.4785
Epoch 5/5, Loss: 1.4705
新模型评估：
Accuracy: 66.33%
loss差为：
0.04659178853034973
单位数据loss差为：
0.00020345759183558834
正在评估DataOwner5的数据质量, 本轮评估的样本数据量为：1536.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 64.29%
Epoch 1/5, Loss: 1.4907
Epoch 2/5, Loss: 1.4303
Epoch 3/5, Loss: 1.3721
Epoch 4/5, Loss: 1.3143
Epoch 5/5, Loss: 1.2565
新模型评估：
Accuracy: 69.10%
loss差为：
0.2341781655947368
单位数据loss差为：
0.00015245974322574012
正在评估DataOwner6的数据质量, 本轮评估的样本数据量为：555.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 64.29%
Epoch 1/5, Loss: 1.5513
Epoch 2/5, Loss: 1.5278
Epoch 3/5, Loss: 1.5020
Epoch 4/5, Loss: 1.4780
Epoch 5/5, Loss: 1.4599
新模型评估：
Accuracy: 67.50%
loss差为：
0.09141357739766454
单位数据loss差为：
0.00016470914846426041
正在评估DataOwner7的数据质量, 本轮评估的样本数据量为：596.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 64.29%
Epoch 1/5, Loss: 1.4908
Epoch 2/5, Loss: 1.4577
Epoch 3/5, Loss: 1.4445
Epoch 4/5, Loss: 1.4186
Epoch 5/5, Loss: 1.3830
新模型评估：
Accuracy: 65.97%
loss差为：
0.10783271789550786
单位数据loss差为：
0.00018092737901930848
正在评估DataOwner8的数据质量, 本轮评估的样本数据量为：673.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 64.29%
Epoch 1/5, Loss: 1.5348
Epoch 2/5, Loss: 1.5126
Epoch 3/5, Loss: 1.4844
Epoch 4/5, Loss: 1.4583
Epoch 5/5, Loss: 1.4321
新模型评估：
Accuracy: 65.14%
loss差为：
0.10274474187330762
单位数据loss差为：
0.00015266677841501875
正在评估DataOwner9的数据质量, 本轮评估的样本数据量为：691.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 64.29%
Epoch 1/5, Loss: 1.5062
Epoch 2/5, Loss: 1.4764
Epoch 3/5, Loss: 1.4494
Epoch 4/5, Loss: 1.4223
Epoch 5/5, Loss: 1.3934
新模型评估：
Accuracy: 66.75%
loss差为：
0.11275283856825391
单位数据loss差为：
0.0001631734277398754
正在评估DataOwner10的数据质量, 本轮评估的样本数据量为：229.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 64.29%
Epoch 1/5, Loss: 1.5270
Epoch 2/5, Loss: 1.5201
Epoch 3/5, Loss: 1.5035
Epoch 4/5, Loss: 1.5025
Epoch 5/5, Loss: 1.4686
新模型评估：
Accuracy: 67.21%
loss差为：
0.05846148729324341
单位数据loss差为：
0.0002552903375250804
经过服务器调节后的真实数据质量：
数据质量列表avg_f_list: [0.00023147160546821538, 0.00014825747915760316, 0.0001990585996393572, 0.00020345759183558834, 0.00015245974322574012, 0.00016470914846426041, 0.00018092737901930848, 0.00015266677841501875, 0.0001631734277398754, 0.0002552903375250804]
归一化后的数据质量列表avg_f_list:[0.9777463365734961, 0.9, 0.9474631073640376, 0.9515730529109724, 0.9039261439264841, 0.9153706717335097, 0.9305232433852597, 0.9041195753572021, 0.9139358593330855, 1.0]
CPC1调整模型中, 本轮训练的数据量为：190.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 64.29%
Epoch 1/5, Loss: 1.5483
Epoch 2/5, Loss: 1.5379
Epoch 3/5, Loss: 1.5262
Epoch 4/5, Loss: 1.5166
Epoch 5/5, Loss: 1.5096
新模型评估：
Accuracy: 64.37%
Model saved to ../../data/model/mnist_cnn_model
CPC2调整模型中, 本轮训练的数据量为：1613.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 64.37%
Epoch 1/5, Loss: 1.4465
Epoch 2/5, Loss: 1.3882
Epoch 3/5, Loss: 1.3259
Epoch 4/5, Loss: 1.2715
Epoch 5/5, Loss: 1.2049
新模型评估：
Accuracy: 70.18%
Model saved to ../../data/model/mnist_cnn_model
CPC3调整模型中, 本轮训练的数据量为：171.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 70.18%
Epoch 1/5, Loss: 1.1754
Epoch 2/5, Loss: 1.1752
Epoch 3/5, Loss: 1.1615
Epoch 4/5, Loss: 1.1584
Epoch 5/5, Loss: 1.1448
新模型评估：
Accuracy: 71.87%
Model saved to ../../data/model/mnist_cnn_model
CPC4调整模型中, 本轮训练的数据量为：229.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 71.87%
Epoch 1/5, Loss: 1.1566
Epoch 2/5, Loss: 1.1423
Epoch 3/5, Loss: 1.1585
Epoch 4/5, Loss: 1.1342
Epoch 5/5, Loss: 1.1093
新模型评估：
Accuracy: 72.81%
Model saved to ../../data/model/mnist_cnn_model
CPC5调整模型中, 本轮训练的数据量为：1536.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 72.81%
Epoch 1/5, Loss: 1.1005
Epoch 2/5, Loss: 1.0475
Epoch 3/5, Loss: 0.9983
Epoch 4/5, Loss: 0.9510
Epoch 5/5, Loss: 0.9065
新模型评估：
Accuracy: 74.02%
Model saved to ../../data/model/mnist_cnn_model
CPC6调整模型中, 本轮训练的数据量为：555.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 74.02%
Epoch 1/5, Loss: 0.9513
Epoch 2/5, Loss: 0.9300
Epoch 3/5, Loss: 0.9131
Epoch 4/5, Loss: 0.8948
Epoch 5/5, Loss: 0.8798
新模型评估：
Accuracy: 75.50%
Model saved to ../../data/model/mnist_cnn_model
CPC7调整模型中, 本轮训练的数据量为：596.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 75.50%
Epoch 1/5, Loss: 0.7819
Epoch 2/5, Loss: 0.7818
Epoch 3/5, Loss: 0.7670
Epoch 4/5, Loss: 0.7503
Epoch 5/5, Loss: 0.7342
新模型评估：
Accuracy: 74.75%
CPC8调整模型中, 本轮训练的数据量为：673.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 75.50%
Epoch 1/5, Loss: 0.8708
Epoch 2/5, Loss: 0.8595
Epoch 3/5, Loss: 0.8408
Epoch 4/5, Loss: 0.8270
Epoch 5/5, Loss: 0.7979
新模型评估：
Accuracy: 74.32%
CPC9调整模型中, 本轮训练的数据量为：691.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 75.50%
Epoch 1/5, Loss: 0.8265
Epoch 2/5, Loss: 0.8062
Epoch 3/5, Loss: 0.7884
Epoch 4/5, Loss: 0.7699
Epoch 5/5, Loss: 0.7548
新模型评估：
Accuracy: 74.92%
CPC10调整模型中, 本轮训练的数据量为：229.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 75.50%
Epoch 1/5, Loss: 0.8832
Epoch 2/5, Loss: 0.8877
Epoch 3/5, Loss: 0.8696
Epoch 4/5, Loss: 0.8633
Epoch 5/5, Loss: 0.8453
新模型评估：
Accuracy: 78.18%
Model saved to ../../data/model/mnist_cnn_model
DONE
========================= literation: 3 =========================
----- literation 3: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3096
DataOwner1的最优x_1 = 0.1585
DataOwner2的最优x_2 = 0.0814
DataOwner3的最优x_3 = 0.1316
DataOwner4的最优x_4 = 0.1355
DataOwner5的最优x_5 = 0.0860
DataOwner6的最优x_6 = 0.0989
DataOwner7的最优x_7 = 0.1150
DataOwner8的最优x_8 = 0.0862
DataOwner9的最优x_9 = 0.0973
DataOwner10的最优x_10 = 0.1760
每个DataOwner应该贡献数据比例 xn_list = [0.15846850607603902, 0.08143974091942317, 0.13164928852206384, 0.13550773114846237, 0.08601982641779658, 0.09890790460593121, 0.11496417456677381, 0.08624334044626925, 0.09732901295227193, 0.17597495422881376]
ModelOwner的最大效用 U(Eta) = 0.5454
Eta开始变化：
eta:0.01
**** log-parameter_analysis 运行时间： 2025-01-16 23:16:20 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
**** log-parameter_analysis 运行时间： 2025-01-16 23:17:03 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
初始化模型的准确率：
Accuracy: 26.82%
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.05061304292405353
DataOwner2: noise random: 0.07209495379438274
DataOwner3: noise random: 0.019392753636367478
DataOwner4: noise random: 0.03150579655057539
DataOwner5: noise random: 0.029215295941000398
DataOwner6: noise random: 0.09730857922041566
DataOwner7: noise random: 0.03401022819651341
DataOwner8: noise random: 0.04524425263152835
DataOwner9: noise random: 0.05335695663353808
DataOwner10: noise random: 0.01587623647847064
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9989652208362877, 0.9978953534555668, 0.9998479345454895, 0.9995987401835525, 0.9996558357396389, 0.9961663541405955, 0.9995326053834194, 0.9991712219260226, 0.9988512893050251, 0.9998980397804336]
归一化后的数据质量列表avg_f_list: [0.9750027458318714, 0.9463329305264394, 0.9986573028979402, 0.9919795066956925, 0.9935095272171646, 0.9, 0.990207256658681, 0.980523068539007, 0.9719496609190826, 1.0]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3591
DataOwner1的最优x_1 = 0.1234
DataOwner2的最优x_2 = 0.0929
DataOwner3的最优x_3 = 0.1459
DataOwner4的最优x_4 = 0.1398
DataOwner5的最优x_5 = 0.1412
DataOwner6的最优x_6 = 0.0346
DataOwner7的最优x_7 = 0.1381
DataOwner8的最优x_8 = 0.1289
DataOwner9的最优x_9 = 0.1204
DataOwner10的最优x_10 = 0.1471
每个DataOwner应该贡献数据比例 xn_list = [0.12342877930136804, 0.09288388904264357, 0.14590554092513597, 0.13979238029042346, 0.14120869892531246, 0.03455184982718184, 0.1381400383174937, 0.12888290666747879, 0.12035543424392725, 0.14711349811976918]
ModelOwner的最大效用 U(Eta) = 0.6021
DONE
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3591057383755207
DataOwner1的分配到的支付 ： 0.1373
DataOwner2的分配到的支付 ： 0.1003
DataOwner3的分配到的支付 ： 0.1662
DataOwner4的分配到的支付 ： 0.1582
DataOwner5的分配到的支付 ： 0.1601
DataOwner6的分配到的支付 ： 0.0355
DataOwner7的分配到的支付 ： 0.1561
DataOwner8的分配到的支付 ： 0.1442
DataOwner9的分配到的支付 ： 0.1335
DataOwner10的分配到的支付 ： 0.1678
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC1', 'DataOwner2': 'CPC2', 'DataOwner3': 'CPC3', 'DataOwner4': 'CPC4', 'DataOwner5': 'CPC5', 'DataOwner6': 'CPC6', 'DataOwner7': 'CPC7', 'DataOwner8': 'CPC8', 'DataOwner9': 'CPC9', 'DataOwner10': 'CPC10'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC2
DataOwner3 把数据交给 CPC3
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC8
DataOwner9 把数据交给 CPC9
DataOwner10 把数据交给 CPC10
DONE
----- literation 1: 模型训练 -----
CPC1调整模型中, 本轮训练的数据量为：978.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 26.82%
Epoch 1/5, Loss: 2.2987
Epoch 2/5, Loss: 2.2939
Epoch 3/5, Loss: 2.2885
Epoch 4/5, Loss: 2.2846
Epoch 5/5, Loss: 2.2765
新模型评估：
Accuracy: 36.64%
Model saved to ../../../data/model/mnist_cnn_model
CPC2调整模型中, 本轮训练的数据量为：1366.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 36.64%
Epoch 1/5, Loss: 2.2655
Epoch 2/5, Loss: 2.2522
Epoch 3/5, Loss: 2.2340
Epoch 4/5, Loss: 2.2144
Epoch 5/5, Loss: 2.1843
新模型评估：
Accuracy: 45.30%
Model saved to ../../../data/model/mnist_cnn_model
CPC3调整模型中, 本轮训练的数据量为：660.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 45.30%
Epoch 1/5, Loss: 2.1653
Epoch 2/5, Loss: 2.1494
Epoch 3/5, Loss: 2.1301
Epoch 4/5, Loss: 2.1140
Epoch 5/5, Loss: 2.0990
新模型评估：
Accuracy: 49.87%
Model saved to ../../../data/model/mnist_cnn_model
CPC4调整模型中, 本轮训练的数据量为：316.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 49.87%
Epoch 1/5, Loss: 2.0970
Epoch 2/5, Loss: 2.0875
Epoch 3/5, Loss: 2.0784
Epoch 4/5, Loss: 2.0698
Epoch 5/5, Loss: 2.0610
新模型评估：
Accuracy: 50.24%
Model saved to ../../../data/model/mnist_cnn_model
CPC5调整模型中, 本轮训练的数据量为：319.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 50.24%
Epoch 1/5, Loss: 2.0477
Epoch 2/5, Loss: 2.0383
Epoch 3/5, Loss: 2.0292
Epoch 4/5, Loss: 2.0199
Epoch 5/5, Loss: 2.0099
新模型评估：
Accuracy: 50.42%
Model saved to ../../../data/model/mnist_cnn_model
CPC6调整模型中, 本轮训练的数据量为：273.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 50.42%
Epoch 1/5, Loss: 2.0450
Epoch 2/5, Loss: 2.0356
Epoch 3/5, Loss: 2.0135
Epoch 4/5, Loss: 2.0091
Epoch 5/5, Loss: 2.0233
新模型评估：
Accuracy: 51.16%
Model saved to ../../../data/model/mnist_cnn_model
CPC7调整模型中, 本轮训练的数据量为：312.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 51.16%
Epoch 1/5, Loss: 1.9815
Epoch 2/5, Loss: 1.9713
Epoch 3/5, Loss: 1.9574
Epoch 4/5, Loss: 1.9449
Epoch 5/5, Loss: 1.9339
新模型评估：
Accuracy: 52.89%
Model saved to ../../../data/model/mnist_cnn_model
CPC8调整模型中, 本轮训练的数据量为：875.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 52.89%
Epoch 1/5, Loss: 1.9059
Epoch 2/5, Loss: 1.8731
Epoch 3/5, Loss: 1.8419
Epoch 4/5, Loss: 1.8121
Epoch 5/5, Loss: 1.7771
新模型评估：
Accuracy: 58.15%
Model saved to ../../../data/model/mnist_cnn_model
CPC9调整模型中, 本轮训练的数据量为：408.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 58.15%
Epoch 1/5, Loss: 1.7874
Epoch 2/5, Loss: 1.7737
Epoch 3/5, Loss: 1.7501
Epoch 4/5, Loss: 1.7424
Epoch 5/5, Loss: 1.7193
新模型评估：
Accuracy: 59.21%
Model saved to ../../../data/model/mnist_cnn_model
CPC10调整模型中, 本轮训练的数据量为：1165.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 59.21%
Epoch 1/5, Loss: 1.6959
Epoch 2/5, Loss: 1.6506
Epoch 3/5, Loss: 1.5954
Epoch 4/5, Loss: 1.5524
Epoch 5/5, Loss: 1.5081
新模型评估：
Accuracy: 67.93%
Model saved to ../../../data/model/mnist_cnn_model
DONE
========================= literation: 2 =========================
----- literation 2: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3591
DataOwner1的最优x_1 = 0.1234
DataOwner2的最优x_2 = 0.0929
DataOwner3的最优x_3 = 0.1459
DataOwner4的最优x_4 = 0.1398
DataOwner5的最优x_5 = 0.1412
DataOwner6的最优x_6 = 0.0346
DataOwner7的最优x_7 = 0.1381
DataOwner8的最优x_8 = 0.1289
DataOwner9的最优x_9 = 0.1204
DataOwner10的最优x_10 = 0.1471
每个DataOwner应该贡献数据比例 xn_list = [0.12342877930136804, 0.09288388904264357, 0.14590554092513597, 0.13979238029042346, 0.14120869892531246, 0.03455184982718184, 0.1381400383174937, 0.12888290666747879, 0.12035543424392725, 0.14711349811976918]
ModelOwner的最大效用 U(Eta) = 0.6021
DONE
----- literation 2: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3591057383755207
DataOwner1的分配到的支付 ： 0.1373
DataOwner2的分配到的支付 ： 0.1003
DataOwner3的分配到的支付 ： 0.1662
DataOwner4的分配到的支付 ： 0.1582
DataOwner5的分配到的支付 ： 0.1601
DataOwner6的分配到的支付 ： 0.0355
DataOwner7的分配到的支付 ： 0.1561
DataOwner8的分配到的支付 ： 0.1442
DataOwner9的分配到的支付 ： 0.1335
DataOwner10的分配到的支付 ： 0.1678
DONE
----- literation 2: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC2
DataOwner3 把数据交给 CPC3
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC8
DataOwner9 把数据交给 CPC9
DataOwner10 把数据交给 CPC10
DONE
----- literation 2: 模型训练 -----
重新调整fn，进而调整xn、Eta
正在评估DataOwner1的数据质量, 本轮评估的样本数据量为：978.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 67.93%
Epoch 1/5, Loss: 1.4837
Epoch 2/5, Loss: 1.4330
Epoch 3/5, Loss: 1.3941
Epoch 4/5, Loss: 1.3462
Epoch 5/5, Loss: 1.3222
新模型评估：
Accuracy: 71.69%
loss差为：
0.1614663526415825
单位数据loss差为：
0.00016509852008341768
正在评估DataOwner2的数据质量, 本轮评估的样本数据量为：1366.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 67.93%
Epoch 1/5, Loss: 1.4399
Epoch 2/5, Loss: 1.3841
Epoch 3/5, Loss: 1.3272
Epoch 4/5, Loss: 1.2779
Epoch 5/5, Loss: 1.2348
新模型评估：
Accuracy: 71.47%
loss差为：
0.2050615820017727
单位数据loss差为：
0.00015011828843467987
正在评估DataOwner3的数据质量, 本轮评估的样本数据量为：660.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 67.93%
Epoch 1/5, Loss: 1.4879
Epoch 2/5, Loss: 1.4557
Epoch 3/5, Loss: 1.4271
Epoch 4/5, Loss: 1.3910
Epoch 5/5, Loss: 1.3614
新模型评估：
Accuracy: 68.34%
loss差为：
0.126499436118386
单位数据loss差为：
0.00019166581230058483
正在评估DataOwner4的数据质量, 本轮评估的样本数据量为：316.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 67.93%
Epoch 1/5, Loss: 1.4778
Epoch 2/5, Loss: 1.4645
Epoch 3/5, Loss: 1.4483
Epoch 4/5, Loss: 1.4336
Epoch 5/5, Loss: 1.4194
新模型评估：
Accuracy: 68.08%
loss差为：
0.05844395160675053
单位数据loss差为：
0.00018494921394541307
正在评估DataOwner5的数据质量, 本轮评估的样本数据量为：319.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 67.93%
Epoch 1/5, Loss: 1.5024
Epoch 2/5, Loss: 1.4843
Epoch 3/5, Loss: 1.4677
Epoch 4/5, Loss: 1.4516
Epoch 5/5, Loss: 1.4362
新模型评估：
Accuracy: 65.47%
loss差为：
0.0661638498306274
单位数据loss差为：
0.0002074101875568257
正在评估DataOwner6的数据质量, 本轮评估的样本数据量为：273.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 67.93%
Epoch 1/5, Loss: 1.4951
Epoch 2/5, Loss: 1.4399
Epoch 3/5, Loss: 1.4396
Epoch 4/5, Loss: 1.4224
Epoch 5/5, Loss: 1.4084
新模型评估：
Accuracy: 68.15%
loss差为：
0.0866431951522828
单位数据loss差为：
0.00031737434121715315
正在评估DataOwner7的数据质量, 本轮评估的样本数据量为：312.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 67.93%
Epoch 1/5, Loss: 1.4629
Epoch 2/5, Loss: 1.4473
Epoch 3/5, Loss: 1.4318
Epoch 4/5, Loss: 1.4144
Epoch 5/5, Loss: 1.4059
新模型评估：
Accuracy: 67.56%
loss差为：
0.056969642639160156
单位数据loss差为：
0.00018259500845884666
正在评估DataOwner8的数据质量, 本轮评估的样本数据量为：875.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 67.93%
Epoch 1/5, Loss: 1.4451
Epoch 2/5, Loss: 1.4072
Epoch 3/5, Loss: 1.3682
Epoch 4/5, Loss: 1.3292
Epoch 5/5, Loss: 1.2947
新模型评估：
Accuracy: 69.23%
loss差为：
0.15036771127155846
单位数据loss差为：
0.0001718488128817811
正在评估DataOwner9的数据质量, 本轮评估的样本数据量为：408.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 67.93%
Epoch 1/5, Loss: 1.4493
Epoch 2/5, Loss: 1.4253
Epoch 3/5, Loss: 1.4159
Epoch 4/5, Loss: 1.4016
Epoch 5/5, Loss: 1.3587
新模型评估：
Accuracy: 65.89%
loss差为：
0.09057218687874924
单位数据loss差为：
0.00022199065411458146
正在评估DataOwner10的数据质量, 本轮评估的样本数据量为：1165.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 67.93%
Epoch 1/5, Loss: 1.4368
Epoch 2/5, Loss: 1.3944
Epoch 3/5, Loss: 1.3461
Epoch 4/5, Loss: 1.2989
Epoch 5/5, Loss: 1.2595
新模型评估：
Accuracy: 69.78%
loss差为：
0.17734144863329426
单位数据loss差为：
0.0001522244194277204
经过服务器调节后的真实数据质量：
数据质量列表avg_f_list: [0.00016509852008341768, 0.00015011828843467987, 0.00019166581230058483, 0.00018494921394541307, 0.0002074101875568257, 0.00031737434121715315, 0.00018259500845884666, 0.0001718488128817811, 0.00022199065411458146, 0.0001522244194277204]
归一化后的数据质量列表avg_f_list:[0.9089564660886865, 0.9, 0.9248406698440624, 0.9208249118230913, 0.9342540064583836, 1.0, 0.9194173660587368, 0.9129923695349688, 0.942971458721064, 0.9012592255753995]
CPC1调整模型中, 本轮训练的数据量为：978.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 67.93%
Epoch 1/5, Loss: 1.4910
Epoch 2/5, Loss: 1.4359
Epoch 3/5, Loss: 1.4020
Epoch 4/5, Loss: 1.3575
Epoch 5/5, Loss: 1.3264
新模型评估：
Accuracy: 71.41%
Model saved to ../../../data/model/mnist_cnn_model
CPC2调整模型中, 本轮训练的数据量为：1366.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 71.41%
Epoch 1/5, Loss: 1.2505
Epoch 2/5, Loss: 1.1988
Epoch 3/5, Loss: 1.1505
Epoch 4/5, Loss: 1.1056
Epoch 5/5, Loss: 1.0624
新模型评估：
Accuracy: 74.43%
Model saved to ../../../data/model/mnist_cnn_model
CPC3调整模型中, 本轮训练的数据量为：660.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 74.43%
Epoch 1/5, Loss: 1.0832
Epoch 2/5, Loss: 1.0564
Epoch 3/5, Loss: 1.0293
Epoch 4/5, Loss: 1.0100
Epoch 5/5, Loss: 0.9703
新模型评估：
Accuracy: 74.20%
CPC4调整模型中, 本轮训练的数据量为：316.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 74.43%
Epoch 1/5, Loss: 1.0591
Epoch 2/5, Loss: 1.0413
Epoch 3/5, Loss: 1.0292
Epoch 4/5, Loss: 1.0155
Epoch 5/5, Loss: 1.0013
新模型评估：
Accuracy: 74.22%
CPC5调整模型中, 本轮训练的数据量为：319.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 74.43%
Epoch 1/5, Loss: 1.0742
Epoch 2/5, Loss: 1.0561
Epoch 3/5, Loss: 1.0426
Epoch 4/5, Loss: 1.0289
Epoch 5/5, Loss: 1.0165
新模型评估：
Accuracy: 71.38%
CPC6调整模型中, 本轮训练的数据量为：273.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 74.43%
Epoch 1/5, Loss: 1.0710
Epoch 2/5, Loss: 1.0600
Epoch 3/5, Loss: 1.0073
Epoch 4/5, Loss: 1.0426
Epoch 5/5, Loss: 0.9997
新模型评估：
Accuracy: 74.84%
Model saved to ../../../data/model/mnist_cnn_model
CPC7调整模型中, 本轮训练的数据量为：312.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 74.84%
Epoch 1/5, Loss: 1.0129
Epoch 2/5, Loss: 1.0023
Epoch 3/5, Loss: 0.9872
Epoch 4/5, Loss: 0.9768
Epoch 5/5, Loss: 0.9630
新模型评估：
Accuracy: 74.68%
CPC8调整模型中, 本轮训练的数据量为：875.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 74.84%
Epoch 1/5, Loss: 0.9860
Epoch 2/5, Loss: 0.9572
Epoch 3/5, Loss: 0.9218
Epoch 4/5, Loss: 0.9010
Epoch 5/5, Loss: 0.8784
新模型评估：
Accuracy: 75.47%
Model saved to ../../../data/model/mnist_cnn_model
CPC9调整模型中, 本轮训练的数据量为：408.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 75.47%
Epoch 1/5, Loss: 0.8914
Epoch 2/5, Loss: 0.8496
Epoch 3/5, Loss: 0.8779
Epoch 4/5, Loss: 0.8420
Epoch 5/5, Loss: 0.8206
新模型评估：
Accuracy: 73.89%
CPC10调整模型中, 本轮训练的数据量为：1165.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 75.47%
Epoch 1/5, Loss: 0.8597
Epoch 2/5, Loss: 0.8260
Epoch 3/5, Loss: 0.7921
Epoch 4/5, Loss: 0.7683
Epoch 5/5, Loss: 0.7393
新模型评估：
Accuracy: 75.82%
Model saved to ../../../data/model/mnist_cnn_model
DONE
========================= literation: 3 =========================
----- literation 3: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.2998
DataOwner1的最优x_1 = 0.0993
DataOwner2的最优x_2 = 0.0893
DataOwner3的最优x_3 = 0.1160
DataOwner4的最优x_4 = 0.1119
DataOwner5的最优x_5 = 0.1254
DataOwner6的最优x_6 = 0.1807
DataOwner7的最优x_7 = 0.1105
DataOwner8的最优x_8 = 0.1037
DataOwner9的最优x_9 = 0.1337
DataOwner10的最优x_10 = 0.0908
每个DataOwner应该贡献数据比例 xn_list = [0.09931525674025675, 0.08932699431354675, 0.11604472564701845, 0.11193011868650679, 0.12539768152589625, 0.18065186664736363, 0.1104698509467405, 0.10368266646184639, 0.1337070536244652, 0.09075643941680754]
ModelOwner的最大效用 U(Eta) = 0.5347
Eta开始变化：
eta:0.01
eta:0.02
eta:0.03
eta:0.04
eta:0.05
eta:0.060000000000000005
eta:0.06999999999999999
eta:0.08
eta:0.09
eta:0.09999999999999999
eta:0.11
eta:0.12
eta:0.13
eta:0.14
eta:0.15000000000000002
eta:0.16
new: DataOwner1的最优x_1 = 0.0122
new: DataOwner2的最优x_2 = 0.0110
new: DataOwner3的最优x_3 = 0.0143
new: DataOwner4的最优x_4 = 0.0138
new: DataOwner5的最优x_5 = 0.0154
new: DataOwner6的最优x_6 = 0.0222
new: DataOwner7的最优x_7 = 0.0136
new: DataOwner8的最优x_8 = 0.0128
new: DataOwner9的最优x_9 = 0.0165
new: DataOwner10的最优x_10 = 0.0112
eta:0.17
new: DataOwner1的最优x_1 = 0.0130
new: DataOwner2的最优x_2 = 0.0117
new: DataOwner3的最优x_3 = 0.0152
new: DataOwner4的最优x_4 = 0.0146
new: DataOwner5的最优x_5 = 0.0164
new: DataOwner6的最优x_6 = 0.0236
new: DataOwner7的最优x_7 = 0.0144
new: DataOwner8的最优x_8 = 0.0136
new: DataOwner9的最优x_9 = 0.0175
new: DataOwner10的最优x_10 = 0.0119
eta:0.18000000000000002
eta:0.19
eta:0.2
eta:0.21000000000000002
eta:0.22
eta:0.23
eta:0.24000000000000002
new: DataOwner1的最优x_1 = 0.0183
new: DataOwner2的最优x_2 = 0.0165
new: DataOwner3的最优x_3 = 0.0214
new: DataOwner4的最优x_4 = 0.0207
new: DataOwner5的最优x_5 = 0.0232
new: DataOwner6的最优x_6 = 0.0334
new: DataOwner7的最优x_7 = 0.0204
new: DataOwner8的最优x_8 = 0.0191
new: DataOwner9的最优x_9 = 0.0247
new: DataOwner10的最优x_10 = 0.0168
eta:0.25
eta:0.26
new: DataOwner1的最优x_1 = 0.0199
new: DataOwner2的最优x_2 = 0.0179
new: DataOwner3的最优x_3 = 0.0232
new: DataOwner4的最优x_4 = 0.0224
new: DataOwner5的最优x_5 = 0.0251
new: DataOwner6的最优x_6 = 0.0361
new: DataOwner7的最优x_7 = 0.0221
new: DataOwner8的最优x_8 = 0.0207
new: DataOwner9的最优x_9 = 0.0267
new: DataOwner10的最优x_10 = 0.0182
eta:0.27
new: DataOwner1的最优x_1 = 0.0206
new: DataOwner2的最优x_2 = 0.0186
new: DataOwner3的最优x_3 = 0.0241
new: DataOwner4的最优x_4 = 0.0233
new: DataOwner5的最优x_5 = 0.0260
new: DataOwner6的最优x_6 = 0.0375
new: DataOwner7的最优x_7 = 0.0229
new: DataOwner8的最优x_8 = 0.0215
new: DataOwner9的最优x_9 = 0.0278
new: DataOwner10的最优x_10 = 0.0189
eta:0.28
new: DataOwner1的最优x_1 = 0.0214
new: DataOwner2的最优x_2 = 0.0192
new: DataOwner3的最优x_3 = 0.0250
new: DataOwner4的最优x_4 = 0.0241
new: DataOwner5的最优x_5 = 0.0270
new: DataOwner6的最优x_6 = 0.0389
new: DataOwner7的最优x_7 = 0.0238
new: DataOwner8的最优x_8 = 0.0223
new: DataOwner9的最优x_9 = 0.0288
new: DataOwner10的最优x_10 = 0.0196
eta:0.29000000000000004
new: DataOwner1的最优x_1 = 0.0222
new: DataOwner2的最优x_2 = 0.0199
new: DataOwner3的最优x_3 = 0.0259
new: DataOwner4的最优x_4 = 0.0250
new: DataOwner5的最优x_5 = 0.0280
new: DataOwner6的最优x_6 = 0.0403
new: DataOwner7的最优x_7 = 0.0246
new: DataOwner8的最优x_8 = 0.0231
new: DataOwner9的最优x_9 = 0.0298
new: DataOwner10的最优x_10 = 0.0202
eta:0.3
new: DataOwner1的最优x_1 = 0.0229
new: DataOwner2的最优x_2 = 0.0206
new: DataOwner3的最优x_3 = 0.0268
new: DataOwner4的最优x_4 = 0.0258
new: DataOwner5的最优x_5 = 0.0289
new: DataOwner6的最优x_6 = 0.0417
new: DataOwner7的最优x_7 = 0.0255
new: DataOwner8的最优x_8 = 0.0239
new: DataOwner9的最优x_9 = 0.0309
new: DataOwner10的最优x_10 = 0.0209
eta:0.31
new: DataOwner1的最优x_1 = 0.0237
new: DataOwner2的最优x_2 = 0.0213
new: DataOwner3的最优x_3 = 0.0277
new: DataOwner4的最优x_4 = 0.0267
new: DataOwner5的最优x_5 = 0.0299
new: DataOwner6的最优x_6 = 0.0431
new: DataOwner7的最优x_7 = 0.0263
new: DataOwner8的最优x_8 = 0.0247
new: DataOwner9的最优x_9 = 0.0319
new: DataOwner10的最优x_10 = 0.0216
eta:0.32
new: DataOwner1的最优x_1 = 0.0245
new: DataOwner2的最优x_2 = 0.0220
new: DataOwner3的最优x_3 = 0.0286
new: DataOwner4的最优x_4 = 0.0276
new: DataOwner5的最优x_5 = 0.0309
new: DataOwner6的最优x_6 = 0.0445
new: DataOwner7的最优x_7 = 0.0272
new: DataOwner8的最优x_8 = 0.0255
new: DataOwner9的最优x_9 = 0.0329
new: DataOwner10的最优x_10 = 0.0223
eta:0.33
new: DataOwner1的最优x_1 = 0.0252
new: DataOwner2的最优x_2 = 0.0227
new: DataOwner3的最优x_3 = 0.0295
new: DataOwner4的最优x_4 = 0.0284
new: DataOwner5的最优x_5 = 0.0318
new: DataOwner6的最优x_6 = 0.0459
new: DataOwner7的最优x_7 = 0.0280
new: DataOwner8的最优x_8 = 0.0263
new: DataOwner9的最优x_9 = 0.0339
new: DataOwner10的最优x_10 = 0.0230
eta:0.34
new: DataOwner1的最优x_1 = 0.0260
new: DataOwner2的最优x_2 = 0.0234
new: DataOwner3的最优x_3 = 0.0304
new: DataOwner4的最优x_4 = 0.0293
new: DataOwner5的最优x_5 = 0.0328
new: DataOwner6的最优x_6 = 0.0473
new: DataOwner7的最优x_7 = 0.0289
new: DataOwner8的最优x_8 = 0.0271
new: DataOwner9的最优x_9 = 0.0350
new: DataOwner10的最优x_10 = 0.0237
eta:0.35000000000000003
new: DataOwner1的最优x_1 = 0.0267
new: DataOwner2的最优x_2 = 0.0241
new: DataOwner3的最优x_3 = 0.0312
new: DataOwner4的最优x_4 = 0.0301
new: DataOwner5的最优x_5 = 0.0338
new: DataOwner6的最优x_6 = 0.0486
new: DataOwner7的最优x_7 = 0.0297
new: DataOwner8的最优x_8 = 0.0279
new: DataOwner9的最优x_9 = 0.0360
new: DataOwner10的最优x_10 = 0.0244
eta:0.36000000000000004
new: DataOwner1的最优x_1 = 0.0275
new: DataOwner2的最优x_2 = 0.0247
new: DataOwner3的最优x_3 = 0.0321
new: DataOwner4的最优x_4 = 0.0310
new: DataOwner5的最优x_5 = 0.0347
new: DataOwner6的最优x_6 = 0.0500
new: DataOwner7的最优x_7 = 0.0306
new: DataOwner8的最优x_8 = 0.0287
new: DataOwner9的最优x_9 = 0.0370
new: DataOwner10的最优x_10 = 0.0251
eta:0.37
eta:0.38
new: DataOwner1的最优x_1 = 0.0290
new: DataOwner2的最优x_2 = 0.0261
new: DataOwner3的最优x_3 = 0.0339
new: DataOwner4的最优x_4 = 0.0327
new: DataOwner5的最优x_5 = 0.0367
new: DataOwner6的最优x_6 = 0.0528
new: DataOwner7的最优x_7 = 0.0323
new: DataOwner8的最优x_8 = 0.0303
new: DataOwner9的最优x_9 = 0.0391
new: DataOwner10的最优x_10 = 0.0265
eta:0.39
new: DataOwner1的最优x_1 = 0.0298
new: DataOwner2的最优x_2 = 0.0268
new: DataOwner3的最优x_3 = 0.0348
new: DataOwner4的最优x_4 = 0.0336
new: DataOwner5的最优x_5 = 0.0376
new: DataOwner6的最优x_6 = 0.0542
new: DataOwner7的最优x_7 = 0.0331
new: DataOwner8的最优x_8 = 0.0311
new: DataOwner9的最优x_9 = 0.0401
new: DataOwner10的最优x_10 = 0.0272
eta:0.4
eta:0.41000000000000003
eta:0.42000000000000004
new: DataOwner1的最优x_1 = 0.0321
new: DataOwner2的最优x_2 = 0.0289
new: DataOwner3的最优x_3 = 0.0375
new: DataOwner4的最优x_4 = 0.0362
new: DataOwner5的最优x_5 = 0.0405
new: DataOwner6的最优x_6 = 0.0584
new: DataOwner7的最优x_7 = 0.0357
new: DataOwner8的最优x_8 = 0.0335
new: DataOwner9的最优x_9 = 0.0432
new: DataOwner10的最优x_10 = 0.0293
eta:0.43
eta:0.44
new: DataOwner1的最优x_1 = 0.0336
new: DataOwner2的最优x_2 = 0.0302
new: DataOwner3的最优x_3 = 0.0393
new: DataOwner4的最优x_4 = 0.0379
new: DataOwner5的最优x_5 = 0.0424
new: DataOwner6的最优x_6 = 0.0612
new: DataOwner7的最优x_7 = 0.0374
new: DataOwner8的最优x_8 = 0.0351
new: DataOwner9的最优x_9 = 0.0453
new: DataOwner10的最优x_10 = 0.0307
eta:0.45
new: DataOwner1的最优x_1 = 0.0344
new: DataOwner2的最优x_2 = 0.0309
new: DataOwner3的最优x_3 = 0.0402
new: DataOwner4的最优x_4 = 0.0388
new: DataOwner5的最优x_5 = 0.0434
new: DataOwner6的最优x_6 = 0.0625
new: DataOwner7的最优x_7 = 0.0382
new: DataOwner8的最优x_8 = 0.0359
new: DataOwner9的最优x_9 = 0.0463
new: DataOwner10的最优x_10 = 0.0314
eta:0.46
new: DataOwner1的最优x_1 = 0.0351
new: DataOwner2的最优x_2 = 0.0316
new: DataOwner3的最优x_3 = 0.0411
new: DataOwner4的最优x_4 = 0.0396
new: DataOwner5的最优x_5 = 0.0444
new: DataOwner6的最优x_6 = 0.0639
new: DataOwner7的最优x_7 = 0.0391
new: DataOwner8的最优x_8 = 0.0367
new: DataOwner9的最优x_9 = 0.0473
new: DataOwner10的最优x_10 = 0.0321
eta:0.47000000000000003
new: DataOwner1的最优x_1 = 0.0359
new: DataOwner2的最优x_2 = 0.0323
new: DataOwner3的最优x_3 = 0.0420
new: DataOwner4的最优x_4 = 0.0405
new: DataOwner5的最优x_5 = 0.0453
new: DataOwner6的最优x_6 = 0.0653
new: DataOwner7的最优x_7 = 0.0399
new: DataOwner8的最优x_8 = 0.0375
new: DataOwner9的最优x_9 = 0.0483
new: DataOwner10的最优x_10 = 0.0328
eta:0.48000000000000004
new: DataOwner1的最优x_1 = 0.0367
new: DataOwner2的最优x_2 = 0.0330
new: DataOwner3的最优x_3 = 0.0429
new: DataOwner4的最优x_4 = 0.0413
new: DataOwner5的最优x_5 = 0.0463
new: DataOwner6的最优x_6 = 0.0667
new: DataOwner7的最优x_7 = 0.0408
new: DataOwner8的最优x_8 = 0.0383
new: DataOwner9的最优x_9 = 0.0494
new: DataOwner10的最优x_10 = 0.0335
eta:0.49
eta:0.5
eta:0.51
eta:0.52
eta:0.53
new: DataOwner1的最优x_1 = 0.0405
new: DataOwner2的最优x_2 = 0.0364
new: DataOwner3的最优x_3 = 0.0473
new: DataOwner4的最优x_4 = 0.0456
new: DataOwner5的最优x_5 = 0.0511
new: DataOwner6的最优x_6 = 0.0737
new: DataOwner7的最优x_7 = 0.0450
new: DataOwner8的最优x_8 = 0.0423
new: DataOwner9的最优x_9 = 0.0545
new: DataOwner10的最优x_10 = 0.0370
eta:0.54
new: DataOwner1的最优x_1 = 0.0413
new: DataOwner2的最优x_2 = 0.0371
new: DataOwner3的最优x_3 = 0.0482
new: DataOwner4的最优x_4 = 0.0465
new: DataOwner5的最优x_5 = 0.0521
new: DataOwner6的最优x_6 = 0.0751
new: DataOwner7的最优x_7 = 0.0459
new: DataOwner8的最优x_8 = 0.0431
new: DataOwner9的最优x_9 = 0.0555
new: DataOwner10的最优x_10 = 0.0377
eta:0.55
new: DataOwner1的最优x_1 = 0.0420
new: DataOwner2的最优x_2 = 0.0378
new: DataOwner3的最优x_3 = 0.0491
new: DataOwner4的最优x_4 = 0.0474
new: DataOwner5的最优x_5 = 0.0531
new: DataOwner6的最优x_6 = 0.0764
new: DataOwner7的最优x_7 = 0.0467
new: DataOwner8的最优x_8 = 0.0439
new: DataOwner9的最优x_9 = 0.0566
new: DataOwner10的最优x_10 = 0.0384
eta:0.56
new: DataOwner1的最优x_1 = 0.0428
new: DataOwner2的最优x_2 = 0.0385
new: DataOwner3的最优x_3 = 0.0500
new: DataOwner4的最优x_4 = 0.0482
new: DataOwner5的最优x_5 = 0.0540
new: DataOwner6的最优x_6 = 0.0778
new: DataOwner7的最优x_7 = 0.0476
new: DataOwner8的最优x_8 = 0.0447
new: DataOwner9的最优x_9 = 0.0576
new: DataOwner10的最优x_10 = 0.0391
eta:0.5700000000000001
eta:0.5800000000000001
new: DataOwner1的最优x_1 = 0.0443
new: DataOwner2的最优x_2 = 0.0399
new: DataOwner3的最优x_3 = 0.0518
new: DataOwner4的最优x_4 = 0.0499
new: DataOwner5的最优x_5 = 0.0560
new: DataOwner6的最优x_6 = 0.0806
new: DataOwner7的最优x_7 = 0.0493
new: DataOwner8的最优x_8 = 0.0463
new: DataOwner9的最优x_9 = 0.0597
new: DataOwner10的最优x_10 = 0.0405
eta:0.59
new: DataOwner1的最优x_1 = 0.0451
new: DataOwner2的最优x_2 = 0.0405
new: DataOwner3的最优x_3 = 0.0527
new: DataOwner4的最优x_4 = 0.0508
new: DataOwner5的最优x_5 = 0.0569
new: DataOwner6的最优x_6 = 0.0820
new: DataOwner7的最优x_7 = 0.0501
new: DataOwner8的最优x_8 = 0.0471
new: DataOwner9的最优x_9 = 0.0607
new: DataOwner10的最优x_10 = 0.0412
eta:0.6
new: DataOwner1的最优x_1 = 0.0458
new: DataOwner2的最优x_2 = 0.0412
new: DataOwner3的最优x_3 = 0.0536
new: DataOwner4的最优x_4 = 0.0517
new: DataOwner5的最优x_5 = 0.0579
new: DataOwner6的最优x_6 = 0.0834
new: DataOwner7的最优x_7 = 0.0510
new: DataOwner8的最优x_8 = 0.0479
new: DataOwner9的最优x_9 = 0.0617
new: DataOwner10的最优x_10 = 0.0419
eta:0.61
new: DataOwner1的最优x_1 = 0.0466
new: DataOwner2的最优x_2 = 0.0419
new: DataOwner3的最优x_3 = 0.0545
new: DataOwner4的最优x_4 = 0.0525
new: DataOwner5的最优x_5 = 0.0588
new: DataOwner6的最优x_6 = 0.0848
new: DataOwner7的最优x_7 = 0.0518
new: DataOwner8的最优x_8 = 0.0487
new: DataOwner9的最优x_9 = 0.0627
new: DataOwner10的最优x_10 = 0.0426
eta:0.62
new: DataOwner1的最优x_1 = 0.0474
new: DataOwner2的最优x_2 = 0.0426
new: DataOwner3的最优x_3 = 0.0554
new: DataOwner4的最优x_4 = 0.0534
new: DataOwner5的最优x_5 = 0.0598
new: DataOwner6的最优x_6 = 0.0862
new: DataOwner7的最优x_7 = 0.0527
new: DataOwner8的最优x_8 = 0.0495
new: DataOwner9的最优x_9 = 0.0638
new: DataOwner10的最优x_10 = 0.0433
eta:0.63
new: DataOwner1的最优x_1 = 0.0481
new: DataOwner2的最优x_2 = 0.0433
new: DataOwner3的最优x_3 = 0.0562
new: DataOwner4的最优x_4 = 0.0543
new: DataOwner5的最优x_5 = 0.0608
new: DataOwner6的最优x_6 = 0.0876
new: DataOwner7的最优x_7 = 0.0535
new: DataOwner8的最优x_8 = 0.0503
new: DataOwner9的最优x_9 = 0.0648
new: DataOwner10的最优x_10 = 0.0440
eta:0.64
new: DataOwner1的最优x_1 = 0.0489
new: DataOwner2的最优x_2 = 0.0440
new: DataOwner3的最优x_3 = 0.0571
new: DataOwner4的最优x_4 = 0.0551
new: DataOwner5的最优x_5 = 0.0617
new: DataOwner6的最优x_6 = 0.0890
new: DataOwner7的最优x_7 = 0.0544
new: DataOwner8的最优x_8 = 0.0511
new: DataOwner9的最优x_9 = 0.0658
new: DataOwner10的最优x_10 = 0.0447
eta:0.65
new: DataOwner1的最优x_1 = 0.0497
new: DataOwner2的最优x_2 = 0.0447
new: DataOwner3的最优x_3 = 0.0580
new: DataOwner4的最优x_4 = 0.0560
new: DataOwner5的最优x_5 = 0.0627
new: DataOwner6的最优x_6 = 0.0903
new: DataOwner7的最优x_7 = 0.0552
new: DataOwner8的最优x_8 = 0.0518
new: DataOwner9的最优x_9 = 0.0669
new: DataOwner10的最优x_10 = 0.0454
eta:0.66
new: DataOwner1的最优x_1 = 0.0504
new: DataOwner2的最优x_2 = 0.0454
new: DataOwner3的最优x_3 = 0.0589
new: DataOwner4的最优x_4 = 0.0568
new: DataOwner5的最优x_5 = 0.0637
new: DataOwner6的最优x_6 = 0.0917
new: DataOwner7的最优x_7 = 0.0561
new: DataOwner8的最优x_8 = 0.0526
new: DataOwner9的最优x_9 = 0.0679
new: DataOwner10的最优x_10 = 0.0461
eta:0.67
eta:0.68
new: DataOwner1的最优x_1 = 0.0520
new: DataOwner2的最优x_2 = 0.0467
new: DataOwner3的最优x_3 = 0.0607
new: DataOwner4的最优x_4 = 0.0586
new: DataOwner5的最优x_5 = 0.0656
new: DataOwner6的最优x_6 = 0.0945
new: DataOwner7的最优x_7 = 0.0578
new: DataOwner8的最优x_8 = 0.0542
new: DataOwner9的最优x_9 = 0.0700
new: DataOwner10的最优x_10 = 0.0475
eta:0.6900000000000001
new: DataOwner1的最优x_1 = 0.0527
new: DataOwner2的最优x_2 = 0.0474
new: DataOwner3的最优x_3 = 0.0616
new: DataOwner4的最优x_4 = 0.0594
new: DataOwner5的最优x_5 = 0.0666
new: DataOwner6的最优x_6 = 0.0959
new: DataOwner7的最优x_7 = 0.0586
new: DataOwner8的最优x_8 = 0.0550
new: DataOwner9的最优x_9 = 0.0710
new: DataOwner10的最优x_10 = 0.0482
eta:0.7000000000000001
new: DataOwner1的最优x_1 = 0.0535
new: DataOwner2的最优x_2 = 0.0481
new: DataOwner3的最优x_3 = 0.0625
new: DataOwner4的最优x_4 = 0.0603
new: DataOwner5的最优x_5 = 0.0675
new: DataOwner6的最优x_6 = 0.0973
new: DataOwner7的最优x_7 = 0.0595
new: DataOwner8的最优x_8 = 0.0558
new: DataOwner9的最优x_9 = 0.0720
new: DataOwner10的最优x_10 = 0.0489
eta:0.7100000000000001
new: DataOwner1的最优x_1 = 0.0543
new: DataOwner2的最优x_2 = 0.0488
new: DataOwner3的最优x_3 = 0.0634
new: DataOwner4的最优x_4 = 0.0611
new: DataOwner5的最优x_5 = 0.0685
new: DataOwner6的最优x_6 = 0.0987
new: DataOwner7的最优x_7 = 0.0603
new: DataOwner8的最优x_8 = 0.0566
new: DataOwner9的最优x_9 = 0.0730
new: DataOwner10的最优x_10 = 0.0496
eta:0.72
new: DataOwner1的最优x_1 = 0.0550
new: DataOwner2的最优x_2 = 0.0495
new: DataOwner3的最优x_3 = 0.0643
new: DataOwner4的最优x_4 = 0.0620
new: DataOwner5的最优x_5 = 0.0695
new: DataOwner6的最优x_6 = 0.1001
new: DataOwner7的最优x_7 = 0.0612
new: DataOwner8的最优x_8 = 0.0574
new: DataOwner9的最优x_9 = 0.0741
new: DataOwner10的最优x_10 = 0.0503
eta:0.73
new: DataOwner1的最优x_1 = 0.0558
new: DataOwner2的最优x_2 = 0.0502
new: DataOwner3的最优x_3 = 0.0652
new: DataOwner4的最优x_4 = 0.0629
new: DataOwner5的最优x_5 = 0.0704
new: DataOwner6的最优x_6 = 0.1015
new: DataOwner7的最优x_7 = 0.0620
new: DataOwner8的最优x_8 = 0.0582
new: DataOwner9的最优x_9 = 0.0751
new: DataOwner10的最优x_10 = 0.0510
eta:0.74
new: DataOwner1的最优x_1 = 0.0565
new: DataOwner2的最优x_2 = 0.0509
new: DataOwner3的最优x_3 = 0.0661
new: DataOwner4的最优x_4 = 0.0637
new: DataOwner5的最优x_5 = 0.0714
new: DataOwner6的最优x_6 = 0.1028
new: DataOwner7的最优x_7 = 0.0629
new: DataOwner8的最优x_8 = 0.0590
new: DataOwner9的最优x_9 = 0.0761
new: DataOwner10的最优x_10 = 0.0517
eta:0.75
new: DataOwner1的最优x_1 = 0.0573
new: DataOwner2的最优x_2 = 0.0515
new: DataOwner3的最优x_3 = 0.0670
new: DataOwner4的最优x_4 = 0.0646
new: DataOwner5的最优x_5 = 0.0724
new: DataOwner6的最优x_6 = 0.1042
new: DataOwner7的最优x_7 = 0.0637
new: DataOwner8的最优x_8 = 0.0598
new: DataOwner9的最优x_9 = 0.0772
new: DataOwner10的最优x_10 = 0.0524
eta:0.76
new: DataOwner1的最优x_1 = 0.0581
new: DataOwner2的最优x_2 = 0.0522
new: DataOwner3的最优x_3 = 0.0679
new: DataOwner4的最优x_4 = 0.0654
new: DataOwner5的最优x_5 = 0.0733
new: DataOwner6的最优x_6 = 0.1056
new: DataOwner7的最优x_7 = 0.0646
new: DataOwner8的最优x_8 = 0.0606
new: DataOwner9的最优x_9 = 0.0782
new: DataOwner10的最优x_10 = 0.0531
eta:0.77
new: DataOwner1的最优x_1 = 0.0588
new: DataOwner2的最优x_2 = 0.0529
new: DataOwner3的最优x_3 = 0.0687
new: DataOwner4的最优x_4 = 0.0663
new: DataOwner5的最优x_5 = 0.0743
new: DataOwner6的最优x_6 = 0.1070
new: DataOwner7的最优x_7 = 0.0654
new: DataOwner8的最优x_8 = 0.0614
new: DataOwner9的最优x_9 = 0.0792
new: DataOwner10的最优x_10 = 0.0538
eta:0.78
new: DataOwner1的最优x_1 = 0.0596
new: DataOwner2的最优x_2 = 0.0536
new: DataOwner3的最优x_3 = 0.0696
new: DataOwner4的最优x_4 = 0.0672
new: DataOwner5的最优x_5 = 0.0753
new: DataOwner6的最优x_6 = 0.1084
new: DataOwner7的最优x_7 = 0.0663
new: DataOwner8的最优x_8 = 0.0622
new: DataOwner9的最优x_9 = 0.0802
new: DataOwner10的最优x_10 = 0.0545
eta:0.79
new: DataOwner1的最优x_1 = 0.0604
new: DataOwner2的最优x_2 = 0.0543
new: DataOwner3的最优x_3 = 0.0705
new: DataOwner4的最优x_4 = 0.0680
new: DataOwner5的最优x_5 = 0.0762
new: DataOwner6的最优x_6 = 0.1098
new: DataOwner7的最优x_7 = 0.0671
new: DataOwner8的最优x_8 = 0.0630
new: DataOwner9的最优x_9 = 0.0813
new: DataOwner10的最优x_10 = 0.0552
eta:0.8
new: DataOwner1的最优x_1 = 0.0611
new: DataOwner2的最优x_2 = 0.0550
new: DataOwner3的最优x_3 = 0.0714
new: DataOwner4的最优x_4 = 0.0689
new: DataOwner5的最优x_5 = 0.0772
new: DataOwner6的最优x_6 = 0.1112
new: DataOwner7的最优x_7 = 0.0680
new: DataOwner8的最优x_8 = 0.0638
new: DataOwner9的最优x_9 = 0.0823
new: DataOwner10的最优x_10 = 0.0559
eta:0.81
new: DataOwner1的最优x_1 = 0.0619
new: DataOwner2的最优x_2 = 0.0557
new: DataOwner3的最优x_3 = 0.0723
new: DataOwner4的最优x_4 = 0.0698
new: DataOwner5的最优x_5 = 0.0781
new: DataOwner6的最优x_6 = 0.1126
new: DataOwner7的最优x_7 = 0.0688
new: DataOwner8的最优x_8 = 0.0646
new: DataOwner9的最优x_9 = 0.0833
new: DataOwner10的最优x_10 = 0.0566
eta:0.8200000000000001
new: DataOwner1的最优x_1 = 0.0627
new: DataOwner2的最优x_2 = 0.0564
new: DataOwner3的最优x_3 = 0.0732
new: DataOwner4的最优x_4 = 0.0706
new: DataOwner5的最优x_5 = 0.0791
new: DataOwner6的最优x_6 = 0.1140
new: DataOwner7的最优x_7 = 0.0697
new: DataOwner8的最优x_8 = 0.0654
new: DataOwner9的最优x_9 = 0.0844
new: DataOwner10的最优x_10 = 0.0573
eta:0.8300000000000001
new: DataOwner1的最优x_1 = 0.0634
new: DataOwner2的最优x_2 = 0.0570
new: DataOwner3的最优x_3 = 0.0741
new: DataOwner4的最优x_4 = 0.0715
new: DataOwner5的最优x_5 = 0.0801
new: DataOwner6的最优x_6 = 0.1154
new: DataOwner7的最优x_7 = 0.0705
new: DataOwner8的最优x_8 = 0.0662
new: DataOwner9的最优x_9 = 0.0854
new: DataOwner10的最优x_10 = 0.0580
eta:0.8400000000000001
new: DataOwner1的最优x_1 = 0.0642
new: DataOwner2的最优x_2 = 0.0577
new: DataOwner3的最优x_3 = 0.0750
new: DataOwner4的最优x_4 = 0.0723
new: DataOwner5的最优x_5 = 0.0810
new: DataOwner6的最优x_6 = 0.1167
new: DataOwner7的最优x_7 = 0.0714
new: DataOwner8的最优x_8 = 0.0670
new: DataOwner9的最优x_9 = 0.0864
new: DataOwner10的最优x_10 = 0.0587
eta:0.85
new: DataOwner1的最优x_1 = 0.0649
new: DataOwner2的最优x_2 = 0.0584
new: DataOwner3的最优x_3 = 0.0759
new: DataOwner4的最优x_4 = 0.0732
new: DataOwner5的最优x_5 = 0.0820
new: DataOwner6的最优x_6 = 0.1181
new: DataOwner7的最优x_7 = 0.0722
new: DataOwner8的最优x_8 = 0.0678
new: DataOwner9的最优x_9 = 0.0874
new: DataOwner10的最优x_10 = 0.0594
eta:0.86
new: DataOwner1的最优x_1 = 0.0657
new: DataOwner2的最优x_2 = 0.0591
new: DataOwner3的最优x_3 = 0.0768
new: DataOwner4的最优x_4 = 0.0741
new: DataOwner5的最优x_5 = 0.0830
new: DataOwner6的最优x_6 = 0.1195
new: DataOwner7的最优x_7 = 0.0731
new: DataOwner8的最优x_8 = 0.0686
new: DataOwner9的最优x_9 = 0.0885
new: DataOwner10的最优x_10 = 0.0600
eta:0.87
new: DataOwner1的最优x_1 = 0.0665
new: DataOwner2的最优x_2 = 0.0598
new: DataOwner3的最优x_3 = 0.0777
new: DataOwner4的最优x_4 = 0.0749
new: DataOwner5的最优x_5 = 0.0839
new: DataOwner6的最优x_6 = 0.1209
new: DataOwner7的最优x_7 = 0.0739
new: DataOwner8的最优x_8 = 0.0694
new: DataOwner9的最优x_9 = 0.0895
new: DataOwner10的最优x_10 = 0.0607
eta:0.88
new: DataOwner1的最优x_1 = 0.0672
new: DataOwner2的最优x_2 = 0.0605
new: DataOwner3的最优x_3 = 0.0786
new: DataOwner4的最优x_4 = 0.0758
new: DataOwner5的最优x_5 = 0.0849
new: DataOwner6的最优x_6 = 0.1223
new: DataOwner7的最优x_7 = 0.0748
new: DataOwner8的最优x_8 = 0.0702
new: DataOwner9的最优x_9 = 0.0905
new: DataOwner10的最优x_10 = 0.0614
eta:0.89
new: DataOwner1的最优x_1 = 0.0680
new: DataOwner2的最优x_2 = 0.0612
new: DataOwner3的最优x_3 = 0.0795
new: DataOwner4的最优x_4 = 0.0766
new: DataOwner5的最优x_5 = 0.0859
new: DataOwner6的最优x_6 = 0.1237
new: DataOwner7的最优x_7 = 0.0756
new: DataOwner8的最优x_8 = 0.0710
new: DataOwner9的最优x_9 = 0.0916
new: DataOwner10的最优x_10 = 0.0621
eta:0.9
new: DataOwner1的最优x_1 = 0.0688
new: DataOwner2的最优x_2 = 0.0619
new: DataOwner3的最优x_3 = 0.0804
new: DataOwner4的最优x_4 = 0.0775
new: DataOwner5的最优x_5 = 0.0868
new: DataOwner6的最优x_6 = 0.1251
new: DataOwner7的最优x_7 = 0.0765
new: DataOwner8的最优x_8 = 0.0718
new: DataOwner9的最优x_9 = 0.0926
new: DataOwner10的最优x_10 = 0.0628
eta:0.91
new: DataOwner1的最优x_1 = 0.0695
new: DataOwner2的最优x_2 = 0.0625
new: DataOwner3的最优x_3 = 0.0812
new: DataOwner4的最优x_4 = 0.0784
new: DataOwner5的最优x_5 = 0.0878
new: DataOwner6的最优x_6 = 0.1265
new: DataOwner7的最优x_7 = 0.0773
new: DataOwner8的最优x_8 = 0.0726
new: DataOwner9的最优x_9 = 0.0936
new: DataOwner10的最优x_10 = 0.0635
eta:0.92
new: DataOwner1的最优x_1 = 0.0703
new: DataOwner2的最优x_2 = 0.0632
new: DataOwner3的最优x_3 = 0.0821
new: DataOwner4的最优x_4 = 0.0792
new: DataOwner5的最优x_5 = 0.0888
new: DataOwner6的最优x_6 = 0.1279
new: DataOwner7的最优x_7 = 0.0782
new: DataOwner8的最优x_8 = 0.0734
new: DataOwner9的最优x_9 = 0.0946
new: DataOwner10的最优x_10 = 0.0642
eta:0.93
new: DataOwner1的最优x_1 = 0.0711
new: DataOwner2的最优x_2 = 0.0639
new: DataOwner3的最优x_3 = 0.0830
new: DataOwner4的最优x_4 = 0.0801
new: DataOwner5的最优x_5 = 0.0897
new: DataOwner6的最优x_6 = 0.1293
new: DataOwner7的最优x_7 = 0.0790
new: DataOwner8的最优x_8 = 0.0742
new: DataOwner9的最优x_9 = 0.0957
new: DataOwner10的最优x_10 = 0.0649
eta:0.9400000000000001
new: DataOwner1的最优x_1 = 0.0718
new: DataOwner2的最优x_2 = 0.0646
new: DataOwner3的最优x_3 = 0.0839
new: DataOwner4的最优x_4 = 0.0809
new: DataOwner5的最优x_5 = 0.0907
new: DataOwner6的最优x_6 = 0.1306
new: DataOwner7的最优x_7 = 0.0799
new: DataOwner8的最优x_8 = 0.0750
new: DataOwner9的最优x_9 = 0.0967
new: DataOwner10的最优x_10 = 0.0656
eta:0.9500000000000001
new: DataOwner1的最优x_1 = 0.0726
new: DataOwner2的最优x_2 = 0.0653
new: DataOwner3的最优x_3 = 0.0848
new: DataOwner4的最优x_4 = 0.0818
new: DataOwner5的最优x_5 = 0.0917
new: DataOwner6的最优x_6 = 0.1320
new: DataOwner7的最优x_7 = 0.0807
new: DataOwner8的最优x_8 = 0.0758
new: DataOwner9的最优x_9 = 0.0977
new: DataOwner10的最优x_10 = 0.0663
eta:0.9600000000000001
new: DataOwner1的最优x_1 = 0.0734
new: DataOwner2的最优x_2 = 0.0660
new: DataOwner3的最优x_3 = 0.0857
new: DataOwner4的最优x_4 = 0.0827
new: DataOwner5的最优x_5 = 0.0926
new: DataOwner6的最优x_6 = 0.1334
new: DataOwner7的最优x_7 = 0.0816
new: DataOwner8的最优x_8 = 0.0766
new: DataOwner9的最优x_9 = 0.0988
new: DataOwner10的最优x_10 = 0.0670
eta:0.97
new: DataOwner1的最优x_1 = 0.0741
new: DataOwner2的最优x_2 = 0.0667
new: DataOwner3的最优x_3 = 0.0866
new: DataOwner4的最优x_4 = 0.0835
new: DataOwner5的最优x_5 = 0.0936
new: DataOwner6的最优x_6 = 0.1348
new: DataOwner7的最优x_7 = 0.0824
new: DataOwner8的最优x_8 = 0.0774
new: DataOwner9的最优x_9 = 0.0998
new: DataOwner10的最优x_10 = 0.0677
eta:0.98
new: DataOwner1的最优x_1 = 0.0749
new: DataOwner2的最优x_2 = 0.0673
new: DataOwner3的最优x_3 = 0.0875
new: DataOwner4的最优x_4 = 0.0844
new: DataOwner5的最优x_5 = 0.0945
new: DataOwner6的最优x_6 = 0.1362
new: DataOwner7的最优x_7 = 0.0833
new: DataOwner8的最优x_8 = 0.0782
new: DataOwner9的最优x_9 = 0.1008
new: DataOwner10的最优x_10 = 0.0684
eta:0.99
new: DataOwner1的最优x_1 = 0.0756
new: DataOwner2的最优x_2 = 0.0680
new: DataOwner3的最优x_3 = 0.0884
new: DataOwner4的最优x_4 = 0.0853
new: DataOwner5的最优x_5 = 0.0955
new: DataOwner6的最优x_6 = 0.1376
new: DataOwner7的最优x_7 = 0.0841
new: DataOwner8的最优x_8 = 0.0790
new: DataOwner9的最优x_9 = 0.1018
new: DataOwner10的最优x_10 = 0.0691
eta:1.0
new: DataOwner1的最优x_1 = 0.0764
new: DataOwner2的最优x_2 = 0.0687
new: DataOwner3的最优x_3 = 0.0893
new: DataOwner4的最优x_4 = 0.0861
new: DataOwner5的最优x_5 = 0.0965
new: DataOwner6的最优x_6 = 0.1390
new: DataOwner7的最优x_7 = 0.0850
new: DataOwner8的最优x_8 = 0.0798
new: DataOwner9的最优x_9 = 0.1029
new: DataOwner10的最优x_10 = 0.0698
eta:1.01
new: DataOwner1的最优x_1 = 0.0772
new: DataOwner2的最优x_2 = 0.0694
new: DataOwner3的最优x_3 = 0.0902
new: DataOwner4的最优x_4 = 0.0870
new: DataOwner5的最优x_5 = 0.0974
new: DataOwner6的最优x_6 = 0.1404
new: DataOwner7的最优x_7 = 0.0858
new: DataOwner8的最优x_8 = 0.0806
new: DataOwner9的最优x_9 = 0.1039
new: DataOwner10的最优x_10 = 0.0705
eta:1.02
new: DataOwner1的最优x_1 = 0.0779
new: DataOwner2的最优x_2 = 0.0701
new: DataOwner3的最优x_3 = 0.0911
new: DataOwner4的最优x_4 = 0.0878
new: DataOwner5的最优x_5 = 0.0984
new: DataOwner6的最优x_6 = 0.1418
new: DataOwner7的最优x_7 = 0.0867
new: DataOwner8的最优x_8 = 0.0814
new: DataOwner9的最优x_9 = 0.1049
new: DataOwner10的最优x_10 = 0.0712
eta:1.03
new: DataOwner1的最优x_1 = 0.0787
new: DataOwner2的最优x_2 = 0.0708
new: DataOwner3的最优x_3 = 0.0920
new: DataOwner4的最优x_4 = 0.0887
new: DataOwner5的最优x_5 = 0.0994
new: DataOwner6的最优x_6 = 0.1432
new: DataOwner7的最优x_7 = 0.0875
new: DataOwner8的最优x_8 = 0.0822
new: DataOwner9的最优x_9 = 0.1060
new: DataOwner10的最优x_10 = 0.0719
eta:1.04
new: DataOwner1的最优x_1 = 0.0795
new: DataOwner2的最优x_2 = 0.0715
new: DataOwner3的最优x_3 = 0.0929
new: DataOwner4的最优x_4 = 0.0896
new: DataOwner5的最优x_5 = 0.1003
new: DataOwner6的最优x_6 = 0.1445
new: DataOwner7的最优x_7 = 0.0884
new: DataOwner8的最优x_8 = 0.0830
new: DataOwner9的最优x_9 = 0.1070
new: DataOwner10的最优x_10 = 0.0726
eta:1.05
new: DataOwner1的最优x_1 = 0.0802
new: DataOwner2的最优x_2 = 0.0722
new: DataOwner3的最优x_3 = 0.0937
new: DataOwner4的最优x_4 = 0.0904
new: DataOwner5的最优x_5 = 0.1013
new: DataOwner6的最优x_6 = 0.1459
new: DataOwner7的最优x_7 = 0.0892
new: DataOwner8的最优x_8 = 0.0838
new: DataOwner9的最优x_9 = 0.1080
new: DataOwner10的最优x_10 = 0.0733
eta:1.06
new: DataOwner1的最优x_1 = 0.0810
new: DataOwner2的最优x_2 = 0.0728
new: DataOwner3的最优x_3 = 0.0946
new: DataOwner4的最优x_4 = 0.0913
new: DataOwner5的最优x_5 = 0.1023
new: DataOwner6的最优x_6 = 0.1473
new: DataOwner7的最优x_7 = 0.0901
new: DataOwner8的最优x_8 = 0.0846
new: DataOwner9的最优x_9 = 0.1090
new: DataOwner10的最优x_10 = 0.0740
eta:1.07
new: DataOwner1的最优x_1 = 0.0818
new: DataOwner2的最优x_2 = 0.0735
new: DataOwner3的最优x_3 = 0.0955
new: DataOwner4的最优x_4 = 0.0921
new: DataOwner5的最优x_5 = 0.1032
new: DataOwner6的最优x_6 = 0.1487
new: DataOwner7的最优x_7 = 0.0909
new: DataOwner8的最优x_8 = 0.0854
new: DataOwner9的最优x_9 = 0.1101
new: DataOwner10的最优x_10 = 0.0747
eta:1.08
new: DataOwner1的最优x_1 = 0.0825
new: DataOwner2的最优x_2 = 0.0742
new: DataOwner3的最优x_3 = 0.0964
new: DataOwner4的最优x_4 = 0.0930
new: DataOwner5的最优x_5 = 0.1042
new: DataOwner6的最优x_6 = 0.1501
new: DataOwner7的最优x_7 = 0.0918
new: DataOwner8的最优x_8 = 0.0862
new: DataOwner9的最优x_9 = 0.1111
new: DataOwner10的最优x_10 = 0.0754
eta:1.09
new: DataOwner1的最优x_1 = 0.0833
new: DataOwner2的最优x_2 = 0.0749
new: DataOwner3的最优x_3 = 0.0973
new: DataOwner4的最优x_4 = 0.0939
new: DataOwner5的最优x_5 = 0.1052
new: DataOwner6的最优x_6 = 0.1515
new: DataOwner7的最优x_7 = 0.0926
new: DataOwner8的最优x_8 = 0.0869
new: DataOwner9的最优x_9 = 0.1121
new: DataOwner10的最优x_10 = 0.0761
eta:1.1
new: DataOwner1的最优x_1 = 0.0840
new: DataOwner2的最优x_2 = 0.0756
new: DataOwner3的最优x_3 = 0.0982
new: DataOwner4的最优x_4 = 0.0947
new: DataOwner5的最优x_5 = 0.1061
new: DataOwner6的最优x_6 = 0.1529
new: DataOwner7的最优x_7 = 0.0935
new: DataOwner8的最优x_8 = 0.0877
new: DataOwner9的最优x_9 = 0.1132
new: DataOwner10的最优x_10 = 0.0768
eta:1.11
new: DataOwner1的最优x_1 = 0.0848
new: DataOwner2的最优x_2 = 0.0763
new: DataOwner3的最优x_3 = 0.0991
new: DataOwner4的最优x_4 = 0.0956
new: DataOwner5的最优x_5 = 0.1071
new: DataOwner6的最优x_6 = 0.1543
new: DataOwner7的最优x_7 = 0.0943
new: DataOwner8的最优x_8 = 0.0885
new: DataOwner9的最优x_9 = 0.1142
new: DataOwner10的最优x_10 = 0.0775
eta:1.12
new: DataOwner1的最优x_1 = 0.0856
new: DataOwner2的最优x_2 = 0.0770
new: DataOwner3的最优x_3 = 0.1000
new: DataOwner4的最优x_4 = 0.0964
new: DataOwner5的最优x_5 = 0.1081
new: DataOwner6的最优x_6 = 0.1557
new: DataOwner7的最优x_7 = 0.0952
new: DataOwner8的最优x_8 = 0.0893
new: DataOwner9的最优x_9 = 0.1152
new: DataOwner10的最优x_10 = 0.0782
eta:1.1300000000000001
new: DataOwner1的最优x_1 = 0.0863
new: DataOwner2的最优x_2 = 0.0777
new: DataOwner3的最优x_3 = 0.1009
new: DataOwner4的最优x_4 = 0.0973
new: DataOwner5的最优x_5 = 0.1090
new: DataOwner6的最优x_6 = 0.1571
new: DataOwner7的最优x_7 = 0.0960
new: DataOwner8的最优x_8 = 0.0901
new: DataOwner9的最优x_9 = 0.1162
new: DataOwner10的最优x_10 = 0.0789
eta:1.1400000000000001
new: DataOwner1的最优x_1 = 0.0871
new: DataOwner2的最优x_2 = 0.0783
new: DataOwner3的最优x_3 = 0.1018
new: DataOwner4的最优x_4 = 0.0982
new: DataOwner5的最优x_5 = 0.1100
new: DataOwner6的最优x_6 = 0.1584
new: DataOwner7的最优x_7 = 0.0969
new: DataOwner8的最优x_8 = 0.0909
new: DataOwner9的最优x_9 = 0.1173
new: DataOwner10的最优x_10 = 0.0796
eta:1.1500000000000001
new: DataOwner1的最优x_1 = 0.0879
new: DataOwner2的最优x_2 = 0.0790
new: DataOwner3的最优x_3 = 0.1027
new: DataOwner4的最优x_4 = 0.0990
new: DataOwner5的最优x_5 = 0.1109
new: DataOwner6的最优x_6 = 0.1598
new: DataOwner7的最优x_7 = 0.0977
new: DataOwner8的最优x_8 = 0.0917
new: DataOwner9的最优x_9 = 0.1183
new: DataOwner10的最优x_10 = 0.0803
eta:1.1600000000000001
new: DataOwner1的最优x_1 = 0.0886
new: DataOwner2的最优x_2 = 0.0797
new: DataOwner3的最优x_3 = 0.1036
new: DataOwner4的最优x_4 = 0.0999
new: DataOwner5的最优x_5 = 0.1119
new: DataOwner6的最优x_6 = 0.1612
new: DataOwner7的最优x_7 = 0.0986
new: DataOwner8的最优x_8 = 0.0925
new: DataOwner9的最优x_9 = 0.1193
new: DataOwner10的最优x_10 = 0.0810
eta:1.17
new: DataOwner1的最优x_1 = 0.0894
new: DataOwner2的最优x_2 = 0.0804
new: DataOwner3的最优x_3 = 0.1045
new: DataOwner4的最优x_4 = 0.1008
new: DataOwner5的最优x_5 = 0.1129
new: DataOwner6的最优x_6 = 0.1626
new: DataOwner7的最优x_7 = 0.0994
new: DataOwner8的最优x_8 = 0.0933
new: DataOwner9的最优x_9 = 0.1204
new: DataOwner10的最优x_10 = 0.0817
eta:1.18
new: DataOwner1的最优x_1 = 0.0902
new: DataOwner2的最优x_2 = 0.0811
new: DataOwner3的最优x_3 = 0.1053
new: DataOwner4的最优x_4 = 0.1016
new: DataOwner5的最优x_5 = 0.1138
new: DataOwner6的最优x_6 = 0.1640
new: DataOwner7的最优x_7 = 0.1003
new: DataOwner8的最优x_8 = 0.0941
new: DataOwner9的最优x_9 = 0.1214
new: DataOwner10的最优x_10 = 0.0824
eta:1.19
new: DataOwner1的最优x_1 = 0.0909
new: DataOwner2的最优x_2 = 0.0818
new: DataOwner3的最优x_3 = 0.1062
new: DataOwner4的最优x_4 = 0.1025
new: DataOwner5的最优x_5 = 0.1148
new: DataOwner6的最优x_6 = 0.1654
new: DataOwner7的最优x_7 = 0.1011
new: DataOwner8的最优x_8 = 0.0949
new: DataOwner9的最优x_9 = 0.1224
new: DataOwner10的最优x_10 = 0.0831
eta:1.2
new: DataOwner1的最优x_1 = 0.0917
new: DataOwner2的最优x_2 = 0.0825
new: DataOwner3的最优x_3 = 0.1071
new: DataOwner4的最优x_4 = 0.1033
new: DataOwner5的最优x_5 = 0.1158
new: DataOwner6的最优x_6 = 0.1668
new: DataOwner7的最优x_7 = 0.1020
new: DataOwner8的最优x_8 = 0.0957
new: DataOwner9的最优x_9 = 0.1234
new: DataOwner10的最优x_10 = 0.0838
eta:1.21
new: DataOwner1的最优x_1 = 0.0925
new: DataOwner2的最优x_2 = 0.0832
new: DataOwner3的最优x_3 = 0.1080
new: DataOwner4的最优x_4 = 0.1042
new: DataOwner5的最优x_5 = 0.1167
new: DataOwner6的最优x_6 = 0.1682
new: DataOwner7的最优x_7 = 0.1028
new: DataOwner8的最优x_8 = 0.0965
new: DataOwner9的最优x_9 = 0.1245
new: DataOwner10的最优x_10 = 0.0845
eta:1.22
new: DataOwner1的最优x_1 = 0.0932
new: DataOwner2的最优x_2 = 0.0838
new: DataOwner3的最优x_3 = 0.1089
new: DataOwner4的最优x_4 = 0.1051
new: DataOwner5的最优x_5 = 0.1177
new: DataOwner6的最优x_6 = 0.1696
new: DataOwner7的最优x_7 = 0.1037
new: DataOwner8的最优x_8 = 0.0973
new: DataOwner9的最优x_9 = 0.1255
new: DataOwner10的最优x_10 = 0.0852
eta:1.23
new: DataOwner1的最优x_1 = 0.0940
new: DataOwner2的最优x_2 = 0.0845
new: DataOwner3的最优x_3 = 0.1098
new: DataOwner4的最优x_4 = 0.1059
new: DataOwner5的最优x_5 = 0.1187
new: DataOwner6的最优x_6 = 0.1710
new: DataOwner7的最优x_7 = 0.1045
new: DataOwner8的最优x_8 = 0.0981
new: DataOwner9的最优x_9 = 0.1265
new: DataOwner10的最优x_10 = 0.0859
eta:1.24
new: DataOwner1的最优x_1 = 0.0947
new: DataOwner2的最优x_2 = 0.0852
new: DataOwner3的最优x_3 = 0.1107
new: DataOwner4的最优x_4 = 0.1068
new: DataOwner5的最优x_5 = 0.1196
new: DataOwner6的最优x_6 = 0.1723
new: DataOwner7的最优x_7 = 0.1054
new: DataOwner8的最优x_8 = 0.0989
new: DataOwner9的最优x_9 = 0.1276
new: DataOwner10的最优x_10 = 0.0866
eta:1.25
new: DataOwner1的最优x_1 = 0.0955
new: DataOwner2的最优x_2 = 0.0859
new: DataOwner3的最优x_3 = 0.1116
new: DataOwner4的最优x_4 = 0.1076
new: DataOwner5的最优x_5 = 0.1206
new: DataOwner6的最优x_6 = 0.1737
new: DataOwner7的最优x_7 = 0.1062
new: DataOwner8的最优x_8 = 0.0997
new: DataOwner9的最优x_9 = 0.1286
new: DataOwner10的最优x_10 = 0.0873
eta:1.26
new: DataOwner1的最优x_1 = 0.0963
new: DataOwner2的最优x_2 = 0.0866
new: DataOwner3的最优x_3 = 0.1125
new: DataOwner4的最优x_4 = 0.1085
new: DataOwner5的最优x_5 = 0.1216
new: DataOwner6的最优x_6 = 0.1751
new: DataOwner7的最优x_7 = 0.1071
new: DataOwner8的最优x_8 = 0.1005
new: DataOwner9的最优x_9 = 0.1296
new: DataOwner10的最优x_10 = 0.0880
eta:1.27
new: DataOwner1的最优x_1 = 0.0970
new: DataOwner2的最优x_2 = 0.0873
new: DataOwner3的最优x_3 = 0.1134
new: DataOwner4的最优x_4 = 0.1094
new: DataOwner5的最优x_5 = 0.1225
new: DataOwner6的最优x_6 = 0.1765
new: DataOwner7的最优x_7 = 0.1079
new: DataOwner8的最优x_8 = 0.1013
new: DataOwner9的最优x_9 = 0.1306
new: DataOwner10的最优x_10 = 0.0887
eta:1.28
new: DataOwner1的最优x_1 = 0.0978
new: DataOwner2的最优x_2 = 0.0880
new: DataOwner3的最优x_3 = 0.1143
new: DataOwner4的最优x_4 = 0.1102
new: DataOwner5的最优x_5 = 0.1235
new: DataOwner6的最优x_6 = 0.1779
new: DataOwner7的最优x_7 = 0.1088
new: DataOwner8的最优x_8 = 0.1021
new: DataOwner9的最优x_9 = 0.1317
new: DataOwner10的最优x_10 = 0.0894
eta:1.29
new: DataOwner1的最优x_1 = 0.0986
new: DataOwner2的最优x_2 = 0.0887
new: DataOwner3的最优x_3 = 0.1152
new: DataOwner4的最优x_4 = 0.1111
new: DataOwner5的最优x_5 = 0.1245
new: DataOwner6的最优x_6 = 0.1793
new: DataOwner7的最优x_7 = 0.1096
new: DataOwner8的最优x_8 = 0.1029
new: DataOwner9的最优x_9 = 0.1327
new: DataOwner10的最优x_10 = 0.0901
eta:1.3
new: DataOwner1的最优x_1 = 0.0993
new: DataOwner2的最优x_2 = 0.0893
new: DataOwner3的最优x_3 = 0.1161
new: DataOwner4的最优x_4 = 0.1119
new: DataOwner5的最优x_5 = 0.1254
new: DataOwner6的最优x_6 = 0.1807
new: DataOwner7的最优x_7 = 0.1105
new: DataOwner8的最优x_8 = 0.1037
new: DataOwner9的最优x_9 = 0.1337
new: DataOwner10的最优x_10 = 0.0908
eta:1.31
new: DataOwner1的最优x_1 = 0.1001
new: DataOwner2的最优x_2 = 0.0900
new: DataOwner3的最优x_3 = 0.1170
new: DataOwner4的最优x_4 = 0.1128
new: DataOwner5的最优x_5 = 0.1264
new: DataOwner6的最优x_6 = 0.1821
new: DataOwner7的最优x_7 = 0.1113
new: DataOwner8的最优x_8 = 0.1045
new: DataOwner9的最优x_9 = 0.1348
new: DataOwner10的最优x_10 = 0.0915
eta:1.32
new: DataOwner1的最优x_1 = 0.1009
new: DataOwner2的最优x_2 = 0.0907
new: DataOwner3的最优x_3 = 0.1178
new: DataOwner4的最优x_4 = 0.1137
new: DataOwner5的最优x_5 = 0.1273
new: DataOwner6的最优x_6 = 0.1835
new: DataOwner7的最优x_7 = 0.1122
new: DataOwner8的最优x_8 = 0.1053
new: DataOwner9的最优x_9 = 0.1358
new: DataOwner10的最优x_10 = 0.0922
eta:1.33
new: DataOwner1的最优x_1 = 0.1016
new: DataOwner2的最优x_2 = 0.0914
new: DataOwner3的最优x_3 = 0.1187
new: DataOwner4的最优x_4 = 0.1145
new: DataOwner5的最优x_5 = 0.1283
new: DataOwner6的最优x_6 = 0.1849
new: DataOwner7的最优x_7 = 0.1130
new: DataOwner8的最优x_8 = 0.1061
new: DataOwner9的最优x_9 = 0.1368
new: DataOwner10的最优x_10 = 0.0929
eta:1.34
new: DataOwner1的最优x_1 = 0.1024
new: DataOwner2的最优x_2 = 0.0921
new: DataOwner3的最优x_3 = 0.1196
new: DataOwner4的最优x_4 = 0.1154
new: DataOwner5的最优x_5 = 0.1293
new: DataOwner6的最优x_6 = 0.1862
new: DataOwner7的最优x_7 = 0.1139
new: DataOwner8的最优x_8 = 0.1069
new: DataOwner9的最优x_9 = 0.1378
new: DataOwner10的最优x_10 = 0.0936
eta:1.35
new: DataOwner1的最优x_1 = 0.1032
new: DataOwner2的最优x_2 = 0.0928
new: DataOwner3的最优x_3 = 0.1205
new: DataOwner4的最优x_4 = 0.1163
new: DataOwner5的最优x_5 = 0.1302
new: DataOwner6的最优x_6 = 0.1876
new: DataOwner7的最优x_7 = 0.1147
new: DataOwner8的最优x_8 = 0.1077
new: DataOwner9的最优x_9 = 0.1389
new: DataOwner10的最优x_10 = 0.0943
eta:1.36
new: DataOwner1的最优x_1 = 0.1039
new: DataOwner2的最优x_2 = 0.0935
new: DataOwner3的最优x_3 = 0.1214
new: DataOwner4的最优x_4 = 0.1171
new: DataOwner5的最优x_5 = 0.1312
new: DataOwner6的最优x_6 = 0.1890
new: DataOwner7的最优x_7 = 0.1156
new: DataOwner8的最优x_8 = 0.1085
new: DataOwner9的最优x_9 = 0.1399
new: DataOwner10的最优x_10 = 0.0950
eta:1.37
new: DataOwner1的最优x_1 = 0.1047
new: DataOwner2的最优x_2 = 0.0942
new: DataOwner3的最优x_3 = 0.1223
new: DataOwner4的最优x_4 = 0.1180
new: DataOwner5的最优x_5 = 0.1322
new: DataOwner6的最优x_6 = 0.1904
new: DataOwner7的最优x_7 = 0.1164
new: DataOwner8的最优x_8 = 0.1093
new: DataOwner9的最优x_9 = 0.1409
new: DataOwner10的最优x_10 = 0.0957
eta:1.3800000000000001
new: DataOwner1的最优x_1 = 0.1054
new: DataOwner2的最优x_2 = 0.0948
new: DataOwner3的最优x_3 = 0.1232
new: DataOwner4的最优x_4 = 0.1188
new: DataOwner5的最优x_5 = 0.1331
new: DataOwner6的最优x_6 = 0.1918
new: DataOwner7的最优x_7 = 0.1173
new: DataOwner8的最优x_8 = 0.1101
new: DataOwner9的最优x_9 = 0.1420
new: DataOwner10的最优x_10 = 0.0964
eta:1.3900000000000001
new: DataOwner1的最优x_1 = 0.1062
new: DataOwner2的最优x_2 = 0.0955
new: DataOwner3的最优x_3 = 0.1241
new: DataOwner4的最优x_4 = 0.1197
new: DataOwner5的最优x_5 = 0.1341
new: DataOwner6的最优x_6 = 0.1932
new: DataOwner7的最优x_7 = 0.1181
new: DataOwner8的最优x_8 = 0.1109
new: DataOwner9的最优x_9 = 0.1430
new: DataOwner10的最优x_10 = 0.0971
eta:1.4000000000000001
new: DataOwner1的最优x_1 = 0.1070
new: DataOwner2的最优x_2 = 0.0962
new: DataOwner3的最优x_3 = 0.1250
new: DataOwner4的最优x_4 = 0.1206
new: DataOwner5的最优x_5 = 0.1351
new: DataOwner6的最优x_6 = 0.1946
new: DataOwner7的最优x_7 = 0.1190
new: DataOwner8的最优x_8 = 0.1117
new: DataOwner9的最优x_9 = 0.1440
new: DataOwner10的最优x_10 = 0.0978
eta:1.4100000000000001
new: DataOwner1的最优x_1 = 0.1077
new: DataOwner2的最优x_2 = 0.0969
new: DataOwner3的最优x_3 = 0.1259
new: DataOwner4的最优x_4 = 0.1214
new: DataOwner5的最优x_5 = 0.1360
new: DataOwner6的最优x_6 = 0.1960
new: DataOwner7的最优x_7 = 0.1198
new: DataOwner8的最优x_8 = 0.1125
new: DataOwner9的最优x_9 = 0.1450
new: DataOwner10的最优x_10 = 0.0985
eta:1.42
new: DataOwner1的最优x_1 = 0.1085
new: DataOwner2的最优x_2 = 0.0976
new: DataOwner3的最优x_3 = 0.1268
new: DataOwner4的最优x_4 = 0.1223
new: DataOwner5的最优x_5 = 0.1370
new: DataOwner6的最优x_6 = 0.1974
new: DataOwner7的最优x_7 = 0.1207
new: DataOwner8的最优x_8 = 0.1133
new: DataOwner9的最优x_9 = 0.1461
new: DataOwner10的最优x_10 = 0.0991
eta:1.43
new: DataOwner1的最优x_1 = 0.1093
new: DataOwner2的最优x_2 = 0.0983
new: DataOwner3的最优x_3 = 0.1277
new: DataOwner4的最优x_4 = 0.1231
new: DataOwner5的最优x_5 = 0.1380
new: DataOwner6的最优x_6 = 0.1987
new: DataOwner7的最优x_7 = 0.1215
new: DataOwner8的最优x_8 = 0.1141
new: DataOwner9的最优x_9 = 0.1471
new: DataOwner10的最优x_10 = 0.0998
eta:1.44
new: DataOwner1的最优x_1 = 0.1100
new: DataOwner2的最优x_2 = 0.0990
new: DataOwner3的最优x_3 = 0.1286
new: DataOwner4的最优x_4 = 0.1240
new: DataOwner5的最优x_5 = 0.1389
new: DataOwner6的最优x_6 = 0.2001
new: DataOwner7的最优x_7 = 0.1224
new: DataOwner8的最优x_8 = 0.1149
new: DataOwner9的最优x_9 = 0.1481
new: DataOwner10的最优x_10 = 0.1005
eta:1.45
new: DataOwner1的最优x_1 = 0.1108
new: DataOwner2的最优x_2 = 0.0997
new: DataOwner3的最优x_3 = 0.1295
new: DataOwner4的最优x_4 = 0.1249
new: DataOwner5的最优x_5 = 0.1399
new: DataOwner6的最优x_6 = 0.2015
new: DataOwner7的最优x_7 = 0.1232
new: DataOwner8的最优x_8 = 0.1157
new: DataOwner9的最优x_9 = 0.1492
new: DataOwner10的最优x_10 = 0.1012
eta:1.46
new: DataOwner1的最优x_1 = 0.1116
new: DataOwner2的最优x_2 = 0.1003
new: DataOwner3的最优x_3 = 0.1303
new: DataOwner4的最优x_4 = 0.1257
new: DataOwner5的最优x_5 = 0.1409
new: DataOwner6的最优x_6 = 0.2029
new: DataOwner7的最优x_7 = 0.1241
new: DataOwner8的最优x_8 = 0.1165
new: DataOwner9的最优x_9 = 0.1502
new: DataOwner10的最优x_10 = 0.1019
eta:1.47
new: DataOwner1的最优x_1 = 0.1123
new: DataOwner2的最优x_2 = 0.1010
new: DataOwner3的最优x_3 = 0.1312
new: DataOwner4的最优x_4 = 0.1266
new: DataOwner5的最优x_5 = 0.1418
new: DataOwner6的最优x_6 = 0.2043
new: DataOwner7的最优x_7 = 0.1249
new: DataOwner8的最优x_8 = 0.1173
new: DataOwner9的最优x_9 = 0.1512
new: DataOwner10的最优x_10 = 0.1026
eta:1.48
new: DataOwner1的最优x_1 = 0.1131
new: DataOwner2的最优x_2 = 0.1017
new: DataOwner3的最优x_3 = 0.1321
new: DataOwner4的最优x_4 = 0.1274
new: DataOwner5的最优x_5 = 0.1428
new: DataOwner6的最优x_6 = 0.2057
new: DataOwner7的最优x_7 = 0.1258
new: DataOwner8的最优x_8 = 0.1181
new: DataOwner9的最优x_9 = 0.1522
new: DataOwner10的最优x_10 = 0.1033
eta:1.49
new: DataOwner1的最优x_1 = 0.1138
new: DataOwner2的最优x_2 = 0.1024
new: DataOwner3的最优x_3 = 0.1330
new: DataOwner4的最优x_4 = 0.1283
new: DataOwner5的最优x_5 = 0.1437
new: DataOwner6的最优x_6 = 0.2071
new: DataOwner7的最优x_7 = 0.1266
new: DataOwner8的最优x_8 = 0.1189
new: DataOwner9的最优x_9 = 0.1533
new: DataOwner10的最优x_10 = 0.1040
eta:1.5
new: DataOwner1的最优x_1 = 0.1146
new: DataOwner2的最优x_2 = 0.1031
new: DataOwner3的最优x_3 = 0.1339
new: DataOwner4的最优x_4 = 0.1292
new: DataOwner5的最优x_5 = 0.1447
new: DataOwner6的最优x_6 = 0.2085
new: DataOwner7的最优x_7 = 0.1275
new: DataOwner8的最优x_8 = 0.1197
new: DataOwner9的最优x_9 = 0.1543
new: DataOwner10的最优x_10 = 0.1047
eta:1.51
new: DataOwner1的最优x_1 = 0.1154
new: DataOwner2的最优x_2 = 0.1038
new: DataOwner3的最优x_3 = 0.1348
new: DataOwner4的最优x_4 = 0.1300
new: DataOwner5的最优x_5 = 0.1457
new: DataOwner6的最优x_6 = 0.2099
new: DataOwner7的最优x_7 = 0.1283
new: DataOwner8的最优x_8 = 0.1205
new: DataOwner9的最优x_9 = 0.1553
new: DataOwner10的最优x_10 = 0.1054
eta:1.52
new: DataOwner1的最优x_1 = 0.1161
new: DataOwner2的最优x_2 = 0.1045
new: DataOwner3的最优x_3 = 0.1357
new: DataOwner4的最优x_4 = 0.1309
new: DataOwner5的最优x_5 = 0.1466
new: DataOwner6的最优x_6 = 0.2113
new: DataOwner7的最优x_7 = 0.1292
new: DataOwner8的最优x_8 = 0.1212
new: DataOwner9的最优x_9 = 0.1564
new: DataOwner10的最优x_10 = 0.1061
eta:1.53
new: DataOwner1的最优x_1 = 0.1169
new: DataOwner2的最优x_2 = 0.1051
new: DataOwner3的最优x_3 = 0.1366
new: DataOwner4的最优x_4 = 0.1318
new: DataOwner5的最优x_5 = 0.1476
new: DataOwner6的最优x_6 = 0.2126
new: DataOwner7的最优x_7 = 0.1300
new: DataOwner8的最优x_8 = 0.1220
new: DataOwner9的最优x_9 = 0.1574
new: DataOwner10的最优x_10 = 0.1068
eta:1.54
new: DataOwner1的最优x_1 = 0.1177
new: DataOwner2的最优x_2 = 0.1058
new: DataOwner3的最优x_3 = 0.1375
new: DataOwner4的最优x_4 = 0.1326
new: DataOwner5的最优x_5 = 0.1486
new: DataOwner6的最优x_6 = 0.2140
new: DataOwner7的最优x_7 = 0.1309
new: DataOwner8的最优x_8 = 0.1228
new: DataOwner9的最优x_9 = 0.1584
new: DataOwner10的最优x_10 = 0.1075
eta:1.55
new: DataOwner1的最优x_1 = 0.1184
new: DataOwner2的最优x_2 = 0.1065
new: DataOwner3的最优x_3 = 0.1384
new: DataOwner4的最优x_4 = 0.1335
new: DataOwner5的最优x_5 = 0.1495
new: DataOwner6的最优x_6 = 0.2154
new: DataOwner7的最优x_7 = 0.1317
new: DataOwner8的最优x_8 = 0.1236
new: DataOwner9的最优x_9 = 0.1594
new: DataOwner10的最优x_10 = 0.1082
eta:1.56
new: DataOwner1的最优x_1 = 0.1192
new: DataOwner2的最优x_2 = 0.1072
new: DataOwner3的最优x_3 = 0.1393
new: DataOwner4的最优x_4 = 0.1343
new: DataOwner5的最优x_5 = 0.1505
new: DataOwner6的最优x_6 = 0.2168
new: DataOwner7的最优x_7 = 0.1326
new: DataOwner8的最优x_8 = 0.1244
new: DataOwner9的最优x_9 = 0.1605
new: DataOwner10的最优x_10 = 0.1089
eta:1.57
new: DataOwner1的最优x_1 = 0.1200
new: DataOwner2的最优x_2 = 0.1079
new: DataOwner3的最优x_3 = 0.1402
new: DataOwner4的最优x_4 = 0.1352
new: DataOwner5的最优x_5 = 0.1515
new: DataOwner6的最优x_6 = 0.2182
new: DataOwner7的最优x_7 = 0.1334
new: DataOwner8的最优x_8 = 0.1252
new: DataOwner9的最优x_9 = 0.1615
new: DataOwner10的最优x_10 = 0.1096
eta:1.58
new: DataOwner1的最优x_1 = 0.1207
new: DataOwner2的最优x_2 = 0.1086
new: DataOwner3的最优x_3 = 0.1411
new: DataOwner4的最优x_4 = 0.1361
new: DataOwner5的最优x_5 = 0.1524
new: DataOwner6的最优x_6 = 0.2196
new: DataOwner7的最优x_7 = 0.1343
new: DataOwner8的最优x_8 = 0.1260
new: DataOwner9的最优x_9 = 0.1625
new: DataOwner10的最优x_10 = 0.1103
eta:1.59
new: DataOwner1的最优x_1 = 0.1215
new: DataOwner2的最优x_2 = 0.1093
new: DataOwner3的最优x_3 = 0.1420
new: DataOwner4的最优x_4 = 0.1369
new: DataOwner5的最优x_5 = 0.1534
new: DataOwner6的最优x_6 = 0.2210
new: DataOwner7的最优x_7 = 0.1351
new: DataOwner8的最优x_8 = 0.1268
new: DataOwner9的最优x_9 = 0.1636
new: DataOwner10的最优x_10 = 0.1110
eta:1.6
new: DataOwner1的最优x_1 = 0.1223
new: DataOwner2的最优x_2 = 0.1100
new: DataOwner3的最优x_3 = 0.1428
new: DataOwner4的最优x_4 = 0.1378
new: DataOwner5的最优x_5 = 0.1544
new: DataOwner6的最优x_6 = 0.2224
new: DataOwner7的最优x_7 = 0.1360
new: DataOwner8的最优x_8 = 0.1276
new: DataOwner9的最优x_9 = 0.1646
new: DataOwner10的最优x_10 = 0.1117
eta:1.61
new: DataOwner1的最优x_1 = 0.1230
new: DataOwner2的最优x_2 = 0.1106
new: DataOwner3的最优x_3 = 0.1437
new: DataOwner4的最优x_4 = 0.1386
new: DataOwner5的最优x_5 = 0.1553
new: DataOwner6的最优x_6 = 0.2238
new: DataOwner7的最优x_7 = 0.1368
new: DataOwner8的最优x_8 = 0.1284
new: DataOwner9的最优x_9 = 0.1656
new: DataOwner10的最优x_10 = 0.1124
eta:1.62
new: DataOwner1的最优x_1 = 0.1238
new: DataOwner2的最优x_2 = 0.1113
new: DataOwner3的最优x_3 = 0.1446
new: DataOwner4的最优x_4 = 0.1395
new: DataOwner5的最优x_5 = 0.1563
new: DataOwner6的最优x_6 = 0.2252
new: DataOwner7的最优x_7 = 0.1377
new: DataOwner8的最优x_8 = 0.1292
new: DataOwner9的最优x_9 = 0.1666
new: DataOwner10的最优x_10 = 0.1131
eta:1.6300000000000001
new: DataOwner1的最优x_1 = 0.1245
new: DataOwner2的最优x_2 = 0.1120
new: DataOwner3的最优x_3 = 0.1455
new: DataOwner4的最优x_4 = 0.1404
new: DataOwner5的最优x_5 = 0.1573
new: DataOwner6的最优x_6 = 0.2265
new: DataOwner7的最优x_7 = 0.1385
new: DataOwner8的最优x_8 = 0.1300
new: DataOwner9的最优x_9 = 0.1677
new: DataOwner10的最优x_10 = 0.1138
eta:1.6400000000000001
new: DataOwner1的最优x_1 = 0.1253
new: DataOwner2的最优x_2 = 0.1127
new: DataOwner3的最优x_3 = 0.1464
new: DataOwner4的最优x_4 = 0.1412
new: DataOwner5的最优x_5 = 0.1582
new: DataOwner6的最优x_6 = 0.2279
new: DataOwner7的最优x_7 = 0.1394
new: DataOwner8的最优x_8 = 0.1308
new: DataOwner9的最优x_9 = 0.1687
new: DataOwner10的最优x_10 = 0.1145
eta:1.6500000000000001
new: DataOwner1的最优x_1 = 0.1261
new: DataOwner2的最优x_2 = 0.1134
new: DataOwner3的最优x_3 = 0.1473
new: DataOwner4的最优x_4 = 0.1421
new: DataOwner5的最优x_5 = 0.1592
new: DataOwner6的最优x_6 = 0.2293
new: DataOwner7的最优x_7 = 0.1402
new: DataOwner8的最优x_8 = 0.1316
new: DataOwner9的最优x_9 = 0.1697
new: DataOwner10的最优x_10 = 0.1152
eta:1.6600000000000001
new: DataOwner1的最优x_1 = 0.1268
new: DataOwner2的最优x_2 = 0.1141
new: DataOwner3的最优x_3 = 0.1482
new: DataOwner4的最优x_4 = 0.1429
new: DataOwner5的最优x_5 = 0.1601
new: DataOwner6的最优x_6 = 0.2307
new: DataOwner7的最优x_7 = 0.1411
new: DataOwner8的最优x_8 = 0.1324
new: DataOwner9的最优x_9 = 0.1708
new: DataOwner10的最优x_10 = 0.1159
eta:1.6700000000000002
new: DataOwner1的最优x_1 = 0.1276
new: DataOwner2的最优x_2 = 0.1148
new: DataOwner3的最优x_3 = 0.1491
new: DataOwner4的最优x_4 = 0.1438
new: DataOwner5的最优x_5 = 0.1611
new: DataOwner6的最优x_6 = 0.2321
new: DataOwner7的最优x_7 = 0.1419
new: DataOwner8的最优x_8 = 0.1332
new: DataOwner9的最优x_9 = 0.1718
new: DataOwner10的最优x_10 = 0.1166
eta:1.68
new: DataOwner1的最优x_1 = 0.1284
new: DataOwner2的最优x_2 = 0.1155
new: DataOwner3的最优x_3 = 0.1500
new: DataOwner4的最优x_4 = 0.1447
new: DataOwner5的最优x_5 = 0.1621
new: DataOwner6的最优x_6 = 0.2335
new: DataOwner7的最优x_7 = 0.1428
new: DataOwner8的最优x_8 = 0.1340
new: DataOwner9的最优x_9 = 0.1728
new: DataOwner10的最优x_10 = 0.1173
eta:1.69
new: DataOwner1的最优x_1 = 0.1291
new: DataOwner2的最优x_2 = 0.1161
new: DataOwner3的最优x_3 = 0.1509
new: DataOwner4的最优x_4 = 0.1455
new: DataOwner5的最优x_5 = 0.1630
new: DataOwner6的最优x_6 = 0.2349
new: DataOwner7的最优x_7 = 0.1436
new: DataOwner8的最优x_8 = 0.1348
new: DataOwner9的最优x_9 = 0.1738
new: DataOwner10的最优x_10 = 0.1180
eta:1.7
new: DataOwner1的最优x_1 = 0.1299
new: DataOwner2的最优x_2 = 0.1168
new: DataOwner3的最优x_3 = 0.1518
new: DataOwner4的最优x_4 = 0.1464
new: DataOwner5的最优x_5 = 0.1640
new: DataOwner6的最优x_6 = 0.2363
new: DataOwner7的最优x_7 = 0.1445
new: DataOwner8的最优x_8 = 0.1356
new: DataOwner9的最优x_9 = 0.1749
new: DataOwner10的最优x_10 = 0.1187
eta:1.71
new: DataOwner1的最优x_1 = 0.1307
new: DataOwner2的最优x_2 = 0.1175
new: DataOwner3的最优x_3 = 0.1527
new: DataOwner4的最优x_4 = 0.1473
new: DataOwner5的最优x_5 = 0.1650
new: DataOwner6的最优x_6 = 0.2377
new: DataOwner7的最优x_7 = 0.1453
new: DataOwner8的最优x_8 = 0.1364
new: DataOwner9的最优x_9 = 0.1759
new: DataOwner10的最优x_10 = 0.1194
eta:1.72
new: DataOwner1的最优x_1 = 0.1314
new: DataOwner2的最优x_2 = 0.1182
new: DataOwner3的最优x_3 = 0.1536
new: DataOwner4的最优x_4 = 0.1481
new: DataOwner5的最优x_5 = 0.1659
new: DataOwner6的最优x_6 = 0.2391
new: DataOwner7的最优x_7 = 0.1462
new: DataOwner8的最优x_8 = 0.1372
new: DataOwner9的最优x_9 = 0.1769
new: DataOwner10的最优x_10 = 0.1201
eta:1.73
new: DataOwner1的最优x_1 = 0.1322
new: DataOwner2的最优x_2 = 0.1189
new: DataOwner3的最优x_3 = 0.1545
new: DataOwner4的最优x_4 = 0.1490
new: DataOwner5的最优x_5 = 0.1669
new: DataOwner6的最优x_6 = 0.2404
new: DataOwner7的最优x_7 = 0.1470
new: DataOwner8的最优x_8 = 0.1380
new: DataOwner9的最优x_9 = 0.1780
new: DataOwner10的最优x_10 = 0.1208
eta:1.74
new: DataOwner1的最优x_1 = 0.1330
new: DataOwner2的最优x_2 = 0.1196
new: DataOwner3的最优x_3 = 0.1553
new: DataOwner4的最优x_4 = 0.1498
new: DataOwner5的最优x_5 = 0.1679
new: DataOwner6的最优x_6 = 0.2418
new: DataOwner7的最优x_7 = 0.1479
new: DataOwner8的最优x_8 = 0.1388
new: DataOwner9的最优x_9 = 0.1790
new: DataOwner10的最优x_10 = 0.1215
eta:1.75
new: DataOwner1的最优x_1 = 0.1337
new: DataOwner2的最优x_2 = 0.1203
new: DataOwner3的最优x_3 = 0.1562
new: DataOwner4的最优x_4 = 0.1507
new: DataOwner5的最优x_5 = 0.1688
new: DataOwner6的最优x_6 = 0.2432
new: DataOwner7的最优x_7 = 0.1487
new: DataOwner8的最优x_8 = 0.1396
new: DataOwner9的最优x_9 = 0.1800
new: DataOwner10的最优x_10 = 0.1222
eta:1.76
new: DataOwner1的最优x_1 = 0.1345
new: DataOwner2的最优x_2 = 0.1210
new: DataOwner3的最优x_3 = 0.1571
new: DataOwner4的最优x_4 = 0.1516
new: DataOwner5的最优x_5 = 0.1698
new: DataOwner6的最优x_6 = 0.2446
new: DataOwner7的最优x_7 = 0.1496
new: DataOwner8的最优x_8 = 0.1404
new: DataOwner9的最优x_9 = 0.1810
new: DataOwner10的最优x_10 = 0.1229
eta:1.77
new: DataOwner1的最优x_1 = 0.1352
new: DataOwner2的最优x_2 = 0.1216
new: DataOwner3的最优x_3 = 0.1580
new: DataOwner4的最优x_4 = 0.1524
new: DataOwner5的最优x_5 = 0.1708
new: DataOwner6的最优x_6 = 0.2460
new: DataOwner7的最优x_7 = 0.1504
new: DataOwner8的最优x_8 = 0.1412
new: DataOwner9的最优x_9 = 0.1821
new: DataOwner10的最优x_10 = 0.1236
eta:1.78
new: DataOwner1的最优x_1 = 0.1360
new: DataOwner2的最优x_2 = 0.1223
new: DataOwner3的最优x_3 = 0.1589
new: DataOwner4的最优x_4 = 0.1533
new: DataOwner5的最优x_5 = 0.1717
new: DataOwner6的最优x_6 = 0.2474
new: DataOwner7的最优x_7 = 0.1513
new: DataOwner8的最优x_8 = 0.1420
new: DataOwner9的最优x_9 = 0.1831
new: DataOwner10的最优x_10 = 0.1243
eta:1.79
new: DataOwner1的最优x_1 = 0.1368
new: DataOwner2的最优x_2 = 0.1230
new: DataOwner3的最优x_3 = 0.1598
new: DataOwner4的最优x_4 = 0.1541
new: DataOwner5的最优x_5 = 0.1727
new: DataOwner6的最优x_6 = 0.2488
new: DataOwner7的最优x_7 = 0.1521
new: DataOwner8的最优x_8 = 0.1428
new: DataOwner9的最优x_9 = 0.1841
new: DataOwner10的最优x_10 = 0.1250
eta:1.8
new: DataOwner1的最优x_1 = 0.1375
new: DataOwner2的最优x_2 = 0.1237
new: DataOwner3的最优x_3 = 0.1607
new: DataOwner4的最优x_4 = 0.1550
new: DataOwner5的最优x_5 = 0.1737
new: DataOwner6的最优x_6 = 0.2502
new: DataOwner7的最优x_7 = 0.1530
new: DataOwner8的最优x_8 = 0.1436
new: DataOwner9的最优x_9 = 0.1852
new: DataOwner10的最优x_10 = 0.1257
eta:1.81
new: DataOwner1的最优x_1 = 0.1383
new: DataOwner2的最优x_2 = 0.1244
new: DataOwner3的最优x_3 = 0.1616
new: DataOwner4的最优x_4 = 0.1559
new: DataOwner5的最优x_5 = 0.1746
new: DataOwner6的最优x_6 = 0.2516
new: DataOwner7的最优x_7 = 0.1538
new: DataOwner8的最优x_8 = 0.1444
new: DataOwner9的最优x_9 = 0.1862
new: DataOwner10的最优x_10 = 0.1264
eta:1.82
new: DataOwner1的最优x_1 = 0.1391
new: DataOwner2的最优x_2 = 0.1251
new: DataOwner3的最优x_3 = 0.1625
new: DataOwner4的最优x_4 = 0.1567
new: DataOwner5的最优x_5 = 0.1756
new: DataOwner6的最优x_6 = 0.2530
new: DataOwner7的最优x_7 = 0.1547
new: DataOwner8的最优x_8 = 0.1452
new: DataOwner9的最优x_9 = 0.1872
new: DataOwner10的最优x_10 = 0.1271
eta:1.83
new: DataOwner1的最优x_1 = 0.1398
new: DataOwner2的最优x_2 = 0.1258
new: DataOwner3的最优x_3 = 0.1634
new: DataOwner4的最优x_4 = 0.1576
new: DataOwner5的最优x_5 = 0.1765
new: DataOwner6的最优x_6 = 0.2543
new: DataOwner7的最优x_7 = 0.1555
new: DataOwner8的最优x_8 = 0.1460
new: DataOwner9的最优x_9 = 0.1882
new: DataOwner10的最优x_10 = 0.1278
eta:1.84
new: DataOwner1的最优x_1 = 0.1406
new: DataOwner2的最优x_2 = 0.1265
new: DataOwner3的最优x_3 = 0.1643
new: DataOwner4的最优x_4 = 0.1584
new: DataOwner5的最优x_5 = 0.1775
new: DataOwner6的最优x_6 = 0.2557
new: DataOwner7的最优x_7 = 0.1564
new: DataOwner8的最优x_8 = 0.1468
new: DataOwner9的最优x_9 = 0.1893
new: DataOwner10的最优x_10 = 0.1285
eta:1.85
new: DataOwner1的最优x_1 = 0.1414
new: DataOwner2的最优x_2 = 0.1271
new: DataOwner3的最优x_3 = 0.1652
new: DataOwner4的最优x_4 = 0.1593
new: DataOwner5的最优x_5 = 0.1785
new: DataOwner6的最优x_6 = 0.2571
new: DataOwner7的最优x_7 = 0.1572
new: DataOwner8的最优x_8 = 0.1476
new: DataOwner9的最优x_9 = 0.1903
new: DataOwner10的最优x_10 = 0.1292
eta:1.86
new: DataOwner1的最优x_1 = 0.1421
new: DataOwner2的最优x_2 = 0.1278
new: DataOwner3的最优x_3 = 0.1661
new: DataOwner4的最优x_4 = 0.1602
new: DataOwner5的最优x_5 = 0.1794
new: DataOwner6的最优x_6 = 0.2585
new: DataOwner7的最优x_7 = 0.1581
new: DataOwner8的最优x_8 = 0.1484
new: DataOwner9的最优x_9 = 0.1913
new: DataOwner10的最优x_10 = 0.1299
eta:1.87
new: DataOwner1的最优x_1 = 0.1429
new: DataOwner2的最优x_2 = 0.1285
new: DataOwner3的最优x_3 = 0.1670
new: DataOwner4的最优x_4 = 0.1610
new: DataOwner5的最优x_5 = 0.1804
new: DataOwner6的最优x_6 = 0.2599
new: DataOwner7的最优x_7 = 0.1589
new: DataOwner8的最优x_8 = 0.1492
new: DataOwner9的最优x_9 = 0.1924
new: DataOwner10的最优x_10 = 0.1306
eta:1.8800000000000001
new: DataOwner1的最优x_1 = 0.1436
new: DataOwner2的最优x_2 = 0.1292
new: DataOwner3的最优x_3 = 0.1678
new: DataOwner4的最优x_4 = 0.1619
new: DataOwner5的最优x_5 = 0.1814
new: DataOwner6的最优x_6 = 0.2613
new: DataOwner7的最优x_7 = 0.1598
new: DataOwner8的最优x_8 = 0.1500
new: DataOwner9的最优x_9 = 0.1934
new: DataOwner10的最优x_10 = 0.1313
eta:1.8900000000000001
new: DataOwner1的最优x_1 = 0.1444
new: DataOwner2的最优x_2 = 0.1299
new: DataOwner3的最优x_3 = 0.1687
new: DataOwner4的最优x_4 = 0.1628
new: DataOwner5的最优x_5 = 0.1823
new: DataOwner6的最优x_6 = 0.2627
new: DataOwner7的最优x_7 = 0.1606
new: DataOwner8的最优x_8 = 0.1508
new: DataOwner9的最优x_9 = 0.1944
new: DataOwner10的最优x_10 = 0.1320
eta:1.9000000000000001
new: DataOwner1的最优x_1 = 0.1452
new: DataOwner2的最优x_2 = 0.1306
new: DataOwner3的最优x_3 = 0.1696
new: DataOwner4的最优x_4 = 0.1636
new: DataOwner5的最优x_5 = 0.1833
new: DataOwner6的最优x_6 = 0.2641
new: DataOwner7的最优x_7 = 0.1615
new: DataOwner8的最优x_8 = 0.1516
new: DataOwner9的最优x_9 = 0.1954
new: DataOwner10的最优x_10 = 0.1327
eta:1.9100000000000001
new: DataOwner1的最优x_1 = 0.1459
new: DataOwner2的最优x_2 = 0.1313
new: DataOwner3的最优x_3 = 0.1705
new: DataOwner4的最优x_4 = 0.1645
new: DataOwner5的最优x_5 = 0.1843
new: DataOwner6的最优x_6 = 0.2655
new: DataOwner7的最优x_7 = 0.1623
new: DataOwner8的最优x_8 = 0.1524
new: DataOwner9的最优x_9 = 0.1965
new: DataOwner10的最优x_10 = 0.1334
eta:1.9200000000000002
new: DataOwner1的最优x_1 = 0.1467
new: DataOwner2的最优x_2 = 0.1320
new: DataOwner3的最优x_3 = 0.1714
new: DataOwner4的最优x_4 = 0.1653
new: DataOwner5的最优x_5 = 0.1852
new: DataOwner6的最优x_6 = 0.2669
new: DataOwner7的最优x_7 = 0.1632
new: DataOwner8的最优x_8 = 0.1532
new: DataOwner9的最优x_9 = 0.1975
new: DataOwner10的最优x_10 = 0.1341
eta:1.93
new: DataOwner1的最优x_1 = 0.1475
new: DataOwner2的最优x_2 = 0.1326
new: DataOwner3的最优x_3 = 0.1723
new: DataOwner4的最优x_4 = 0.1662
new: DataOwner5的最优x_5 = 0.1862
new: DataOwner6的最优x_6 = 0.2682
new: DataOwner7的最优x_7 = 0.1640
new: DataOwner8的最优x_8 = 0.1540
new: DataOwner9的最优x_9 = 0.1985
new: DataOwner10的最优x_10 = 0.1348
eta:1.94
new: DataOwner1的最优x_1 = 0.1482
new: DataOwner2的最优x_2 = 0.1333
new: DataOwner3的最优x_3 = 0.1732
new: DataOwner4的最优x_4 = 0.1671
new: DataOwner5的最优x_5 = 0.1872
new: DataOwner6的最优x_6 = 0.2696
new: DataOwner7的最优x_7 = 0.1649
new: DataOwner8的最优x_8 = 0.1548
new: DataOwner9的最优x_9 = 0.1996
new: DataOwner10的最优x_10 = 0.1355
eta:1.95
new: DataOwner1的最优x_1 = 0.1490
new: DataOwner2的最优x_2 = 0.1340
new: DataOwner3的最优x_3 = 0.1741
new: DataOwner4的最优x_4 = 0.1679
new: DataOwner5的最优x_5 = 0.1881
new: DataOwner6的最优x_6 = 0.2710
new: DataOwner7的最优x_7 = 0.1657
new: DataOwner8的最优x_8 = 0.1555
new: DataOwner9的最优x_9 = 0.2006
new: DataOwner10的最优x_10 = 0.1362
eta:1.96
new: DataOwner1的最优x_1 = 0.1498
new: DataOwner2的最优x_2 = 0.1347
new: DataOwner3的最优x_3 = 0.1750
new: DataOwner4的最优x_4 = 0.1688
new: DataOwner5的最优x_5 = 0.1891
new: DataOwner6的最优x_6 = 0.2724
new: DataOwner7的最优x_7 = 0.1666
new: DataOwner8的最优x_8 = 0.1563
new: DataOwner9的最优x_9 = 0.2016
new: DataOwner10的最优x_10 = 0.1369
eta:1.97
new: DataOwner1的最优x_1 = 0.1505
new: DataOwner2的最优x_2 = 0.1354
new: DataOwner3的最优x_3 = 0.1759
new: DataOwner4的最优x_4 = 0.1696
new: DataOwner5的最优x_5 = 0.1901
new: DataOwner6的最优x_6 = 0.2738
new: DataOwner7的最优x_7 = 0.1674
new: DataOwner8的最优x_8 = 0.1571
new: DataOwner9的最优x_9 = 0.2027
new: DataOwner10的最优x_10 = 0.1376
eta:1.98
new: DataOwner1的最优x_1 = 0.1513
new: DataOwner2的最优x_2 = 0.1361
new: DataOwner3的最优x_3 = 0.1768
new: DataOwner4的最优x_4 = 0.1705
new: DataOwner5的最优x_5 = 0.1910
new: DataOwner6的最优x_6 = 0.2752
new: DataOwner7的最优x_7 = 0.1683
new: DataOwner8的最优x_8 = 0.1579
new: DataOwner9的最优x_9 = 0.2037
new: DataOwner10的最优x_10 = 0.1383
eta:1.99
new: DataOwner1的最优x_1 = 0.1521
new: DataOwner2的最优x_2 = 0.1368
new: DataOwner3的最优x_3 = 0.1777
new: DataOwner4的最优x_4 = 0.1714
new: DataOwner5的最优x_5 = 0.1920
new: DataOwner6的最优x_6 = 0.2766
new: DataOwner7的最优x_7 = 0.1691
new: DataOwner8的最优x_8 = 0.1587
new: DataOwner9的最优x_9 = 0.2047
new: DataOwner10的最优x_10 = 0.1389
eta:2.0
new: DataOwner1的最优x_1 = 0.1528
new: DataOwner2的最优x_2 = 0.1374
new: DataOwner3的最优x_3 = 0.1786
new: DataOwner4的最优x_4 = 0.1722
new: DataOwner5的最优x_5 = 0.1930
new: DataOwner6的最优x_6 = 0.2780
new: DataOwner7的最优x_7 = 0.1700
new: DataOwner8的最优x_8 = 0.1595
new: DataOwner9的最优x_9 = 0.2057
new: DataOwner10的最优x_10 = 0.1396
eta:2.01
new: DataOwner1的最优x_1 = 0.1536
new: DataOwner2的最优x_2 = 0.1381
new: DataOwner3的最优x_3 = 0.1795
new: DataOwner4的最优x_4 = 0.1731
new: DataOwner5的最优x_5 = 0.1939
new: DataOwner6的最优x_6 = 0.2794
new: DataOwner7的最优x_7 = 0.1708
new: DataOwner8的最优x_8 = 0.1603
new: DataOwner9的最优x_9 = 0.2068
new: DataOwner10的最优x_10 = 0.1403
eta:2.02
new: DataOwner1的最优x_1 = 0.1543
new: DataOwner2的最优x_2 = 0.1388
new: DataOwner3的最优x_3 = 0.1803
new: DataOwner4的最优x_4 = 0.1740
new: DataOwner5的最优x_5 = 0.1949
new: DataOwner6的最优x_6 = 0.2808
new: DataOwner7的最优x_7 = 0.1717
new: DataOwner8的最优x_8 = 0.1611
new: DataOwner9的最优x_9 = 0.2078
new: DataOwner10的最优x_10 = 0.1410
eta:2.03
new: DataOwner1的最优x_1 = 0.1551
new: DataOwner2的最优x_2 = 0.1395
new: DataOwner3的最优x_3 = 0.1812
new: DataOwner4的最优x_4 = 0.1748
new: DataOwner5的最优x_5 = 0.1958
new: DataOwner6的最优x_6 = 0.2821
new: DataOwner7的最优x_7 = 0.1725
new: DataOwner8的最优x_8 = 0.1619
new: DataOwner9的最优x_9 = 0.2088
new: DataOwner10的最优x_10 = 0.1417
eta:2.04
new: DataOwner1的最优x_1 = 0.1559
new: DataOwner2的最优x_2 = 0.1402
new: DataOwner3的最优x_3 = 0.1821
new: DataOwner4的最优x_4 = 0.1757
new: DataOwner5的最优x_5 = 0.1968
new: DataOwner6的最优x_6 = 0.2835
new: DataOwner7的最优x_7 = 0.1734
new: DataOwner8的最优x_8 = 0.1627
new: DataOwner9的最优x_9 = 0.2099
new: DataOwner10的最优x_10 = 0.1424
eta:2.05
new: DataOwner1的最优x_1 = 0.1566
new: DataOwner2的最优x_2 = 0.1409
new: DataOwner3的最优x_3 = 0.1830
new: DataOwner4的最优x_4 = 0.1765
new: DataOwner5的最优x_5 = 0.1978
new: DataOwner6的最优x_6 = 0.2849
new: DataOwner7的最优x_7 = 0.1742
new: DataOwner8的最优x_8 = 0.1635
new: DataOwner9的最优x_9 = 0.2109
new: DataOwner10的最优x_10 = 0.1431
eta:2.0599999999999996
new: DataOwner1的最优x_1 = 0.1574
new: DataOwner2的最优x_2 = 0.1416
new: DataOwner3的最优x_3 = 0.1839
new: DataOwner4的最优x_4 = 0.1774
new: DataOwner5的最优x_5 = 0.1987
new: DataOwner6的最优x_6 = 0.2863
new: DataOwner7的最优x_7 = 0.1751
new: DataOwner8的最优x_8 = 0.1643
new: DataOwner9的最优x_9 = 0.2119
new: DataOwner10的最优x_10 = 0.1438
eta:2.07
new: DataOwner1的最优x_1 = 0.1582
new: DataOwner2的最优x_2 = 0.1423
new: DataOwner3的最优x_3 = 0.1848
new: DataOwner4的最优x_4 = 0.1783
new: DataOwner5的最优x_5 = 0.1997
new: DataOwner6的最优x_6 = 0.2877
new: DataOwner7的最优x_7 = 0.1759
new: DataOwner8的最优x_8 = 0.1651
new: DataOwner9的最优x_9 = 0.2129
new: DataOwner10的最优x_10 = 0.1445
eta:2.0799999999999996
new: DataOwner1的最优x_1 = 0.1589
new: DataOwner2的最优x_2 = 0.1429
new: DataOwner3的最优x_3 = 0.1857
new: DataOwner4的最优x_4 = 0.1791
new: DataOwner5的最优x_5 = 0.2007
new: DataOwner6的最优x_6 = 0.2891
new: DataOwner7的最优x_7 = 0.1768
new: DataOwner8的最优x_8 = 0.1659
new: DataOwner9的最优x_9 = 0.2140
new: DataOwner10的最优x_10 = 0.1452
eta:2.09
new: DataOwner1的最优x_1 = 0.1597
new: DataOwner2的最优x_2 = 0.1436
new: DataOwner3的最优x_3 = 0.1866
new: DataOwner4的最优x_4 = 0.1800
new: DataOwner5的最优x_5 = 0.2016
new: DataOwner6的最优x_6 = 0.2905
new: DataOwner7的最优x_7 = 0.1776
new: DataOwner8的最优x_8 = 0.1667
new: DataOwner9的最优x_9 = 0.2150
new: DataOwner10的最优x_10 = 0.1459
eta:2.0999999999999996
new: DataOwner1的最优x_1 = 0.1605
new: DataOwner2的最优x_2 = 0.1443
new: DataOwner3的最优x_3 = 0.1875
new: DataOwner4的最优x_4 = 0.1808
new: DataOwner5的最优x_5 = 0.2026
new: DataOwner6的最优x_6 = 0.2919
new: DataOwner7的最优x_7 = 0.1785
new: DataOwner8的最优x_8 = 0.1675
new: DataOwner9的最优x_9 = 0.2160
new: DataOwner10的最优x_10 = 0.1466
eta:2.11
new: DataOwner1的最优x_1 = 0.1612
new: DataOwner2的最优x_2 = 0.1450
new: DataOwner3的最优x_3 = 0.1884
new: DataOwner4的最优x_4 = 0.1817
new: DataOwner5的最优x_5 = 0.2036
new: DataOwner6的最优x_6 = 0.2933
new: DataOwner7的最优x_7 = 0.1793
new: DataOwner8的最优x_8 = 0.1683
new: DataOwner9的最优x_9 = 0.2171
new: DataOwner10的最优x_10 = 0.1473
eta:2.1199999999999997
new: DataOwner1的最优x_1 = 0.1620
new: DataOwner2的最优x_2 = 0.1457
new: DataOwner3的最优x_3 = 0.1893
new: DataOwner4的最优x_4 = 0.1826
new: DataOwner5的最优x_5 = 0.2045
new: DataOwner6的最优x_6 = 0.2946
new: DataOwner7的最优x_7 = 0.1802
new: DataOwner8的最优x_8 = 0.1691
new: DataOwner9的最优x_9 = 0.2181
new: DataOwner10的最优x_10 = 0.1480
eta:2.13
new: DataOwner1的最优x_1 = 0.1628
new: DataOwner2的最优x_2 = 0.1464
new: DataOwner3的最优x_3 = 0.1902
new: DataOwner4的最优x_4 = 0.1834
new: DataOwner5的最优x_5 = 0.2055
new: DataOwner6的最优x_6 = 0.2960
new: DataOwner7的最优x_7 = 0.1810
new: DataOwner8的最优x_8 = 0.1699
new: DataOwner9的最优x_9 = 0.2191
new: DataOwner10的最优x_10 = 0.1487
eta:2.1399999999999997
new: DataOwner1的最优x_1 = 0.1635
new: DataOwner2的最优x_2 = 0.1471
new: DataOwner3的最优x_3 = 0.1911
new: DataOwner4的最优x_4 = 0.1843
new: DataOwner5的最优x_5 = 0.2065
new: DataOwner6的最优x_6 = 0.2974
new: DataOwner7的最优x_7 = 0.1819
new: DataOwner8的最优x_8 = 0.1707
new: DataOwner9的最优x_9 = 0.2201
new: DataOwner10的最优x_10 = 0.1494
eta:2.15
new: DataOwner1的最优x_1 = 0.1643
new: DataOwner2的最优x_2 = 0.1478
new: DataOwner3的最优x_3 = 0.1920
new: DataOwner4的最优x_4 = 0.1851
new: DataOwner5的最优x_5 = 0.2074
new: DataOwner6的最优x_6 = 0.2988
new: DataOwner7的最优x_7 = 0.1827
new: DataOwner8的最优x_8 = 0.1715
new: DataOwner9的最优x_9 = 0.2212
new: DataOwner10的最优x_10 = 0.1501
eta:2.1599999999999997
new: DataOwner1的最优x_1 = 0.1650
new: DataOwner2的最优x_2 = 0.1484
new: DataOwner3的最优x_3 = 0.1928
new: DataOwner4的最优x_4 = 0.1860
new: DataOwner5的最优x_5 = 0.2084
new: DataOwner6的最优x_6 = 0.3002
new: DataOwner7的最优x_7 = 0.1836
new: DataOwner8的最优x_8 = 0.1723
new: DataOwner9的最优x_9 = 0.2222
new: DataOwner10的最优x_10 = 0.1508
eta:2.17
new: DataOwner1的最优x_1 = 0.1658
new: DataOwner2的最优x_2 = 0.1491
new: DataOwner3的最优x_3 = 0.1937
new: DataOwner4的最优x_4 = 0.1869
new: DataOwner5的最优x_5 = 0.2094
new: DataOwner6的最优x_6 = 0.3016
new: DataOwner7的最优x_7 = 0.1844
new: DataOwner8的最优x_8 = 0.1731
new: DataOwner9的最优x_9 = 0.2232
new: DataOwner10的最优x_10 = 0.1515
eta:2.1799999999999997
new: DataOwner1的最优x_1 = 0.1666
new: DataOwner2的最优x_2 = 0.1498
new: DataOwner3的最优x_3 = 0.1946
new: DataOwner4的最优x_4 = 0.1877
new: DataOwner5的最优x_5 = 0.2103
new: DataOwner6的最优x_6 = 0.3030
new: DataOwner7的最优x_7 = 0.1853
new: DataOwner8的最优x_8 = 0.1739
new: DataOwner9的最优x_9 = 0.2243
new: DataOwner10的最优x_10 = 0.1522
eta:2.19
new: DataOwner1的最优x_1 = 0.1673
new: DataOwner2的最优x_2 = 0.1505
new: DataOwner3的最优x_3 = 0.1955
new: DataOwner4的最优x_4 = 0.1886
new: DataOwner5的最优x_5 = 0.2113
new: DataOwner6的最优x_6 = 0.3044
new: DataOwner7的最优x_7 = 0.1861
new: DataOwner8的最优x_8 = 0.1747
new: DataOwner9的最优x_9 = 0.2253
new: DataOwner10的最优x_10 = 0.1529
eta:2.1999999999999997
new: DataOwner1的最优x_1 = 0.1681
new: DataOwner2的最优x_2 = 0.1512
new: DataOwner3的最优x_3 = 0.1964
new: DataOwner4的最优x_4 = 0.1895
new: DataOwner5的最优x_5 = 0.2122
new: DataOwner6的最优x_6 = 0.3058
new: DataOwner7的最优x_7 = 0.1870
new: DataOwner8的最优x_8 = 0.1755
new: DataOwner9的最优x_9 = 0.2263
new: DataOwner10的最优x_10 = 0.1536
eta:2.21
new: DataOwner1的最优x_1 = 0.1689
new: DataOwner2的最优x_2 = 0.1519
new: DataOwner3的最优x_3 = 0.1973
new: DataOwner4的最优x_4 = 0.1903
new: DataOwner5的最优x_5 = 0.2132
new: DataOwner6的最优x_6 = 0.3072
new: DataOwner7的最优x_7 = 0.1878
new: DataOwner8的最优x_8 = 0.1763
new: DataOwner9的最优x_9 = 0.2273
new: DataOwner10的最优x_10 = 0.1543
eta:2.2199999999999998
new: DataOwner1的最优x_1 = 0.1696
new: DataOwner2的最优x_2 = 0.1526
new: DataOwner3的最优x_3 = 0.1982
new: DataOwner4的最优x_4 = 0.1912
new: DataOwner5的最优x_5 = 0.2142
new: DataOwner6的最优x_6 = 0.3085
new: DataOwner7的最优x_7 = 0.1887
new: DataOwner8的最优x_8 = 0.1771
new: DataOwner9的最优x_9 = 0.2284
new: DataOwner10的最优x_10 = 0.1550
eta:2.23
new: DataOwner1的最优x_1 = 0.1704
new: DataOwner2的最优x_2 = 0.1533
new: DataOwner3的最优x_3 = 0.1991
new: DataOwner4的最优x_4 = 0.1920
new: DataOwner5的最优x_5 = 0.2151
new: DataOwner6的最优x_6 = 0.3099
new: DataOwner7的最优x_7 = 0.1895
new: DataOwner8的最优x_8 = 0.1779
new: DataOwner9的最优x_9 = 0.2294
new: DataOwner10的最优x_10 = 0.1557
eta:2.2399999999999998
new: DataOwner1的最优x_1 = 0.1712
new: DataOwner2的最优x_2 = 0.1539
new: DataOwner3的最优x_3 = 0.2000
new: DataOwner4的最优x_4 = 0.1929
new: DataOwner5的最优x_5 = 0.2161
new: DataOwner6的最优x_6 = 0.3113
new: DataOwner7的最优x_7 = 0.1904
new: DataOwner8的最优x_8 = 0.1787
new: DataOwner9的最优x_9 = 0.2304
new: DataOwner10的最优x_10 = 0.1564
eta:2.25
new: DataOwner1的最优x_1 = 0.1719
new: DataOwner2的最优x_2 = 0.1546
new: DataOwner3的最优x_3 = 0.2009
new: DataOwner4的最优x_4 = 0.1938
new: DataOwner5的最优x_5 = 0.2171
new: DataOwner6的最优x_6 = 0.3127
new: DataOwner7的最优x_7 = 0.1912
new: DataOwner8的最优x_8 = 0.1795
new: DataOwner9的最优x_9 = 0.2315
new: DataOwner10的最优x_10 = 0.1571
eta:2.26
new: DataOwner1的最优x_1 = 0.1727
new: DataOwner2的最优x_2 = 0.1553
new: DataOwner3的最优x_3 = 0.2018
new: DataOwner4的最优x_4 = 0.1946
new: DataOwner5的最优x_5 = 0.2180
new: DataOwner6的最优x_6 = 0.3141
new: DataOwner7的最优x_7 = 0.1921
new: DataOwner8的最优x_8 = 0.1803
new: DataOwner9的最优x_9 = 0.2325
new: DataOwner10的最优x_10 = 0.1578
eta:2.27
new: DataOwner1的最优x_1 = 0.1734
new: DataOwner2的最优x_2 = 0.1560
new: DataOwner3的最优x_3 = 0.2027
new: DataOwner4的最优x_4 = 0.1955
new: DataOwner5的最优x_5 = 0.2190
new: DataOwner6的最优x_6 = 0.3155
new: DataOwner7的最优x_7 = 0.1929
new: DataOwner8的最优x_8 = 0.1811
new: DataOwner9的最优x_9 = 0.2335
new: DataOwner10的最优x_10 = 0.1585
eta:2.28
new: DataOwner1的最优x_1 = 0.1742
new: DataOwner2的最优x_2 = 0.1567
new: DataOwner3的最优x_3 = 0.2036
new: DataOwner4的最优x_4 = 0.1963
new: DataOwner5的最优x_5 = 0.2200
new: DataOwner6的最优x_6 = 0.3169
new: DataOwner7的最优x_7 = 0.1938
new: DataOwner8的最优x_8 = 0.1819
new: DataOwner9的最优x_9 = 0.2345
new: DataOwner10的最优x_10 = 0.1592
eta:2.29
new: DataOwner1的最优x_1 = 0.1750
new: DataOwner2的最优x_2 = 0.1574
new: DataOwner3的最优x_3 = 0.2045
new: DataOwner4的最优x_4 = 0.1972
new: DataOwner5的最优x_5 = 0.2209
new: DataOwner6的最优x_6 = 0.3183
new: DataOwner7的最优x_7 = 0.1946
new: DataOwner8的最优x_8 = 0.1827
new: DataOwner9的最优x_9 = 0.2356
new: DataOwner10的最优x_10 = 0.1599
eta:2.3
new: DataOwner1的最优x_1 = 0.1757
new: DataOwner2的最优x_2 = 0.1581
new: DataOwner3的最优x_3 = 0.2053
new: DataOwner4的最优x_4 = 0.1981
new: DataOwner5的最优x_5 = 0.2219
new: DataOwner6的最优x_6 = 0.3197
new: DataOwner7的最优x_7 = 0.1955
new: DataOwner8的最优x_8 = 0.1835
new: DataOwner9的最优x_9 = 0.2366
new: DataOwner10的最优x_10 = 0.1606
eta:2.31
new: DataOwner1的最优x_1 = 0.1765
new: DataOwner2的最优x_2 = 0.1588
new: DataOwner3的最优x_3 = 0.2062
new: DataOwner4的最优x_4 = 0.1989
new: DataOwner5的最优x_5 = 0.2229
new: DataOwner6的最优x_6 = 0.3211
new: DataOwner7的最优x_7 = 0.1963
new: DataOwner8的最优x_8 = 0.1843
new: DataOwner9的最优x_9 = 0.2376
new: DataOwner10的最优x_10 = 0.1613
eta:2.32
new: DataOwner1的最优x_1 = 0.1773
new: DataOwner2的最优x_2 = 0.1594
new: DataOwner3的最优x_3 = 0.2071
new: DataOwner4的最优x_4 = 0.1998
new: DataOwner5的最优x_5 = 0.2238
new: DataOwner6的最优x_6 = 0.3224
new: DataOwner7的最优x_7 = 0.1972
new: DataOwner8的最优x_8 = 0.1851
new: DataOwner9的最优x_9 = 0.2387
new: DataOwner10的最优x_10 = 0.1620
eta:2.3299999999999996
new: DataOwner1的最优x_1 = 0.1780
new: DataOwner2的最优x_2 = 0.1601
new: DataOwner3的最优x_3 = 0.2080
new: DataOwner4的最优x_4 = 0.2006
new: DataOwner5的最优x_5 = 0.2248
new: DataOwner6的最优x_6 = 0.3238
new: DataOwner7的最优x_7 = 0.1980
new: DataOwner8的最优x_8 = 0.1859
new: DataOwner9的最优x_9 = 0.2397
new: DataOwner10的最优x_10 = 0.1627
eta:2.34
new: DataOwner1的最优x_1 = 0.1788
new: DataOwner2的最优x_2 = 0.1608
new: DataOwner3的最优x_3 = 0.2089
new: DataOwner4的最优x_4 = 0.2015
new: DataOwner5的最优x_5 = 0.2258
new: DataOwner6的最优x_6 = 0.3252
new: DataOwner7的最优x_7 = 0.1989
new: DataOwner8的最优x_8 = 0.1867
new: DataOwner9的最优x_9 = 0.2407
new: DataOwner10的最优x_10 = 0.1634
eta:2.3499999999999996
new: DataOwner1的最优x_1 = 0.1796
new: DataOwner2的最优x_2 = 0.1615
new: DataOwner3的最优x_3 = 0.2098
new: DataOwner4的最优x_4 = 0.2024
new: DataOwner5的最优x_5 = 0.2267
new: DataOwner6的最优x_6 = 0.3266
new: DataOwner7的最优x_7 = 0.1997
new: DataOwner8的最优x_8 = 0.1875
new: DataOwner9的最优x_9 = 0.2417
new: DataOwner10的最优x_10 = 0.1641
eta:2.36
new: DataOwner1的最优x_1 = 0.1803
new: DataOwner2的最优x_2 = 0.1622
new: DataOwner3的最优x_3 = 0.2107
new: DataOwner4的最优x_4 = 0.2032
new: DataOwner5的最优x_5 = 0.2277
new: DataOwner6的最优x_6 = 0.3280
new: DataOwner7的最优x_7 = 0.2006
new: DataOwner8的最优x_8 = 0.1883
new: DataOwner9的最优x_9 = 0.2428
new: DataOwner10的最优x_10 = 0.1648
eta:2.3699999999999997
new: DataOwner1的最优x_1 = 0.1811
new: DataOwner2的最优x_2 = 0.1629
new: DataOwner3的最优x_3 = 0.2116
new: DataOwner4的最优x_4 = 0.2041
new: DataOwner5的最优x_5 = 0.2286
new: DataOwner6的最优x_6 = 0.3294
new: DataOwner7的最优x_7 = 0.2014
new: DataOwner8的最优x_8 = 0.1891
new: DataOwner9的最优x_9 = 0.2438
new: DataOwner10的最优x_10 = 0.1655
eta:2.38
new: DataOwner1的最优x_1 = 0.1819
new: DataOwner2的最优x_2 = 0.1636
new: DataOwner3的最优x_3 = 0.2125
new: DataOwner4的最优x_4 = 0.2050
new: DataOwner5的最优x_5 = 0.2296
new: DataOwner6的最优x_6 = 0.3308
new: DataOwner7的最优x_7 = 0.2023
new: DataOwner8的最优x_8 = 0.1898
new: DataOwner9的最优x_9 = 0.2448
new: DataOwner10的最优x_10 = 0.1662
eta:2.3899999999999997
new: DataOwner1的最优x_1 = 0.1826
new: DataOwner2的最优x_2 = 0.1643
new: DataOwner3的最优x_3 = 0.2134
new: DataOwner4的最优x_4 = 0.2058
new: DataOwner5的最优x_5 = 0.2306
new: DataOwner6的最优x_6 = 0.3322
new: DataOwner7的最优x_7 = 0.2031
new: DataOwner8的最优x_8 = 0.1906
new: DataOwner9的最优x_9 = 0.2459
new: DataOwner10的最优x_10 = 0.1669
eta:2.4
new: DataOwner1的最优x_1 = 0.1834
new: DataOwner2的最优x_2 = 0.1649
new: DataOwner3的最优x_3 = 0.2143
new: DataOwner4的最优x_4 = 0.2067
new: DataOwner5的最优x_5 = 0.2315
new: DataOwner6的最优x_6 = 0.3336
new: DataOwner7的最优x_7 = 0.2040
new: DataOwner8的最优x_8 = 0.1914
new: DataOwner9的最优x_9 = 0.2469
new: DataOwner10的最优x_10 = 0.1676
eta:2.4099999999999997
new: DataOwner1的最优x_1 = 0.1841
new: DataOwner2的最优x_2 = 0.1656
new: DataOwner3的最优x_3 = 0.2152
new: DataOwner4的最优x_4 = 0.2075
new: DataOwner5的最优x_5 = 0.2325
new: DataOwner6的最优x_6 = 0.3350
new: DataOwner7的最优x_7 = 0.2048
new: DataOwner8的最优x_8 = 0.1922
new: DataOwner9的最优x_9 = 0.2479
new: DataOwner10的最优x_10 = 0.1683
eta:2.42
new: DataOwner1的最优x_1 = 0.1849
new: DataOwner2的最优x_2 = 0.1663
new: DataOwner3的最优x_3 = 0.2161
new: DataOwner4的最优x_4 = 0.2084
new: DataOwner5的最优x_5 = 0.2335
new: DataOwner6的最优x_6 = 0.3363
new: DataOwner7的最优x_7 = 0.2057
new: DataOwner8的最优x_8 = 0.1930
new: DataOwner9的最优x_9 = 0.2489
new: DataOwner10的最优x_10 = 0.1690
eta:2.4299999999999997
new: DataOwner1的最优x_1 = 0.1857
new: DataOwner2的最优x_2 = 0.1670
new: DataOwner3的最优x_3 = 0.2169
new: DataOwner4的最优x_4 = 0.2093
new: DataOwner5的最优x_5 = 0.2344
new: DataOwner6的最优x_6 = 0.3377
new: DataOwner7的最优x_7 = 0.2065
new: DataOwner8的最优x_8 = 0.1938
new: DataOwner9的最优x_9 = 0.2500
new: DataOwner10的最优x_10 = 0.1697
eta:2.44
new: DataOwner1的最优x_1 = 0.1864
new: DataOwner2的最优x_2 = 0.1677
new: DataOwner3的最优x_3 = 0.2178
new: DataOwner4的最优x_4 = 0.2101
new: DataOwner5的最优x_5 = 0.2354
new: DataOwner6的最优x_6 = 0.3391
new: DataOwner7的最优x_7 = 0.2074
new: DataOwner8的最优x_8 = 0.1946
new: DataOwner9的最优x_9 = 0.2510
new: DataOwner10的最优x_10 = 0.1704
eta:2.4499999999999997
new: DataOwner1的最优x_1 = 0.1872
new: DataOwner2的最优x_2 = 0.1684
new: DataOwner3的最优x_3 = 0.2187
new: DataOwner4的最优x_4 = 0.2110
new: DataOwner5的最优x_5 = 0.2364
new: DataOwner6的最优x_6 = 0.3405
new: DataOwner7的最优x_7 = 0.2082
new: DataOwner8的最优x_8 = 0.1954
new: DataOwner9的最优x_9 = 0.2520
new: DataOwner10的最优x_10 = 0.1711
eta:2.46
new: DataOwner1的最优x_1 = 0.1880
new: DataOwner2的最优x_2 = 0.1691
new: DataOwner3的最优x_3 = 0.2196
new: DataOwner4的最优x_4 = 0.2118
new: DataOwner5的最优x_5 = 0.2373
new: DataOwner6的最优x_6 = 0.3419
new: DataOwner7的最优x_7 = 0.2091
new: DataOwner8的最优x_8 = 0.1962
new: DataOwner9的最优x_9 = 0.2531
new: DataOwner10的最优x_10 = 0.1718
eta:2.4699999999999998
new: DataOwner1的最优x_1 = 0.1887
new: DataOwner2的最优x_2 = 0.1697
new: DataOwner3的最优x_3 = 0.2205
new: DataOwner4的最优x_4 = 0.2127
new: DataOwner5的最优x_5 = 0.2383
new: DataOwner6的最优x_6 = 0.3433
new: DataOwner7的最优x_7 = 0.2099
new: DataOwner8的最优x_8 = 0.1970
new: DataOwner9的最优x_9 = 0.2541
new: DataOwner10的最优x_10 = 0.1725
eta:2.48
new: DataOwner1的最优x_1 = 0.1895
new: DataOwner2的最优x_2 = 0.1704
new: DataOwner3的最优x_3 = 0.2214
new: DataOwner4的最优x_4 = 0.2136
new: DataOwner5的最优x_5 = 0.2393
new: DataOwner6的最优x_6 = 0.3447
new: DataOwner7的最优x_7 = 0.2108
new: DataOwner8的最优x_8 = 0.1978
new: DataOwner9的最优x_9 = 0.2551
new: DataOwner10的最优x_10 = 0.1732
eta:2.4899999999999998
new: DataOwner1的最优x_1 = 0.1903
new: DataOwner2的最优x_2 = 0.1711
new: DataOwner3的最优x_3 = 0.2223
new: DataOwner4的最优x_4 = 0.2144
new: DataOwner5的最优x_5 = 0.2402
new: DataOwner6的最优x_6 = 0.3461
new: DataOwner7的最优x_7 = 0.2116
new: DataOwner8的最优x_8 = 0.1986
new: DataOwner9的最优x_9 = 0.2561
new: DataOwner10的最优x_10 = 0.1739
eta:2.5
new: DataOwner1的最优x_1 = 0.1910
new: DataOwner2的最优x_2 = 0.1718
new: DataOwner3的最优x_3 = 0.2232
new: DataOwner4的最优x_4 = 0.2153
new: DataOwner5的最优x_5 = 0.2412
new: DataOwner6的最优x_6 = 0.3475
new: DataOwner7的最优x_7 = 0.2125
new: DataOwner8的最优x_8 = 0.1994
new: DataOwner9的最优x_9 = 0.2572
new: DataOwner10的最优x_10 = 0.1746
eta:2.51
new: DataOwner1的最优x_1 = 0.1918
new: DataOwner2的最优x_2 = 0.1725
new: DataOwner3的最优x_3 = 0.2241
new: DataOwner4的最优x_4 = 0.2161
new: DataOwner5的最优x_5 = 0.2422
new: DataOwner6的最优x_6 = 0.3489
new: DataOwner7的最优x_7 = 0.2133
new: DataOwner8的最优x_8 = 0.2002
new: DataOwner9的最优x_9 = 0.2582
new: DataOwner10的最优x_10 = 0.1753
eta:2.52
new: DataOwner1的最优x_1 = 0.1925
new: DataOwner2的最优x_2 = 0.1732
new: DataOwner3的最优x_3 = 0.2250
new: DataOwner4的最优x_4 = 0.2170
new: DataOwner5的最优x_5 = 0.2431
new: DataOwner6的最优x_6 = 0.3502
new: DataOwner7的最优x_7 = 0.2142
new: DataOwner8的最优x_8 = 0.2010
new: DataOwner9的最优x_9 = 0.2592
new: DataOwner10的最优x_10 = 0.1760
eta:2.53
new: DataOwner1的最优x_1 = 0.1933
new: DataOwner2的最优x_2 = 0.1739
new: DataOwner3的最优x_3 = 0.2259
new: DataOwner4的最优x_4 = 0.2179
new: DataOwner5的最优x_5 = 0.2441
new: DataOwner6的最优x_6 = 0.3516
new: DataOwner7的最优x_7 = 0.2150
new: DataOwner8的最优x_8 = 0.2018
new: DataOwner9的最优x_9 = 0.2603
new: DataOwner10的最优x_10 = 0.1767
eta:2.54
new: DataOwner1的最优x_1 = 0.1941
new: DataOwner2的最优x_2 = 0.1746
new: DataOwner3的最优x_3 = 0.2268
new: DataOwner4的最优x_4 = 0.2187
new: DataOwner5的最优x_5 = 0.2450
new: DataOwner6的最优x_6 = 0.3530
new: DataOwner7的最优x_7 = 0.2159
new: DataOwner8的最优x_8 = 0.2026
new: DataOwner9的最优x_9 = 0.2613
new: DataOwner10的最优x_10 = 0.1774
eta:2.55
new: DataOwner1的最优x_1 = 0.1948
new: DataOwner2的最优x_2 = 0.1752
new: DataOwner3的最优x_3 = 0.2277
new: DataOwner4的最优x_4 = 0.2196
new: DataOwner5的最优x_5 = 0.2460
new: DataOwner6的最优x_6 = 0.3544
new: DataOwner7的最优x_7 = 0.2167
new: DataOwner8的最优x_8 = 0.2034
new: DataOwner9的最优x_9 = 0.2623
new: DataOwner10的最优x_10 = 0.1781
eta:2.56
new: DataOwner1的最优x_1 = 0.1956
new: DataOwner2的最优x_2 = 0.1759
new: DataOwner3的最优x_3 = 0.2286
new: DataOwner4的最优x_4 = 0.2205
new: DataOwner5的最优x_5 = 0.2470
new: DataOwner6的最优x_6 = 0.3558
new: DataOwner7的最优x_7 = 0.2176
new: DataOwner8的最优x_8 = 0.2042
new: DataOwner9的最优x_9 = 0.2633
new: DataOwner10的最优x_10 = 0.1787
eta:2.57
new: DataOwner1的最优x_1 = 0.1964
new: DataOwner2的最优x_2 = 0.1766
new: DataOwner3的最优x_3 = 0.2294
new: DataOwner4的最优x_4 = 0.2213
new: DataOwner5的最优x_5 = 0.2479
new: DataOwner6的最优x_6 = 0.3572
new: DataOwner7的最优x_7 = 0.2184
new: DataOwner8的最优x_8 = 0.2050
new: DataOwner9的最优x_9 = 0.2644
new: DataOwner10的最优x_10 = 0.1794
eta:2.5799999999999996
new: DataOwner1的最优x_1 = 0.1971
new: DataOwner2的最优x_2 = 0.1773
new: DataOwner3的最优x_3 = 0.2303
new: DataOwner4的最优x_4 = 0.2222
new: DataOwner5的最优x_5 = 0.2489
new: DataOwner6的最优x_6 = 0.3586
new: DataOwner7的最优x_7 = 0.2193
new: DataOwner8的最优x_8 = 0.2058
new: DataOwner9的最优x_9 = 0.2654
new: DataOwner10的最优x_10 = 0.1801
eta:2.59
new: DataOwner1的最优x_1 = 0.1979
new: DataOwner2的最优x_2 = 0.1780
new: DataOwner3的最优x_3 = 0.2312
new: DataOwner4的最优x_4 = 0.2230
new: DataOwner5的最优x_5 = 0.2499
new: DataOwner6的最优x_6 = 0.3600
new: DataOwner7的最优x_7 = 0.2201
new: DataOwner8的最优x_8 = 0.2066
new: DataOwner9的最优x_9 = 0.2664
new: DataOwner10的最优x_10 = 0.1808
eta:2.5999999999999996
new: DataOwner1的最优x_1 = 0.1987
new: DataOwner2的最优x_2 = 0.1787
new: DataOwner3的最优x_3 = 0.2321
new: DataOwner4的最优x_4 = 0.2239
new: DataOwner5的最优x_5 = 0.2508
new: DataOwner6的最优x_6 = 0.3614
new: DataOwner7的最优x_7 = 0.2210
new: DataOwner8的最优x_8 = 0.2074
new: DataOwner9的最优x_9 = 0.2675
new: DataOwner10的最优x_10 = 0.1815
eta:2.61
new: DataOwner1的最优x_1 = 0.1994
new: DataOwner2的最优x_2 = 0.1794
new: DataOwner3的最优x_3 = 0.2330
new: DataOwner4的最优x_4 = 0.2248
new: DataOwner5的最优x_5 = 0.2518
new: DataOwner6的最优x_6 = 0.3628
new: DataOwner7的最优x_7 = 0.2218
new: DataOwner8的最优x_8 = 0.2082
new: DataOwner9的最优x_9 = 0.2685
new: DataOwner10的最优x_10 = 0.1822
eta:2.6199999999999997
new: DataOwner1的最优x_1 = 0.2002
new: DataOwner2的最优x_2 = 0.1801
new: DataOwner3的最优x_3 = 0.2339
new: DataOwner4的最优x_4 = 0.2256
new: DataOwner5的最优x_5 = 0.2528
new: DataOwner6的最优x_6 = 0.3641
new: DataOwner7的最优x_7 = 0.2227
new: DataOwner8的最优x_8 = 0.2090
new: DataOwner9的最优x_9 = 0.2695
new: DataOwner10的最优x_10 = 0.1829
eta:2.63
new: DataOwner1的最优x_1 = 0.2010
new: DataOwner2的最优x_2 = 0.1807
new: DataOwner3的最优x_3 = 0.2348
new: DataOwner4的最优x_4 = 0.2265
new: DataOwner5的最优x_5 = 0.2537
new: DataOwner6的最优x_6 = 0.3655
new: DataOwner7的最优x_7 = 0.2235
new: DataOwner8的最优x_8 = 0.2098
new: DataOwner9的最优x_9 = 0.2705
new: DataOwner10的最优x_10 = 0.1836
eta:2.6399999999999997
new: DataOwner1的最优x_1 = 0.2017
new: DataOwner2的最优x_2 = 0.1814
new: DataOwner3的最优x_3 = 0.2357
new: DataOwner4的最优x_4 = 0.2273
new: DataOwner5的最优x_5 = 0.2547
new: DataOwner6的最优x_6 = 0.3669
new: DataOwner7的最优x_7 = 0.2244
new: DataOwner8的最优x_8 = 0.2106
new: DataOwner9的最优x_9 = 0.2716
new: DataOwner10的最优x_10 = 0.1843
eta:2.65
new: DataOwner1的最优x_1 = 0.2025
new: DataOwner2的最优x_2 = 0.1821
new: DataOwner3的最优x_3 = 0.2366
new: DataOwner4的最优x_4 = 0.2282
new: DataOwner5的最优x_5 = 0.2557
new: DataOwner6的最优x_6 = 0.3683
new: DataOwner7的最优x_7 = 0.2252
new: DataOwner8的最优x_8 = 0.2114
new: DataOwner9的最优x_9 = 0.2726
new: DataOwner10的最优x_10 = 0.1850
eta:2.6599999999999997
new: DataOwner1的最优x_1 = 0.2032
new: DataOwner2的最优x_2 = 0.1828
new: DataOwner3的最优x_3 = 0.2375
new: DataOwner4的最优x_4 = 0.2291
new: DataOwner5的最优x_5 = 0.2566
new: DataOwner6的最优x_6 = 0.3697
new: DataOwner7的最优x_7 = 0.2261
new: DataOwner8的最优x_8 = 0.2122
new: DataOwner9的最优x_9 = 0.2736
new: DataOwner10的最优x_10 = 0.1857
eta:2.67
new: DataOwner1的最优x_1 = 0.2040
new: DataOwner2的最优x_2 = 0.1835
new: DataOwner3的最优x_3 = 0.2384
new: DataOwner4的最优x_4 = 0.2299
new: DataOwner5的最优x_5 = 0.2576
new: DataOwner6的最优x_6 = 0.3711
new: DataOwner7的最优x_7 = 0.2269
new: DataOwner8的最优x_8 = 0.2130
new: DataOwner9的最优x_9 = 0.2747
new: DataOwner10的最优x_10 = 0.1864
eta:2.6799999999999997
new: DataOwner1的最优x_1 = 0.2048
new: DataOwner2的最优x_2 = 0.1842
new: DataOwner3的最优x_3 = 0.2393
new: DataOwner4的最优x_4 = 0.2308
new: DataOwner5的最优x_5 = 0.2586
new: DataOwner6的最优x_6 = 0.3725
new: DataOwner7的最优x_7 = 0.2278
new: DataOwner8的最优x_8 = 0.2138
new: DataOwner9的最优x_9 = 0.2757
new: DataOwner10的最优x_10 = 0.1871
eta:2.69
new: DataOwner1的最优x_1 = 0.2055
new: DataOwner2的最优x_2 = 0.1849
new: DataOwner3的最优x_3 = 0.2402
new: DataOwner4的最优x_4 = 0.2316
new: DataOwner5的最优x_5 = 0.2595
new: DataOwner6的最优x_6 = 0.3739
new: DataOwner7的最优x_7 = 0.2286
new: DataOwner8的最优x_8 = 0.2146
new: DataOwner9的最优x_9 = 0.2767
new: DataOwner10的最优x_10 = 0.1878
eta:2.6999999999999997
new: DataOwner1的最优x_1 = 0.2063
new: DataOwner2的最优x_2 = 0.1856
new: DataOwner3的最优x_3 = 0.2411
new: DataOwner4的最优x_4 = 0.2325
new: DataOwner5的最优x_5 = 0.2605
new: DataOwner6的最优x_6 = 0.3753
new: DataOwner7的最优x_7 = 0.2295
new: DataOwner8的最优x_8 = 0.2154
new: DataOwner9的最优x_9 = 0.2777
new: DataOwner10的最优x_10 = 0.1885
eta:2.71
new: DataOwner1的最优x_1 = 0.2071
new: DataOwner2的最优x_2 = 0.1862
new: DataOwner3的最优x_3 = 0.2419
new: DataOwner4的最优x_4 = 0.2334
new: DataOwner5的最优x_5 = 0.2614
new: DataOwner6的最优x_6 = 0.3767
new: DataOwner7的最优x_7 = 0.2303
new: DataOwner8的最优x_8 = 0.2162
new: DataOwner9的最优x_9 = 0.2788
new: DataOwner10的最优x_10 = 0.1892
eta:2.7199999999999998
new: DataOwner1的最优x_1 = 0.2078
new: DataOwner2的最优x_2 = 0.1869
new: DataOwner3的最优x_3 = 0.2428
new: DataOwner4的最优x_4 = 0.2342
new: DataOwner5的最优x_5 = 0.2624
new: DataOwner6的最优x_6 = 0.3780
new: DataOwner7的最优x_7 = 0.2312
new: DataOwner8的最优x_8 = 0.2170
new: DataOwner9的最优x_9 = 0.2798
new: DataOwner10的最优x_10 = 0.1899
eta:2.73
new: DataOwner1的最优x_1 = 0.2086
new: DataOwner2的最优x_2 = 0.1876
new: DataOwner3的最优x_3 = 0.2437
new: DataOwner4的最优x_4 = 0.2351
new: DataOwner5的最优x_5 = 0.2634
new: DataOwner6的最优x_6 = 0.3794
new: DataOwner7的最优x_7 = 0.2320
new: DataOwner8的最优x_8 = 0.2178
new: DataOwner9的最优x_9 = 0.2808
new: DataOwner10的最优x_10 = 0.1906
eta:2.7399999999999998
new: DataOwner1的最优x_1 = 0.2094
new: DataOwner2的最优x_2 = 0.1883
new: DataOwner3的最优x_3 = 0.2446
new: DataOwner4的最优x_4 = 0.2360
new: DataOwner5的最优x_5 = 0.2643
new: DataOwner6的最优x_6 = 0.3808
new: DataOwner7的最优x_7 = 0.2329
new: DataOwner8的最优x_8 = 0.2186
new: DataOwner9的最优x_9 = 0.2819
new: DataOwner10的最优x_10 = 0.1913
eta:2.75
new: DataOwner1的最优x_1 = 0.2101
new: DataOwner2的最优x_2 = 0.1890
new: DataOwner3的最优x_3 = 0.2455
new: DataOwner4的最优x_4 = 0.2368
new: DataOwner5的最优x_5 = 0.2653
new: DataOwner6的最优x_6 = 0.3822
new: DataOwner7的最优x_7 = 0.2337
new: DataOwner8的最优x_8 = 0.2194
new: DataOwner9的最优x_9 = 0.2829
new: DataOwner10的最优x_10 = 0.1920
eta:2.76
new: DataOwner1的最优x_1 = 0.2109
new: DataOwner2的最优x_2 = 0.1897
new: DataOwner3的最优x_3 = 0.2464
new: DataOwner4的最优x_4 = 0.2377
new: DataOwner5的最优x_5 = 0.2663
new: DataOwner6的最优x_6 = 0.3836
new: DataOwner7的最优x_7 = 0.2346
new: DataOwner8的最优x_8 = 0.2202
new: DataOwner9的最优x_9 = 0.2839
new: DataOwner10的最优x_10 = 0.1927
eta:2.77
new: DataOwner1的最优x_1 = 0.2117
new: DataOwner2的最优x_2 = 0.1904
new: DataOwner3的最优x_3 = 0.2473
new: DataOwner4的最优x_4 = 0.2385
new: DataOwner5的最优x_5 = 0.2672
new: DataOwner6的最优x_6 = 0.3850
new: DataOwner7的最优x_7 = 0.2354
new: DataOwner8的最优x_8 = 0.2210
new: DataOwner9的最优x_9 = 0.2849
new: DataOwner10的最优x_10 = 0.1934
eta:2.78
new: DataOwner1的最优x_1 = 0.2124
new: DataOwner2的最优x_2 = 0.1911
new: DataOwner3的最优x_3 = 0.2482
new: DataOwner4的最优x_4 = 0.2394
new: DataOwner5的最优x_5 = 0.2682
new: DataOwner6的最优x_6 = 0.3864
new: DataOwner7的最优x_7 = 0.2363
new: DataOwner8的最优x_8 = 0.2218
new: DataOwner9的最优x_9 = 0.2860
new: DataOwner10的最优x_10 = 0.1941
eta:2.79
new: DataOwner1的最优x_1 = 0.2132
new: DataOwner2的最优x_2 = 0.1917
new: DataOwner3的最优x_3 = 0.2491
new: DataOwner4的最优x_4 = 0.2403
new: DataOwner5的最优x_5 = 0.2692
new: DataOwner6的最优x_6 = 0.3878
new: DataOwner7的最优x_7 = 0.2371
new: DataOwner8的最优x_8 = 0.2226
new: DataOwner9的最优x_9 = 0.2870
new: DataOwner10的最优x_10 = 0.1948
eta:2.8
new: DataOwner1的最优x_1 = 0.2139
new: DataOwner2的最优x_2 = 0.1924
new: DataOwner3的最优x_3 = 0.2500
new: DataOwner4的最优x_4 = 0.2411
new: DataOwner5的最优x_5 = 0.2701
new: DataOwner6的最优x_6 = 0.3892
new: DataOwner7的最优x_7 = 0.2380
new: DataOwner8的最优x_8 = 0.2234
new: DataOwner9的最优x_9 = 0.2880
new: DataOwner10的最优x_10 = 0.1955
eta:2.81
new: DataOwner1的最优x_1 = 0.2147
new: DataOwner2的最优x_2 = 0.1931
new: DataOwner3的最优x_3 = 0.2509
new: DataOwner4的最优x_4 = 0.2420
new: DataOwner5的最优x_5 = 0.2711
new: DataOwner6的最优x_6 = 0.3905
new: DataOwner7的最优x_7 = 0.2388
new: DataOwner8的最优x_8 = 0.2242
new: DataOwner9的最优x_9 = 0.2891
new: DataOwner10的最优x_10 = 0.1962
eta:2.82
new: DataOwner1的最优x_1 = 0.2155
new: DataOwner2的最优x_2 = 0.1938
new: DataOwner3的最优x_3 = 0.2518
new: DataOwner4的最优x_4 = 0.2428
new: DataOwner5的最优x_5 = 0.2721
new: DataOwner6的最优x_6 = 0.3919
new: DataOwner7的最优x_7 = 0.2397
new: DataOwner8的最优x_8 = 0.2249
new: DataOwner9的最优x_9 = 0.2901
new: DataOwner10的最优x_10 = 0.1969
eta:2.8299999999999996
new: DataOwner1的最优x_1 = 0.2162
new: DataOwner2的最优x_2 = 0.1945
new: DataOwner3的最优x_3 = 0.2527
new: DataOwner4的最优x_4 = 0.2437
new: DataOwner5的最优x_5 = 0.2730
new: DataOwner6的最优x_6 = 0.3933
new: DataOwner7的最优x_7 = 0.2405
new: DataOwner8的最优x_8 = 0.2257
new: DataOwner9的最优x_9 = 0.2911
new: DataOwner10的最优x_10 = 0.1976
eta:2.84
new: DataOwner1的最优x_1 = 0.2170
new: DataOwner2的最优x_2 = 0.1952
new: DataOwner3的最优x_3 = 0.2536
new: DataOwner4的最优x_4 = 0.2446
new: DataOwner5的最优x_5 = 0.2740
new: DataOwner6的最优x_6 = 0.3947
new: DataOwner7的最优x_7 = 0.2414
new: DataOwner8的最优x_8 = 0.2265
new: DataOwner9的最优x_9 = 0.2921
new: DataOwner10的最优x_10 = 0.1983
eta:2.8499999999999996
new: DataOwner1的最优x_1 = 0.2178
new: DataOwner2的最优x_2 = 0.1959
new: DataOwner3的最优x_3 = 0.2544
new: DataOwner4的最优x_4 = 0.2454
new: DataOwner5的最优x_5 = 0.2750
new: DataOwner6的最优x_6 = 0.3961
new: DataOwner7的最优x_7 = 0.2422
new: DataOwner8的最优x_8 = 0.2273
new: DataOwner9的最优x_9 = 0.2932
new: DataOwner10的最优x_10 = 0.1990
eta:2.86
new: DataOwner1的最优x_1 = 0.2185
new: DataOwner2的最优x_2 = 0.1966
new: DataOwner3的最优x_3 = 0.2553
new: DataOwner4的最优x_4 = 0.2463
new: DataOwner5的最优x_5 = 0.2759
new: DataOwner6的最优x_6 = 0.3975
new: DataOwner7的最优x_7 = 0.2431
new: DataOwner8的最优x_8 = 0.2281
new: DataOwner9的最优x_9 = 0.2942
new: DataOwner10的最优x_10 = 0.1997
eta:2.8699999999999997
new: DataOwner1的最优x_1 = 0.2193
new: DataOwner2的最优x_2 = 0.1972
new: DataOwner3的最优x_3 = 0.2562
new: DataOwner4的最优x_4 = 0.2471
new: DataOwner5的最优x_5 = 0.2769
new: DataOwner6的最优x_6 = 0.3989
new: DataOwner7的最优x_7 = 0.2439
new: DataOwner8的最优x_8 = 0.2289
new: DataOwner9的最优x_9 = 0.2952
new: DataOwner10的最优x_10 = 0.2004
eta:2.88
new: DataOwner1的最优x_1 = 0.2201
new: DataOwner2的最优x_2 = 0.1979
new: DataOwner3的最优x_3 = 0.2571
new: DataOwner4的最优x_4 = 0.2480
new: DataOwner5的最优x_5 = 0.2778
new: DataOwner6的最优x_6 = 0.4003
new: DataOwner7的最优x_7 = 0.2448
new: DataOwner8的最优x_8 = 0.2297
new: DataOwner9的最优x_9 = 0.2963
new: DataOwner10的最优x_10 = 0.2011
eta:2.8899999999999997
new: DataOwner1的最优x_1 = 0.2208
new: DataOwner2的最优x_2 = 0.1986
new: DataOwner3的最优x_3 = 0.2580
new: DataOwner4的最优x_4 = 0.2489
new: DataOwner5的最优x_5 = 0.2788
new: DataOwner6的最优x_6 = 0.4017
new: DataOwner7的最优x_7 = 0.2456
new: DataOwner8的最优x_8 = 0.2305
new: DataOwner9的最优x_9 = 0.2973
new: DataOwner10的最优x_10 = 0.2018
eta:2.9
new: DataOwner1的最优x_1 = 0.2216
new: DataOwner2的最优x_2 = 0.1993
new: DataOwner3的最优x_3 = 0.2589
new: DataOwner4的最优x_4 = 0.2497
new: DataOwner5的最优x_5 = 0.2798
new: DataOwner6的最优x_6 = 0.4031
new: DataOwner7的最优x_7 = 0.2465
new: DataOwner8的最优x_8 = 0.2313
new: DataOwner9的最优x_9 = 0.2983
new: DataOwner10的最优x_10 = 0.2025
eta:2.9099999999999997
new: DataOwner1的最优x_1 = 0.2223
new: DataOwner2的最优x_2 = 0.2000
new: DataOwner3的最优x_3 = 0.2598
new: DataOwner4的最优x_4 = 0.2506
new: DataOwner5的最优x_5 = 0.2807
new: DataOwner6的最优x_6 = 0.4044
new: DataOwner7的最优x_7 = 0.2473
new: DataOwner8的最优x_8 = 0.2321
new: DataOwner9的最优x_9 = 0.2993
new: DataOwner10的最优x_10 = 0.2032
eta:2.92
new: DataOwner1的最优x_1 = 0.2231
new: DataOwner2的最优x_2 = 0.2007
new: DataOwner3的最优x_3 = 0.2607
new: DataOwner4的最优x_4 = 0.2515
new: DataOwner5的最优x_5 = 0.2817
new: DataOwner6的最优x_6 = 0.4058
new: DataOwner7的最优x_7 = 0.2482
new: DataOwner8的最优x_8 = 0.2329
new: DataOwner9的最优x_9 = 0.3004
new: DataOwner10的最优x_10 = 0.2039
eta:2.9299999999999997
new: DataOwner1的最优x_1 = 0.2239
new: DataOwner2的最优x_2 = 0.2014
new: DataOwner3的最优x_3 = 0.2616
new: DataOwner4的最优x_4 = 0.2523
new: DataOwner5的最优x_5 = 0.2827
new: DataOwner6的最优x_6 = 0.4072
new: DataOwner7的最优x_7 = 0.2490
new: DataOwner8的最优x_8 = 0.2337
new: DataOwner9的最优x_9 = 0.3014
new: DataOwner10的最优x_10 = 0.2046
eta:2.94
new: DataOwner1的最优x_1 = 0.2246
new: DataOwner2的最优x_2 = 0.2020
new: DataOwner3的最优x_3 = 0.2625
new: DataOwner4的最优x_4 = 0.2532
new: DataOwner5的最优x_5 = 0.2836
new: DataOwner6的最优x_6 = 0.4086
new: DataOwner7的最优x_7 = 0.2499
new: DataOwner8的最优x_8 = 0.2345
new: DataOwner9的最优x_9 = 0.3024
new: DataOwner10的最优x_10 = 0.2053
eta:2.9499999999999997
new: DataOwner1的最优x_1 = 0.2254
new: DataOwner2的最优x_2 = 0.2027
new: DataOwner3的最优x_3 = 0.2634
new: DataOwner4的最优x_4 = 0.2540
new: DataOwner5的最优x_5 = 0.2846
new: DataOwner6的最优x_6 = 0.4100
new: DataOwner7的最优x_7 = 0.2507
new: DataOwner8的最优x_8 = 0.2353
new: DataOwner9的最优x_9 = 0.3035
new: DataOwner10的最优x_10 = 0.2060
eta:2.96
new: DataOwner1的最优x_1 = 0.2262
new: DataOwner2的最优x_2 = 0.2034
new: DataOwner3的最优x_3 = 0.2643
new: DataOwner4的最优x_4 = 0.2549
new: DataOwner5的最优x_5 = 0.2856
new: DataOwner6的最优x_6 = 0.4114
new: DataOwner7的最优x_7 = 0.2516
new: DataOwner8的最优x_8 = 0.2361
new: DataOwner9的最优x_9 = 0.3045
new: DataOwner10的最优x_10 = 0.2067
eta:2.9699999999999998
new: DataOwner1的最优x_1 = 0.2269
new: DataOwner2的最优x_2 = 0.2041
new: DataOwner3的最优x_3 = 0.2652
new: DataOwner4的最优x_4 = 0.2558
new: DataOwner5的最优x_5 = 0.2865
new: DataOwner6的最优x_6 = 0.4128
new: DataOwner7的最优x_7 = 0.2524
new: DataOwner8的最优x_8 = 0.2369
new: DataOwner9的最优x_9 = 0.3055
new: DataOwner10的最优x_10 = 0.2074
eta:2.98
new: DataOwner1的最优x_1 = 0.2277
new: DataOwner2的最优x_2 = 0.2048
new: DataOwner3的最优x_3 = 0.2661
new: DataOwner4的最优x_4 = 0.2566
new: DataOwner5的最优x_5 = 0.2875
new: DataOwner6的最优x_6 = 0.4142
new: DataOwner7的最优x_7 = 0.2533
new: DataOwner8的最优x_8 = 0.2377
new: DataOwner9的最优x_9 = 0.3065
new: DataOwner10的最优x_10 = 0.2081
eta:2.9899999999999998
new: DataOwner1的最优x_1 = 0.2285
new: DataOwner2的最优x_2 = 0.2055
new: DataOwner3的最优x_3 = 0.2669
new: DataOwner4的最优x_4 = 0.2575
new: DataOwner5的最优x_5 = 0.2885
new: DataOwner6的最优x_6 = 0.4156
new: DataOwner7的最优x_7 = 0.2541
new: DataOwner8的最优x_8 = 0.2385
new: DataOwner9的最优x_9 = 0.3076
new: DataOwner10的最优x_10 = 0.2088
eta:3.0
new: DataOwner1的最优x_1 = 0.2292
new: DataOwner2的最优x_2 = 0.2062
new: DataOwner3的最优x_3 = 0.2678
new: DataOwner4的最优x_4 = 0.2583
new: DataOwner5的最优x_5 = 0.2894
new: DataOwner6的最优x_6 = 0.4170
new: DataOwner7的最优x_7 = 0.2550
new: DataOwner8的最优x_8 = 0.2393
new: DataOwner9的最优x_9 = 0.3086
new: DataOwner10的最优x_10 = 0.2095
DONE
----- literation 3: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.2997899392939436
DataOwner1的分配到的支付 ： 0.1143
DataOwner2的分配到的支付 ： 0.0852
DataOwner3的分配到的支付 ： 0.1375
DataOwner4的分配到的支付 ： 0.1312
DataOwner5的分配到的支付 ： 0.1345
DataOwner6的分配到的支付 ： 0.1841
DataOwner7的分配到的支付 ： 0.1294
DataOwner8的分配到的支付 ： 0.1199
DataOwner9的分配到的支付 ： 0.1285
DataOwner10的分配到的支付 ： 0.1351
DONE
----- literation 3: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC2
DataOwner3 把数据交给 CPC3
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC8
DataOwner9 把数据交给 CPC9
DataOwner10 把数据交给 CPC10
DONE
----- literation 3: 模型训练 -----
CPC1调整模型中, 本轮训练的数据量为：978.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 75.82%
Epoch 1/5, Loss: 0.7502
Epoch 2/5, Loss: 0.7413
Epoch 3/5, Loss: 0.7049
Epoch 4/5, Loss: 0.7021
Epoch 5/5, Loss: 0.6747
新模型评估：
Accuracy: 76.38%
Model saved to ../../../data/model/mnist_cnn_model
CPC2调整模型中, 本轮训练的数据量为：1366.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 76.38%
Epoch 1/5, Loss: 0.6711
Epoch 2/5, Loss: 0.6490
Epoch 3/5, Loss: 0.6287
Epoch 4/5, Loss: 0.5999
Epoch 5/5, Loss: 0.5815
新模型评估：
Accuracy: 77.17%
Model saved to ../../../data/model/mnist_cnn_model
CPC3调整模型中, 本轮训练的数据量为：660.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.17%
Epoch 1/5, Loss: 0.5289
Epoch 2/5, Loss: 0.5071
Epoch 3/5, Loss: 0.4943
Epoch 4/5, Loss: 0.4968
Epoch 5/5, Loss: 0.4973
新模型评估：
Accuracy: 78.80%
Model saved to ../../../data/model/mnist_cnn_model
CPC4调整模型中, 本轮训练的数据量为：316.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.80%
Epoch 1/5, Loss: 0.5694
Epoch 2/5, Loss: 0.5597
Epoch 3/5, Loss: 0.5491
Epoch 4/5, Loss: 0.5424
Epoch 5/5, Loss: 0.5372
新模型评估：
Accuracy: 76.74%
CPC5调整模型中, 本轮训练的数据量为：319.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.80%
Epoch 1/5, Loss: 0.5419
Epoch 2/5, Loss: 0.5280
Epoch 3/5, Loss: 0.5189
Epoch 4/5, Loss: 0.5120
Epoch 5/5, Loss: 0.5065
新模型评估：
Accuracy: 77.59%
CPC6调整模型中, 本轮训练的数据量为：1431.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.80%
Epoch 1/5, Loss: 0.5739
Epoch 2/5, Loss: 0.5557
Epoch 3/5, Loss: 0.5404
Epoch 4/5, Loss: 0.5217
Epoch 5/5, Loss: 0.5122
新模型评估：
Accuracy: 76.51%
CPC7调整模型中, 本轮训练的数据量为：312.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.80%
Epoch 1/5, Loss: 0.5659
Epoch 2/5, Loss: 0.5575
Epoch 3/5, Loss: 0.5527
Epoch 4/5, Loss: 0.5362
Epoch 5/5, Loss: 0.5365
新模型评估：
Accuracy: 76.96%
CPC8调整模型中, 本轮训练的数据量为：875.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.80%
Epoch 1/5, Loss: 0.5198
Epoch 2/5, Loss: 0.5028
Epoch 3/5, Loss: 0.4968
Epoch 4/5, Loss: 0.4824
Epoch 5/5, Loss: 0.4749
新模型评估：
Accuracy: 77.72%
CPC9调整模型中, 本轮训练的数据量为：454.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.80%
Epoch 1/5, Loss: 0.5322
Epoch 2/5, Loss: 0.5415
Epoch 3/5, Loss: 0.5637
Epoch 4/5, Loss: 0.5571
Epoch 5/5, Loss: 0.5641
新模型评估：
Accuracy: 76.28%
CPC10调整模型中, 本轮训练的数据量为：1165.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.80%
Epoch 1/5, Loss: 0.5409
Epoch 2/5, Loss: 0.5378
Epoch 3/5, Loss: 0.5167
Epoch 4/5, Loss: 0.5083
Epoch 5/5, Loss: 0.5011
新模型评估：
Accuracy: 77.22%
DONE
最终的列表：
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.15285673453012114, 0.1611690360336873, 0.1611690360336873, 0.1611690360336873, 0.1611690360336873, 0.1611690360336873, 0.1611690360336873, 0.1611690360336873, 0.21573146763191528, 0.21573146763191528, 0.2302098123104489, 0.2372722952806745, 0.24421911586688416, 0.2510518316129473, 0.25777196880940006, 0.26438102330883356, 0.27088046141856353, 0.27727172049219656, 0.28355620987446256, 0.2897353115269479, 0.295810380736725, 0.295810380736725, 0.3076537138499581, 0.3134245610842403, 0.3134245610842403, 0.3134245610842403, 0.3301488201150091, 0.3301488201150091, 0.3408201260223009, 0.3460158141262499, 0.3511196963862255, 0.35613287545654276, 0.3610564342345392, 0.3610564342345392, 0.3610564342345392, 0.3610564342345392, 0.3610564342345392, 0.3843665008270294, 0.38877403025715107, 0.3930990044634607, 0.3973423641456786, 0.3973423641456786, 0.4055879231287358, 0.40959192533381683, 0.41351791946630423, 0.4173667698614162, 0.4211393263042197, 0.4248364249955151, 0.42845888820827593, 0.43200752492426864, 0.4354831310221291, 0.4354831310221291, 0.442218371200111, 0.44547953413748587, 0.44867072469973246, 0.45179267743247176, 0.4548461153665311, 0.4578317502816396, 0.46075028290452824, 0.4636024031654795, 0.46638879040945125, 0.4691101136050986, 0.4717670315655649, 0.4743601931450787, 0.47689023744286785, 0.4793577940068625, 0.48176348297026816, 0.4841079153438992, 0.486391693091085, 0.48861540934906167, 0.49077964860734535, 0.4928849868574189, 0.49493199176667, 0.4969212228341201, 0.49885323155173233, 0.5007285615399467, 0.502547748726821, 0.5043113214633171, 0.506019800681, 0.5076737000408355, 0.5092735259846805, 0.5108197780460677, 0.5123129487952933, 0.5137535240576601, 0.5151419830226942, 0.516478798351405, 0.5177644363214511, 0.5189993569034468, 0.520184013908995, 0.5213188550800976, 0.5224043222036907, 0.5234408512155813, 0.5244288723046828, 0.5253688100081948, 0.5262610833205366, 0.5271061057787574, 0.5279042855633724, 0.528656025586596, 0.5293617235869295, 0.5300217722131502, 0.5306365591135662, 0.5312064670186711, 0.5317318738255825, 0.5322131526780516, 0.5326506720463844, 0.5330447958045457, 0.533395883307209, 0.5337042894635102, 0.533970364810362, 0.5341944555839624, 0.5343769037898578, 0.5345180472726745, 0.5346182197779281, 0.53467775103165, 0.5346969667802199, 0.5346761888860612, 0.5346157353629202, 0.5345159204482197, 0.5343770546614148, 0.5341994448608098, 0.5339833943035714, 0.5337292026991927, 0.5334371662659396, 0.5331075777836256, 0.5327407268661579, 0.5323368989300961, 0.5318963774019971, 0.5314194416211506, 0.5309063680157937, 0.5303574296303541, 0.5297728968008206, 0.5291530365343804, 0.5284981130507957, 0.5278083873998571, 0.527084117871282, 0.5263255598396348, 0.5255329659130079, 0.5247065858371143, 0.5238466666694293, 0.5229534527671136, 0.5220271858058365, 0.521068104856343, 0.520076446395755, 0.5190524443575446, 0.5179963301651496, 0.5169083327654505, 0.5157886786659502, 0.5146375919576969, 0.5134552944063095, 0.5122420053702561, 0.5109979419490549, 0.5097233189576322, 0.5084183489697016, 0.5070832423479312, 0.5057182072769069, 0.5043234497909963, 0.5028991737981212, 0.5014455811331806, 0.4999628715315112, 0.4984512427421799, 0.4969108904723216, 0.49534200846010545, 0.49374478847522085, 0.49211942041665147, 0.49046609217560655, 0.48878498990506136, 0.48707629777236616, 0.48534019819790863, 0.48357687177885667, 0.48178649733261647, 0.4799692519220462, 0.47812531087241084, 0.4762548478135902, 0.4743580346657099, 0.4724350416959784, 0.4704860375248412, 0.46851118914356094, 0.4665106619504231, 0.4644846197498307, 0.46243322479245097, 0.4603566377816817, 0.45825501789899525, 0.4561285228214409, 0.4539773087428378, 0.45180153038825344, 0.4496013410361739, 0.4473768925260746, 0.4451283353050499, 0.44285581840510524, 0.44055948948042456, 0.4382394948538595, 0.43589597948335745, 0.4335290869771318, 0.4311389596614008, 0.42872573855919915, 0.4262895634127264, 0.42383057270369706, 0.4213489036473015, 0.41884469225091436, 0.4163180732837799, 0.4137691803350023, 0.41119814572431324, 0.4086051007119136, 0.40599017531683623, 0.4033534984310134, 0.4006951978068227, 0.3980154000775822, 0.3953142307663784, 0.39259181430013346, 0.389848274022337, 0.387083732205487, 0.3842983100635493, 0.38149212776394803, 0.3786653044434227, 0.37581795820223407, 0.3729502061513217, 0.37006216438637063, 0.36715394802023305, 0.3642256711884224, 0.36127744706062925, 0.358309387852104, 0.35532160483199116, 0.35231420834151006, 0.34928730778438677, 0.34624101166913634, 0.34317542759218655, 0.34009066225748086, 0.3369868214871272, 0.3338640102293384, 0.3307223325688087, 0.3275618917359724, 0.32438279011757976, 0.3211851292654191, 0.31796900988694476, 0.31473453190909817, 0.3114817944174706, 0.3082108957133416, 0.304921933289525, 0.30161500387293927, 0.29829020342519863, 0.29494762711205924, 0.2915873693077713, 0.28820952381896525, 0.28481418345795717, 0.2814014404437062, 0.27797138625089746, 0.27452411162793355, 0.27105970661141265, 0.2675782605342074, 0.2640798620312692, 0.2605645990483567, 0.2570325588486164, 0.2534838280193332, 0.24991849248126385, 0.2463366374898177, 0.242738347647395, 0.23912370690841156, 0.23549279858516714, 0.2318457053548899, 0.22818250926629124, 0.2245032917454628, 0.2208081336025609, 0.21709711503791507, 0.2133703156480964, 0.2096278144328938, 0.20586968979819797, 0.2020960195642294, 0.19830688097471194, 0.194502350695394, 0.19068250482551807, 0.18684741890015788, 0.18299716789729636, 0.1791318262434567, 0.17525146781607326, 0.17135616595929548, 0.16744599346972633, 0.16352102262107948, 0.15958132515952705, 0.155626972309999, 0.15165803478318374, 0.1476745827789938, 0.14367668599085004, 0.13966441361215365, 0.13563783434013432, 0.13159701638070453]
**** log-parameter_analysis 运行时间： 2025-01-17 22:34:51 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.09393294726302918
DataOwner2: noise random: 0.006922930992007293
DataOwner3: noise random: 0.04180116398420369
DataOwner4: noise random: 0.03709630802766041
DataOwner5: noise random: 0.07037700683146399
DataOwner6: noise random: 0.04740013205791659
DataOwner7: noise random: 0.07178372572622836
DataOwner8: noise random: 0.03690370479838099
DataOwner9: noise random: 0.03455150614895173
DataOwner10: noise random: 0.02731898898927353
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9964072362859457, 0.9999806907703578, 0.9992933878935594, 0.999442432771095, 0.9979974633721433, 0.9990899287030421, 0.9979185594159252, 0.9994492673544436, 0.9995160019213243, 0.9996970559903172]
归一化后的数据质量列表avg_f_list: [0.9, 1.0, 0.9807664298007284, 0.9849373204105234, 0.9445011149053226, 0.9750728022085833, 0.9422930566646953, 0.9851285802510619, 0.9869960887689897, 0.9920627286207835]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3525
DataOwner1的最优x_1 = 0.0416
DataOwner2的最优x_2 = 0.1515
DataOwner3的最优x_3 = 0.1340
DataOwner4的最优x_4 = 0.1379
DataOwner5的最优x_5 = 0.0965
DataOwner6的最优x_6 = 0.1285
DataOwner7的最优x_7 = 0.0941
DataOwner8的最优x_8 = 0.1381
DataOwner9的最优x_9 = 0.1398
DataOwner10的最优x_10 = 0.1445
每个DataOwner应该贡献数据比例 xn_list = [0.04157022869840666, 0.15152926076661963, 0.13396476348371167, 0.13789977485519053, 0.09653787092599003, 0.12847598157743456, 0.09405997030914102, 0.1380785052493272, 0.13981585512874026, 0.14445871613330444]
ModelOwner的最大效用 U(Eta) = 0.5942
DONE
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3524595127368417
DataOwner1的分配到的支付 ： 0.0429
DataOwner2的分配到的支付 ： 0.1739
DataOwner3的分配到的支付 ： 0.1508
DataOwner4的分配到的支付 ： 0.1559
DataOwner5的分配到的支付 ： 0.1046
DataOwner6的分配到的支付 ： 0.1438
DataOwner7的分配到的支付 ： 0.1017
DataOwner8的分配到的支付 ： 0.1561
DataOwner9的分配到的支付 ： 0.1584
DataOwner10的分配到的支付 ： 0.1645
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC2', 'DataOwner5': 'CPC1', 'DataOwner2': 'CPC3', 'DataOwner3': 'CPC4', 'DataOwner4': 'CPC5', 'DataOwner6': 'CPC6', 'DataOwner7': 'CPC7', 'DataOwner8': 'CPC8', 'DataOwner9': 'CPC9', 'DataOwner10': 'CPC10'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC2
DataOwner5 把数据交给 CPC1
DataOwner2 把数据交给 CPC3
DataOwner3 把数据交给 CPC4
DataOwner4 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC8
DataOwner9 把数据交给 CPC9
DataOwner10 把数据交给 CPC10
DONE
----- literation 1: 模型训练 -----
CPC2调整模型中, 本轮训练的数据量为：67.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 26.82%
Epoch 1/5, Loss: 2.2614
Epoch 2/5, Loss: 2.2852
Epoch 3/5, Loss: 2.2642
Epoch 4/5, Loss: 2.2725
Epoch 5/5, Loss: 2.2734
新模型评估：
Accuracy: 27.53%
Model saved to ../../../data/model/mnist_cnn_model
CPC1调整模型中, 本轮训练的数据量为：782.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 27.53%
Epoch 1/5, Loss: 2.2932
Epoch 2/5, Loss: 2.2963
Epoch 3/5, Loss: 2.2892
Epoch 4/5, Loss: 2.2860
Epoch 5/5, Loss: 2.2785
新模型评估：
Accuracy: 40.84%
Model saved to ../../../data/model/mnist_cnn_model
CPC3调整模型中, 本轮训练的数据量为：245.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 40.84%
Epoch 1/5, Loss: 2.2811
Epoch 2/5, Loss: 2.2789
Epoch 3/5, Loss: 2.2762
Epoch 4/5, Loss: 2.2750
Epoch 5/5, Loss: 2.2729
新模型评估：
Accuracy: 44.09%
Model saved to ../../../data/model/mnist_cnn_model
CPC4调整模型中, 本轮训练的数据量为：1086.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 44.09%
Epoch 1/5, Loss: 2.2630
Epoch 2/5, Loss: 2.2522
Epoch 3/5, Loss: 2.2396
Epoch 4/5, Loss: 2.2241
Epoch 5/5, Loss: 2.2053
新模型评估：
Accuracy: 47.09%
Model saved to ../../../data/model/mnist_cnn_model
CPC5调整模型中, 本轮训练的数据量为：894.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 47.09%
Epoch 1/5, Loss: 2.1888
Epoch 2/5, Loss: 2.1725
Epoch 3/5, Loss: 2.1548
Epoch 4/5, Loss: 2.1361
Epoch 5/5, Loss: 2.1150
新模型评估：
Accuracy: 48.67%
Model saved to ../../../data/model/mnist_cnn_model
CPC6调整模型中, 本轮训练的数据量为：2291.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 48.67%
Epoch 1/5, Loss: 2.0806
Epoch 2/5, Loss: 2.0212
Epoch 3/5, Loss: 1.9485
Epoch 4/5, Loss: 1.8650
Epoch 5/5, Loss: 1.7702
新模型评估：
Accuracy: 56.54%
Model saved to ../../../data/model/mnist_cnn_model
CPC7调整模型中, 本轮训练的数据量为：152.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 56.54%
Epoch 1/5, Loss: 1.7158
Epoch 2/5, Loss: 1.6818
Epoch 3/5, Loss: 1.6921
Epoch 4/5, Loss: 1.6965
Epoch 5/5, Loss: 1.7030
新模型评估：
Accuracy: 57.26%
Model saved to ../../../data/model/mnist_cnn_model
CPC8调整模型中, 本轮训练的数据量为：1343.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 57.26%
Epoch 1/5, Loss: 1.6713
Epoch 2/5, Loss: 1.6190
Epoch 3/5, Loss: 1.5673
Epoch 4/5, Loss: 1.5149
Epoch 5/5, Loss: 1.4621
新模型评估：
Accuracy: 65.45%
Model saved to ../../../data/model/mnist_cnn_model
CPC9调整模型中, 本轮训练的数据量为：226.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 65.45%
Epoch 1/5, Loss: 1.5026
Epoch 2/5, Loss: 1.4877
Epoch 3/5, Loss: 1.4771
Epoch 4/5, Loss: 1.4633
Epoch 5/5, Loss: 1.4474
新模型评估：
Accuracy: 65.53%
Model saved to ../../../data/model/mnist_cnn_model
CPC10调整模型中, 本轮训练的数据量为：468.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 65.53%
Epoch 1/5, Loss: 1.3545
Epoch 2/5, Loss: 1.3446
Epoch 3/5, Loss: 1.3109
Epoch 4/5, Loss: 1.3016
Epoch 5/5, Loss: 1.2647
新模型评估：
Accuracy: 70.02%
Model saved to ../../../data/model/mnist_cnn_model
DONE
========================= literation: 2 =========================
----- literation 2: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3525
DataOwner1的最优x_1 = 0.0416
DataOwner2的最优x_2 = 0.1515
DataOwner3的最优x_3 = 0.1340
DataOwner4的最优x_4 = 0.1379
DataOwner5的最优x_5 = 0.0965
DataOwner6的最优x_6 = 0.1285
DataOwner7的最优x_7 = 0.0941
DataOwner8的最优x_8 = 0.1381
DataOwner9的最优x_9 = 0.1398
DataOwner10的最优x_10 = 0.1445
每个DataOwner应该贡献数据比例 xn_list = [0.04157022869840666, 0.15152926076661963, 0.13396476348371167, 0.13789977485519053, 0.09653787092599003, 0.12847598157743456, 0.09405997030914102, 0.1380785052493272, 0.13981585512874026, 0.14445871613330444]
ModelOwner的最大效用 U(Eta) = 0.5942
DONE
----- literation 2: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3524595127368417
DataOwner1的分配到的支付 ： 0.0429
DataOwner2的分配到的支付 ： 0.1739
DataOwner3的分配到的支付 ： 0.1508
DataOwner4的分配到的支付 ： 0.1559
DataOwner5的分配到的支付 ： 0.1046
DataOwner6的分配到的支付 ： 0.1438
DataOwner7的分配到的支付 ： 0.1017
DataOwner8的分配到的支付 ： 0.1561
DataOwner9的分配到的支付 ： 0.1584
DataOwner10的分配到的支付 ： 0.1645
DONE
----- literation 2: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC2
DataOwner5 把数据交给 CPC1
DataOwner2 把数据交给 CPC3
DataOwner3 把数据交给 CPC4
DataOwner4 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC8
DataOwner9 把数据交给 CPC9
DataOwner10 把数据交给 CPC10
DONE
----- literation 2: 模型训练 -----
重新调整fn，进而调整xn、Eta
正在评估DataOwner1的数据质量, 本轮评估的样本数据量为：67.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 70.02%
Epoch 1/5, Loss: 1.2291
Epoch 2/5, Loss: 1.3420
Epoch 3/5, Loss: 1.2084
Epoch 4/5, Loss: 1.2915
Epoch 5/5, Loss: 1.0498
新模型评估：
Accuracy: 70.22%
loss差为：
0.17935866117477417
单位数据loss差为：
0.0026769949429070773
正在评估DataOwner5的数据质量, 本轮评估的样本数据量为：782.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 70.02%
Epoch 1/5, Loss: 1.3440
Epoch 2/5, Loss: 1.2938
Epoch 3/5, Loss: 1.2627
Epoch 4/5, Loss: 1.2380
Epoch 5/5, Loss: 1.2130
新模型评估：
Accuracy: 69.72%
loss差为：
0.13103307210482096
单位数据loss差为：
0.0001675614732798222
正在评估DataOwner2的数据质量, 本轮评估的样本数据量为：245.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 72.01%
Epoch 1/5, Loss: 0.9629
Epoch 2/5, Loss: 0.9524
Epoch 3/5, Loss: 0.9439
Epoch 4/5, Loss: 0.9286
Epoch 5/5, Loss: 0.9230
新模型评估：
Accuracy: 75.05%
loss差为：
0.0398348867893219
单位数据loss差为：
0.00016259137465029347
正在评估DataOwner3的数据质量, 本轮评估的样本数据量为：1086.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 72.01%
Epoch 1/5, Loss: 0.9453
Epoch 2/5, Loss: 0.9103
Epoch 3/5, Loss: 0.8807
Epoch 4/5, Loss: 0.8536
Epoch 5/5, Loss: 0.8259
新模型评估：
Accuracy: 75.34%
loss差为：
0.11943992797066183
单位数据loss差为：
0.00010998151746838105
正在评估DataOwner4的数据质量, 本轮评估的样本数据量为：894.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 72.01%
Epoch 1/5, Loss: 0.9093
Epoch 2/5, Loss: 0.8799
Epoch 3/5, Loss: 0.8547
Epoch 4/5, Loss: 0.8310
Epoch 5/5, Loss: 0.8073
新模型评估：
Accuracy: 73.53%
loss差为：
0.10206811342920574
单位数据loss差为：
0.00011417014924967085
正在评估DataOwner6的数据质量, 本轮评估的样本数据量为：2291.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 26.82%
Epoch 1/5, Loss: 2.2916
Epoch 2/5, Loss: 2.2799
Epoch 3/5, Loss: 2.2585
Epoch 4/5, Loss: 2.2217
Epoch 5/5, Loss: 2.1622
新模型评估：
Accuracy: 53.52%
loss差为：
0.12938055064943077
单位数据loss差为：
5.647339618045865e-05
正在评估DataOwner7的数据质量, 本轮评估的样本数据量为：152.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
**** log-parameter_analysis 运行时间： 2025-01-17 22:42:03 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.03909528644185309
DataOwner2: noise random: 0.059804858563670384
DataOwner3: noise random: 0.08621001502060306
DataOwner4: noise random: 0.019022035563856024
DataOwner5: noise random: 0.08014997925842934
DataOwner6: noise random: 0.06159052367419313
DataOwner7: noise random: 0.004156411603494492
DataOwner8: noise random: 0.0777477156427723
DataOwner9: noise random: 0.04785129147860178
DataOwner10: noise random: 0.007695907074791209
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9993813432112229, 0.9985554273657783, 0.9969846961074036, 0.9998535226000185, 0.9974016162118857, 0.9984672014688297, 0.9999930167392177, 0.9975493377086392, 0.9990723208339377, 0.9999759790462467]
归一化后的数据质量列表avg_f_list: [0.9796672761032792, 0.952212893857245, 0.9, 0.9953630561275941, 0.9138588985520043, 0.9492801646788607, 1.0, 0.9187693291487715, 0.9693950207453526, 0.9994336477039438]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3383
DataOwner1的最优x_1 = 0.1426
DataOwner2的最优x_2 = 0.1161
DataOwner3的最优x_3 = 0.0557
DataOwner4的最优x_4 = 0.1564
DataOwner5的最优x_5 = 0.0731
DataOwner6的最优x_6 = 0.1131
DataOwner7的最优x_7 = 0.1603
DataOwner8的最优x_8 = 0.0791
DataOwner9的最优x_9 = 0.1331
DataOwner10的最优x_10 = 0.1599
每个DataOwner应该贡献数据比例 xn_list = [0.14264580520771064, 0.11610569012407498, 0.055703950198206856, 0.15643435480398474, 0.07314585511653897, 0.11307473263459368, 0.16032913747310895, 0.079067897825616, 0.13309131806658234, 0.15985766924909664]
ModelOwner的最大效用 U(Eta) = 0.5778
DONE
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3383375142427196
DataOwner1的分配到的支付 ： 0.1623
DataOwner2的分配到的支付 ： 0.1284
DataOwner3的分配到的支付 ： 0.0582
DataOwner4的分配到的支付 ： 0.1809
DataOwner5的分配到的支付 ： 0.0777
DataOwner6的分配到的支付 ： 0.1247
DataOwner7的分配到的支付 ： 0.1862
DataOwner8的分配到的支付 ： 0.0844
DataOwner9的分配到的支付 ： 0.1499
DataOwner10的分配到的支付 ： 0.1856
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC1', 'DataOwner2': 'CPC2', 'DataOwner3': 'CPC3', 'DataOwner4': 'CPC4', 'DataOwner5': 'CPC5', 'DataOwner6': 'CPC9', 'DataOwner8': 'CPC6', 'DataOwner7': 'CPC10', 'DataOwner9': 'CPC7', 'DataOwner10': 'CPC8'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC2
DataOwner3 把数据交给 CPC3
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC9
DataOwner8 把数据交给 CPC6
DataOwner7 把数据交给 CPC10
DataOwner9 把数据交给 CPC7
DataOwner10 把数据交给 CPC8
DONE
----- literation 1: 模型训练 -----
CPC1调整模型中, 本轮训练的数据量为：1352.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 26.82%
Epoch 1/5, Loss: 2.2906
Epoch 2/5, Loss: 2.2877
Epoch 3/5, Loss: 2.2785
Epoch 4/5, Loss: 2.2643
Epoch 5/5, Loss: 2.2528
新模型评估：
Accuracy: 39.15%
Model saved to ../../../data/model/mnist_cnn_model
CPC2调整模型中, 本轮训练的数据量为：366.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 39.15%
Epoch 1/5, Loss: 2.2325
Epoch 2/5, Loss: 2.2268
Epoch 3/5, Loss: 2.2209
Epoch 4/5, Loss: 2.2144
Epoch 5/5, Loss: 2.2071
新模型评估：
Accuracy: 43.75%
Model saved to ../../../data/model/mnist_cnn_model
CPC3调整模型中, 本轮训练的数据量为：263.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
**** log-parameter_analysis 运行时间： 2025-01-17 22:43:05 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.01141658216311292
DataOwner2: noise random: 0.05565190315100332
DataOwner3: noise random: 0.010916139971950733
DataOwner4: noise random: 0.0769201904117881
DataOwner5: noise random: 0.08815028355753296
DataOwner6: noise random: 0.0624994012975338
DataOwner7: noise random: 0.05529858492801413
DataOwner8: noise random: 0.005483372368984474
DataOwner9: noise random: 0.03809842906985659
DataOwner10: noise random: 0.06667113618827175
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9999471873173084, 0.9987432504538906, 0.9999517448753736, 0.9976119634542615, 0.9968531824745663, 0.9984188375826696, 0.9987643065341315, 0.9999878154433819, 0.9994128779125186, 0.9982000609764101]
归一化后的数据质量列表avg_f_list: [0.9987038952732996, 0.9602963089499568, 0.9988492889481131, 0.92420637399159, 0.9, 0.9499469993354547, 0.9609680328950061, 1.0, 0.9816585374880273, 0.9429676620913202]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3434
DataOwner1的最优x_1 = 0.1562
DataOwner2的最优x_2 = 0.1205
DataOwner3的最优x_3 = 0.1563
DataOwner4的最优x_4 = 0.0810
DataOwner5的最优x_5 = 0.0508
DataOwner6的最优x_6 = 0.1098
DataOwner7的最优x_7 = 0.1212
DataOwner8的最优x_8 = 0.1573
DataOwner9的最优x_9 = 0.1411
DataOwner10的最优x_10 = 0.1024
每个DataOwner应该贡献数据比例 xn_list = [0.1561627231351038, 0.12052825671408214, 0.15628652932141768, 0.08104903908944151, 0.05075644114984305, 0.10984783103649408, 0.12120469649055904, 0.15726361617988255, 0.1410878961132977, 0.10236310190127434]
ModelOwner的最大效用 U(Eta) = 0.5836
DONE
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3433998984317428
DataOwner1的分配到的支付 ： 0.1804
DataOwner2的分配到的支付 ： 0.1339
DataOwner3的分配到的支付 ： 0.1806
DataOwner4的分配到的支付 ： 0.0866
DataOwner5的分配到的支付 ： 0.0528
DataOwner6的分配到的支付 ： 0.1207
DataOwner7的分配到的支付 ： 0.1347
DataOwner8的分配到的支付 ： 0.1819
DataOwner9的分配到的支付 ： 0.1602
DataOwner10的分配到的支付 ： 0.1116
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC2', 'DataOwner3': 'CPC1', 'DataOwner2': 'CPC8', 'DataOwner5': 'CPC3', 'DataOwner4': 'CPC9', 'DataOwner6': 'CPC4', 'DataOwner7': 'CPC5', 'DataOwner9': 'CPC6', 'DataOwner10': 'CPC7', 'DataOwner8': 'CPC10'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC2
DataOwner3 把数据交给 CPC1
DataOwner2 把数据交给 CPC8
DataOwner5 把数据交给 CPC3
DataOwner4 把数据交给 CPC9
DataOwner6 把数据交给 CPC4
DataOwner7 把数据交给 CPC5
DataOwner9 把数据交给 CPC6
DataOwner10 把数据交给 CPC7
DataOwner8 把数据交给 CPC10
DONE
----- literation 1: 模型训练 -----
CPC2调整模型中, 本轮训练的数据量为：670.00 :
**** log-parameter_analysis 运行时间： 2025-01-17 22:43:19 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.08065760343487995
DataOwner2: noise random: 0.09687856469168825
DataOwner3: noise random: 0.061060884961292465
DataOwner4: noise random: 0.08522541577276416
DataOwner5: noise random: 0.05161926395959814
DataOwner6: noise random: 0.041944524868440115
DataOwner7: noise random: 0.024401955417553214
DataOwner8: noise random: 0.0266746620789178
DataOwner9: noise random: 0.0235813870487416
DataOwner10: noise random: 0.0428312153758623
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9973624715747037, 0.996209191841275, 0.9984899384588928, 0.9970648000066891, 0.9989237292684428, 0.999289443626023, 0.9997593075702995, 0.9997119774705103, 0.9997754948607911, 0.9992575432502593]
归一化后的数据质量列表avg_f_list: [0.932338242911989, 0.9, 0.9639526872825085, 0.9239914600843473, 0.9761162865946283, 0.9863710057135293, 0.9995461044559872, 0.9982189570001996, 1.0, 0.9854765114546524]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3490
DataOwner1的最优x_1 = 0.0856
DataOwner2的最优x_2 = 0.0451
DataOwner3的最优x_3 = 0.1200
DataOwner4的最优x_4 = 0.0757
DataOwner5的最优x_5 = 0.1320
DataOwner6的最优x_6 = 0.1416
DataOwner7的最优x_7 = 0.1533
DataOwner8的最优x_8 = 0.1522
DataOwner9的最优x_9 = 0.1537
DataOwner10的最优x_10 = 0.1408
每个DataOwner应该贡献数据比例 xn_list = [0.08561678691061485, 0.04508518847886938, 0.1199716184755712, 0.07571152539799639, 0.13196348483884654, 0.14158790352619505, 0.15333655356129425, 0.15218344053767285, 0.15372940034487728, 0.14076548149004456]
ModelOwner的最大效用 U(Eta) = 0.5902
DONE
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3490396993980278
DataOwner1的分配到的支付 ： 0.0919
DataOwner2的分配到的支付 ： 0.0467
DataOwner3的分配到的支付 ： 0.1331
DataOwner4的分配到的支付 ： 0.0805
DataOwner5的分配到的支付 ： 0.1483
DataOwner6的分配到的支付 ： 0.1607
DataOwner7的分配到的支付 ： 0.1764
DataOwner8的分配到的支付 ： 0.1748
DataOwner9的分配到的支付 ： 0.1769
DataOwner10的分配到的支付 ： 0.1597
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC1', 'DataOwner2': 'CPC2', 'DataOwner3': 'CPC3', 'DataOwner4': 'CPC4', 'DataOwner5': 'CPC5', 'DataOwner6': 'CPC6', 'DataOwner7': 'CPC7', 'DataOwner8': 'CPC8', 'DataOwner9': 'CPC9', 'DataOwner10': 'CPC10'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC2
DataOwner3 把数据交给 CPC3
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC8
DataOwner9 把数据交给 CPC9
DataOwner10 把数据交给 CPC10
DONE
----- literation 1: 模型训练 -----
CPC1调整模型中, 本轮训练的数据量为：571.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 26.82%
Epoch 1/5, Loss: 2.2988
Epoch 2/5, Loss: 2.2967
Epoch 3/5, Loss: 2.2945
Epoch 4/5, Loss: 2.2921
Epoch 5/5, Loss: 2.2893
新模型评估：
Accuracy: 35.42%
Model saved to ../../../data/model/mnist_cnn_model
CPC2调整模型中, 本轮训练的数据量为：100.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 35.42%
Epoch 1/5, Loss: 2.2936
Epoch 2/5, Loss: 2.2983
Epoch 3/5, Loss: 2.2968
Epoch 4/5, Loss: 2.2921
Epoch 5/5, Loss: 2.2938
新模型评估：
Accuracy: 36.47%
Model saved to ../../../data/model/mnist_cnn_model
CPC3调整模型中, 本轮训练的数据量为：533.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 36.47%
Epoch 1/5, Loss: 2.2868
Epoch 2/5, Loss: 2.2824
Epoch 3/5, Loss: 2.2788
Epoch 4/5, Loss: 2.2720
Epoch 5/5, Loss: 2.2677
新模型评估：
Accuracy: 43.41%
Model saved to ../../../data/model/mnist_cnn_model
CPC4调整模型中, 本轮训练的数据量为：504.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 43.41%
Epoch 1/5, Loss: 2.2720
Epoch 2/5, Loss: 2.2672
Epoch 3/5, Loss: 2.2624
Epoch 4/5, Loss: 2.2569
Epoch 5/5, Loss: 2.2512
新模型评估：
Accuracy: 40.88%
CPC5调整模型中, 本轮训练的数据量为：293.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 71.31%
Epoch 1/5, Loss: 1.1043
Epoch 2/5, Loss: 1.0923
Epoch 3/5, Loss: 1.0586
Epoch 4/5, Loss: 1.0741
Epoch 5/5, Loss: 1.0501
新模型评估：
Accuracy: 73.88%
Model saved to ../../../data/model/mnist_cnn_model
CPC6调整模型中, 本轮训练的数据量为：943.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 73.88%
Epoch 1/5, Loss: 1.0132
Epoch 2/5, Loss: 0.9765
Epoch 3/5, Loss: 0.9464
Epoch 4/5, Loss: 0.9219
Epoch 5/5, Loss: 0.8979
新模型评估：
Accuracy: 75.44%
Model saved to ../../../data/model/mnist_cnn_model
CPC7调整模型中, 本轮训练的数据量为：3066.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 75.44%
Epoch 1/5, Loss: 0.8453
Epoch 2/5, Loss: 0.7749
Epoch 3/5, Loss: 0.7161
Epoch 4/5, Loss: 0.6662
Epoch 5/5, Loss: 0.6223
新模型评估：
Accuracy: 77.65%
Model saved to ../../../data/model/mnist_cnn_model
CPC8调整模型中, 本轮训练的数据量为：1014.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.65%
Epoch 1/5, Loss: 0.5992
Epoch 2/5, Loss: 0.5852
Epoch 3/5, Loss: 0.5708
Epoch 4/5, Loss: 0.5583
Epoch 5/5, Loss: 0.5451
新模型评估：
Accuracy: 75.79%
CPC9调整模型中, 本轮训练的数据量为：341.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.65%
Epoch 1/5, Loss: 0.6515
Epoch 2/5, Loss: 0.6490
Epoch 3/5, Loss: 0.6213
Epoch 4/5, Loss: 0.6672
Epoch 5/5, Loss: 0.6370
新模型评估：
Accuracy: 75.86%
CPC10调整模型中, 本轮训练的数据量为：312.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.65%
Epoch 1/5, Loss: 0.6340
Epoch 2/5, Loss: 0.6217
Epoch 3/5, Loss: 0.6196
Epoch 4/5, Loss: 0.6092
Epoch 5/5, Loss: 0.6048
新模型评估：
Accuracy: 77.10%
DONE
========================= literation: 2 =========================
----- literation 2: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3490
DataOwner1的最优x_1 = 0.0856
DataOwner2的最优x_2 = 0.0451
DataOwner3的最优x_3 = 0.1200
DataOwner4的最优x_4 = 0.0757
DataOwner5的最优x_5 = 0.1320
DataOwner6的最优x_6 = 0.1416
DataOwner7的最优x_7 = 0.1533
DataOwner8的最优x_8 = 0.1522
DataOwner9的最优x_9 = 0.1537
DataOwner10的最优x_10 = 0.1408
每个DataOwner应该贡献数据比例 xn_list = [0.08561678691061485, 0.04508518847886938, 0.1199716184755712, 0.07571152539799639, 0.13196348483884654, 0.14158790352619505, 0.15333655356129425, 0.15218344053767285, 0.15372940034487728, 0.14076548149004456]
ModelOwner的最大效用 U(Eta) = 0.5902
DONE
----- literation 2: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3490396993980278
DataOwner1的分配到的支付 ： 0.0919
DataOwner2的分配到的支付 ： 0.0467
DataOwner3的分配到的支付 ： 0.1331
DataOwner4的分配到的支付 ： 0.0805
DataOwner5的分配到的支付 ： 0.1483
DataOwner6的分配到的支付 ： 0.1607
DataOwner7的分配到的支付 ： 0.1764
DataOwner8的分配到的支付 ： 0.1748
DataOwner9的分配到的支付 ： 0.1769
DataOwner10的分配到的支付 ： 0.1597
DONE
----- literation 2: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC2
DataOwner3 把数据交给 CPC3
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC8
DataOwner9 把数据交给 CPC9
DataOwner10 把数据交给 CPC10
DONE
----- literation 2: 模型训练 -----
重新调整fn，进而调整xn、Eta
正在评估DataOwner1的数据质量, 本轮评估的样本数据量为：571.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.65%
Epoch 1/5, Loss: 0.6091
Epoch 2/5, Loss: 0.5949
Epoch 3/5, Loss: 0.5861
Epoch 4/5, Loss: 0.5762
Epoch 5/5, Loss: 0.5664
新模型评估：
Accuracy: 77.04%
loss差为：
0.042641447650061726
单位数据loss差为：
7.467854229432877e-05
正在评估DataOwner2的数据质量, 本轮评估的样本数据量为：100.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.65%
Epoch 1/5, Loss: 0.6414
Epoch 2/5, Loss: 0.5945
Epoch 3/5, Loss: 0.6208
Epoch 4/5, Loss: 0.6126
Epoch 5/5, Loss: 0.5808
新模型评估：
Accuracy: 76.48%
loss差为：
0.06051735579967499
单位数据loss差为：
0.0006051735579967499
正在评估DataOwner3的数据质量, 本轮评估的样本数据量为：533.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.65%
Epoch 1/5, Loss: 0.6102
Epoch 2/5, Loss: 0.5829
Epoch 3/5, Loss: 0.5893
Epoch 4/5, Loss: 0.5679
Epoch 5/5, Loss: 0.5571
新模型评估：
Accuracy: 75.96%
loss差为：
0.05316678351826143
单位数据loss差为：
9.975006288604396e-05
正在评估DataOwner4的数据质量, 本轮评估的样本数据量为：504.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.65%
Epoch 1/5, Loss: 0.6516
Epoch 2/5, Loss: 0.6384
Epoch 3/5, Loss: 0.6307
Epoch 4/5, Loss: 0.6231
Epoch 5/5, Loss: 0.6141
新模型评估：
Accuracy: 75.68%
loss差为：
0.037495166063308716
单位数据loss差为：
7.439517076053316e-05
正在评估DataOwner5的数据质量, 本轮评估的样本数据量为：293.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.65%
Epoch 1/5, Loss: 0.6927
Epoch 2/5, Loss: 0.6792
Epoch 3/5, Loss: 0.6532
Epoch 4/5, Loss: 0.6647
Epoch 5/5, Loss: 0.6449
新模型评估：
Accuracy: 77.98%
loss差为：
0.04776606559753416
单位数据loss差为：
0.00016302411466735206
正在评估DataOwner6的数据质量, 本轮评估的样本数据量为：943.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.65%
Epoch 1/5, Loss: 0.6389
Epoch 2/5, Loss: 0.6189
Epoch 3/5, Loss: 0.6026
Epoch 4/5, Loss: 0.5897
Epoch 5/5, Loss: 0.5782
新模型评估：
Accuracy: 75.94%
loss差为：
0.06075703303019209
单位数据loss差为：
6.442951540847517e-05
正在评估DataOwner7的数据质量, 本轮评估的样本数据量为：3066.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.65%
Epoch 1/5, Loss: 0.6144
Epoch 2/5, Loss: 0.5796
Epoch 3/5, Loss: 0.5508
Epoch 4/5, Loss: 0.5267
Epoch 5/5, Loss: 0.5048
新模型评估：
Accuracy: 76.49%
loss差为：
0.10962621929744887
单位数据loss差为：
3.575545313028339e-05
正在评估DataOwner8的数据质量, 本轮评估的样本数据量为：1014.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.65%
Epoch 1/5, Loss: 0.5918
Epoch 2/5, Loss: 0.5750
Epoch 3/5, Loss: 0.5607
Epoch 4/5, Loss: 0.5464
Epoch 5/5, Loss: 0.5359
新模型评估：
Accuracy: 76.26%
loss差为：
0.0559641532599926
单位数据loss差为：
5.519147264299073e-05
正在评估DataOwner9的数据质量, 本轮评估的样本数据量为：341.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.65%
Epoch 1/5, Loss: 0.7079
Epoch 2/5, Loss: 0.6735
Epoch 3/5, Loss: 0.6808
Epoch 4/5, Loss: 0.6865
Epoch 5/5, Loss: 0.7045
新模型评估：
Accuracy: 77.41%
loss差为：
0.003337611754735348
单位数据loss差为：
9.787717755822135e-06
正在评估DataOwner10的数据质量, 本轮评估的样本数据量为：312.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.65%
Epoch 1/5, Loss: 0.6767
Epoch 2/5, Loss: 0.6686
Epoch 3/5, Loss: 0.6573
Epoch 4/5, Loss: 0.6494
Epoch 5/5, Loss: 0.6459
新模型评估：
Accuracy: 76.90%
loss差为：
0.03088845014572139
单位数据loss差为：
9.900144277474804e-05
经过服务器调节后的真实数据质量：
数据质量列表avg_f_list: [7.467854229432877e-05, 0.0006051735579967499, 9.975006288604396e-05, 7.439517076053316e-05, 0.00016302411466735206, 6.442951540847517e-05, 3.575545313028339e-05, 5.519147264299073e-05, 9.787717755822135e-06, 9.900144277474804e-05]
归一化后的数据质量列表avg_f_list:[0.9108989532757864, 1.0, 0.9151099235235117, 0.9108513586716418, 0.9257373263780546, 0.9091775440327137, 0.9043614969687478, 0.9076259379747419, 0.9, 0.914984186554189]
CPC1调整模型中, 本轮训练的数据量为：571.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.72%
Epoch 1/5, Loss: 0.4205
Epoch 2/5, Loss: 0.4088
Epoch 3/5, Loss: 0.4020
Epoch 4/5, Loss: 0.3969
Epoch 5/5, Loss: 0.3929
新模型评估：
Accuracy: 77.34%
CPC2调整模型中, 本轮训练的数据量为：100.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.72%
Epoch 1/5, Loss: 0.4171
Epoch 2/5, Loss: 0.3845
Epoch 3/5, Loss: 0.3751
Epoch 4/5, Loss: 0.3817
Epoch 5/5, Loss: 0.3778
新模型评估：
Accuracy: 77.24%
CPC3调整模型中, 本轮训练的数据量为：533.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.72%
Epoch 1/5, Loss: 0.3992
Epoch 2/5, Loss: 0.4002
Epoch 3/5, Loss: 0.3981
Epoch 4/5, Loss: 0.4235
Epoch 5/5, Loss: 0.3808
新模型评估：
Accuracy: 76.96%
CPC4调整模型中, 本轮训练的数据量为：504.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.72%
Epoch 1/5, Loss: 0.4824
Epoch 2/5, Loss: 0.4699
Epoch 3/5, Loss: 0.4676
Epoch 4/5, Loss: 0.4617
Epoch 5/5, Loss: 0.4572
新模型评估：
Accuracy: 76.19%
CPC5调整模型中, 本轮训练的数据量为：293.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.72%
Epoch 1/5, Loss: 0.5114
Epoch 2/5, Loss: 0.4997
Epoch 3/5, Loss: 0.4743
Epoch 4/5, Loss: 0.4824
Epoch 5/5, Loss: 0.4710
新模型评估：
Accuracy: 78.36%
Model saved to ../../../data/model/mnist_cnn_model
CPC6调整模型中, 本轮训练的数据量为：943.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.36%
Epoch 1/5, Loss: 0.4444
Epoch 2/5, Loss: 0.4287
Epoch 3/5, Loss: 0.4219
Epoch 4/5, Loss: 0.4196
Epoch 5/5, Loss: 0.4108
新模型评估：
Accuracy: 76.60%
CPC7调整模型中, 本轮训练的数据量为：3066.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.36%
Epoch 1/5, Loss: 0.4449
Epoch 2/5, Loss: 0.4320
Epoch 3/5, Loss: 0.4215
Epoch 4/5, Loss: 0.4115
Epoch 5/5, Loss: 0.4035
新模型评估：
Accuracy: 76.53%
CPC8调整模型中, 本轮训练的数据量为：1014.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.77%
Epoch 1/5, Loss: 0.4106
Epoch 2/5, Loss: 0.3992
Epoch 3/5, Loss: 0.3939
Epoch 4/5, Loss: 0.3869
Epoch 5/5, Loss: 0.3829
新模型评估：
Accuracy: 76.83%
CPC9调整模型中, 本轮训练的数据量为：341.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.77%
Epoch 1/5, Loss: 0.5724
Epoch 2/5, Loss: 0.4827
Epoch 3/5, Loss: 0.5135
Epoch 4/5, Loss: 0.4921
Epoch 5/5, Loss: 0.4852
新模型评估：
Accuracy: 78.08%
CPC10调整模型中, 本轮训练的数据量为：312.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.77%
Epoch 1/5, Loss: 0.4977
Epoch 2/5, Loss: 0.4865
Epoch 3/5, Loss: 0.4768
Epoch 4/5, Loss: 0.4702
Epoch 5/5, Loss: 0.4682
新模型评估：
Accuracy: 78.32%
DONE
========================= literation: 3 =========================
----- literation 3: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.2911
DataOwner1的最优x_1 = 0.1077
DataOwner2的最优x_2 = 0.1846
DataOwner3的最优x_3 = 0.1121
DataOwner4的最优x_4 = 0.1077
DataOwner5的最优x_5 = 0.1228
DataOwner6的最优x_6 = 0.1059
DataOwner7的最优x_7 = 0.1008
DataOwner8的最优x_8 = 0.1043
DataOwner9的最优x_9 = 0.0960
DataOwner10的最优x_10 = 0.1120
每个DataOwner应该贡献数据比例 xn_list = [0.10773713368985087, 0.1845524759813923, 0.1121182399576983, 0.10768712569034947, 0.12280278037600495, 0.10592139500671763, 0.10076365209395839, 0.10427228628819651, 0.09599198246262075, 0.11198865716275275]
ModelOwner的最大效用 U(Eta) = 0.5253
xn开始变化：
DONE
----- literation 3: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.2910956996974834
DataOwner1的分配到的支付 ： 0.5634
DataOwner2的分配到的支付 ： 0.1131
DataOwner3的分配到的支付 ： 0.0673
DataOwner4的分配到的支付 ： 0.0601
DataOwner5的分配到的支付 ： 0.0749
DataOwner6的分配到的支付 ： 0.0789
DataOwner7的分配到的支付 ： 0.0850
DataOwner8的分配到的支付 ： 0.0847
DataOwner9的分配到的支付 ： 0.0848
DataOwner10的分配到的支付 ： 0.0789
DONE
----- literation 3: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC2
DataOwner3 把数据交给 CPC3
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC8
DataOwner9 把数据交给 CPC9
DataOwner10 把数据交给 CPC10
DONE
----- literation 3: 模型训练 -----
CPC1调整模型中, 本轮训练的数据量为：6670.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.77%
Epoch 1/5, Loss: 0.4326
Epoch 2/5, Loss: 0.4098
Epoch 3/5, Loss: 0.3925
Epoch 4/5, Loss: 0.3814
Epoch 5/5, Loss: 0.3664
新模型评估：
Accuracy: 78.96%
Model saved to ../../../data/model/mnist_cnn_model
CPC2调整模型中, 本轮训练的数据量为：410.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.96%
Epoch 1/5, Loss: 0.3337
Epoch 2/5, Loss: 0.3212
Epoch 3/5, Loss: 0.3185
Epoch 4/5, Loss: 0.3133
Epoch 5/5, Loss: 0.3057
新模型评估：
Accuracy: 77.65%
CPC3调整模型中, 本轮训练的数据量为：533.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.96%
Epoch 1/5, Loss: 0.3079
Epoch 2/5, Loss: 0.3039
Epoch 3/5, Loss: 0.2903
Epoch 4/5, Loss: 0.3159
Epoch 5/5, Loss: 0.3199
新模型评估：
Accuracy: 78.05%
CPC4调整模型中, 本轮训练的数据量为：717.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 45.28%
Epoch 1/5, Loss: 2.2291
Epoch 2/5, Loss: 2.2164
Epoch 3/5, Loss: 2.2094
Epoch 4/5, Loss: 2.1958
Epoch 5/5, Loss: 2.1740
新模型评估：
Accuracy: 46.77%
Model saved to ../../../data/model/mnist_cnn_model
CPC5调整模型中, 本轮训练的数据量为：293.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 46.77%
Epoch 1/5, Loss: 2.1364
Epoch 2/5, Loss: 2.1362
Epoch 3/5, Loss: 2.1250
Epoch 4/5, Loss: 2.1201
Epoch 5/5, Loss: 2.1048
新模型评估：
Accuracy: 47.94%
Model saved to ../../../data/model/mnist_cnn_model
CPC6调整模型中, 本轮训练的数据量为：943.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 47.94%
Epoch 1/5, Loss: 2.1282
Epoch 2/5, Loss: 2.1087
Epoch 3/5, Loss: 2.0853
Epoch 4/5, Loss: 2.0610
Epoch 5/5, Loss: 2.0363
新模型评估：
Accuracy: 48.07%
Model saved to ../../../data/model/mnist_cnn_model
CPC7调整模型中, 本轮训练的数据量为：3066.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 48.07%
Epoch 1/5, Loss: 1.9835
Epoch 2/5, Loss: 1.8854
Epoch 3/5, Loss: 1.7720
Epoch 4/5, Loss: 1.6462
Epoch 5/5, Loss: 1.5115
新模型评估：
Accuracy: 65.62%
Model saved to ../../../data/model/mnist_cnn_model
CPC8调整模型中, 本轮训练的数据量为：1014.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 65.62%
Epoch 1/5, Loss: 1.4196
Epoch 2/5, Loss: 1.3767
Epoch 3/5, Loss: 1.3387
Epoch 4/5, Loss: 1.2985
Epoch 5/5, Loss: 1.2602
新模型评估：
Accuracy: 68.72%
Model saved to ../../../data/model/mnist_cnn_model
CPC9调整模型中, 本轮训练的数据量为：341.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 68.72%
Epoch 1/5, Loss: 1.2938
Epoch 2/5, Loss: 1.2676
Epoch 3/5, Loss: 1.2744
Epoch 4/5, Loss: 1.2395
Epoch 5/5, Loss: 1.2568
新模型评估：
Accuracy: 70.24%
Model saved to ../../../data/model/mnist_cnn_model
CPC10调整模型中, 本轮训练的数据量为：312.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 70.24%
Epoch 1/5, Loss: 1.1943
Epoch 2/5, Loss: 1.1788
Epoch 3/5, Loss: 1.1639
Epoch 4/5, Loss: 1.1535
Epoch 5/5, Loss: 1.1395
新模型评估：
Accuracy: 70.38%
Model saved to ../../../data/model/mnist_cnn_model
DONE
最终的列表：
[-0.001045737676195901, -0.0011578050891925176, -0.0012712225537129217, -0.001385986993080025, -0.0015020953399583793, -0.0016195445363187226, -0.0017383315334027274, -0.001858453291687899, -0.0019799067808526083, -0.002102688979741321, -0.002226796876329956, -0.002352227467691436, -0.0024789777599613164, -0.0026070447683036874, -0.0027364255168771216, -0.0028671170388008364, -0.0029991163761209916, -0.0031324205797771364, -0.0032670267095688023, -0.0034029318341222796, -0.0035401330308574945, -0.0036786273859550737, -0.003818411994323541, -0.003959483959566641, -0.004101840393950871, -0.0042454784183730936, -0.004390395162328323, -0.004536587763877636, -0.0046840533696162895, -0.004832789134641885, -0.004982792222522736, -0.0051340598052663985, -0.005286589063288251, -0.005440377185380305, -0.005595421368680145, -0.005751718818639916, -0.005909266748995556, -0.0060680623817361695, -0.00622810294707337, -0.006389385683411027, -0.006551907837314873, -0.006715666663482445, -0.006880659424713088, -0.007046883391878048, -0.007214335843890758, -0.007383014067677281, -0.007552915358146742, -0.0077240370181621, -0.007896376358510843, -0.008069930697875968, -0.008244697362806946, -0.00842067368769104, -0.008597857014724437, -0.008776244693883765, -0.008955834082897657, -0.009136622547218358, -0.009318607459993598, -0.009501786202038454, -0.009686156161807416, -0.00987171473536659, -0.010058459326365933, -0.010246387346011708, -0.01043549621303895, -0.010625783353684193, -0.010817246201658248, -0.011009882198118964, -0.011203688791644378, -0.011398663438205767, -0.011594803601140916, -0.011792106751127393, -0.011990570366156209, -0.012190191931505168, -0.012390968939712754, -0.012592898890551832, -0.01279597929100365, -0.013000207655231827, -0.013205581504556599, -0.013412098367428862, -0.013619755779404885, -0.013828551283120452, -0.014038482428265706, -0.014249546771559735, -0.014461741876725442, -0.0146750653144644, -0.014889514662431935, -0.015105087505212236, -0.015321781434293663, -0.015539594048043975, -0.015758522951685935, -0.015978565757272717, -0.016199720083663688, -0.016421983556500097, -0.016645353808181024, -0.01686982847783927, -0.017095405211317555, -0.01732208166114453, -0.017549855486511196, -0.01777872435324722, -0.018008685933797453, -0.01823973790719849, -0.018471877959055333, -0.018705103781518226, -0.01893941307325947, -0.019174803539450433, -0.019411272891738676, -0.019648818848225003, -0.01988743913344082, -0.02012713147832547, -0.020367893620203706, -0.020609723302763228, -0.02085261827603234, -0.02109657629635768, -0.021341595126382104, -0.021587672535022542, -0.021834806297448092, -0.02208299419505806, -0.02233223401546025, -0.02258252355244922, -0.02283386060598462, -0.023086242982169797, -0.023339668493230215, -0.02359413495749231, -0.02384964019936199, -0.024106182049303662, -0.024363758343819097, -0.024622366925426464, -0.024882005642639343, -0.025142672349946077, -0.0254043649077889, -0.025667081182543364, -0.025930819046497716, -0.026195576377832597, -0.02646135106060045, -0.02672814098470537, -0.026995944045882833, -0.027264758145679566, -0.027534581191433496, -0.027805411096253813, -0.028077245779001114, -0.028350083164267473, -0.028623921182356876, -0.028898757769265496, -0.029174590866662173, -0.02945141842186895, -0.029729238387841556, -0.030008048723150355, -0.030287847391960804, -0.030568632364014497, -0.030850401614610085, -0.03113315312458409, -0.031416884880292234, -0.03170159487359048, -0.03198728110181606, -0.03227394156776933, -0.0325615742796945, -0.0328501772512616, -0.03313974850154769, -0.03343028605501877, -0.03372178794151112, -0.034014252196213374, -0.03430767685964822, -0.034602059977654365, -0.03489739960136834, -0.035193693787206826, -0.0354909405968486, -0.035789138097216844, -0.036088284360461326, -0.0363883774639408, -0.03668941549020541, -0.03699139652697919, -0.03729431866714275, -0.03759818000871551, -0.03790297865483891, -0.03820871271375886, -0.038515380298808605, -0.03882297952839181, -0.039131508525965336, -0.0394409654200224, -0.03975134834407565, -0.04006265543664034, -0.040374884841217734, -0.0406880347062781, -0.041002103185244415, -0.04131708843647569, -0.041632988623250405, -0.04194980191375028, -0.04226752648104373, -0.04258616050306971, -0.04290570216262146, -0.04322614964733035, -0.04354750114964975, -0.043869754866839084, -0.04419290900094783, -0.04451696175879952, -0.04484191135197613, -0.04516775599680209, -0.045494493914328665, -0.045822123330318376, -0.04615064247522918, -0.04648004958419927, -0.0468103428970314, -0.047141520658177505, -0.04747358111672337, -0.04780652252637346, -0.048140343145435555, -0.048475041236805655, -0.04881061506795284, -0.049147062910904316, -0.04948438304223038, -0.049822573743029536, -0.05016163329891343, -0.05050155999999248, -0.05084235214086069, -0.05118400802058115, -0.05152652594267146, -0.05186990421508908, -0.052214141150216775, -0.05255923506484836, -0.052905184280173906, -0.053251987121765904, -0.05359964191956451, -0.05394814700786363, -0.054297500725296544, -0.05464770141482195, -0.054998747423709776, -0.055350637103527195, -0.05570336881012464, -0.05605694090362204, -0.056411351748394734, -0.05676659971306, -0.0571226831704629, -0.057479600497663014, -0.057837350075920446, -0.05819593029068254, -0.05855533953157013, -0.058915576192364205, -0.05927663867099228, -0.05963852536951536, -0.060001234694114364, -0.06036476505507685, -0.06072911486678398, -0.06109428254769711, -0.06146026652034489, -0.06182706521131012, -0.062194677051216674, -0.06256310047471655, -0.06293233392047715, -0.06330237583116805, -0.06367322465344849, -0.06404487883795451, -0.06441733683928619, -0.06479059711599514, -0.06516465813057154, -0.06553951834943186, -0.06591517624290641, -0.06629163028522647, -0.06666887895451226, -0.06704692073276033, -0.0674257541058314, -0.06780537756343782, -0.06818578959913157, -0.06856698871029193, -0.0689489733981134, -0.06933174216759364, -0.06971529352752118, -0.07009962599046377, -0.07048473807275615, -0.0708706282944882, -0.07125729517949309, -0.07164473725533538, -0.0720329530532994, -0.07242194110837741, -0.07281169995925782, -0.07320222814831359, -0.07359352422159085, -0.07398558672879701, -0.07437841422328934, -0.07477200526206365, -0.07516635840574265, -0.07556147221856471, -0.07595734526837244, -0.0763539761266015, -0.07675136336826899, -0.07714950557196279, -0.07754840131982987, -0.07794804919756551, -0.07834844779440198, -0.07874959570309756, -0.07915149151992557, -0.07955413384466345, -0.0799575212805817, -0.08036165243443302, -0.08076652591644154, -0.08117214034029205, -0.08157849432311898, -0.08198558648549606, -0.08239341545142528, -0.08280197984832646, -0.08321127830702651, -0.08362130946174917, -0.08403207195010395, -0.0844435644130761, -0.08485578549501602, -0.08526873384362876, -0.08568240810996378, -0.08609680694840452, -0.08651192901665827, -0.08692777297574567, -0.0873443374899906, -0.08776162122701012, -0.08817962285770412, -0.08859834105624542, -0.08901777450006948, -0.0894379218698646, -0.08985878184956175, -0.09028035312632479, -0.09070263439054027, -0.09112562433580784, -0.09154932165893018, -0.0919737250599034, -0.09239883324190709, -0.09282464491129458, -0.09325115877758347, -0.09367837355344569, -0.09410628795469791, -0.09453490070029211, -0.0949642105123058, -0.09539421611593277, -0.09582491623947326, -0.09625630961432474, -0.09668839497497247, -0.09712117105898005, -0.09755463660697997, -0.0979887903626645, -0.09842363107277627, -0.09885915748709906, -0.09929536835844854, -0.09973226244266314, -0.10016983849859487, -0.10060809528810027, -0.10104703157603112, -0.10148664613022562, -0.10192693772149919, -0.10236790512363575, -0.10280954711337831, -0.10325186247042056, -0.1036948499773977, -0.10413850841987754, -0.10458283658635165, -0.10502783326822707, -0.1054734972598167, -0.10591982735833133, -0.1063668223638704, -0.10681448107941383, -0.10726280231081281, -0.10771178486678179, -0.10816142755888941, -0.10861172920155027, -0.10906268861201612, -0.10951430461036793, -0.10996657601950677, -0.11041950166514569, -0.11087308037580151, -0.1113273109827862, -0.11178219232019859, -0.11223772322491599, -0.11269390253658634, -0.11315072909761942, -0.11360820175317876, -0.114066319351174, -0.11452508074225187, -0.11498448477978873, -0.11544453031988217, -0.11590521622134303, -0.11636654134568736, -0.11682850455712823, -0.1172911047225682, -0.11775434071159085, -0.11821821139645317, -0.1186827156520775, -0.11914785235604386, -0.11961362038858181, -0.1200800186325629, -0.1205470459734928, -0.12101470129950359, -0.12148298350134568, -0.12195189147238078, -0.1224214241085736, -0.1228915803084844, -0.12336235897326153, -0.12383375900663374, -0.12430577931490228, -0.12477841880693408, -0.12525167639415352, -0.12572555099053528, -0.12620004151259673, -0.1266751468793909, -0.12715086601249842, -0.1276271978360205, -0.12810414127657177, -0.1285816952632725, -0.12905985872774162, -0.12953863060408916, -0.13001800982890938, -0.13049799534127315, -0.130978586082721, -0.13145978099725564, -0.1319415790313354, -0.13242397913386633, -0.13290698025619568, -0.13339058135210446, -0.13387478137780073, -0.13435957929191222, -0.13484497405547946, -0.13533096463194882, -0.13581754998716566, -0.13630472908936714, -0.1367925009091755, -0.13728086441959103, -0.13776981859598547, -0.13825936241609477, -0.1387494948600126, -0.13924021491018346, -0.13973152155139584, -0.14022341377077563, -0.14071589055777922, -0.14120895090418695, -0.14170259380409633, -0.14219681825391534, -0.14269162325235601, -0.1431870078004276, -0.14368297090143023, -0.14417951156094805, -0.14467662878684284, -0.1451743215892476, -0.14567258898055985, -0.1461714299754353, -0.1466708435907813, -0.1471708288457506, -0.14767138476173464, -0.14817251036235746, -0.14867420467346915, -0.1491764667231395, -0.149679295541652, -0.15018269016149705, -0.150686649617366, -0.15119117294614476, -0.15169625918690788, -0.15220190738091183, -0.15270811657158917, -0.15321488580454223, -0.15372221412753712, -0.1542301005904974, -0.15473854424549793, -0.1552475441467593, -0.15575709935064097, -0.15626720891563595, -0.15677787190236425, -0.15728908737356717, -0.15780085439410135, -0.15831317203093237, -0.15882603935312944, -0.15933945543185907, -0.15985341934037928, -0.16036793015403356, -0.16088298695024522, -0.1613985888085117, -0.1619147348103981, -0.16243142403953198, -0.16294865558159755, -0.16346642852432958, -0.16398474195750767, -0.16450359497295097, -0.16502298666451193, -0.16554291612807098, -0.16606338246153063, -0.16658438476481002, -0.1671059221398391, -0.1676279936905533, -0.16815059852288744, -0.16867373574477068, -0.16919740446612058, -0.16972160379883788, -0.17024633285680085, -0.1707715907558594, -0.17129737661383054, -0.17182368955049182, -0.17235052868757667, -0.1728778931487686, -0.17340578205969603, -0.17393419454792647, -0.1744631297429617, -0.17499258677623208, -0.1755225647810913, -0.17605306289281097, -0.17658408024857547, -0.17711561598747644, -0.17764766925050784, -0.17818023918056036, -0.17871332492241637, -0.1792469256227448, -0.17978104043009563, -0.1803156684948951, -0.1808508089694401, -0.18138646100789352, -0.1819226237662786, -0.18245929640247427, -0.18299647807620983, -0.18353416794905986, -0.18407236518443926, -0.18461106894759793, -0.18515027840561643, -0.18568999272739994, -0.1862302110836742, -0.18677093264697991, -0.18731215659166817, -0.1878538820938953, -0.18839610833161796, -0.1889388344845883, -0.18948205973434884, -0.1900257832642281, -0.1905700042593349, -0.19111472190655443, -0.19165993539454268, -0.19220564391372225, -0.19275184665627698, -0.19329854281614733, -0.19384573158902596, -0.19439341217235234, -0.19494158376530862, -0.1954902455688146, -0.19603939678552285, -0.1965890366198147, -0.19713916427779454, -0.19768977896728607, -0.19824087989782713, -0.19879246628066521, -0.1993445373287529, -0.19989709225674318, -0.20045013028098496, -0.20100365061951814, -0.20155765249206986, -0.20211213512004883, -0.2026670977265418, -0.20322253953630837, -0.2037784597757769, -0.20433485767303988, -0.20489173245784958, -0.20544908336161305, -0.20600690961738855, -0.2065652104598804, -0.2071239851254349, -0.20768323285203583, -0.20824295287930011, -0.20880314444847348, -0.2093638068024259, -0.20992493918564753, -0.21048654084424429, -0.2110486110259332, -0.2116111489800387, -0.21217415395748795, -0.2127376252108064, -0.21330156199411415, -0.213865963563121, -0.2144308291751228, -0.2149961580889967, -0.21556194956519736, -0.21612820286575263, -0.2166949172542592, -0.21726209199587876, -0.21782972635733344, -0.21839781960690213, -0.21896637101441596, -0.21953537985125426, -0.2201048453903407, -0.22067476690613902, -0.22124514367464881, -0.22181597497340189, -0.22238726008145754, -0.2229589982793994, -0.2235311888493306, -0.22410383107487025, -0.22467692424114927, -0.22525046763480633, -0.22582446054398403, -0.22639890225832487, -0.22697379206896734, -0.22754912926854193, -0.22812491315116717, -0.22870114301244593, -0.2292778181494612, -0.2298549378607725, -0.23043250144641164, -0.23101050820787944, -0.23158895744814134, -0.23216784847162386, -0.2327471805842104, -0.23332695309323798, -0.23390716530749317, -0.23448781653720813, -0.23506890609405706, -0.23565043329115226, -0.23623239744304075, -0.23681479786569987, -0.23739763387653434, -0.23798090479437167, -0.23856460993945944, -0.2391487486334608, -0.23973332019945087, -0.24031832396191383, -0.24090375924673813, -0.24148962538121366, -0.2420759216940278, -0.242662647515262, -0.2432498021763878, -0.24383738501026347, -0.24442539535113056, -0.2450138325346099, -0.2456026958976984, -0.24619198477876525, -0.24678169851754878, -0.24737183645515232, -0.247962397934041, -0.24855338229803836, -0.24914478889232256, -0.24973661706342315, -0.2503288661592172, -0.2509215355289265, -0.2515146245231132, -0.2521081324936772, -0.25270205879385227, -0.2532964027782024, -0.25389116380261934, -0.2544863412243176, -0.255081934401833, -0.2556779426950174, -0.25627436546503674, -0.2568712020743668, -0.2574684518867903, -0.25806611426739345, -0.25866418858256246, -0.25926267419998045, -0.25986157048862385, -0.26046087681875946, -0.2610605925619408, -0.2616607170910049, -0.2622612497800696, -0.2628621900045291, -0.26346353714105186, -0.2640652905675767, -0.2646674496633098, -0.2652700138087214, -0.2658729823855425, -0.2664763547767622, -0.2670801303666235, -0.26768430854062103, -0.2682888886854974, -0.2688938701892402, -0.2694992524410787, -0.27010503483148096, -0.2707112167521506, -0.27131779759602315, -0.271924776757264, -0.2725321536312642, -0.2731399276146381, -0.2737480981052198, -0.2743566645020604, -0.27496562620542475, -0.2755749826167884, -0.27618473313883474, -0.2767948771754514, -0.27740541413172815, -0.27801634341395276, -0.27862766442960907, -0.2792393765873729, -0.2798514792971102, -0.2804639719698731, -0.2810768540178974, -0.2816901248545996, -0.2823037838945735, -0.2829178305535881, -0.28353226424858374, -0.2841470843976697, -0.28476229042012124, -0.2853778817363763, -0.28599385776803316, -0.2866102179378469, -0.2872269616697275, -0.28784408838873554, -0.2884615975210806, -0.2890794884941177, -0.2896977607363446, -0.29031641367739924, -0.29093544674805627, -0.29155485938022496, -0.2921746510069456, -0.29279482106238763, -0.29341536898184567, -0.29403629420173777, -0.2946575961596022, -0.2952792742940943, -0.2959013280449846, -0.2965237568531551, -0.29714656016059704, -0.29776973741040813, -0.2983932880467897, -0.29901721151504396, -0.29964150726157135, -0.30026617473386785, -0.300891213380522, -0.3015166226512126, -0.30214240199670567, -0.3027685508688522, -0.3033950687205847, -0.3040219550059155, -0.3046492091799335, -0.3052768306988014, -0.30590481901975364, -0.30653317360109295, -0.3071618939021888, -0.3077909793834735, -0.3084204295064407, -0.3090502437336422, -0.30968042152868536, -0.31031096235623085, -0.3109418656819896, -0.31157313097272077, -0.3122047576962285, -0.3128367453213602, -0.31346909331800304, -0.31410180115708225, -0.3147348683105581, -0.3153682942514234, -0.3160020784537013, -0.3166362203924423, -0.3172707195437223, -0.31790557538463937, -0.31854078739331204, -0.3191763550488763, -0.31981227783148325, -0.3204485552222967, -0.3210851867034905, -0.3217221717582466, -0.3223595098707518, -0.32299720052619596, -0.3236352432107695, -0.3242736374116604, -0.3249123826170526, -0.32555147831612286, -0.326190923999039, -0.32683071915695694, -0.3274708632820186, -0.3281113558673493, -0.32875219640705594, -0.3293933843962239, -0.33003491933091494, -0.3306768007081651, -0.33131902802598195, -0.3319616007833427, -0.3326045184801912, -0.33324778061743643, -0.33389138669694945, -0.3345353362215615, -0.3351796286950616, -0.33582426362219414, -0.3364692405086567, -0.33711455886109754, -0.3377602181871139, -0.33840621799524884, -0.3390525577949898, -0.3396992370967657, -0.3403462554119452, -0.340993612252834, -0.3416413071326727, -0.3422893395656352, -0.34293770906682525, -0.3435864151522753, -0.3442354573389438, -0.34488483514471285, -0.34553454808838663, -0.34618459568968823, -0.34683497746925857, -0.34748569294865317, -0.3481367416503407, -0.34878812309770035, -0.34943983681502, -0.350091882327494, -0.3507442591612205, -0.35139696684320026, -0.3520500049013336, -0.35270337286441866, -0.3533570702621493, -0.3540110966251129, -0.35466545148478834, -0.3553201343735434, -0.35597514482463355, -0.35663048237219896, -0.35728614655126306, -0.3579421368977298, -0.35859845294838216, -0.3592550942408801, -0.35991206031375755, -0.36056935070642177, -0.3612269649591501, -0.36188490261308837, -0.3625431632102489, -0.3632017462935084, -0.3638606514066058, -0.3645198780941405, -0.36517942590157015, -0.3658392943752085, -0.36649948306222363, -0.36715999151063583, -0.36782081926931565, -0.36848196588798177, -0.3691434309171992, -0.3698052139083773, -0.3704673144137674, -0.3711297319864614, -0.3717924661803892, -0.37245551655031744, -0.3731188826518468, -0.3737825640414107, -0.37444656027627293, -0.37511087091452566, -0.375775495515088, -0.37644043363770335, -0.3771056848429383, -0.3777712486921798, -0.37843712474763425, -0.37910331257232455, -0.379769811730089, -0.38043662178557913, -0.3811037423042575, -0.3817711728523963, -0.38243891299707516, -0.38310696230617963, -0.3837753203483986, -0.3844439866932232, -0.3851129609109446, -0.3857822425726519, -0.3864518312502309, -0.3871217265163617, -0.38779192794451695, -0.3884624351089603, -0.3891332475847443, -0.3898043649477087, -0.39047578677447853, -0.3911475126424625, -0.3918195421298507, -0.3924918748156135, -0.393164510279499, -0.3938374481020319, -0.39451068786451116, -0.3951842291490088, -0.39585807153836744, -0.396532214616199, -0.39720665796688287, -0.39788140117556386, -0.3985564438281509, -0.3992317855113148, -0.39990742581248695, -0.400583364319857, -0.4012596006223719, -0.40193613430973335, -0.4026129649723967, -0.4032900922015688, -0.4039675155892065, -0.40464523472801495, -0.40532324921144547, -0.40600155863369475, -0.40668016258970213, -0.40735906067514843, -0.40803825248645426, -0.40871773762077807, -0.40939751567601484, -0.410077586250794, -0.41075794894447815, -0.411438603357161, -0.41211954908966586, -0.41280078574354423, -0.4134823129210735, -0.4141641302252563, -0.41484623725981756, -0.41552863362920417, -0.4162113189385824, -0.41689429279383666, -0.4175775548015678, -0.41826110456909144, -0.4189449417044365, -0.41962906581634346, -0.42031347651426276, -0.4209981734083529, -0.4216831561094796, -0.4223684242292134, -0.4230539773798285, -0.4237398151743011, -0.42442593722630767, -0.4251123431502234, -0.42579903256112095, -0.42648600507476825, -0.4271732603076276, -0.4278607978768536, -0.4285486174002921, -0.4292367184964777, -0.42992510078463353, -0.4306137638846684, -0.43130270741717625, -0.4319919310034338, -0.43268143426540007, -0.4333712168257135, -0.43406127830769153, -0.4347516183353284, -0.43544223653329406, -0.4361331325269326, -0.4368243059422602, -0.43751575640596435, -0.4382074835454023, -0.43889948698859854, -0.43959176636424485, -0.44028432130169737, -0.4409771514309764, -0.44167025638276364, -0.44236363578840193, -0.4430572892798926, -0.44375121648989513, -0.4444454170517247, -0.44513989059935166, -0.4458346367673991, -0.44652965519114185, -0.4472249455065057, -0.4479205073500646, -0.44861634035904013, -0.4493124441713, -0.4500088184253563, -0.45070546276036416, -0.45140237681612044, -0.4520995602330624, -0.45279701265226585, -0.45349473371544435, -0.454192723064947, -0.454890980343758, -0.4555895051954944, -0.45628829726440495, -0.4569873561953689, -0.4576866816338947, -0.45838627322611786, -0.45908613061880066, -0.45978625345932966, -0.4604866413957155, -0.46118729407659, -0.4618882111512065, -0.46258939226943707, -0.46329083708177216, -0.4639925452393183, -0.4646945163937979, -0.46539675019754667, -0.46609924630351296, -0.46680200436525643, -0.4675050240369465, -0.4682083049733611, -0.4689118468298852, -0.4696156492625094, -0.47031971192782973, -0.47102403448304375, -0.47172861658595255, -0.4724334578949564, -0.47313855806905547, -0.47384391676784765, -0.4745495336515275, -0.4752554083808844, -0.4759615406173022, -0.4766679300227573, -0.4773745762598173, -0.4780814789916398, -0.47878863788197157, -0.4794960525951465, -0.4802037227960848, -0.4809116481502915, -0.48161982832385597, -0.4823282629834489, -0.4830369517963228, -0.4837458944303097, -0.48445509055382074, -0.4851645398358434, -0.4858742419459424, -0.48658419655425655, -0.4872944033314983, -0.48800486194895276, -0.4887155720784756, -0.48942653339249287, -0.4901377455639988, -0.4908492082665551, -0.49156092117428996, -0.49227288396189606, -0.49298509630462994, -0.4936975578783106, -0.49441026835931823, -0.49512322742459336, -0.49583643475163486]
**** log-parameter_analysis 运行时间： 2025-01-17 23:04:47 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.060844997898674216
DataOwner2: noise random: 0.08859484346216961
DataOwner3: noise random: 0.07297759892194641
DataOwner4: noise random: 0.03258939539398142
DataOwner5: noise random: 0.014190322344247663
DataOwner6: noise random: 0.09687091208096475
DataOwner7: noise random: 0.012757541076017865
DataOwner8: noise random: 0.0956735099567791
DataOwner9: noise random: 0.06761704240284887
DataOwner10: noise random: 0.010326141665297817
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9984967969145012, 0.9968164249254796, 0.9978449775306767, 0.9995706045057651, 0.9999185989480246, 0.9962084998221096, 0.9999341436382053, 0.9962994461648321, 0.9981477762483458, 0.9999568230947059]
归一化后的数据质量列表avg_f_list: [0.9610485522719235, 0.916218587863393, 0.9436589266601221, 0.9896962305315434, 0.9989802334563627, 0.9, 0.999394943956238, 0.9024263206801679, 0.9517371711350017, 1.0]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3363
DataOwner1的最优x_1 = 0.1265
DataOwner2的最优x_2 = 0.0779
DataOwner3的最优x_3 = 0.1088
DataOwner4的最优x_4 = 0.1529
DataOwner5的最优x_5 = 0.1607
DataOwner6的最优x_6 = 0.0577
DataOwner7的最优x_7 = 0.1611
DataOwner8的最优x_8 = 0.0608
DataOwner9的最优x_9 = 0.1172
DataOwner10的最优x_10 = 0.1616
每个DataOwner应该贡献数据比例 xn_list = [0.12649655758900466, 0.077857812360099, 0.10877858731403715, 0.15286247751951798, 0.16071651715685614, 0.05769794233110358, 0.1610599285227827, 0.06080914759933146, 0.11718023593075123, 0.1615598395336469]
ModelOwner的最大效用 U(Eta) = 0.5754
DONE
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.336258710303773
DataOwner1的分配到的支付 ： 0.1415
DataOwner2的分配到的支付 ： 0.0830
DataOwner3的分配到的支付 ： 0.1195
DataOwner4的分配到的支付 ： 0.1761
DataOwner5的分配到的支付 ： 0.1868
DataOwner6的分配到的支付 ： 0.0604
DataOwner7的分配到的支付 ： 0.1873
DataOwner8的分配到的支付 ： 0.0639
DataOwner9的分配到的支付 ： 0.1298
DataOwner10的分配到的支付 ： 0.1880
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC1', 'DataOwner2': 'CPC2', 'DataOwner3': 'CPC3', 'DataOwner4': 'CPC4', 'DataOwner5': 'CPC5', 'DataOwner6': 'CPC6', 'DataOwner7': 'CPC7', 'DataOwner8': 'CPC8', 'DataOwner9': 'CPC9', 'DataOwner10': 'CPC10'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC2
DataOwner3 把数据交给 CPC3
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC8
DataOwner9 把数据交给 CPC9
DataOwner10 把数据交给 CPC10
DONE
----- literation 1: 模型训练 -----
CPC1调整模型中, 本轮训练的数据量为：220.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 26.82%
Epoch 1/5, Loss: 2.2972
Epoch 2/5, Loss: 2.2912
Epoch 3/5, Loss: 2.2942
Epoch 4/5, Loss: 2.2929
Epoch 5/5, Loss: 2.2885
新模型评估：
Accuracy: 28.87%
Model saved to ../../../data/model/mnist_cnn_model
CPC2调整模型中, 本轮训练的数据量为：203.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 28.87%
Epoch 1/5, Loss: 2.2938
Epoch 2/5, Loss: 2.2862
Epoch 3/5, Loss: 2.2840
Epoch 4/5, Loss: 2.2838
Epoch 5/5, Loss: 2.2756
新模型评估：
Accuracy: 31.28%
Model saved to ../../../data/model/mnist_cnn_model
CPC3调整模型中, 本轮训练的数据量为：756.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 31.28%
Epoch 1/5, Loss: 2.2799
Epoch 2/5, Loss: 2.2749
Epoch 3/5, Loss: 2.2686
Epoch 4/5, Loss: 2.2616
Epoch 5/5, Loss: 2.2536
新模型评估：
Accuracy: 37.18%
Model saved to ../../../data/model/mnist_cnn_model
CPC4调整模型中, 本轮训练的数据量为：1594.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 37.18%
Epoch 1/5, Loss: 2.2503
Epoch 2/5, Loss: 2.2317
Epoch 3/5, Loss: 2.2080
Epoch 4/5, Loss: 2.1781
Epoch 5/5, Loss: 2.1407
新模型评估：
Accuracy: 44.13%
Model saved to ../../../data/model/mnist_cnn_model
CPC5调整模型中, 本轮训练的数据量为：1956.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 44.13%
Epoch 1/5, Loss: 2.1038
Epoch 2/5, Loss: 2.0556
Epoch 3/5, Loss: 2.0001
Epoch 4/5, Loss: 1.9323
Epoch 5/5, Loss: 1.8542
新模型评估：
Accuracy: 55.15%
Model saved to ../../../data/model/mnist_cnn_model
CPC6调整模型中, 本轮训练的数据量为：200.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 55.15%
Epoch 1/5, Loss: 1.8438
Epoch 2/5, Loss: 1.7266
Epoch 3/5, Loss: 1.7231
Epoch 4/5, Loss: 1.7262
Epoch 5/5, Loss: 1.7169
新模型评估：
Accuracy: 56.34%
Model saved to ../../../data/model/mnist_cnn_model
CPC7调整模型中, 本轮训练的数据量为：840.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 56.34%
Epoch 1/5, Loss: 1.7357
Epoch 2/5, Loss: 1.7123
Epoch 3/5, Loss: 1.6747
Epoch 4/5, Loss: 1.6395
Epoch 5/5, Loss: 1.6260
新模型评估：
Accuracy: 60.36%
Model saved to ../../../data/model/mnist_cnn_model
CPC8调整模型中, 本轮训练的数据量为：52.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 60.36%
Epoch 1/5, Loss: 1.5703
Epoch 2/5, Loss: 1.5641
Epoch 3/5, Loss: 1.5580
Epoch 4/5, Loss: 1.5518
Epoch 5/5, Loss: 1.5458
新模型评估：
Accuracy: 60.51%
Model saved to ../../../data/model/mnist_cnn_model
CPC9调整模型中, 本轮训练的数据量为：713.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 60.51%
Epoch 1/5, Loss: 1.6000
Epoch 2/5, Loss: 1.5826
Epoch 3/5, Loss: 1.5531
Epoch 4/5, Loss: 1.5498
Epoch 5/5, Loss: 1.5127
新模型评估：
Accuracy: 65.34%
Model saved to ../../../data/model/mnist_cnn_model
CPC10调整模型中, 本轮训练的数据量为：1685.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 65.34%
Epoch 1/5, Loss: 1.4290
Epoch 2/5, Loss: 1.3617
Epoch 3/5, Loss: 1.3012
Epoch 4/5, Loss: 1.2434
Epoch 5/5, Loss: 1.1781
新模型评估：
Accuracy: 71.18%
Model saved to ../../../data/model/mnist_cnn_model
DONE
========================= literation: 2 =========================
----- literation 2: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3363
DataOwner1的最优x_1 = 0.1265
DataOwner2的最优x_2 = 0.0779
DataOwner3的最优x_3 = 0.1088
DataOwner4的最优x_4 = 0.1529
DataOwner5的最优x_5 = 0.1607
DataOwner6的最优x_6 = 0.0577
DataOwner7的最优x_7 = 0.1611
DataOwner8的最优x_8 = 0.0608
DataOwner9的最优x_9 = 0.1172
DataOwner10的最优x_10 = 0.1616
每个DataOwner应该贡献数据比例 xn_list = [0.12649655758900466, 0.077857812360099, 0.10877858731403715, 0.15286247751951798, 0.16071651715685614, 0.05769794233110358, 0.1610599285227827, 0.06080914759933146, 0.11718023593075123, 0.1615598395336469]
ModelOwner的最大效用 U(Eta) = 0.5754
DONE
----- literation 2: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.336258710303773
DataOwner1的分配到的支付 ： 0.1415
DataOwner2的分配到的支付 ： 0.0830
DataOwner3的分配到的支付 ： 0.1195
DataOwner4的分配到的支付 ： 0.1761
DataOwner5的分配到的支付 ： 0.1868
DataOwner6的分配到的支付 ： 0.0604
DataOwner7的分配到的支付 ： 0.1873
DataOwner8的分配到的支付 ： 0.0639
DataOwner9的分配到的支付 ： 0.1298
DataOwner10的分配到的支付 ： 0.1880
DONE
----- literation 2: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC2
DataOwner3 把数据交给 CPC3
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC8
DataOwner9 把数据交给 CPC9
DataOwner10 把数据交给 CPC10
DONE
----- literation 2: 模型训练 -----
重新调整fn，进而调整xn、Eta
正在评估DataOwner1的数据质量, 本轮评估的样本数据量为：220.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 71.18%
Epoch 1/5, Loss: 1.1288
Epoch 2/5, Loss: 1.0814
Epoch 3/5, Loss: 1.0971
Epoch 4/5, Loss: 1.0759
Epoch 5/5, Loss: 1.0609
新模型评估：
Accuracy: 71.52%
loss差为：
0.067889004945755
单位数据loss差为：
0.0003085863861170682
正在评估DataOwner2的数据质量, 本轮评估的样本数据量为：203.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 66.75%
Epoch 1/5, Loss: 1.2954
Epoch 2/5, Loss: 1.3029
Epoch 3/5, Loss: 1.3381
Epoch 4/5, Loss: 1.3007
Epoch 5/5, Loss: 1.2733
新模型评估：
Accuracy: 67.61%
loss差为：
0.022067010402679443
单位数据loss差为：
0.00010870448474226326
正在评估DataOwner3的数据质量, 本轮评估的样本数据量为：756.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 69.48%
Epoch 1/5, Loss: 1.2581
Epoch 2/5, Loss: 1.2259
Epoch 3/5, Loss: 1.1980
Epoch 4/5, Loss: 1.1706
Epoch 5/5, Loss: 1.1469
新模型评估：
Accuracy: 72.00%
loss差为：
0.11124676465988159
单位数据loss差为：
0.0001471518051056635
正在评估DataOwner4的数据质量, 本轮评估的样本数据量为：1594.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 69.48%
Epoch 1/5, Loss: 1.2490
Epoch 2/5, Loss: 1.1938
Epoch 3/5, Loss: 1.1398
Epoch 4/5, Loss: 1.0878
Epoch 5/5, Loss: 1.0375
新模型评估：
Accuracy: 72.50%
loss差为：
0.21151057243347182
单位数据loss差为：
0.0001326917016521153
正在评估DataOwner5的数据质量, 本轮评估的样本数据量为：1956.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 72.00%
Epoch 1/5, Loss: 1.0400
Epoch 2/5, Loss: 0.9815
Epoch 3/5, Loss: 0.9275
Epoch 4/5, Loss: 0.8808
Epoch 5/5, Loss: 0.8354
新模型评估：
Accuracy: 75.55%
loss差为：
0.20451863542679816
单位数据loss差为：
0.00010455962956380274
正在评估DataOwner6的数据质量, 本轮评估的样本数据量为：200.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 75.03%
Epoch 1/5, Loss: 0.9916
Epoch 2/5, Loss: 0.9052
Epoch 3/5, Loss: 0.8538
Epoch 4/5, Loss: 0.9147
Epoch 5/5, Loss: 0.9569
新模型评估：
Accuracy: 75.75%
loss差为：
0.03463219106197357
单位数据loss差为：
0.00017316095530986786
正在评估DataOwner7的数据质量, 本轮评估的样本数据量为：840.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 75.03%
Epoch 1/5, Loss: 0.8684
Epoch 2/5, Loss: 0.8402
Epoch 3/5, Loss: 0.8238
Epoch 4/5, Loss: 0.8230
Epoch 5/5, Loss: 0.7862
新模型评估：
Accuracy: 76.05%
loss差为：
0.08214651686804642
单位数据loss差为：
9.779347246196002e-05
正在评估DataOwner8的数据质量, 本轮评估的样本数据量为：52.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 76.48%
Epoch 1/5, Loss: 1.0612
Epoch 2/5, Loss: 1.0523
Epoch 3/5, Loss: 1.0436
Epoch 4/5, Loss: 1.0355
Epoch 5/5, Loss: 1.0277
新模型评估：
Accuracy: 76.23%
loss差为：
0.03354454040527344
单位数据loss差为：
0.0006450873154860276
正在评估DataOwner9的数据质量, 本轮评估的样本数据量为：713.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 76.48%
Epoch 1/5, Loss: 0.8851
Epoch 2/5, Loss: 0.8418
Epoch 3/5, Loss: 0.8542
Epoch 4/5, Loss: 0.8131
Epoch 5/5, Loss: 0.8067
新模型评估：
Accuracy: 75.53%
loss差为：
0.07837816576162981
单位数据loss差为：
0.00010992730120845696
正在评估DataOwner10的数据质量, 本轮评估的样本数据量为：1685.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 76.48%
Epoch 1/5, Loss: 0.8653
Epoch 2/5, Loss: 0.8224
Epoch 3/5, Loss: 0.7881
Epoch 4/5, Loss: 0.7615
Epoch 5/5, Loss: 0.7254
新模型评估：
Accuracy: 76.18%
loss差为：
0.13983362471615823
单位数据loss差为：
8.298731437160726e-05
经过服务器调节后的真实数据质量：
数据质量列表avg_f_list: [0.0003085863861170682, 0.00010870448474226326, 0.0001471518051056635, 0.0001326917016521153, 0.00010455962956380274, 0.00017316095530986786, 9.779347246196002e-05, 0.0006450873154860276, 0.00010992730120845696, 8.298731437160726e-05]
归一化后的数据质量列表avg_f_list:[0.9401350420384608, 0.9045751948620653, 0.9114151379837829, 0.9088426235868998, 0.9038378073562402, 0.9160422773099951, 0.9026340790003555, 1.0, 0.9047927391537873, 0.9]
CPC1调整模型中, 本轮训练的数据量为：220.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 76.48%
Epoch 1/5, Loss: 0.7802
Epoch 2/5, Loss: 0.7421
Epoch 3/5, Loss: 0.7607
Epoch 4/5, Loss: 0.7402
Epoch 5/5, Loss: 0.7439
新模型评估：
Accuracy: 75.36%
CPC2调整模型中, 本轮训练的数据量为：203.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 76.48%
Epoch 1/5, Loss: 0.8609
Epoch 2/5, Loss: 0.8407
Epoch 3/5, Loss: 0.8096
Epoch 4/5, Loss: 0.7613
Epoch 5/5, Loss: 0.9122
新模型评估：
Accuracy: 74.70%
CPC3调整模型中, 本轮训练的数据量为：756.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 76.48%
Epoch 1/5, Loss: 0.8584
Epoch 2/5, Loss: 0.8274
Epoch 3/5, Loss: 0.8069
Epoch 4/5, Loss: 0.7877
Epoch 5/5, Loss: 0.7661
新模型评估：
Accuracy: 75.73%
CPC4调整模型中, 本轮训练的数据量为：1594.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 26.82%
Epoch 1/5, Loss: 2.2923
Epoch 2/5, Loss: 2.2848
Epoch 3/5, Loss: 2.2735
Epoch 4/5, Loss: 2.2568
Epoch 5/5, Loss: 2.2324
新模型评估：
Accuracy: 41.16%
Model saved to ../../../data/model/mnist_cnn_model
CPC5调整模型中, 本轮训练的数据量为：1956.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 41.16%
Epoch 1/5, Loss: 2.2136
Epoch 2/5, Loss: 2.1798
Epoch 3/5, Loss: 2.1414
Epoch 4/5, Loss: 2.0894
Epoch 5/5, Loss: 2.0297
新模型评估：
Accuracy: 48.84%
Model saved to ../../../data/model/mnist_cnn_model
CPC6调整模型中, 本轮训练的数据量为：200.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 48.84%
Epoch 1/5, Loss: 1.9802
Epoch 2/5, Loss: 2.0141
Epoch 3/5, Loss: 1.9765
Epoch 4/5, Loss: 1.9809
Epoch 5/5, Loss: 1.9607
新模型评估：
Accuracy: 51.78%
Model saved to ../../../data/model/mnist_cnn_model
CPC7调整模型中, 本轮训练的数据量为：840.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 51.78%
Epoch 1/5, Loss: 1.9473
Epoch 2/5, Loss: 1.9109
Epoch 3/5, Loss: 1.8896
Epoch 4/5, Loss: 1.8604
Epoch 5/5, Loss: 1.8278
新模型评估：
Accuracy: 56.17%
Model saved to ../../../data/model/mnist_cnn_model
CPC8调整模型中, 本轮训练的数据量为：52.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 56.17%
Epoch 1/5, Loss: 1.8958
Epoch 2/5, Loss: 1.8910
Epoch 3/5, Loss: 1.8862
Epoch 4/5, Loss: 1.8814
Epoch 5/5, Loss: 1.8767
新模型评估：
Accuracy: 57.70%
Model saved to ../../../data/model/mnist_cnn_model
CPC9调整模型中, 本轮训练的数据量为：713.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 57.70%
Epoch 1/5, Loss: 1.7988
Epoch 2/5, Loss: 1.7739
Epoch 3/5, Loss: 1.7274
Epoch 4/5, Loss: 1.6807
Epoch 5/5, Loss: 1.7009
新模型评估：
Accuracy: 60.47%
Model saved to ../../../data/model/mnist_cnn_model
CPC10调整模型中, 本轮训练的数据量为：1685.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 60.47%
Epoch 1/5, Loss: 1.6540
Epoch 2/5, Loss: 1.5851
Epoch 3/5, Loss: 1.5227
Epoch 4/5, Loss: 1.4579
Epoch 5/5, Loss: 1.3945
新模型评估：
Accuracy: 66.41%
Model saved to ../../../data/model/mnist_cnn_model
DONE
========================= literation: 3 =========================
----- literation 3: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.2901
DataOwner1的最优x_1 = 0.1371
DataOwner2的最优x_2 = 0.1017
DataOwner3的最优x_3 = 0.1090
DataOwner4的最优x_4 = 0.1063
DataOwner5的最优x_5 = 0.1009
DataOwner6的最优x_6 = 0.1137
DataOwner7的最优x_7 = 0.0996
DataOwner8的最优x_8 = 0.1850
DataOwner9的最优x_9 = 0.1019
DataOwner10的最优x_10 = 0.0967
每个DataOwner应该贡献数据比例 xn_list = [0.13705976587204313, 0.10170727855035702, 0.10896669811019864, 0.10626344575678817, 0.10091079626258781, 0.11374853736794928, 0.09960471266608664, 0.18497603084924624, 0.10194173545609242, 0.09672098315226577]
ModelOwner的最大效用 U(Eta) = 0.5243
xn开始变化：
DONE
----- literation 3: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.290119037327765
DataOwner1的分配到的支付 ： 0.5800
DataOwner2的分配到的支付 ： 0.0563
DataOwner3的分配到的支付 ： 0.0607
DataOwner4的分配到的支付 ： 0.0849
DataOwner5的分配到的支付 ： 0.0888
DataOwner6的分配到的支付 ： 0.0637
DataOwner7的分配到的支付 ： 0.0889
DataOwner8的分配到的支付 ： 0.1131
DataOwner9的分配到的支付 ： 0.0648
DataOwner10的分配到的支付 ： 0.0889
DONE
----- literation 3: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC2
DataOwner3 把数据交给 CPC3
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC8
DataOwner9 把数据交给 CPC9
DataOwner10 把数据交给 CPC10
DONE
----- literation 3: 模型训练 -----
CPC1调整模型中, 本轮训练的数据量为：1745.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 66.41%
Epoch 1/5, Loss: 1.3111
Epoch 2/5, Loss: 1.2520
Epoch 3/5, Loss: 1.1924
Epoch 4/5, Loss: 1.1413
Epoch 5/5, Loss: 1.0688
新模型评估：
Accuracy: 71.92%
Model saved to ../../../data/model/mnist_cnn_model
CPC2调整模型中, 本轮训练的数据量为：265.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 71.92%
Epoch 1/5, Loss: 1.1286
Epoch 2/5, Loss: 1.0371
Epoch 3/5, Loss: 1.0645
Epoch 4/5, Loss: 1.0201
Epoch 5/5, Loss: 1.0380
新模型评估：
Accuracy: 70.77%
CPC3调整模型中, 本轮训练的数据量为：757.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 71.92%
Epoch 1/5, Loss: 1.0207
Epoch 2/5, Loss: 0.9927
Epoch 3/5, Loss: 0.9644
Epoch 4/5, Loss: 0.9426
Epoch 5/5, Loss: 0.9170
新模型评估：
Accuracy: 74.09%
Model saved to ../../../data/model/mnist_cnn_model
CPC4调整模型中, 本轮训练的数据量为：1594.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 74.09%
Epoch 1/5, Loss: 0.9186
Epoch 2/5, Loss: 0.8726
Epoch 3/5, Loss: 0.8338
Epoch 4/5, Loss: 0.7968
Epoch 5/5, Loss: 0.7632
新模型评估：
Accuracy: 76.71%
Model saved to ../../../data/model/mnist_cnn_model
CPC5调整模型中, 本轮训练的数据量为：1956.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 76.71%
Epoch 1/5, Loss: 0.7505
Epoch 2/5, Loss: 0.7109
Epoch 3/5, Loss: 0.6790
Epoch 4/5, Loss: 0.6490
Epoch 5/5, Loss: 0.6239
新模型评估：
Accuracy: 78.30%
Model saved to ../../../data/model/mnist_cnn_model
CPC6调整模型中, 本轮训练的数据量为：395.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.30%
Epoch 1/5, Loss: 0.6495
Epoch 2/5, Loss: 0.5869
Epoch 3/5, Loss: 0.6227
Epoch 4/5, Loss: 0.5882
Epoch 5/5, Loss: 0.6021
新模型评估：
Accuracy: 75.62%
CPC7调整模型中, 本轮训练的数据量为：840.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.10%
Epoch 1/5, Loss: 0.7089
Epoch 2/5, Loss: 0.6868
Epoch 3/5, Loss: 0.7119
Epoch 4/5, Loss: 0.6570
Epoch 5/5, Loss: 0.6426
新模型评估：
Accuracy: 76.89%
CPC8调整模型中, 本轮训练的数据量为：160.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.10%
Epoch 1/5, Loss: 0.8095
Epoch 2/5, Loss: 0.8168
Epoch 3/5, Loss: 0.7830
Epoch 4/5, Loss: 0.7715
Epoch 5/5, Loss: 0.7396
新模型评估：
Accuracy: 77.62%
CPC9调整模型中, 本轮训练的数据量为：713.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.10%
Epoch 1/5, Loss: 0.7301
Epoch 2/5, Loss: 0.6864
Epoch 3/5, Loss: 0.6919
Epoch 4/5, Loss: 0.7069
Epoch 5/5, Loss: 0.6617
新模型评估：
Accuracy: 77.03%
CPC10调整模型中, 本轮训练的数据量为：1685.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.10%
Epoch 1/5, Loss: 0.7096
Epoch 2/5, Loss: 0.6762
Epoch 3/5, Loss: 0.6475
Epoch 4/5, Loss: 0.6260
Epoch 5/5, Loss: 0.5967
新模型评估：
Accuracy: 76.48%
DONE
最终的列表：
[0.00035869405027408145, 0.0003854218104024625, 0.00041050344227540163, 0.000433942901519482, 0.0004557441310976528, 0.0004759110613598884, 0.0004944476100935714, 0.0005113576825736439, 0.0005266451716125185, 0.0005403139576097594, 0.0005523679086015081, 0.0005628108803096797, 0.0005716467161909732, 0.000578879247485533, 0.0005845122932654906, 0.0005885496604832348, 0.0005909951440194183, 0.000591852526730826, 0.0005911255794979059, 0.000588818061272145, 0.0005849337191232012, 0.0005794762882858281, 0.0005724494922065212, 0.0005638570425900644, 0.000553702639445687, 0.0005419899711331458, 0.0005287227144085277, 0.000513904534469839, 0.0004975390850023784, 0.0004796300082239441, 0.0004601809349297631, 0.0004391954845371984, 0.0004166772651303316, 0.00039262987350427436, 0.00036705689520930007, 0.00033996190459472525, 0.00031134846485261075, 0.0002812201280613033, 0.00024958043522868567, 0.00021643291633526018, 0.00018178109037717688, 0.00014562846540872032, 0.00010797853858494216, 6.883479620391209e-05, 2.820071374884381e-05, -1.3920244070056431e-05, -5.752462327378999e-05, -0.00010260898057314061, -0.00014916988332732556, -0.0001972039095028133, -0.00024670764763248115, -0.0002976776967747313, -0.00035011066647306494, -0.00040400317671562824, -0.0004593518578950434, -0.000516153350768378, -0.0005744043064174059, -0.0006341013862088407, -0.0006952412617550058, -0.0007578206148745326, -0.0008218361375531835, -0.0008872845319051054, -0.0009541625101338741, -0.0010224667944941923, -0.0010921941172532673, -0.0011633412206528276, -0.0012359048568709174, -0.0013098817879841768, -0.0013852687859300106, -0.0014620626324692299, -0.001540260119148651, -0.001619858047263903, -0.0017008532278224575, -0.0017832424815068387, -0.0018670226386378053, -0.00195219053913806, -0.0020387430324957795, -0.002126676977728517, -0.0022159892433471773, -0.0023066767073201405, -0.002398736257037584, -0.002492164789276108, -0.002586959210163206, -0.00268311643514213, -0.002780633388936929, -0.0028795070055173955, -0.002979734228064551, -0.003081312008935952, -0.0031842373096312993, -0.0032885071007582034, -0.0033941183619981136, -0.003501068082072248, -0.0036093532587079674, -0.003718970898604998, -0.003829918017402026, -0.003942191639643336, -0.004055788798745477, -0.004170706536964441, -0.004286941905362676, -0.004404491963776264, -0.0045233537807824326, -0.004643524433666987, -0.004765001008392028, -0.00488778059956381, -0.005011860310400754, -0.005137237252701418, -0.00526390854681294, -0.005391871321599281, -0.005521122714409715, -0.005651659871047779, -0.005783479945739772, -0.005916580101103977, -0.00605095750811957, -0.006186609346095759, -0.006323532802641502, -0.006461725073634655, -0.006601183363191737, -0.006741904883637795, -0.00688388685547614, -0.007027126507358317, -0.0071716210760545684, -0.007317367806423863, -0.0074643639513844035, -0.007612606771884217, -0.007762093536871795, -0.007912821523266833, -0.008064788015931301, -0.008217990307640505, -0.008372425699054109, -0.008528091498687745, -0.008684985022884006, -0.00884310359578458, -0.009002444549301464, -0.009163005223089077, -0.00932478296451597, -0.00948777512863716, -0.009651979078165979, -0.009817392183446516, -0.009984011822426309, -0.010151835380628443, -0.01032086025112447, -0.01049108383450717, -0.01066250353886325, -0.01083511677974669, -0.011008920980151576, -0.011183913570485537, -0.01136009198854307, -0.011537453679478726, -0.011715996095781223, -0.011895716697246761, -0.012076612950952997, -0.012258682331232867, -0.012441922319648774, -0.012626330404966496, -0.012811904083129794, -0.012998640857234539, -0.013186538237503154, -0.013375593741259434, -0.013565804892903127, -0.013757169223884591, -0.013949684272679952, -0.01414334758476607, -0.014338156712595557, -0.014534109215572188, -0.014731202660025922, -0.014929434619188775, -0.015128802673170072, -0.015329304408932237, -0.015530937420266427, -0.015733699307768606, -0.01593758767881512, -0.016142600147539293, -0.016348734334807158, -0.0165559878681939, -0.01676435838196022, -0.016973843517028797, -0.017184440920960747, -0.017396148247932453, -0.017608963158712326, -0.017822883320637417, -0.018037906407590842, -0.01825403009997814, -0.01847125208470518, -0.018689570055155008, -0.01890898171116509, -0.019129484759005028, -0.019351076911354043, -0.019573755887278332, -0.01979751941220914, -0.02002236521792039, -0.02024829104250664, -0.02047529463036102, -0.020703373732153474, -0.02093252610480864, -0.021162749511484447, -0.021394041721550405, -0.021626400510565885, -0.021859823660258904, -0.022094308958504505, -0.02232985419930375, -0.022566457182762234, -0.022804115715069295, -0.023042827608476757, -0.023282590681278137, -0.02352340275778758, -0.02376526166831952, -0.024008165249167712, -0.024252111342584565, -0.024497097796760903, -0.024743122465805484, -0.02499018320972446, -0.025238277894401417, -0.02548740439157693, -0.02573756057882884, -0.025988744339552006, -0.026240953562938235, -0.026494186143956877, -0.026748439983334787, -0.02700371298753662, -0.02726000306874543, -0.027517308144843128, -0.027775626139390774, -0.028034954981609517, -0.028295292606361433, -0.028556636954129827, -0.028818985971000688, -0.029082337608643344, -0.029346689824291394, -0.02961204058072414, -0.029878387846247606, -0.03014572959467582, -0.030414063805312225, -0.03068338846293084, -0.0309537015577582, -0.031225001085454673, -0.03149728504709595, -0.03177055144915511, -0.032044798303484334, -0.03232002362729641, -0.03259622544314714, -0.03287340177891726, -0.03315155066779424, -0.03343067014825479, -0.033710758264047014, -0.0339918130641724, -0.034273832602868703, -0.03455681493959206, -0.034840758138999534, -0.035125660270931836, -0.035411519410395775, -0.03569833363754718, -0.03598610103767372, -0.03627481970117749, -0.03656448772355825, -0.03685510320539637, -0.03714666425233559, -0.037439168975066606, -0.03773261548931006, -0.03802700191579966, -0.038322326380266, -0.03861858701341936, -0.03891578195093348, -0.039213909333429164, -0.0395129673064579, -0.039812954020485236, -0.04011386763087485, -0.040415706297872, -0.04071846818658781, -0.04102215146698293, -0.041326754313851355, -0.041632274906805045, -0.04193871143025743, -0.04224606207340781, -0.0425543250302258, -0.0428634984994353, -0.043173580684499197, -0.043484569793603406, -0.04379646403964188, -0.04410926164020043, -0.04442296081754202, -0.04473755979859115, -0.04505305681491861, -0.045369450102726194, -0.04568673790283162, -0.04600491846065349, -0.04632399002619619, -0.04664395085403503, -0.04696479920330099, -0.047286533337666525, -0.04760915152532991, -0.04793265203900121, -0.04825703315588731, -0.0485822931576772, -0.04890843033052772, -0.049235442965048726, -0.049563329356288766, -0.049892087803720564, -0.050221716611227096, -0.05055221408708682, -0.05088357854395964, -0.05121580829887257, -0.05154890167320608, -0.05188285699267953, -0.05221767258733717, -0.05255334679153478, -0.05288987794392502, -0.05322726438744391, -0.05356550446929714, -0.05390459654094626, -0.05424453895809467, -0.05458533008067468, -0.05492696827283322, -0.05526945190291849, -0.05561277934346687, -0.05595694897118908, -0.05630195916695696, -0.05664780831579003, -0.05699449480684243, -0.05734201703338937, -0.05769037339281452, -0.058039562286596125, -0.05838958212029455, -0.058740431303539176, -0.059092108250014874, -0.05944461137744994, -0.05979793910760256, -0.060152089866248315, -0.06050706208316736, -0.060862854192131444, -0.06121946463089162, -0.06157689184116533, -0.061935134268624015, -0.062294190362880475, -0.06265405857747647, -0.06301473736986996, -0.06337622520142328, -0.06373852053739026, -0.06410162184690443, -0.06446552760296614, -0.06483023628243112, -0.06519574636599756, -0.06556205633819445, -0.06592916468736959, -0.06629706990567719, -0.06666577048906625, -0.06703526493726847, -0.06740555175378626, -0.06777662944588103, -0.06814849652456162, -0.06852115150457211, -0.06889459290438038, -0.06926881924616629, -0.06964382905581029, -0.07001962086288172, -0.07039619320062707, -0.07077354460595897, -0.0711516736194443, -0.07153057878529301, -0.07191025865134659, -0.07229071176906693, -0.07267193669352517, -0.07305393198339011, -0.07343669620091703, -0.07382022791193693, -0.07420452568584518, -0.07458958809559035, -0.07497541371766347, -0.07536200113208669, -0.07574934892240248, -0.07613745567566299, -0.07652631998241888, -0.07691594043670852, -0.07730631563604723, -0.07769744418141694, -0.07808932467725449, -0.07848195573144223, -0.07887533595529633, -0.079269463963557, -0.07966433837437731, -0.08005995780931313, -0.08045632089331228, -0.08085342625470449, -0.08125127252519093, -0.08164985833983351, -0.08204918233704506, -0.0824492431585786, -0.08285003944951741, -0.08325156985826482, -0.08365383303653379, -0.08405682763933714, -0.08446055232497729, -0.08486500575503597, -0.08527018659436453, -0.08567609351107408, -0.0860827251765251, -0.08649008026531801, -0.08689815745528262, -0.08730695542746914, -0.08771647286613798, -0.08812670845874981, -0.08853766089595616, -0.08894932887158946, -0.08936171108265362, -0.08977480622931427, -0.09018861301488923, -0.09060313014583898, -0.09101835633175687, -0.09143429028536043, -0.09185093072248063, -0.09226827636205381, -0.09268632592611148, -0.09310507813977109, -0.0935245317312271, -0.09394468543174112, -0.09436553797563324, -0.09478708810027237, -0.0952093345460675, -0.09563227605645802, -0.09605591137790531, -0.09648023925988264, -0.0969052584548673, -0.09733096771833089, -0.09775736580873029, -0.0981844514874991, -0.09861222351903853, -0.09904068067070837, -0.09946982171281837, -0.09989964541861945, -0.10033015056429467, -0.10076133592895059, -0.10119320029460865, -0.10162574244619638, -0.10205896117153856, -0.10249285526134905, -0.1029274235092218, -0.10336266471162248, -0.10379857766787948, -0.10423516118017617, -0.104672414053542, -0.10511033509584383, -0.10554892311777808, -0.10598817693286178, -0.10642809535742465, -0.10686867721060034, -0.10730992131431843, -0.10775182649329629, -0.10819439157503041, -0.10863761538978861, -0.10908149677060158, -0.10952603455325483, -0.10997122757628047, -0.11041707468094941, -0.11086357471126296, -0.11131072651394491, -0.11175852893843347, -0.11220698083687347, -0.11265608106410824, -0.11310582847767159, -0.11355622193778031, -0.11400726030732589, -0.11445894245186672, -0.11491126723962058, -0.11536423354145653, -0.11581784023088737, -0.11627208618406176, -0.11672697027975643, -0.11718249139936893, -0.11763864842690935, -0.11809544024899338, -0.11855286575483415, -0.1190109238362349, -0.1194696133875815, -0.1199289333058346, -0.12038888249052265, -0.12084945984373408, -0.12131066427010984, -0.12177249467683637, -0.12223494997363749, -0.12269802907276778, -0.12316173088900484, -0.12362605433964219, -0.12409099834448178, -0.12455656182582686, -0.12502274370847455, -0.12548954291970887, -0.12595695838929372, -0.1264249890494652, -0.12689363383492475, -0.12736289168283205, -0.127832761532798, -0.1283032423268775, -0.12877433300956248, -0.129246032527775, -0.12971833983086012, -0.13019125387057873, -0.13066477360110113, -0.1311388979789998, -0.1316136259632424, -0.1320889565151852, -0.13256488859856602, -0.13304142117949724, -0.13351855322645956, -0.13399628371029465, -0.13447461160419882, -0.13495353588371595, -0.13543305552673096, -0.13591316951346305, -0.13639387682645937, -0.13687517645058778, -0.13735706737303072, -0.13783954858327863, -0.1383226190731227, -0.13880627783664934, -0.13929052387023283, -0.1397753561725294, -0.14026077374447038, -0.14074677558925563, -0.14123336071234766, -0.14172052812146463, -0.1422082768265744, -0.1426966058398878, -0.1431855141758525, -0.1436750008511467, -0.1441650648846725, -0.14465570529755034, -0.14514692111311167, -0.14563871135689377, -0.14613107505663292, -0.14662401124225805, -0.14711751894588526, -0.1476115972018111, -0.14810624504650666, -0.1486014615186112, -0.14909724565892618, -0.14959359651040943, -0.150090513118169, -0.15058799452945665, -0.15108603979366264, -0.15158464796230897, -0.15208381808904364, -0.15258354922963535, -0.15308384044196643, -0.153584690786028, -0.1540860993239132, -0.1545880651198117, -0.15509058724000413, -0.1555936647528559, -0.15609729672881112, -0.1566014822403876, -0.1571062203621703, -0.157611510170806, -0.1581173507449971, -0.15862374116549677, -0.15913068051510249, -0.15963816787865037, -0.16014620234301014, -0.16065478299707875, -0.161163908931775, -0.16167357924003445, -0.16218379301680297, -0.162694549359032, -0.16320584736567206, -0.16371768613766813, -0.16423006477795365, -0.16474298239144536, -0.16525643808503715, -0.16577043096759553, -0.16628496014995325, -0.16680002474490457, -0.16731562386719978, -0.16783175663353933, -0.16834842216256918, -0.16886561957487461, -0.1693833479929755, -0.16990160654132108, -0.17042039434628398, -0.1709397105361557, -0.17145955424114112, -0.1719799245933526, -0.17250082072680573, -0.1730222417774135, -0.17354418688298145, -0.1740666551832024, -0.17458964581965092, -0.17511315793577859, -0.17563719067690892, -0.17616174319023187, -0.17668681462479924, -0.17721240413151917, -0.17773851086315123, -0.17826513397430138, -0.17879227262141695, -0.17931992596278185, -0.17984809315851097, -0.18037677337054597, -0.1809059657626496, -0.1814356695004014, -0.1819658837511921, -0.1824966076842195, -0.18302784047048282, -0.18355958128277822, -0.18409182929569373, -0.18462458368560475, -0.18515784363066906, -0.1856916083108215, -0.18622587690777004, -0.18676064860499025, -0.187295922587721, -0.1878316980429594, -0.18836797415945628, -0.1889047501277114, -0.18944202513996877, -0.18997979839021167, -0.1905180690741583, -0.19105683638925708, -0.19159609953468182, -0.1921358577113274, -0.1926761101218049, -0.1932168559704368, -0.1937580944632527, -0.19429982480798497, -0.19484204621406376, -0.1953847578926124, -0.19592795905644328, -0.19647164892005292, -0.19701582669961792, -0.19756049161299, -0.19810564287969168, -0.1986512797209123, -0.19919740135950265, -0.1997440070199712, -0.20029109592847966, -0.2008386673128385, -0.20138672040250216, -0.20193525442856547, -0.20248426862375835, -0.20303376222244235, -0.20358373446060574, -0.20413418457585936, -0.2046851118074326, -0.2052365153961685, -0.20578839458451975, -0.20634074861654478, -0.20689357673790332, -0.20744687819585145, -0.20800065223923875, -0.20855489811850259, -0.20910961508566528, -0.20966480239432872, -0.2102204592996712, -0.21077658505844288, -0.21133317892896125, -0.2118902401711073, -0.21244776804632182, -0.21300576181760073, -0.21356422074949116, -0.21412314410808736, -0.21468253116102654, -0.2152423811774854, -0.21580269342817499, -0.21636346718533778, -0.21692470172274314, -0.21748639631568312, -0.21804855024096886, -0.2186111627769266, -0.2191742332033934, -0.21973776080171348, -0.22030174485473414, -0.22086618464680186, -0.22143107946375828, -0.2219964285929366, -0.22256223132315744, -0.2231284869447251, -0.2236951947494233, -0.22426235403051203, -0.22482996408272293, -0.2253980242022563, -0.22596653368677627, -0.22653549183540794, -0.22710489794873334, -0.227674751328787, -0.22824505127905298, -0.2288157971044607, -0.22938698811138147, -0.22995862360762448, -0.23053070290243322, -0.23110322530648153, -0.2316761901318704, -0.23224959669212375, -0.2328234443021851, -0.23339773227841382, -0.23397245993858135, -0.2345476266018675, -0.23512323158885717, -0.23569927422153658, -0.23627575382328925, -0.23685266971889324, -0.23743002123451667, -0.2380078076977145, -0.23858602843742538, -0.23916468278396752, -0.23974377006903524, -0.24032328962569577, -0.2409032407883851, -0.24148362289290531, -0.24206443527642058, -0.24264567727745345, -0.2432273482358817, -0.24380944749293515, -0.24439197439119142, -0.2449749282745733, -0.24555830848834492, -0.246142114379108, -0.24672634529479903, -0.24731100058468564, -0.24789607959936322, -0.2484815816907513, -0.24906750621209067, -0.2496538525179393, -0.25024061996416996, -0.25082780790796577, -0.2514154157078178, -0.2520034427235212, -0.2525918883161721, -0.2531807518481642, -0.25377003268318554, -0.25435973018621555, -0.2549498437235208, -0.2555403726626531, -0.2561313163724449, -0.2567226742230071, -0.2573144455857251, -0.25790662983325613, -0.25849922633952566, -0.25909223447972435, -0.2596856536303045, -0.2602794831689776, -0.2608737224747108, -0.26146837092772324, -0.2620634279094836, -0.2626588928027068, -0.2632547649913506, -0.26385104386061237, -0.2644477287969267, -0.26504481918796174, -0.2656423144226159, -0.26624021389101526, -0.2668385169845103, -0.2674372230956726, -0.26803633161829216, -0.26863584194737405, -0.26923575347913575, -0.2698360656110035, -0.2704367777416096, -0.27103788927078987, -0.2716393995995799, -0.27224130813021213, -0.27284361426611353, -0.27344631741190184, -0.274049416973383, -0.2746529123575481, -0.27525680297257044, -0.2758610882278028, -0.276465767533774, -0.2770708403021862, -0.27767630594591236, -0.2782821638789929, -0.2788884135166327, -0.2794950542751988, -0.2801020855722166, -0.28070950682636814, -0.28131731745748817, -0.28192551688656187, -0.28253410453572203, -0.2831430798282457, -0.2837524421885519, -0.28436219104219873, -0.2849723258158803, -0.2855828459374238, -0.2861937508357876, -0.2868050399410569, -0.2874167126844427, -0.28802876849827763, -0.288641206816014, -0.2892540270722207, -0.28986722870258064, -0.29048081114388746, -0.2910947738340438, -0.29170911621205764, -0.29232383771804027, -0.2929389377932028, -0.2935544158798543, -0.2941702714213985, -0.2947865038623314, -0.2954031126482386, -0.2960200972257926, -0.2966374570427498, -0.29725519154794844, -0.2978733001913052, -0.2984917824238137, -0.29911063769754065, -0.29972986546562386, -0.3003494651822697, -0.3009694363027501, -0.30158977828340006, -0.30221049058161553, -0.30283157265585026, -0.30345302396561347, -0.3040748439714671, -0.3046970321350233, -0.3053195879189423, -0.30594251078692913, -0.3065658002037318, -0.3071894556351382, -0.3078134765479741, -0.30843786241009974, -0.30906261269040836, -0.3096877268588234, -0.3103132043862953, -0.3109390447448003, -0.3115652474073366, -0.31219181184792266, -0.31281873754159484, -0.31344602396440435, -0.3140736705934155, -0.3147016769067027, -0.31533004238334783, -0.3159587665034389, -0.31658784874806634, -0.3172172885993215, -0.31784708554029395, -0.3184772390550683, -0.3191077486287236, -0.3197386137473289, -0.3203698338979425, -0.3210014085686088, -0.32163333724835586, -0.322265619427193, -0.32289825459610944, -0.3235312422470704, -0.3241645818730159, -0.3247982729678581, -0.32543231502647896, -0.32606670754472733, -0.3267014500194181, -0.3273365419483283, -0.3279719828301958, -0.3286077721647167, -0.3292439094525428, -0.3298803941952799, -0.3305172258954848, -0.33115440405666396, -0.3317919281832701, -0.3324297977807009, -0.33306801235529615, -0.333706571414336, -0.33434547446603846, -0.334984721019557, -0.33562431058497855, -0.3362642426733212, -0.3369045167965323, -0.33754513246748563, -0.3381860891999796, -0.33882738650873534, -0.3394690239093937, -0.3401110009185141, -0.34075331705357115, -0.3413959718329538, -0.34203896477596196, -0.34268229540280526, -0.3433259632346005, -0.3439699677933695, -0.3446143086020369, -0.3452589851844282, -0.34590399706526787, -0.3465493437701763, -0.3471950248256692, -0.3478410397591539, -0.3484873880989281, -0.3491340693741779, -0.34978108311497513, -0.350428428852276, -0.3510761061179182, -0.35172411444461926, -0.3523724533659749, -0.3530211224164562, -0.35367012113140783, -0.35431944904704615, -0.35496910570045714, -0.3556190906295942, -0.35626940337327595, -0.3569200434711851, -0.3575710104638655, -0.35822230389272014, -0.35887392330000945, -0.3595258682288499, -0.36017813822321065, -0.36083073282791256, -0.36148365158862616, -0.362136894051869, -0.36279045976500424, -0.3634443482762387, -0.36409855913462075, -0.36475309189003824, -0.3654079460932167, -0.36606312129571705, -0.3667186170499346, -0.3673744329090959, -0.36803056842725745, -0.36868702315930413, -0.3693437966609463, -0.3700008884887187, -0.3706582981999783, -0.3713160253529023, -0.3719740695064865, -0.3726324302205427, -0.3732911070556978, -0.3739500995733913, -0.37460940733587345, -0.3752690299062036, -0.3759289668482483, -0.37658921772667897, -0.3772497821069709, -0.3779106595554005, -0.37857184963904433, -0.37923335192577645, -0.379895165984267, -0.3805572913839803, -0.3812197276951731, -0.3818824744888929, -0.3825455313369752, -0.38320889781204315, -0.3838725734875048, -0.38453655793755137, -0.3852008507371554, -0.3858654514620694, -0.3865303596888241, -0.3871955749947258, -0.3878610969578554, -0.3885269251570663, -0.389193059171983, -0.3898594985829986, -0.39052624297127403, -0.3911932919187352, -0.3918606450080724, -0.39252830182273757, -0.393196261946943, -0.3938645249656597, -0.3945330904646155, -0.39520195803029345, -0.39587112724992957, -0.3965405977115123, -0.3972103690037794, -0.3978804407162171, -0.39855081243905854, -0.3992214837632815, -0.3998924542806065, -0.40056372358349634, -0.40123529126515334, -0.4019071569195177, -0.4025793201412664, -0.4032517805258111, -0.4039245376692966, -0.40459759116859906, -0.40527094062132485, -0.40594458562580793, -0.40661852578110946, -0.4072927606870147, -0.40796728994403275, -0.40864211315339427, -0.40931722991704944, -0.4099926398376674, -0.4106683425186337, -0.41134433756404887, -0.41202062457872723, -0.4126972031681949, -0.41337407293868855, -0.414051233497153, -0.41472868445124056, -0.41540642540930905, -0.4160844559804202, -0.41676277577433785, -0.41744138440152734, -0.41812028147315206, -0.4187994666010739, -0.41947893939785097, -0.420158699476735, -0.4208387464516715, -0.4215190799372972, -0.4221996995489382, -0.4228806049026096, -0.42356179561501284, -0.4242432713035348, -0.42492503158624584, -0.4256070760818992, -0.42628940440992746, -0.426972016190444, -0.42765491104423836, -0.4283380885927771, -0.42902154845820084]
**** log-parameter_analysis 运行时间： 2025-01-17 23:21:39 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.07551143441603081
DataOwner2: noise random: 0.04070712554427178
DataOwner3: noise random: 0.02823653622747249
DataOwner4: noise random: 0.019764649839168216
DataOwner5: noise random: 0.08515412716727835
DataOwner6: noise random: 0.020087710822993522
DataOwner7: noise random: 0.009880279141453775
DataOwner8: noise random: 0.049396118774037226
DataOwner9: noise random: 0.07307698522038925
DataOwner10: noise random: 0.03084922818151108
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9977008621642304, 0.9993295748813182, 0.9996777088468973, 0.999842078254062, 0.9970687507188313, 0.9998365332926609, 0.9999604666760351, 0.999011726033934, 0.9978424636474736, 0.9996158289167915]
归一化后的数据质量列表avg_f_list: [0.9218593891915415, 0.9781827882110898, 0.9902217979454941, 0.9959059456832833, 0.9, 0.995714192361615, 1.0, 0.9671910845967546, 0.9267561869869998, 0.9880818944756629]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3487
DataOwner1的最优x_1 = 0.0734
DataOwner2的最优x_2 = 0.1342
DataOwner3的最优x_3 = 0.1453
DataOwner4的最优x_4 = 0.1504
DataOwner5的最优x_5 = 0.0454
DataOwner6的最优x_6 = 0.1502
DataOwner7的最优x_7 = 0.1539
DataOwner8的最优x_8 = 0.1235
DataOwner9的最优x_9 = 0.0793
DataOwner10的最优x_10 = 0.1434
每个DataOwner应该贡献数据比例 xn_list = [0.0734177478124918, 0.1341659241550416, 0.14530712131282325, 0.15036705600219624, 0.04541205870108592, 0.15019839819440925, 0.15393457224031828, 0.12346742500970169, 0.07932317853509877, 0.14336943550656742]
ModelOwner的最大效用 U(Eta) = 0.5898
DONE
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.348729244257586
DataOwner1的分配到的支付 ： 0.0779
DataOwner2的分配到的支付 ： 0.1511
DataOwner3的分配到的支付 ： 0.1657
DataOwner4的分配到的支付 ： 0.1724
DataOwner5的分配到的支付 ： 0.0471
DataOwner6的分配到的支付 ： 0.1722
DataOwner7的分配到的支付 ： 0.1772
DataOwner8的分配到的支付 ： 0.1375
DataOwner9的分配到的支付 ： 0.0846
DataOwner10的分配到的支付 ： 0.1631
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC1', 'DataOwner2': 'CPC2', 'DataOwner3': 'CPC3', 'DataOwner4': 'CPC10', 'DataOwner5': 'CPC4', 'DataOwner6': 'CPC5', 'DataOwner7': 'CPC6', 'DataOwner8': 'CPC7', 'DataOwner9': 'CPC8', 'DataOwner10': 'CPC9'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC2
DataOwner3 把数据交给 CPC3
DataOwner4 把数据交给 CPC10
DataOwner5 把数据交给 CPC4
DataOwner6 把数据交给 CPC5
DataOwner7 把数据交给 CPC6
DataOwner8 把数据交给 CPC7
DataOwner9 把数据交给 CPC8
DataOwner10 把数据交给 CPC9
DONE
----- literation 1: 模型训练 -----
CPC1调整模型中, 本轮训练的数据量为：440.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 26.82%
Epoch 1/5, Loss: 2.2953
Epoch 2/5, Loss: 2.2939
Epoch 3/5, Loss: 2.2922
Epoch 4/5, Loss: 2.2906
Epoch 5/5, Loss: 2.2884
新模型评估：
Accuracy: 34.56%
Model saved to ../../../data/model/mnist_cnn_model
CPC2调整模型中, 本轮训练的数据量为：1006.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 34.56%
Epoch 1/5, Loss: 2.2897
Epoch 2/5, Loss: 2.2836
Epoch 3/5, Loss: 2.2767
Epoch 4/5, Loss: 2.2680
Epoch 5/5, Loss: 2.2571
新模型评估：
Accuracy: 45.51%
Model saved to ../../../data/model/mnist_cnn_model
CPC3调整模型中, 本轮训练的数据量为：1307.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 45.51%
Epoch 1/5, Loss: 2.2383
Epoch 2/5, Loss: 2.2214
Epoch 3/5, Loss: 2.1992
Epoch 4/5, Loss: 2.1735
Epoch 5/5, Loss: 2.1435
新模型评估：
Accuracy: 50.19%
Model saved to ../../../data/model/mnist_cnn_model
CPC10调整模型中, 本轮训练的数据量为：1127.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 50.19%
Epoch 1/5, Loss: 2.1211
Epoch 2/5, Loss: 2.0934
Epoch 3/5, Loss: 2.0661
Epoch 4/5, Loss: 2.0373
Epoch 5/5, Loss: 2.0011
新模型评估：
Accuracy: 53.05%
Model saved to ../../../data/model/mnist_cnn_model
CPC4调整模型中, 本轮训练的数据量为：136.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 53.05%
Epoch 1/5, Loss: 2.0714
Epoch 2/5, Loss: 2.0280
Epoch 3/5, Loss: 2.0310
Epoch 4/5, Loss: 2.0262
Epoch 5/5, Loss: 1.9987
新模型评估：
Accuracy: 54.05%
Model saved to ../../../data/model/mnist_cnn_model
CPC5调整模型中, 本轮训练的数据量为：1126.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 54.05%
Epoch 1/5, Loss: 1.9483
Epoch 2/5, Loss: 1.9105
Epoch 3/5, Loss: 1.8734
Epoch 4/5, Loss: 1.8332
Epoch 5/5, Loss: 1.7916
新模型评估：
Accuracy: 57.10%
Model saved to ../../../data/model/mnist_cnn_model
CPC6调整模型中, 本轮训练的数据量为：1385.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 57.10%
Epoch 1/5, Loss: 1.7618
Epoch 2/5, Loss: 1.7137
Epoch 3/5, Loss: 1.6660
Epoch 4/5, Loss: 1.6110
Epoch 5/5, Loss: 1.5606
新模型评估：
Accuracy: 64.97%
Model saved to ../../../data/model/mnist_cnn_model
CPC7调整模型中, 本轮训练的数据量为：370.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 64.97%
Epoch 1/5, Loss: 1.4975
Epoch 2/5, Loss: 1.4781
Epoch 3/5, Loss: 1.4657
Epoch 4/5, Loss: 1.4401
Epoch 5/5, Loss: 1.4294
新模型评估：
Accuracy: 65.48%
Model saved to ../../../data/model/mnist_cnn_model
CPC8调整模型中, 本轮训练的数据量为：237.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 65.48%
Epoch 1/5, Loss: 1.4629
Epoch 2/5, Loss: 1.4399
Epoch 3/5, Loss: 1.4234
Epoch 4/5, Loss: 1.4250
Epoch 5/5, Loss: 1.3986
新模型评估：
Accuracy: 67.28%
Model saved to ../../../data/model/mnist_cnn_model
CPC9调整模型中, 本轮训练的数据量为：645.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 67.28%
Epoch 1/5, Loss: 1.4006
Epoch 2/5, Loss: 1.3435
Epoch 3/5, Loss: 1.3282
Epoch 4/5, Loss: 1.2704
Epoch 5/5, Loss: 1.2457
新模型评估：
Accuracy: 69.72%
Model saved to ../../../data/model/mnist_cnn_model
DONE
========================= literation: 2 =========================
----- literation 2: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3487
DataOwner1的最优x_1 = 0.0734
DataOwner2的最优x_2 = 0.1342
DataOwner3的最优x_3 = 0.1453
DataOwner4的最优x_4 = 0.1504
DataOwner5的最优x_5 = 0.0454
DataOwner6的最优x_6 = 0.1502
DataOwner7的最优x_7 = 0.1539
DataOwner8的最优x_8 = 0.1235
DataOwner9的最优x_9 = 0.0793
DataOwner10的最优x_10 = 0.1434
每个DataOwner应该贡献数据比例 xn_list = [0.0734177478124918, 0.1341659241550416, 0.14530712131282325, 0.15036705600219624, 0.04541205870108592, 0.15019839819440925, 0.15393457224031828, 0.12346742500970169, 0.07932317853509877, 0.14336943550656742]
ModelOwner的最大效用 U(Eta) = 0.5898
DONE
----- literation 2: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.348729244257586
DataOwner1的分配到的支付 ： 0.0779
DataOwner2的分配到的支付 ： 0.1511
DataOwner3的分配到的支付 ： 0.1657
DataOwner4的分配到的支付 ： 0.1724
DataOwner5的分配到的支付 ： 0.0471
DataOwner6的分配到的支付 ： 0.1722
DataOwner7的分配到的支付 ： 0.1772
DataOwner8的分配到的支付 ： 0.1375
DataOwner9的分配到的支付 ： 0.0846
DataOwner10的分配到的支付 ： 0.1631
DONE
----- literation 2: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC2
DataOwner3 把数据交给 CPC3
DataOwner4 把数据交给 CPC10
DataOwner5 把数据交给 CPC4
DataOwner6 把数据交给 CPC5
DataOwner7 把数据交给 CPC6
DataOwner8 把数据交给 CPC7
DataOwner9 把数据交给 CPC8
DataOwner10 把数据交给 CPC9
DONE
----- literation 2: 模型训练 -----
重新调整fn，进而调整xn、Eta
正在评估DataOwner1的数据质量, 本轮评估的样本数据量为：440.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 69.72%
Epoch 1/5, Loss: 1.3210
Epoch 2/5, Loss: 1.2941
Epoch 3/5, Loss: 1.2751
Epoch 4/5, Loss: 1.2572
Epoch 5/5, Loss: 1.2402
新模型评估：
Accuracy: 68.34%
loss差为：
0.0807795354298182
单位数据loss差为：
0.0001835898532495868
正在评估DataOwner2的数据质量, 本轮评估的样本数据量为：1006.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 69.72%
Epoch 1/5, Loss: 1.2855
Epoch 2/5, Loss: 1.2469
Epoch 3/5, Loss: 1.2118
Epoch 4/5, Loss: 1.1751
Epoch 5/5, Loss: 1.1425
新模型评估：
Accuracy: 71.50%
loss差为：
0.14302995055913925
单位数据loss差为：
0.00014217688922379647
正在评估DataOwner3的数据质量, 本轮评估的样本数据量为：1307.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 69.72%
Epoch 1/5, Loss: 1.2111
Epoch 2/5, Loss: 1.1683
Epoch 3/5, Loss: 1.1118
Epoch 4/5, Loss: 1.0661
Epoch 5/5, Loss: 1.0312
新模型评估：
Accuracy: 72.71%
loss差为：
0.17994001649674907
单位数据loss差为：
0.00013767407536094037
正在评估DataOwner4的数据质量, 本轮评估的样本数据量为：1127.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 69.72%
Epoch 1/5, Loss: 1.3004
Epoch 2/5, Loss: 1.2575
Epoch 3/5, Loss: 1.2222
Epoch 4/5, Loss: 1.1838
Epoch 5/5, Loss: 1.1414
新模型评估：
Accuracy: 74.52%
loss差为：
0.15907244549857258
单位数据loss差为：
0.00014114680168462519
正在评估DataOwner5的数据质量, 本轮评估的样本数据量为：136.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 69.72%
Epoch 1/5, Loss: 1.2964
Epoch 2/5, Loss: 1.2764
Epoch 3/5, Loss: 1.2712
Epoch 4/5, Loss: 1.2412
Epoch 5/5, Loss: 1.2548
新模型评估：
Accuracy: 70.05%
loss差为：
0.0415571928024292
单位数据loss差为：
0.00030556759413550883
正在评估DataOwner6的数据质量, 本轮评估的样本数据量为：1126.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 69.72%
Epoch 1/5, Loss: 1.3085
Epoch 2/5, Loss: 1.2639
Epoch 3/5, Loss: 1.2279
Epoch 4/5, Loss: 1.1903
Epoch 5/5, Loss: 1.1517
新模型评估：
Accuracy: 74.38%
loss差为：
0.15677164660559773
单位数据loss差为：
0.00013922881581314184
正在评估DataOwner7的数据质量, 本轮评估的样本数据量为：1385.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 69.72%
Epoch 1/5, Loss: 1.2718
Epoch 2/5, Loss: 1.2212
Epoch 3/5, Loss: 1.1769
Epoch 4/5, Loss: 1.1290
Epoch 5/5, Loss: 1.0830
新模型评估：
Accuracy: 74.07%
loss差为：
0.18882001800970594
单位数据loss差为：
0.00013633214296729671
正在评估DataOwner8的数据质量, 本轮评估的样本数据量为：370.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 69.72%
Epoch 1/5, Loss: 1.2820
Epoch 2/5, Loss: 1.2566
Epoch 3/5, Loss: 1.2437
Epoch 4/5, Loss: 1.2288
Epoch 5/5, Loss: 1.2178
新模型评估：
Accuracy: 70.99%
loss差为：
0.06415724754333496
单位数据loss差为：
0.00017339796633333772
正在评估DataOwner9的数据质量, 本轮评估的样本数据量为：237.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 69.72%
Epoch 1/5, Loss: 1.3422
Epoch 2/5, Loss: 1.3410
Epoch 3/5, Loss: 1.3269
Epoch 4/5, Loss: 1.3086
Epoch 5/5, Loss: 1.2919
新模型评估：
Accuracy: 68.84%
loss差为：
0.050275593996047974
单位数据loss差为：
0.00021213330800020242
正在评估DataOwner10的数据质量, 本轮评估的样本数据量为：645.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 69.72%
Epoch 1/5, Loss: 1.2823
Epoch 2/5, Loss: 1.2752
Epoch 3/5, Loss: 1.2705
Epoch 4/5, Loss: 1.2100
Epoch 5/5, Loss: 1.2285
新模型评估：
Accuracy: 70.02%
loss差为：
0.05386860804124316
单位数据loss差为：
8.351722176936925e-05
经过服务器调节后的真实数据质量：
数据质量列表avg_f_list: [0.0001835898532495868, 0.00014217688922379647, 0.00013767407536094037, 0.00014114680168462519, 0.00030556759413550883, 0.00013922881581314184, 0.00013633214296729671, 0.00017339796633333772, 0.00021213330800020242, 8.351722176936925e-05]
归一化后的数据质量列表avg_f_list:[0.9450675359891798, 0.9264172794800376, 0.924389445068019, 0.9259533813436847, 1.0, 0.9250896197336295, 0.9237851081424178, 0.940477637396511, 0.9579220313212345, 0.9]
CPC1调整模型中, 本轮训练的数据量为：440.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 69.72%
Epoch 1/5, Loss: 1.3213
Epoch 2/5, Loss: 1.2984
Epoch 3/5, Loss: 1.2765
Epoch 4/5, Loss: 1.2597
Epoch 5/5, Loss: 1.2430
新模型评估：
Accuracy: 68.10%
CPC2调整模型中, 本轮训练的数据量为：1006.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 69.72%
Epoch 1/5, Loss: 1.2880
Epoch 2/5, Loss: 1.2480
Epoch 3/5, Loss: 1.2112
Epoch 4/5, Loss: 1.1762
Epoch 5/5, Loss: 1.1460
新模型评估：
Accuracy: 71.98%
Model saved to ../../../data/model/mnist_cnn_model
CPC3调整模型中, 本轮训练的数据量为：1307.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 71.98%
Epoch 1/5, Loss: 1.0434
Epoch 2/5, Loss: 0.9942
Epoch 3/5, Loss: 0.9593
Epoch 4/5, Loss: 0.9176
Epoch 5/5, Loss: 0.8761
新模型评估：
Accuracy: 74.31%
Model saved to ../../../data/model/mnist_cnn_model
CPC10调整模型中, 本轮训练的数据量为：1127.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 74.31%
Epoch 1/5, Loss: 0.9407
Epoch 2/5, Loss: 0.9072
Epoch 3/5, Loss: 0.8756
Epoch 4/5, Loss: 0.8491
Epoch 5/5, Loss: 0.8215
新模型评估：
Accuracy: 76.92%
Model saved to ../../../data/model/mnist_cnn_model
CPC4调整模型中, 本轮训练的数据量为：136.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 76.92%
Epoch 1/5, Loss: 0.8637
Epoch 2/5, Loss: 0.8577
Epoch 3/5, Loss: 0.8629
Epoch 4/5, Loss: 0.6564
Epoch 5/5, Loss: 0.7263
新模型评估：
Accuracy: 76.70%
CPC5调整模型中, 本轮训练的数据量为：1126.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 76.92%
Epoch 1/5, Loss: 0.8222
Epoch 2/5, Loss: 0.7969
Epoch 3/5, Loss: 0.7703
Epoch 4/5, Loss: 0.7499
Epoch 5/5, Loss: 0.7316
新模型评估：
Accuracy: 78.42%
Model saved to ../../../data/model/mnist_cnn_model
CPC6调整模型中, 本轮训练的数据量为：1385.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.42%
Epoch 1/5, Loss: 0.6894
Epoch 2/5, Loss: 0.6640
Epoch 3/5, Loss: 0.6435
Epoch 4/5, Loss: 0.6192
Epoch 5/5, Loss: 0.6014
新模型评估：
Accuracy: 78.12%
CPC7调整模型中, 本轮训练的数据量为：370.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.42%
Epoch 1/5, Loss: 0.6607
Epoch 2/5, Loss: 0.6402
Epoch 3/5, Loss: 0.6333
Epoch 4/5, Loss: 0.6249
Epoch 5/5, Loss: 0.6130
新模型评估：
Accuracy: 76.60%
CPC8调整模型中, 本轮训练的数据量为：237.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.42%
Epoch 1/5, Loss: 0.7149
Epoch 2/5, Loss: 0.7142
Epoch 3/5, Loss: 0.6970
Epoch 4/5, Loss: 0.6873
Epoch 5/5, Loss: 0.6726
新模型评估：
Accuracy: 74.87%
CPC9调整模型中, 本轮训练的数据量为：645.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.42%
Epoch 1/5, Loss: 0.7437
Epoch 2/5, Loss: 0.6877
Epoch 3/5, Loss: 0.6787
Epoch 4/5, Loss: 0.6935
Epoch 5/5, Loss: 0.6586
新模型评估：
Accuracy: 76.25%
DONE
========================= literation: 3 =========================
----- literation 3: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3132
DataOwner1的最优x_1 = 0.1269
DataOwner2的最优x_2 = 0.1081
DataOwner3的最优x_3 = 0.1059
DataOwner4的最优x_4 = 0.1076
DataOwner5的最优x_5 = 0.1742
DataOwner6的最优x_6 = 0.1066
DataOwner7的最优x_7 = 0.1053
DataOwner8的最优x_8 = 0.1224
DataOwner9的最优x_9 = 0.1391
DataOwner10的最优x_10 = 0.0784
每个DataOwner应该贡献数据比例 xn_list = [0.12693609243944445, 0.10805307459901223, 0.10590174703339068, 0.10756267802204363, 0.17415713178396075, 0.10664681035408176, 0.10525675031969639, 0.12243608624519058, 0.13905298594901133, 0.07840149914229286]
ModelOwner的最大效用 U(Eta) = 0.5494
xn开始变化：
DONE
----- literation 3: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3132073461145717
DataOwner1的分配到的支付 ： 0.5702
DataOwner2的分配到的支付 ： 0.0750
DataOwner3的分配到的支付 ： 0.0810
DataOwner4的分配到的支付 ： 0.0840
DataOwner5的分配到的支付 ： 0.1051
DataOwner6的分配到的支付 ： 0.0838
DataOwner7的分配到的支付 ： 0.0858
DataOwner8的分配到的支付 ： 0.0701
DataOwner9的分配到的支付 ： 0.0804
DataOwner10的分配到的支付 ： 0.0779
DONE
----- literation 3: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC2
DataOwner3 把数据交给 CPC3
DataOwner4 把数据交给 CPC10
DataOwner5 把数据交给 CPC4
DataOwner6 把数据交给 CPC5
DataOwner7 把数据交给 CPC6
DataOwner8 把数据交给 CPC7
DataOwner9 把数据交给 CPC8
DataOwner10 把数据交给 CPC9
DONE
----- literation 3: 模型训练 -----
CPC1调整模型中, 本轮训练的数据量为：6000.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.42%
Epoch 1/5, Loss: 0.6606
Epoch 2/5, Loss: 0.5874
Epoch 3/5, Loss: 0.5346
Epoch 4/5, Loss: 0.4954
Epoch 5/5, Loss: 0.4630
新模型评估：
Accuracy: 75.68%
CPC2调整模型中, 本轮训练的数据量为：1006.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.42%
Epoch 1/5, Loss: 0.6975
Epoch 2/5, Loss: 0.6777
Epoch 3/5, Loss: 0.6600
Epoch 4/5, Loss: 0.6441
Epoch 5/5, Loss: 0.6285
新模型评估：
Accuracy: 77.90%
CPC3调整模型中, 本轮训练的数据量为：1307.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.42%
Epoch 1/5, Loss: 0.6560
Epoch 2/5, Loss: 0.6212
Epoch 3/5, Loss: 0.6050
Epoch 4/5, Loss: 0.5861
Epoch 5/5, Loss: 0.5619
新模型评估：
Accuracy: 77.93%
CPC10调整模型中, 本轮训练的数据量为：1127.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.42%
Epoch 1/5, Loss: 0.6991
Epoch 2/5, Loss: 0.6769
Epoch 3/5, Loss: 0.6650
Epoch 4/5, Loss: 0.6402
Epoch 5/5, Loss: 0.6241
新模型评估：
Accuracy: 78.57%
Model saved to ../../../data/model/mnist_cnn_model
CPC4调整模型中, 本轮训练的数据量为：522.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.57%
Epoch 1/5, Loss: 0.6259
Epoch 2/5, Loss: 0.6237
Epoch 3/5, Loss: 0.6111
Epoch 4/5, Loss: 0.5995
Epoch 5/5, Loss: 0.6075
新模型评估：
Accuracy: 75.93%
CPC5调整模型中, 本轮训练的数据量为：1126.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.57%
Epoch 1/5, Loss: 0.6059
Epoch 2/5, Loss: 0.5817
Epoch 3/5, Loss: 0.5728
Epoch 4/5, Loss: 0.5569
Epoch 5/5, Loss: 0.5402
新模型评估：
Accuracy: 77.72%
CPC6调整模型中, 本轮训练的数据量为：1385.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.57%
Epoch 1/5, Loss: 0.6053
Epoch 2/5, Loss: 0.5855
Epoch 3/5, Loss: 0.5657
Epoch 4/5, Loss: 0.5510
Epoch 5/5, Loss: 0.5380
新模型评估：
Accuracy: 77.61%
CPC7调整模型中, 本轮训练的数据量为：370.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.57%
Epoch 1/5, Loss: 0.5927
Epoch 2/5, Loss: 0.5796
Epoch 3/5, Loss: 0.5684
Epoch 4/5, Loss: 0.5621
Epoch 5/5, Loss: 0.5552
新模型评估：
Accuracy: 76.70%
CPC8调整模型中, 本轮训练的数据量为：417.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.57%
Epoch 1/5, Loss: 0.6327
Epoch 2/5, Loss: 0.6122
Epoch 3/5, Loss: 0.6099
Epoch 4/5, Loss: 0.5992
Epoch 5/5, Loss: 0.5893
新模型评估：
Accuracy: 76.07%
CPC9调整模型中, 本轮训练的数据量为：645.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.57%
Epoch 1/5, Loss: 0.6285
Epoch 2/5, Loss: 0.5637
Epoch 3/5, Loss: 0.5787
Epoch 4/5, Loss: 0.5366
Epoch 5/5, Loss: 0.5277
新模型评估：
Accuracy: 76.28%
DONE
最终的列表：
[9.084331145450819e-07, -0.00014935838399141727, -0.00044739838236248705, -0.000889910813844319, -0.0014736925070257534, -0.002195634287985644, -0.00305271755744288, -0.004042011016393676, -0.005160667532774915, -0.006405921142117771, -0.007775084175554281, -0.009265544508912288, -0.010874762926984416, -0.012600270597384, -0.014439666648709104, -0.016390615848023732, -0.018450846372937646, -0.02061814767381817, -0.022890368421910362, -0.02526541453936318, -0.027741247307373262, -0.030315881548855672, -0.03298738388223907, -0.035753871043158414, -0.0386135082709855, -0.041564507757293506, -0.044605127153498986, -0.04773366813506552, -0.0509484750197835, -0.05424793343776402, -0.057630469050904076, -0.061094546319687804, -0.0646386673152971, -0.06826137057509823, -0.07196122999967047, -0.07573685378962713, -0.0795868834205638, -0.08350999265454845, -0.08750488658664224, -0.0915703007250126, -0.09570500010326372, -0.09990777842367721, -0.1041774572301139, -0.108512885109386, -0.11291293691996246, -0.11737651304692204, -0.12190253868211859, -0.12648996312856858, -0.13113775912811432, -0.13584492221146122, -0.14061047006972288, -0.14543344194664992, -0.15031289805075237, -0.1552479189865596, -0.16023760520429486, -0.16528107646727375, -0.17037747133636189, -0.17552594667086086, -0.1807256771452112, -0.18597585478093492, -0.19127568849325532, -0.1966244036518638, -0.2020212416553192, -0.20746545951858947, -0.21295632947326387, -0.21849313857998387, -0.22407518835266155, -0.22970179439406535, -0.23537228604237725, -0.24108600602833757, -0.24684231014260716, -0.2526405669129953, -0.25848015729121426, -0.2643604743488309, -0.2702809229821063, -0.27624091962541714, -0.2822398919729732, -0.28827727870854913, -0.294352529242964, -0.3004651034590513, -0.30661447146386833, -0.31280011334791047, -0.3190215189510974, -0.3252781876353108, -0.3315696280632706, -0.33789535798354653, -0.3442549040215024, -0.35064780147598884, -0.3570735941215981, -0.36353183401630196, -0.3700220813143077, -0.3765439040839629, -0.3830968781305548, -0.389680586823847, -0.39629462093021184, -0.4029385784492111, -0.40961206445449116, -0.41631469093885975, -0.42304607666341587, -0.42980584701060964]
**** log-parameter_analysis 运行时间： 2025-01-17 23:45:20 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.05865967695000649
DataOwner2: noise random: 0.017118920390866776
DataOwner3: noise random: 0.0556531704516965
DataOwner4: noise random: 0.003336871706305178
DataOwner5: noise random: 0.08199438610118182
DataOwner6: noise random: 0.01958503040917018
DataOwner7: noise random: 0.0873878437757346
DataOwner8: noise random: 0.03196876253454185
DataOwner9: noise random: 0.027561147637308294
DataOwner10: noise random: 0.036046981169154016
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9986089767412821, 0.9998814855991429, 0.9987455181226688, 0.9999955010011535, 0.9972687806704239, 0.9998446133750168, 0.99692381698804, 0.9995861679373431, 0.9996929183539068, 0.9994728275897179]
归一化后的数据质量列表avg_f_list: [0.9548611037479083, 0.9962881793334255, 0.9593062674041867, 1.0, 0.9112304417027011, 0.9950877881483733, 0.9, 0.986673985277688, 0.9901492912045992, 0.9829841412982502]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3503
DataOwner1的最优x_1 = 0.1096
DataOwner2的最优x_2 = 0.1497
DataOwner3的最优x_3 = 0.1143
DataOwner4的最优x_4 = 0.1530
DataOwner5的最优x_5 = 0.0587
DataOwner6的最优x_6 = 0.1486
DataOwner7的最优x_7 = 0.0438
DataOwner8的最优x_8 = 0.1410
DataOwner9的最优x_9 = 0.1442
DataOwner10的最优x_10 = 0.1376
每个DataOwner应该贡献数据比例 xn_list = [0.10961210378669053, 0.1497013727018006, 0.11427135324392541, 0.15295129020735582, 0.058650358807870465, 0.14863906590895334, 0.043840364457304576, 0.14103496181834013, 0.144209701129785, 0.13761106142760743]
ModelOwner的最大效用 U(Eta) = 0.5916
DONE
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.350259967138263
DataOwner1的分配到的支付 ： 0.1203
DataOwner2的分配到的支付 ： 0.1715
DataOwner3的分配到的支付 ： 0.1260
DataOwner4的分配到的支付 ： 0.1759
DataOwner5的分配到的支付 ： 0.0614
DataOwner6的分配到的支付 ： 0.1701
DataOwner7的分配到的支付 ： 0.0454
DataOwner8的分配到的支付 ： 0.1600
DataOwner9的分配到的支付 ： 0.1642
DataOwner10的分配到的支付 ： 0.1555
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC2', 'DataOwner3': 'CPC1', 'DataOwner2': 'CPC3', 'DataOwner4': 'CPC4', 'DataOwner5': 'CPC5', 'DataOwner6': 'CPC6', 'DataOwner7': 'CPC7', 'DataOwner8': 'CPC8', 'DataOwner9': 'CPC9', 'DataOwner10': 'CPC10'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC2
DataOwner3 把数据交给 CPC1
DataOwner2 把数据交给 CPC3
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC8
DataOwner9 把数据交给 CPC9
DataOwner10 把数据交给 CPC10
DONE
----- literation 1: 模型训练 -----
CPC2调整模型中, 本轮训练的数据量为：212.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 26.82%
Epoch 1/5, Loss: 2.2869
Epoch 2/5, Loss: 2.2989
Epoch 3/5, Loss: 2.2937
Epoch 4/5, Loss: 2.2919
Epoch 5/5, Loss: 2.2836
新模型评估：
Accuracy: 28.91%
Model saved to ../../../data/model/mnist_cnn_model
CPC1调整模型中, 本轮训练的数据量为：442.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 28.91%
Epoch 1/5, Loss: 2.2951
Epoch 2/5, Loss: 2.2930
Epoch 3/5, Loss: 2.2905
Epoch 4/5, Loss: 2.2886
Epoch 5/5, Loss: 2.2859
新模型评估：
Accuracy: 34.50%
Model saved to ../../../data/model/mnist_cnn_model
CPC3调整模型中, 本轮训练的数据量为：1158.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 34.50%
Epoch 1/5, Loss: 2.2802
Epoch 2/5, Loss: 2.2793
Epoch 3/5, Loss: 2.2668
Epoch 4/5, Loss: 2.2487
Epoch 5/5, Loss: 2.2326
新模型评估：
Accuracy: 43.94%
Model saved to ../../../data/model/mnist_cnn_model
CPC4调整模型中, 本轮训练的数据量为：2072.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 43.94%
Epoch 1/5, Loss: 2.2021
Epoch 2/5, Loss: 2.1649
Epoch 3/5, Loss: 2.1142
Epoch 4/5, Loss: 2.0547
Epoch 5/5, Loss: 1.9768
新模型评估：
Accuracy: 53.05%
Model saved to ../../../data/model/mnist_cnn_model
CPC5调整模型中, 本轮训练的数据量为：113.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 53.05%
Epoch 1/5, Loss: 1.9571
Epoch 2/5, Loss: 1.9578
Epoch 3/5, Loss: 1.9534
Epoch 4/5, Loss: 1.9514
Epoch 5/5, Loss: 1.9341
新模型评估：
Accuracy: 52.85%
CPC6调整模型中, 本轮训练的数据量为：575.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 53.05%
Epoch 1/5, Loss: 1.9284
Epoch 2/5, Loss: 1.9068
Epoch 3/5, Loss: 1.8855
Epoch 4/5, Loss: 1.8639
Epoch 5/5, Loss: 1.8414
新模型评估：
Accuracy: 52.87%
CPC7调整模型中, 本轮训练的数据量为：84.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 53.05%
Epoch 1/5, Loss: 1.9428
Epoch 2/5, Loss: 1.9090
Epoch 3/5, Loss: 1.9262
Epoch 4/5, Loss: 1.9631
Epoch 5/5, Loss: 1.9381
新模型评估：
Accuracy: 52.56%
CPC8调整模型中, 本轮训练的数据量为：818.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 53.05%
Epoch 1/5, Loss: 1.9337
Epoch 2/5, Loss: 1.9021
Epoch 3/5, Loss: 1.8736
Epoch 4/5, Loss: 1.8408
Epoch 5/5, Loss: 1.8109
新模型评估：
Accuracy: 54.89%
Model saved to ../../../data/model/mnist_cnn_model
CPC9调整模型中, 本轮训练的数据量为：2511.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 54.89%
Epoch 1/5, Loss: 1.7731
Epoch 2/5, Loss: 1.6762
Epoch 3/5, Loss: 1.5705
Epoch 4/5, Loss: 1.4650
Epoch 5/5, Loss: 1.3668
新模型评估：
Accuracy: 69.21%
Model saved to ../../../data/model/mnist_cnn_model
CPC10调整模型中, 本轮训练的数据量为：266.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 69.21%
Epoch 1/5, Loss: 1.2425
Epoch 2/5, Loss: 1.1991
Epoch 3/5, Loss: 1.2384
Epoch 4/5, Loss: 1.2053
Epoch 5/5, Loss: 1.2473
新模型评估：
Accuracy: 71.20%
Model saved to ../../../data/model/mnist_cnn_model
DONE
========================= literation: 2 =========================
----- literation 2: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3503
DataOwner1的最优x_1 = 0.1096
DataOwner2的最优x_2 = 0.1497
DataOwner3的最优x_3 = 0.1143
DataOwner4的最优x_4 = 0.1530
DataOwner5的最优x_5 = 0.0587
DataOwner6的最优x_6 = 0.1486
DataOwner7的最优x_7 = 0.0438
DataOwner8的最优x_8 = 0.1410
DataOwner9的最优x_9 = 0.1442
DataOwner10的最优x_10 = 0.1376
每个DataOwner应该贡献数据比例 xn_list = [0.10961210378669053, 0.1497013727018006, 0.11427135324392541, 0.15295129020735582, 0.058650358807870465, 0.14863906590895334, 0.043840364457304576, 0.14103496181834013, 0.144209701129785, 0.13761106142760743]
ModelOwner的最大效用 U(Eta) = 0.5916
DONE
----- literation 2: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.350259967138263
DataOwner1的分配到的支付 ： 0.1203
DataOwner2的分配到的支付 ： 0.1715
DataOwner3的分配到的支付 ： 0.1260
DataOwner4的分配到的支付 ： 0.1759
DataOwner5的分配到的支付 ： 0.0614
DataOwner6的分配到的支付 ： 0.1701
DataOwner7的分配到的支付 ： 0.0454
DataOwner8的分配到的支付 ： 0.1600
DataOwner9的分配到的支付 ： 0.1642
DataOwner10的分配到的支付 ： 0.1555
DONE
----- literation 2: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC2
DataOwner3 把数据交给 CPC1
DataOwner2 把数据交给 CPC3
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC8
DataOwner9 把数据交给 CPC9
DataOwner10 把数据交给 CPC10
DONE
----- literation 2: 模型训练 -----
重新调整fn，进而调整xn、Eta
正在评估DataOwner1的数据质量, 本轮评估的样本数据量为：212.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 71.20%
Epoch 1/5, Loss: 1.2719
Epoch 2/5, Loss: 1.2810
Epoch 3/5, Loss: 1.2235
Epoch 4/5, Loss: 1.2057
Epoch 5/5, Loss: 1.1953
新模型评估：
Accuracy: 72.70%
loss差为：
0.07656246423721313
单位数据loss差为：
0.00036114369923213744
正在评估DataOwner3的数据质量, 本轮评估的样本数据量为：442.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 71.20%
Epoch 1/5, Loss: 1.2649
Epoch 2/5, Loss: 1.2407
Epoch 3/5, Loss: 1.2206
Epoch 4/5, Loss: 1.2023
Epoch 5/5, Loss: 1.1859
新模型评估：
Accuracy: 72.67%
loss差为：
0.07893712180001389
单位数据loss差为：
0.00017859077330319883
正在评估DataOwner2的数据质量, 本轮评估的样本数据量为：1158.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 71.20%
Epoch 1/5, Loss: 1.2660
Epoch 2/5, Loss: 1.2088
Epoch 3/5, Loss: 1.1809
Epoch 4/5, Loss: 1.1319
Epoch 5/5, Loss: 1.1020
新模型评估：
Accuracy: 73.84%
loss差为：
0.16403943927664488
单位数据loss差为：
0.0001416575468710232
正在评估DataOwner4的数据质量, 本轮评估的样本数据量为：2072.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 71.20%
Epoch 1/5, Loss: 1.2480
Epoch 2/5, Loss: 1.1691
Epoch 3/5, Loss: 1.1060
Epoch 4/5, Loss: 1.0380
Epoch 5/5, Loss: 0.9788
新模型评估：
Accuracy: 74.40%
loss差为：
0.26918480432394765
单位数据loss差为：
0.00012991544610229134
正在评估DataOwner5的数据质量, 本轮评估的样本数据量为：113.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 71.20%
Epoch 1/5, Loss: 1.1950
Epoch 2/5, Loss: 1.1885
Epoch 3/5, Loss: 1.1614
Epoch 4/5, Loss: 1.1627
Epoch 5/5, Loss: 1.1538
新模型评估：
Accuracy: 69.91%
loss差为：
0.0411759614944458
单位数据loss差为：
0.00036438903977385664
正在评估DataOwner6的数据质量, 本轮评估的样本数据量为：575.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 71.20%
Epoch 1/5, Loss: 1.2404
Epoch 2/5, Loss: 1.2165
Epoch 3/5, Loss: 1.1935
Epoch 4/5, Loss: 1.1726
Epoch 5/5, Loss: 1.1512
新模型评估：
Accuracy: 73.96%
loss差为：
0.0892343521118164
单位数据loss差为：
0.00015519017758576766
正在评估DataOwner7的数据质量, 本轮评估的样本数据量为：84.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 71.20%
Epoch 1/5, Loss: 1.2154
Epoch 2/5, Loss: 1.2156
Epoch 3/5, Loss: 1.2911
Epoch 4/5, Loss: 1.2667
Epoch 5/5, Loss: 1.1843
新模型评估：
Accuracy: 71.00%
loss差为：
0.031066536903381348
单位数据loss差为：
0.00036983972504025413
正在评估DataOwner8的数据质量, 本轮评估的样本数据量为：818.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 71.20%
Epoch 1/5, Loss: 1.2257
Epoch 2/5, Loss: 1.1875
Epoch 3/5, Loss: 1.1588
Epoch 4/5, Loss: 1.1261
Epoch 5/5, Loss: 1.1010
新模型评估：
Accuracy: 73.75%
loss差为：
0.12476970599247861
单位数据loss差为：
0.00015253020292479047
正在评估DataOwner9的数据质量, 本轮评估的样本数据量为：2511.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 71.20%
Epoch 1/5, Loss: 1.2289
Epoch 2/5, Loss: 1.1455
Epoch 3/5, Loss: 1.0595
Epoch 4/5, Loss: 0.9817
Epoch 5/5, Loss: 0.9129
新模型评估：
Accuracy: 75.84%
loss差为：
0.31598547101020813
单位数据loss差为：
0.00012584049024699646
正在评估DataOwner10的数据质量, 本轮评估的样本数据量为：266.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 71.20%
Epoch 1/5, Loss: 1.2330
Epoch 2/5, Loss: 1.2950
Epoch 3/5, Loss: 1.1854
Epoch 4/5, Loss: 1.2043
Epoch 5/5, Loss: 1.2240
新模型评估：
Accuracy: 70.31%
loss差为：
0.008962392807006836
单位数据loss差为：
3.3693206041379085e-05
经过服务器调节后的真实数据质量：
数据质量列表avg_f_list: [0.00036114369923213744, 0.0001416575468710232, 0.00017859077330319883, 0.00012991544610229134, 0.00036438903977385664, 0.00015519017758576766, 0.00036983972504025413, 0.00015253020292479047, 0.00012584049024699646, 3.3693206041379085e-05]
归一化后的数据质量列表avg_f_list:[0.9974130251790155, 0.9321182385440694, 0.9431054790313936, 0.9286250889485589, 0.9983784793361445, 0.936144051679082, 1.0, 0.9353527376208852, 0.9274128330943465, 0.9]
CPC2调整模型中, 本轮训练的数据量为：212.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 71.20%
Epoch 1/5, Loss: 1.2589
Epoch 2/5, Loss: 1.2900
Epoch 3/5, Loss: 1.2188
Epoch 4/5, Loss: 1.2243
Epoch 5/5, Loss: 1.1768
新模型评估：
Accuracy: 72.93%
Model saved to ../../../data/model/mnist_cnn_model
CPC1调整模型中, 本轮训练的数据量为：442.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 72.93%
Epoch 1/5, Loss: 1.2262
Epoch 2/5, Loss: 1.2013
Epoch 3/5, Loss: 1.1801
Epoch 4/5, Loss: 1.1618
Epoch 5/5, Loss: 1.1445
新模型评估：
Accuracy: 73.01%
Model saved to ../../../data/model/mnist_cnn_model
CPC3调整模型中, 本轮训练的数据量为：1158.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 73.01%
Epoch 1/5, Loss: 1.1524
Epoch 2/5, Loss: 1.0770
Epoch 3/5, Loss: 1.0737
Epoch 4/5, Loss: 1.0217
Epoch 5/5, Loss: 0.9931
新模型评估：
Accuracy: 73.85%
Model saved to ../../../data/model/mnist_cnn_model
CPC4调整模型中, 本轮训练的数据量为：2072.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 73.85%
Epoch 1/5, Loss: 0.9638
Epoch 2/5, Loss: 0.9066
Epoch 3/5, Loss: 0.8595
Epoch 4/5, Loss: 0.8069
Epoch 5/5, Loss: 0.7620
新模型评估：
Accuracy: 75.59%
Model saved to ../../../data/model/mnist_cnn_model
CPC5调整模型中, 本轮训练的数据量为：113.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 75.59%
Epoch 1/5, Loss: 0.6744
Epoch 2/5, Loss: 0.6608
Epoch 3/5, Loss: 0.6517
Epoch 4/5, Loss: 0.6359
Epoch 5/5, Loss: 0.6227
新模型评估：
Accuracy: 74.71%
CPC6调整模型中, 本轮训练的数据量为：575.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 75.59%
Epoch 1/5, Loss: 0.7220
Epoch 2/5, Loss: 0.7052
Epoch 3/5, Loss: 0.6933
Epoch 4/5, Loss: 0.6800
Epoch 5/5, Loss: 0.6694
新模型评估：
Accuracy: 77.58%
Model saved to ../../../data/model/mnist_cnn_model
CPC7调整模型中, 本轮训练的数据量为：84.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.58%
Epoch 1/5, Loss: 0.6066
Epoch 2/5, Loss: 0.7220
Epoch 3/5, Loss: 0.7781
Epoch 4/5, Loss: 0.6397
Epoch 5/5, Loss: 0.6149
新模型评估：
Accuracy: 76.12%
CPC8调整模型中, 本轮训练的数据量为：818.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.58%
Epoch 1/5, Loss: 0.6538
Epoch 2/5, Loss: 0.6345
Epoch 3/5, Loss: 0.6243
Epoch 4/5, Loss: 0.6117
Epoch 5/5, Loss: 0.5953
新模型评估：
Accuracy: 76.93%
CPC9调整模型中, 本轮训练的数据量为：2511.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.58%
Epoch 1/5, Loss: 0.6756
Epoch 2/5, Loss: 0.6414
Epoch 3/5, Loss: 0.6057
Epoch 4/5, Loss: 0.5767
Epoch 5/5, Loss: 0.5487
新模型评估：
Accuracy: 76.83%
CPC10调整模型中, 本轮训练的数据量为：266.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.58%
Epoch 1/5, Loss: 0.6840
Epoch 2/5, Loss: 0.6332
Epoch 3/5, Loss: 0.6590
Epoch 4/5, Loss: 0.6757
Epoch 5/5, Loss: 0.6435
新模型评估：
Accuracy: 75.16%
DONE
========================= literation: 3 =========================
----- literation 3: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3288
DataOwner1的最优x_1 = 0.1638
DataOwner2的最优x_2 = 0.1022
DataOwner3的最优x_3 = 0.1139
DataOwner4的最优x_4 = 0.0984
DataOwner5的最优x_5 = 0.1645
DataOwner6的最优x_6 = 0.1066
DataOwner7的最优x_7 = 0.1659
DataOwner8的最优x_8 = 0.1057
DataOwner9的最优x_9 = 0.0971
DataOwner10的最优x_10 = 0.0647
每个DataOwner应该贡献数据比例 xn_list = [0.16376416538514993, 0.10224938090024879, 0.11389559198737591, 0.09842435460550782, 0.16454649274820438, 0.10658361711274231, 0.16585296452623385, 0.10573787311444983, 0.09708276804931043, 0.06469082933254318]
ModelOwner的最大效用 U(Eta) = 0.5668
xn开始变化：
new_U_qn_list: []
DONE
----- literation 3: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3287834509116727
DataOwner1的分配到的支付 ： 0.5902
DataOwner2的分配到的支付 ： 0.0826
DataOwner3的分配到的支付 ： 0.0638
DataOwner4的分配到的支付 ： 0.0840
DataOwner5的分配到的支付 ： 0.0972
DataOwner6的分配到的支付 ： 0.0823
DataOwner7的分配到的支付 ： 0.0981
DataOwner8的分配到的支付 ： 0.0781
DataOwner9的分配到的支付 ： 0.0791
DataOwner10的分配到的支付 ： 0.0733
DONE
----- literation 3: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC2
DataOwner3 把数据交给 CPC1
DataOwner2 把数据交给 CPC3
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC8
DataOwner9 把数据交给 CPC9
DataOwner10 把数据交给 CPC10
DONE
----- literation 3: 模型训练 -----
CPC2调整模型中, 本轮训练的数据量为：1941.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.58%
Epoch 1/5, Loss: 0.7006
Epoch 2/5, Loss: 0.6747
Epoch 3/5, Loss: 0.6439
Epoch 4/5, Loss: 0.6182
Epoch 5/5, Loss: 0.6000
新模型评估：
Accuracy: 76.24%
CPC1调整模型中, 本轮训练的数据量为：442.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.58%
Epoch 1/5, Loss: 0.7250
Epoch 2/5, Loss: 0.7086
Epoch 3/5, Loss: 0.6995
Epoch 4/5, Loss: 0.6878
Epoch 5/5, Loss: 0.6793
新模型评估：
Accuracy: 76.21%
CPC3调整模型中, 本轮训练的数据量为：1158.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.58%
Epoch 1/5, Loss: 0.6432
Epoch 2/5, Loss: 0.6309
Epoch 3/5, Loss: 0.6125
Epoch 4/5, Loss: 0.5904
Epoch 5/5, Loss: 0.5689
新模型评估：
Accuracy: 75.50%
CPC4调整模型中, 本轮训练的数据量为：2072.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.58%
Epoch 1/5, Loss: 0.6452
Epoch 2/5, Loss: 0.6226
Epoch 3/5, Loss: 0.5886
Epoch 4/5, Loss: 0.5658
Epoch 5/5, Loss: 0.5454
新模型评估：
Accuracy: 76.35%
CPC5调整模型中, 本轮训练的数据量为：318.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.58%
Epoch 1/5, Loss: 0.7859
Epoch 2/5, Loss: 0.7698
Epoch 3/5, Loss: 0.7578
Epoch 4/5, Loss: 0.7486
Epoch 5/5, Loss: 0.7409
新模型评估：
Accuracy: 76.30%
CPC6调整模型中, 本轮训练的数据量为：575.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.58%
Epoch 1/5, Loss: 0.6588
Epoch 2/5, Loss: 0.6390
Epoch 3/5, Loss: 0.6262
Epoch 4/5, Loss: 0.6146
Epoch 5/5, Loss: 0.6049
新模型评估：
Accuracy: 75.32%
CPC7调整模型中, 本轮训练的数据量为：320.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.58%
Epoch 1/5, Loss: 0.6914
Epoch 2/5, Loss: 0.6742
Epoch 3/5, Loss: 0.6617
Epoch 4/5, Loss: 0.6520
Epoch 5/5, Loss: 0.6435
新模型评估：
Accuracy: 76.31%
CPC8调整模型中, 本轮训练的数据量为：818.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.58%
Epoch 1/5, Loss: 0.6757
Epoch 2/5, Loss: 0.6519
Epoch 3/5, Loss: 0.6350
Epoch 4/5, Loss: 0.6198
Epoch 5/5, Loss: 0.6082
新模型评估：
Accuracy: 75.48%
CPC9调整模型中, 本轮训练的数据量为：2511.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.58%
Epoch 1/5, Loss: 0.6686
Epoch 2/5, Loss: 0.6306
Epoch 3/5, Loss: 0.6020
Epoch 4/5, Loss: 0.5808
Epoch 5/5, Loss: 0.5495
新模型评估：
Accuracy: 76.68%
CPC10调整模型中, 本轮训练的数据量为：266.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.58%
Epoch 1/5, Loss: 0.7344
Epoch 2/5, Loss: 0.6814
Epoch 3/5, Loss: 0.7017
Epoch 4/5, Loss: 0.6539
Epoch 5/5, Loss: 0.6621
新模型评估：
Accuracy: 76.48%
DONE
最终的列表：
[0.0005344418723075596, 0.0009031658576833627, 0.0011100518148399428, 0.0011588594244734615, 0.0010532328066272037, 0.0007967049267938731, 0.00039270180194816984, -0.000155453482971038, -0.0008445369382374945, -0.0016714200028665194, -0.002633066039737564, -0.0037265269840614096, -0.00494894013742167, -0.0062975251000667865, -0.007769580834546375, -0.009362482854178233, -0.011073680530197721, -0.01290069451178566, -0.014841114253493187, -0.01689259564488299, -0.019052858737491574, -0.02131968556448191, -0.023690918048606985, -0.02616445599433867, -0.028738255160238646, -0.03141032540785388, -0.0341787289236164, -0.03704157851041018, -0.039997035945641135, -0.04304331040280934, -0.04617865693373696, -0.04940137500874958, -0.052709807112245655, -0.05610233739121723, -0.05957739035440884, -0.06313342961991247, -0.06676895670910943, -0.0704825098849699, -0.07427266303281721, -0.078138024581759, -0.08207723646507004, -0.08608897311789582, -0.09017194051072341, -0.09432487521713973, -0.09854654351446468, -0.10283574051591704, -0.1071912893330289, -0.1116120402670861, -0.11609687002842894, -0.1206446809824977, -0.12525440042156194, -0.12992497986111756, -0.13465539435998258, -0.13944464186316569, -0.14429174256662203, -0.14919573830305177, -0.15415569194793094, -0.15917068684500274, -0.164239826250488, -0.16936223279530954, -0.17453704796464925, -0.17976343159419367, -0.1850405613824433, -0.19036763241849441, -0.19574385672472155, -0.20116846281381578, -0.20664069525965556, -0.21215981428150937, -0.21772509534108836, -0.22333582875198926, -0.2289913193010843, -0.23469088588143444, -0.24043386113631954, -0.24621959111399228, -0.2520474349327843, -0.2579167644562005, -0.26382696397765903, -0.2697774299145419, -0.2757675705112389, -0.2817968055508766, -0.28786456607543787, -0.2939702941139889, -0.30011344241874083, -0.3062934742086826, -0.3125098629205354, -0.31876209196678307, -0.325049654500546, -0.3313720531870762, -0.33772879998165184, -0.34411941591366924, -0.3505434308767257, -0.35700038342450524, -0.36348982057227697, -0.3700112976038292, -0.37656437788366537, -0.38314863267429633, -0.38976364095846594, -0.3964089892661592, -0.4030842715062396, -0.4097890888025729]
**** log-parameter_analysis 运行时间： 2025-01-17 23:58:00 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.03429654041700715
DataOwner2: noise random: 0.051581109733914776
DataOwner3: noise random: 0.052654130700702254
DataOwner4: noise random: 0.06735070894685188
DataOwner5: noise random: 0.05885984624075233
DataOwner6: noise random: 0.048427533207979594
DataOwner7: noise random: 0.042945622102513674
DataOwner8: noise random: 0.0830848920039093
DataOwner9: noise random: 0.07171997801542394
DataOwner10: noise random: 0.0010146959564973025
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9995225731125665, 0.998923936426997, 0.9988701950424489, 0.9981613522805676, 0.9986033593622502, 0.9990519610466909, 0.9992520979116599, 0.9972037442077079, 0.9979194571094367, 0.999999582633949]
归一化后的数据质量列表avg_f_list: [0.9829385876914307, 0.9615268823528488, 0.9596046902818174, 0.9342511950573335, 0.9500606594932626, 0.9661059960273859, 0.9732643805424052, 0.9, 0.9255992225806534, 1.0]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3359
DataOwner1的最优x_1 = 0.1471
DataOwner2的最优x_2 = 0.1272
DataOwner3的最优x_3 = 0.1253
DataOwner4的最优x_4 = 0.0989
DataOwner5的最优x_5 = 0.1157
DataOwner6的最优x_6 = 0.1316
DataOwner7的最优x_7 = 0.1384
DataOwner8的最优x_8 = 0.0580
DataOwner9的最优x_9 = 0.0891
DataOwner10的最优x_10 = 0.1617
每个DataOwner应该贡献数据比例 xn_list = [0.1471391414137425, 0.12718574779746322, 0.12530010474241382, 0.09885595895445935, 0.11569499889951142, 0.13161348239225265, 0.1383576018712863, 0.057993608198503833, 0.08912259426348496, 0.16174208514360713]
ModelOwner的最大效用 U(Eta) = 0.5750
DONE
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3359485287806874
DataOwner1的分配到的支付 ： 0.1684
DataOwner2的分配到的支付 ： 0.1424
DataOwner3的分配到的支付 ： 0.1400
DataOwner4的分配到的支付 ： 0.1075
DataOwner5的分配到的支付 ： 0.1279
DataOwner6的分配到的支付 ： 0.1480
DataOwner7的分配到的支付 ： 0.1567
DataOwner8的分配到的支付 ： 0.0608
DataOwner9的分配到的支付 ： 0.0960
DataOwner10的分配到的支付 ： 0.1883
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC1', 'DataOwner2': 'CPC2', 'DataOwner3': 'CPC3', 'DataOwner4': 'CPC4', 'DataOwner5': 'CPC5', 'DataOwner6': 'CPC6', 'DataOwner7': 'CPC7', 'DataOwner8': 'CPC8', 'DataOwner9': 'CPC9', 'DataOwner10': 'CPC10'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC2
DataOwner3 把数据交给 CPC3
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC8
DataOwner9 把数据交给 CPC9
DataOwner10 把数据交给 CPC10
DONE
----- literation 1: 模型训练 -----
CPC1调整模型中, 本轮训练的数据量为：201.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 26.82%
Epoch 1/5, Loss: 2.3061
Epoch 2/5, Loss: 2.2920
Epoch 3/5, Loss: 2.2899
Epoch 4/5, Loss: 2.2834
Epoch 5/5, Loss: 2.2983
新模型评估：
Accuracy: 28.25%
Model saved to ../../../data/model/mnist_cnn_model
CPC2调整模型中, 本轮训练的数据量为：693.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 28.25%
Epoch 1/5, Loss: 2.2914
Epoch 2/5, Loss: 2.2878
Epoch 3/5, Loss: 2.2839
Epoch 4/5, Loss: 2.2791
Epoch 5/5, Loss: 2.2733
新模型评估：
Accuracy: 36.93%
Model saved to ../../../data/model/mnist_cnn_model
CPC3调整模型中, 本轮训练的数据量为：341.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 36.93%
Epoch 1/5, Loss: 2.2676
Epoch 2/5, Loss: 2.2636
Epoch 3/5, Loss: 2.2630
Epoch 4/5, Loss: 2.2623
Epoch 5/5, Loss: 2.2518
新模型评估：
Accuracy: 38.74%
Model saved to ../../../data/model/mnist_cnn_model
CPC4调整模型中, 本轮训练的数据量为：539.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 38.74%
Epoch 1/5, Loss: 2.2460
Epoch 2/5, Loss: 2.2375
Epoch 3/5, Loss: 2.2273
Epoch 4/5, Loss: 2.2211
Epoch 5/5, Loss: 2.2131
新模型评估：
Accuracy: 43.77%
Model saved to ../../../data/model/mnist_cnn_model
CPC5调整模型中, 本轮训练的数据量为：157.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 43.77%
Epoch 1/5, Loss: 2.2461
Epoch 2/5, Loss: 2.2403
Epoch 3/5, Loss: 2.2466
Epoch 4/5, Loss: 2.2389
Epoch 5/5, Loss: 2.2276
新模型评估：
Accuracy: 42.30%
CPC6调整模型中, 本轮训练的数据量为：538.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 43.77%
Epoch 1/5, Loss: 2.2276
Epoch 2/5, Loss: 2.2172
Epoch 3/5, Loss: 2.2121
Epoch 4/5, Loss: 2.2056
Epoch 5/5, Loss: 2.1883
新模型评估：
Accuracy: 44.88%
Model saved to ../../../data/model/mnist_cnn_model
CPC7调整模型中, 本轮训练的数据量为：188.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 44.88%
Epoch 1/5, Loss: 2.1831
Epoch 2/5, Loss: 2.1792
Epoch 3/5, Loss: 2.1748
Epoch 4/5, Loss: 2.1713
Epoch 5/5, Loss: 2.1656
新模型评估：
Accuracy: 44.76%
CPC8调整模型中, 本轮训练的数据量为：79.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 44.88%
Epoch 1/5, Loss: 2.1552
Epoch 2/5, Loss: 2.1643
Epoch 3/5, Loss: 2.1502
Epoch 4/5, Loss: 2.1397
Epoch 5/5, Loss: 2.1738
新模型评估：
Accuracy: 46.00%
Model saved to ../../../data/model/mnist_cnn_model
CPC9调整模型中, 本轮训练的数据量为：2795.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 46.00%
Epoch 1/5, Loss: 2.1530
Epoch 2/5, Loss: 2.0916
Epoch 3/5, Loss: 2.0093
Epoch 4/5, Loss: 1.9059
Epoch 5/5, Loss: 1.7832
新模型评估：
Accuracy: 57.05%
Model saved to ../../../data/model/mnist_cnn_model
CPC10调整模型中, 本轮训练的数据量为：882.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 57.05%
Epoch 1/5, Loss: 1.6878
Epoch 2/5, Loss: 1.6537
Epoch 3/5, Loss: 1.6193
Epoch 4/5, Loss: 1.5862
Epoch 5/5, Loss: 1.5506
新模型评估：
Accuracy: 62.83%
Model saved to ../../../data/model/mnist_cnn_model
DONE
========================= literation: 2 =========================
----- literation 2: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3359
DataOwner1的最优x_1 = 0.1471
DataOwner2的最优x_2 = 0.1272
DataOwner3的最优x_3 = 0.1253
DataOwner4的最优x_4 = 0.0989
DataOwner5的最优x_5 = 0.1157
DataOwner6的最优x_6 = 0.1316
DataOwner7的最优x_7 = 0.1384
DataOwner8的最优x_8 = 0.0580
DataOwner9的最优x_9 = 0.0891
DataOwner10的最优x_10 = 0.1617
每个DataOwner应该贡献数据比例 xn_list = [0.1471391414137425, 0.12718574779746322, 0.12530010474241382, 0.09885595895445935, 0.11569499889951142, 0.13161348239225265, 0.1383576018712863, 0.057993608198503833, 0.08912259426348496, 0.16174208514360713]
ModelOwner的最大效用 U(Eta) = 0.5750
DONE
最终的列表：
**** log-parameter_analysis 运行时间： 2025-01-18 00:01:25 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.09762852919849112
DataOwner2: noise random: 0.07762487588949622
DataOwner3: noise random: 0.07316004837419555
DataOwner4: noise random: 0.03435927126038773
DataOwner5: noise random: 0.02381142276240572
DataOwner6: noise random: 0.06198857748847456
DataOwner7: noise random: 0.07184154092387193
DataOwner8: noise random: 0.07250891078545167
DataOwner9: noise random: 0.02580131155617642
DataOwner10: noise random: 0.05778103545140544
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9961401461043454, 0.9975671550252484, 0.9978320947369989, 0.9995213018151529, 0.9997705500985905, 0.9984353901037903, 0.9979085778914053, 0.9978773006365703, 0.9997307749076578, 0.9986459328410289]
归一化后的数据质量列表avg_f_list: [0.9, 0.939307165901235, 0.9466049683543625, 0.9931344201958584, 1.0, 0.9632228259742812, 0.9487117078392157, 0.9478501713577506, 0.9989043866468931, 0.969022255943295]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3423
DataOwner1的最优x_1 = 0.0519
DataOwner2的最优x_2 = 0.0993
DataOwner3的最优x_3 = 0.1072
DataOwner4的最优x_4 = 0.1521
DataOwner5的最优x_5 = 0.1579
DataOwner6的最优x_6 = 0.1243
DataOwner7的最优x_7 = 0.1094
DataOwner8的最优x_8 = 0.1085
DataOwner9的最优x_9 = 0.1570
DataOwner10的最优x_10 = 0.1300
每个DataOwner应该贡献数据比例 xn_list = [0.05185166758054109, 0.09925680527899347, 0.10717510277140546, 0.15206429619181722, 0.15794365952033243, 0.12427573328346891, 0.1094135136019216, 0.10850066520684538, 0.15701723555063024, 0.12995351028079152]
ModelOwner的最大效用 U(Eta) = 0.5823
DONE
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3422907221722253
DataOwner1的分配到的支付 ： 0.0540
DataOwner2的分配到的支付 ： 0.1079
DataOwner3的分配到的支付 ： 0.1175
DataOwner4的分配到的支付 ： 0.1748
DataOwner5的分配到的支付 ： 0.1829
DataOwner6的分配到的支付 ： 0.1386
DataOwner7的分配到的支付 ： 0.1202
DataOwner8的分配到的支付 ： 0.1191
DataOwner9的分配到的支付 ： 0.1816
DataOwner10的分配到的支付 ： 0.1458
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC2', 'DataOwner9': 'CPC1', 'DataOwner2': 'CPC3', 'DataOwner3': 'CPC4', 'DataOwner4': 'CPC5', 'DataOwner5': 'CPC10', 'DataOwner6': 'CPC6', 'DataOwner7': 'CPC7', 'DataOwner8': 'CPC8', 'DataOwner10': 'CPC9'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC2
DataOwner9 把数据交给 CPC1
DataOwner2 把数据交给 CPC3
DataOwner3 把数据交给 CPC4
DataOwner4 把数据交给 CPC5
DataOwner5 把数据交给 CPC10
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC8
DataOwner10 把数据交给 CPC9
DONE
----- literation 1: 模型训练 -----
CPC2调整模型中, 本轮训练的数据量为：259.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 26.82%
Epoch 1/5, Loss: 2.3074
Epoch 2/5, Loss: 2.3178
Epoch 3/5, Loss: 2.3005
Epoch 4/5, Loss: 2.2967
Epoch 5/5, Loss: 2.3095
新模型评估：
Accuracy: 29.44%
Model saved to ../../../data/model/mnist_cnn_model
CPC1调整模型中, 本轮训练的数据量为：1099.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 29.44%
Epoch 1/5, Loss: 2.2915
Epoch 2/5, Loss: 2.2852
Epoch 3/5, Loss: 2.2786
Epoch 4/5, Loss: 2.2697
Epoch 5/5, Loss: 2.2551
新模型评估：
Accuracy: 43.47%
Model saved to ../../../data/model/mnist_cnn_model
CPC3调整模型中, 本轮训练的数据量为：496.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 43.47%
Epoch 1/5, Loss: 2.2526
Epoch 2/5, Loss: 2.2453
Epoch 3/5, Loss: 2.2397
Epoch 4/5, Loss: 2.2335
Epoch 5/5, Loss: 2.2269
新模型评估：
Accuracy: 43.30%
CPC4调整模型中, 本轮训练的数据量为：1071.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 43.47%
Epoch 1/5, Loss: 2.2477
Epoch 2/5, Loss: 2.2347
Epoch 3/5, Loss: 2.2198
Epoch 4/5, Loss: 2.2012
Epoch 5/5, Loss: 2.1813
新模型评估：
Accuracy: 49.64%
Model saved to ../../../data/model/mnist_cnn_model
CPC5调整模型中, 本轮训练的数据量为：304.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 49.64%
Epoch 1/5, Loss: 2.1789
Epoch 2/5, Loss: 2.1730
Epoch 3/5, Loss: 2.1659
Epoch 4/5, Loss: 2.1609
Epoch 5/5, Loss: 2.1550
新模型评估：
Accuracy: 50.67%
Model saved to ../../../data/model/mnist_cnn_model
CPC10调整模型中, 本轮训练的数据量为：789.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 50.67%
Epoch 1/5, Loss: 2.1451
Epoch 2/5, Loss: 2.1254
Epoch 3/5, Loss: 2.1094
Epoch 4/5, Loss: 2.0886
Epoch 5/5, Loss: 2.0644
新模型评估：
Accuracy: 53.87%
Model saved to ../../../data/model/mnist_cnn_model
CPC6调整模型中, 本轮训练的数据量为：124.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 53.87%
Epoch 1/5, Loss: 2.0557
Epoch 2/5, Loss: 2.0471
Epoch 3/5, Loss: 2.0439
Epoch 4/5, Loss: 2.0406
Epoch 5/5, Loss: 2.0359
新模型评估：
Accuracy: 53.59%
CPC7调整模型中, 本轮训练的数据量为：1422.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 53.87%
Epoch 1/5, Loss: 2.0343
Epoch 2/5, Loss: 1.9901
Epoch 3/5, Loss: 1.9500
Epoch 4/5, Loss: 1.8888
Epoch 5/5, Loss: 1.8349
新模型评估：
Accuracy: 56.96%
Model saved to ../../../data/model/mnist_cnn_model
CPC8调整模型中, 本轮训练的数据量为：1193.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 56.96%
Epoch 1/5, Loss: 1.7950
Epoch 2/5, Loss: 1.7487
Epoch 3/5, Loss: 1.7028
Epoch 4/5, Loss: 1.6609
Epoch 5/5, Loss: 1.6135
新模型评估：
Accuracy: 62.18%
Model saved to ../../../data/model/mnist_cnn_model
CPC9调整模型中, 本轮训练的数据量为：129.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 62.18%
Epoch 1/5, Loss: 1.7141
Epoch 2/5, Loss: 1.4350
Epoch 3/5, Loss: 1.5308
Epoch 4/5, Loss: 1.3926
Epoch 5/5, Loss: 1.7408
新模型评估：
Accuracy: 61.34%
DONE
========================= literation: 2 =========================
----- literation 2: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3423
DataOwner1的最优x_1 = 0.0519
DataOwner2的最优x_2 = 0.0993
DataOwner3的最优x_3 = 0.1072
DataOwner4的最优x_4 = 0.1521
DataOwner5的最优x_5 = 0.1579
DataOwner6的最优x_6 = 0.1243
DataOwner7的最优x_7 = 0.1094
DataOwner8的最优x_8 = 0.1085
DataOwner9的最优x_9 = 0.1570
DataOwner10的最优x_10 = 0.1300
每个DataOwner应该贡献数据比例 xn_list = [0.05185166758054109, 0.09925680527899347, 0.10717510277140546, 0.15206429619181722, 0.15794365952033243, 0.12427573328346891, 0.1094135136019216, 0.10850066520684538, 0.15701723555063024, 0.12995351028079152]
ModelOwner的最大效用 U(Eta) = 0.5823
DONE
----- literation 2: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3422907221722253
DataOwner1的分配到的支付 ： 0.0540
DataOwner2的分配到的支付 ： 0.1079
DataOwner3的分配到的支付 ： 0.1175
DataOwner4的分配到的支付 ： 0.1748
DataOwner5的分配到的支付 ： 0.1829
DataOwner6的分配到的支付 ： 0.1386
DataOwner7的分配到的支付 ： 0.1202
DataOwner8的分配到的支付 ： 0.1191
DataOwner9的分配到的支付 ： 0.1816
DataOwner10的分配到的支付 ： 0.1458
DONE
----- literation 2: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC2
DataOwner9 把数据交给 CPC1
DataOwner2 把数据交给 CPC3
DataOwner3 把数据交给 CPC4
DataOwner4 把数据交给 CPC5
DataOwner5 把数据交给 CPC10
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC8
DataOwner10 把数据交给 CPC9
DONE
----- literation 2: 模型训练 -----
重新调整fn，进而调整xn、Eta
正在评估DataOwner1的数据质量, 本轮评估的样本数据量为：259.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 62.18%
Epoch 1/5, Loss: 1.5828
Epoch 2/5, Loss: 1.6244
Epoch 3/5, Loss: 1.5924
Epoch 4/5, Loss: 1.6171
Epoch 5/5, Loss: 1.6916
新模型评估：
Accuracy: 61.38%
loss差为：
-0.1087210893630981
单位数据loss差为：
-0.0004197725458034676
正在评估DataOwner9的数据质量, 本轮评估的样本数据量为：1099.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 62.18%
Epoch 1/5, Loss: 1.5566
Epoch 2/5, Loss: 1.5148
Epoch 3/5, Loss: 1.4846
Epoch 4/5, Loss: 1.4484
Epoch 5/5, Loss: 1.4038
新模型评估：
Accuracy: 65.49%
loss差为：
0.15274850527445483
单位数据loss差为：
0.00013898863082297984
正在评估DataOwner2的数据质量, 本轮评估的样本数据量为：496.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 62.18%
Epoch 1/5, Loss: 1.6212
Epoch 2/5, Loss: 1.5934
Epoch 3/5, Loss: 1.5709
Epoch 4/5, Loss: 1.5605
Epoch 5/5, Loss: 1.5355
新模型评估：
Accuracy: 63.56%
loss差为：
0.08576446771621704
单位数据loss差为：
0.00017291223329882468
正在评估DataOwner3的数据质量, 本轮评估的样本数据量为：1071.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 62.18%
Epoch 1/5, Loss: 1.5790
Epoch 2/5, Loss: 1.5361
Epoch 3/5, Loss: 1.4962
Epoch 4/5, Loss: 1.4558
Epoch 5/5, Loss: 1.4169
新模型评估：
Accuracy: 67.57%
loss差为：
0.16213330801795522
单位数据loss差为：
0.0001513849748066809
正在评估DataOwner4的数据质量, 本轮评估的样本数据量为：304.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 62.18%
Epoch 1/5, Loss: 1.5782
Epoch 2/5, Loss: 1.5657
Epoch 3/5, Loss: 1.5478
Epoch 4/5, Loss: 1.5343
Epoch 5/5, Loss: 1.5255
新模型评估：
Accuracy: 64.43%
loss差为：
0.05271513462066646
单位数据loss差为：
0.00017340504809429757
正在评估DataOwner5的数据质量, 本轮评估的样本数据量为：789.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 62.18%
Epoch 1/5, Loss: 1.5945
Epoch 2/5, Loss: 1.5566
Epoch 3/5, Loss: 1.5330
Epoch 4/5, Loss: 1.5034
Epoch 5/5, Loss: 1.4716
新模型评估：
Accuracy: 64.63%
loss差为：
0.12298402419457055
单位数据loss差为：
0.00015587328795256091
正在评估DataOwner6的数据质量, 本轮评估的样本数据量为：124.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 62.18%
Epoch 1/5, Loss: 1.6618
Epoch 2/5, Loss: 1.6530
Epoch 3/5, Loss: 1.6421
Epoch 4/5, Loss: 1.6340
Epoch 5/5, Loss: 1.6312
新模型评估：
Accuracy: 61.74%
loss差为：
0.03065413236618042
单位数据loss差为：
0.00024721074488855176
正在评估DataOwner7的数据质量, 本轮评估的样本数据量为：1422.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 62.18%
Epoch 1/5, Loss: 1.5811
Epoch 2/5, Loss: 1.5230
Epoch 3/5, Loss: 1.4741
Epoch 4/5, Loss: 1.4200
Epoch 5/5, Loss: 1.3620
新模型评估：
Accuracy: 68.73%
loss差为：
0.21917538020921778
单位数据loss差为：
0.00015413177229902798
正在评估DataOwner8的数据质量, 本轮评估的样本数据量为：1193.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 62.18%
Epoch 1/5, Loss: 1.5616
Epoch 2/5, Loss: 1.5133
Epoch 3/5, Loss: 1.4659
Epoch 4/5, Loss: 1.4250
Epoch 5/5, Loss: 1.3818
新模型评估：
Accuracy: 65.30%
loss差为：
0.17979364018691202
单位数据loss差为：
0.00015070715858081477
正在评估DataOwner10的数据质量, 本轮评估的样本数据量为：129.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 62.18%
Epoch 1/5, Loss: 1.4502
Epoch 2/5, Loss: 1.6423
Epoch 3/5, Loss: 1.3549
Epoch 4/5, Loss: 1.6101
Epoch 5/5, Loss: 1.3449
新模型评估：
Accuracy: 63.98%
loss差为：
0.1053011417388916
单位数据loss差为：
0.000816287920456524
经过服务器调节后的真实数据质量：
数据质量列表avg_f_list: [-0.0004197725458034676, 0.00017291223329882468, 0.0001513849748066809, 0.00017340504809429757, 0.00015587328795256091, 0.00024721074488855176, 0.00015413177229902798, 0.00015070715858081477, 0.00013898863082297984, 0.000816287920456524]
归一化后的数据质量列表avg_f_list:[0.9, 0.9479494972358115, 0.9462078948563356, 0.9479893670325508, 0.9465710092239895, 0.9539604096157321, 0.9464301167918577, 0.9461530580385287, 0.9452050034669517, 1.0]
CPC2调整模型中, 本轮训练的数据量为：259.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 62.18%
Epoch 1/5, Loss: 1.5541
Epoch 2/5, Loss: 1.7054
Epoch 3/5, Loss: 1.5911
Epoch 4/5, Loss: 1.6238
Epoch 5/5, Loss: 1.5827
新模型评估：
Accuracy: 62.24%
Model saved to ../../../data/model/mnist_cnn_model
CPC1调整模型中, 本轮训练的数据量为：1099.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 62.24%
Epoch 1/5, Loss: 1.5190
Epoch 2/5, Loss: 1.4759
Epoch 3/5, Loss: 1.4388
Epoch 4/5, Loss: 1.3951
Epoch 5/5, Loss: 1.3629
新模型评估：
Accuracy: 66.61%
Model saved to ../../../data/model/mnist_cnn_model
CPC3调整模型中, 本轮训练的数据量为：496.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 66.61%
Epoch 1/5, Loss: 1.3904
Epoch 2/5, Loss: 1.3633
Epoch 3/5, Loss: 1.3407
Epoch 4/5, Loss: 1.3305
Epoch 5/5, Loss: 1.3052
新模型评估：
Accuracy: 67.38%
Model saved to ../../../data/model/mnist_cnn_model
CPC4调整模型中, 本轮训练的数据量为：1071.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 67.38%
Epoch 1/5, Loss: 1.2578
Epoch 2/5, Loss: 1.2142
Epoch 3/5, Loss: 1.1758
Epoch 4/5, Loss: 1.1408
Epoch 5/5, Loss: 1.1051
新模型评估：
Accuracy: 72.49%
Model saved to ../../../data/model/mnist_cnn_model
CPC5调整模型中, 本轮训练的数据量为：304.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 72.49%
Epoch 1/5, Loss: 1.0755
Epoch 2/5, Loss: 1.0616
Epoch 3/5, Loss: 1.0537
Epoch 4/5, Loss: 1.0361
Epoch 5/5, Loss: 1.0242
新模型评估：
Accuracy: 72.99%
Model saved to ../../../data/model/mnist_cnn_model
CPC10调整模型中, 本轮训练的数据量为：789.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 72.99%
Epoch 1/5, Loss: 1.0777
Epoch 2/5, Loss: 1.0553
Epoch 3/5, Loss: 1.0224
Epoch 4/5, Loss: 0.9826
Epoch 5/5, Loss: 0.9633
新模型评估：
Accuracy: 75.30%
Model saved to ../../../data/model/mnist_cnn_model
CPC6调整模型中, 本轮训练的数据量为：124.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 75.30%
Epoch 1/5, Loss: 1.0369
Epoch 2/5, Loss: 1.0282
Epoch 3/5, Loss: 1.0195
Epoch 4/5, Loss: 1.0151
Epoch 5/5, Loss: 1.0060
新模型评估：
Accuracy: 73.89%
CPC7调整模型中, 本轮训练的数据量为：1422.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 75.30%
Epoch 1/5, Loss: 0.9287
Epoch 2/5, Loss: 0.8790
Epoch 3/5, Loss: 0.8542
Epoch 4/5, Loss: 0.8089
Epoch 5/5, Loss: 0.7978
新模型评估：
Accuracy: 75.37%
Model saved to ../../../data/model/mnist_cnn_model
CPC8调整模型中, 本轮训练的数据量为：1193.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 75.37%
Epoch 1/5, Loss: 0.7580
Epoch 2/5, Loss: 0.7300
Epoch 3/5, Loss: 0.7103
Epoch 4/5, Loss: 0.6849
Epoch 5/5, Loss: 0.6633
新模型评估：
Accuracy: 75.46%
Model saved to ../../../data/model/mnist_cnn_model
CPC9调整模型中, 本轮训练的数据量为：129.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 75.46%
Epoch 1/5, Loss: 0.4976
Epoch 2/5, Loss: 0.5123
Epoch 3/5, Loss: 0.4806
Epoch 4/5, Loss: 0.5299
Epoch 5/5, Loss: 0.5799
新模型评估：
Accuracy: 76.92%
Model saved to ../../../data/model/mnist_cnn_model
DONE
========================= literation: 3 =========================
----- literation 3: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3273
DataOwner1的最优x_1 = 0.0660
DataOwner2的最优x_2 = 0.1199
DataOwner3的最优x_3 = 0.1181
DataOwner4的最优x_4 = 0.1199
DataOwner5的最优x_5 = 0.1185
DataOwner6的最优x_6 = 0.1259
DataOwner7的最优x_7 = 0.1184
DataOwner8的最优x_8 = 0.1181
DataOwner9的最优x_9 = 0.1171
DataOwner10的最优x_10 = 0.1667
每个DataOwner应该贡献数据比例 xn_list = [0.06600935461483472, 0.11989869202643164, 0.11813862778701073, 0.11993882369399153, 0.11850672694028846, 0.12586885177046955, 0.1183639715621538, 0.1180829860287108, 0.11711884701615327, 0.16665828752058792]
ModelOwner的最大效用 U(Eta) = 0.5652
xn开始变化：
new_xn_list: [0.06600935461483472, 0.11989869202643164, 0.11813862778701073, 0.15206429619181722, 0.15794365952033243, 0.12586885177046955, 0.1183639715621538, 0.1180829860287108, 0.15701723555063024, 0.16665828752058792]
avg_f_list: [0.9, 0.9479494972358115, 0.9462078948563356, 0.9479893670325508, 0.9465710092239895, 0.9539604096157321, 0.9464301167918577, 0.9461530580385287, 0.9452050034669517, 1.0]
============= xn0: 0.01 =============
new_qn: 0.009000000000000001
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.1869953035623877
new_xn: 0.01
Lambda: 1
Rho: 1
============= xn0: 0.02 =============
new_qn: 0.018000000000000002
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.1959953035623878
new_xn: 0.02
Lambda: 1
Rho: 1
============= xn0: 0.03 =============
new_qn: 0.027
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.2049953035623877
new_xn: 0.03
Lambda: 1
Rho: 1
============= xn0: 0.04 =============
new_qn: 0.036000000000000004
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.2139953035623878
new_xn: 0.04
Lambda: 1
Rho: 1
============= xn0: 0.05 =============
new_qn: 0.045000000000000005
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.2229953035623877
new_xn: 0.05
Lambda: 1
Rho: 1
============= xn0: 0.060000000000000005 =============
new_qn: 0.054000000000000006
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.2319953035623878
new_xn: 0.060000000000000005
Lambda: 1
Rho: 1
============= xn0: 0.06999999999999999 =============
new_qn: 0.063
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.2409953035623877
new_xn: 0.06999999999999999
Lambda: 1
Rho: 1
============= xn0: 0.08 =============
new_qn: 0.07200000000000001
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.2499953035623879
new_xn: 0.08
Lambda: 1
Rho: 1
============= xn0: 0.09 =============
new_qn: 0.081
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.2589953035623878
new_xn: 0.09
Lambda: 1
Rho: 1
============= xn0: 0.09999999999999999 =============
new_qn: 0.09
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.2679953035623877
new_xn: 0.09999999999999999
Lambda: 1
Rho: 1
============= xn0: 0.11 =============
new_qn: 0.099
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.2769953035623878
new_xn: 0.11
Lambda: 1
Rho: 1
============= xn0: 0.12 =============
new_qn: 0.108
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.2859953035623877
new_xn: 0.12
Lambda: 1
Rho: 1
============= xn0: 0.13 =============
new_qn: 0.117
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.2949953035623878
new_xn: 0.13
Lambda: 1
Rho: 1
============= xn0: 0.14 =============
new_qn: 0.12600000000000003
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.3039953035623877
new_xn: 0.14
Lambda: 1
Rho: 1
============= xn0: 0.15000000000000002 =============
new_qn: 0.13500000000000004
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.3129953035623878
new_xn: 0.15000000000000002
Lambda: 1
Rho: 1
============= xn0: 0.16 =============
new_qn: 0.14400000000000002
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.3219953035623877
new_xn: 0.16
Lambda: 1
Rho: 1
============= xn0: 0.17 =============
new_qn: 0.15300000000000002
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.3309953035623878
new_xn: 0.17
Lambda: 1
Rho: 1
============= xn0: 0.18000000000000002 =============
new_qn: 0.16200000000000003
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.3399953035623877
new_xn: 0.18000000000000002
Lambda: 1
Rho: 1
============= xn0: 0.19 =============
new_qn: 0.171
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.3489953035623878
new_xn: 0.19
Lambda: 1
Rho: 1
============= xn0: 0.2 =============
new_qn: 0.18000000000000002
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.3579953035623877
new_xn: 0.2
Lambda: 1
Rho: 1
============= xn0: 0.21000000000000002 =============
new_qn: 0.18900000000000003
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.3669953035623879
new_xn: 0.21000000000000002
Lambda: 1
Rho: 1
============= xn0: 0.22 =============
new_qn: 0.198
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.3759953035623878
new_xn: 0.22
Lambda: 1
Rho: 1
============= xn0: 0.23 =============
new_qn: 0.20700000000000002
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.3849953035623879
new_xn: 0.23
Lambda: 1
Rho: 1
============= xn0: 0.24000000000000002 =============
new_qn: 0.21600000000000003
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.3939953035623878
new_xn: 0.24000000000000002
Lambda: 1
Rho: 1
============= xn0: 0.25 =============
new_qn: 0.225
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.4029953035623877
new_xn: 0.25
Lambda: 1
Rho: 1
============= xn0: 0.26 =============
new_qn: 0.234
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.4119953035623878
new_xn: 0.26
Lambda: 1
Rho: 1
============= xn0: 0.27 =============
new_qn: 0.24300000000000002
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.4209953035623877
new_xn: 0.27
Lambda: 1
Rho: 1
============= xn0: 0.28 =============
new_qn: 0.25200000000000006
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.4299953035623878
new_xn: 0.28
Lambda: 1
Rho: 1
============= xn0: 0.29000000000000004 =============
new_qn: 0.26100000000000007
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.438995303562388
new_xn: 0.29000000000000004
Lambda: 1
Rho: 1
============= xn0: 0.3 =============
new_qn: 0.27
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.4479953035623878
new_xn: 0.3
Lambda: 1
Rho: 1
============= xn0: 0.31 =============
new_qn: 0.279
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.4569953035623877
new_xn: 0.31
Lambda: 1
Rho: 1
============= xn0: 0.32 =============
new_qn: 0.28800000000000003
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.4659953035623878
new_xn: 0.32
Lambda: 1
Rho: 1
============= xn0: 0.33 =============
new_qn: 0.29700000000000004
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.4749953035623877
new_xn: 0.33
Lambda: 1
Rho: 1
============= xn0: 0.34 =============
new_qn: 0.30600000000000005
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.4839953035623878
new_xn: 0.34
Lambda: 1
Rho: 1
============= xn0: 0.35000000000000003 =============
new_qn: 0.31500000000000006
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.4929953035623877
new_xn: 0.35000000000000003
Lambda: 1
Rho: 1
============= xn0: 0.36000000000000004 =============
new_qn: 0.32400000000000007
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.5019953035623879
new_xn: 0.36000000000000004
Lambda: 1
Rho: 1
============= xn0: 0.37 =============
new_qn: 0.333
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.5109953035623878
new_xn: 0.37
Lambda: 1
Rho: 1
============= xn0: 0.38 =============
new_qn: 0.342
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.5199953035623879
new_xn: 0.38
Lambda: 1
Rho: 1
============= xn0: 0.39 =============
new_qn: 0.35100000000000003
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.5289953035623878
new_xn: 0.39
Lambda: 1
Rho: 1
============= xn0: 0.4 =============
new_qn: 0.36000000000000004
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.537995303562388
new_xn: 0.4
Lambda: 1
Rho: 1
============= xn0: 0.41000000000000003 =============
new_qn: 0.36900000000000005
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.5469953035623878
new_xn: 0.41000000000000003
Lambda: 1
Rho: 1
============= xn0: 0.42000000000000004 =============
new_qn: 0.37800000000000006
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.555995303562388
new_xn: 0.42000000000000004
Lambda: 1
Rho: 1
============= xn0: 0.43 =============
new_qn: 0.387
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.5649953035623878
new_xn: 0.43
Lambda: 1
Rho: 1
============= xn0: 0.44 =============
new_qn: 0.396
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.5739953035623877
new_xn: 0.44
Lambda: 1
Rho: 1
============= xn0: 0.45 =============
new_qn: 0.405
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.5829953035623878
new_xn: 0.45
Lambda: 1
Rho: 1
============= xn0: 0.46 =============
new_qn: 0.41400000000000003
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.5919953035623877
new_xn: 0.46
Lambda: 1
Rho: 1
============= xn0: 0.47000000000000003 =============
new_qn: 0.42300000000000004
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.6009953035623878
new_xn: 0.47000000000000003
Lambda: 1
Rho: 1
============= xn0: 0.48000000000000004 =============
new_qn: 0.43200000000000005
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.6099953035623877
new_xn: 0.48000000000000004
Lambda: 1
Rho: 1
============= xn0: 0.49 =============
new_qn: 0.441
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.6189953035623876
new_xn: 0.49
Lambda: 1
Rho: 1
============= xn0: 0.5 =============
new_qn: 0.45
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.6279953035623878
new_xn: 0.5
Lambda: 1
Rho: 1
============= xn0: 0.51 =============
new_qn: 0.459
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.6369953035623876
new_xn: 0.51
Lambda: 1
Rho: 1
============= xn0: 0.52 =============
new_qn: 0.468
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.6459953035623878
new_xn: 0.52
Lambda: 1
Rho: 1
============= xn0: 0.53 =============
new_qn: 0.47700000000000004
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.6549953035623877
new_xn: 0.53
Lambda: 1
Rho: 1
============= xn0: 0.54 =============
new_qn: 0.48600000000000004
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.6639953035623878
new_xn: 0.54
Lambda: 1
Rho: 1
============= xn0: 0.55 =============
new_qn: 0.49500000000000005
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.6729953035623877
new_xn: 0.55
Lambda: 1
Rho: 1
============= xn0: 0.56 =============
new_qn: 0.5040000000000001
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.6819953035623878
new_xn: 0.56
Lambda: 1
Rho: 1
============= xn0: 0.5700000000000001 =============
new_qn: 0.5130000000000001
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.690995303562388
new_xn: 0.5700000000000001
Lambda: 1
Rho: 1
============= xn0: 0.5800000000000001 =============
new_qn: 0.5220000000000001
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.6999953035623878
new_xn: 0.5800000000000001
Lambda: 1
Rho: 1
============= xn0: 0.59 =============
new_qn: 0.531
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.7089953035623877
new_xn: 0.59
Lambda: 1
Rho: 1
============= xn0: 0.6 =============
new_qn: 0.54
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.7179953035623878
new_xn: 0.6
Lambda: 1
Rho: 1
============= xn0: 0.61 =============
new_qn: 0.549
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.7269953035623877
new_xn: 0.61
Lambda: 1
Rho: 1
============= xn0: 0.62 =============
new_qn: 0.558
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.7359953035623878
new_xn: 0.62
Lambda: 1
Rho: 1
============= xn0: 0.63 =============
new_qn: 0.5670000000000001
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.7449953035623877
new_xn: 0.63
Lambda: 1
Rho: 1
============= xn0: 0.64 =============
new_qn: 0.5760000000000001
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.7539953035623879
new_xn: 0.64
Lambda: 1
Rho: 1
============= xn0: 0.65 =============
new_qn: 0.5850000000000001
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.7629953035623878
new_xn: 0.65
Lambda: 1
Rho: 1
============= xn0: 0.66 =============
new_qn: 0.5940000000000001
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.7719953035623879
new_xn: 0.66
Lambda: 1
Rho: 1
============= xn0: 0.67 =============
new_qn: 0.6030000000000001
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.7809953035623878
new_xn: 0.67
Lambda: 1
Rho: 1
============= xn0: 0.68 =============
new_qn: 0.6120000000000001
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.789995303562388
new_xn: 0.68
Lambda: 1
Rho: 1
============= xn0: 0.6900000000000001 =============
new_qn: 0.6210000000000001
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.7989953035623878
new_xn: 0.6900000000000001
Lambda: 1
Rho: 1
============= xn0: 0.7000000000000001 =============
new_qn: 0.6300000000000001
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.807995303562388
new_xn: 0.7000000000000001
Lambda: 1
Rho: 1
============= xn0: 0.7100000000000001 =============
new_qn: 0.6390000000000001
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.8169953035623878
new_xn: 0.7100000000000001
Lambda: 1
Rho: 1
============= xn0: 0.72 =============
new_qn: 0.648
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.8259953035623877
new_xn: 0.72
Lambda: 1
Rho: 1
============= xn0: 0.73 =============
new_qn: 0.657
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.8349953035623878
new_xn: 0.73
Lambda: 1
Rho: 1
============= xn0: 0.74 =============
new_qn: 0.666
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.8439953035623877
new_xn: 0.74
Lambda: 1
Rho: 1
============= xn0: 0.75 =============
new_qn: 0.675
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.8529953035623878
new_xn: 0.75
Lambda: 1
Rho: 1
============= xn0: 0.76 =============
new_qn: 0.684
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.8619953035623877
new_xn: 0.76
Lambda: 1
Rho: 1
============= xn0: 0.77 =============
new_qn: 0.6930000000000001
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.8709953035623879
new_xn: 0.77
Lambda: 1
Rho: 1
============= xn0: 0.78 =============
new_qn: 0.7020000000000001
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.8799953035623878
new_xn: 0.78
Lambda: 1
Rho: 1
============= xn0: 0.79 =============
new_qn: 0.7110000000000001
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.8889953035623879
new_xn: 0.79
Lambda: 1
Rho: 1
============= xn0: 0.8 =============
new_qn: 0.7200000000000001
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.8979953035623878
new_xn: 0.8
Lambda: 1
Rho: 1
============= xn0: 0.81 =============
new_qn: 0.7290000000000001
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.9069953035623879
new_xn: 0.81
Lambda: 1
Rho: 1
============= xn0: 0.8200000000000001 =============
new_qn: 0.7380000000000001
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.9159953035623878
new_xn: 0.8200000000000001
Lambda: 1
Rho: 1
============= xn0: 0.8300000000000001 =============
new_qn: 0.7470000000000001
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.924995303562388
new_xn: 0.8300000000000001
Lambda: 1
Rho: 1
============= xn0: 0.8400000000000001 =============
new_qn: 0.7560000000000001
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.9339953035623878
new_xn: 0.8400000000000001
Lambda: 1
Rho: 1
============= xn0: 0.85 =============
new_qn: 0.765
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.9429953035623877
new_xn: 0.85
Lambda: 1
Rho: 1
============= xn0: 0.86 =============
new_qn: 0.774
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.9519953035623878
new_xn: 0.86
Lambda: 1
Rho: 1
============= xn0: 0.87 =============
new_qn: 0.783
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.9609953035623877
new_xn: 0.87
Lambda: 1
Rho: 1
============= xn0: 0.88 =============
new_qn: 0.792
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.9699953035623878
new_xn: 0.88
Lambda: 1
Rho: 1
============= xn0: 0.89 =============
new_qn: 0.801
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.9789953035623877
new_xn: 0.89
Lambda: 1
Rho: 1
============= xn0: 0.9 =============
new_qn: 0.81
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.9879953035623878
new_xn: 0.9
Lambda: 1
Rho: 1
============= xn0: 0.91 =============
new_qn: 0.8190000000000001
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.9969953035623877
new_xn: 0.91
Lambda: 1
Rho: 1
============= xn0: 0.92 =============
new_qn: 0.8280000000000001
best_Eta: 1.3273403386954499
sum(new_qn_list): 2.0059953035623876
new_xn: 0.92
Lambda: 1
Rho: 1
============= xn0: 0.93 =============
new_qn: 0.8370000000000001
best_Eta: 1.3273403386954499
sum(new_qn_list): 2.0149953035623875
new_xn: 0.93
Lambda: 1
Rho: 1
============= xn0: 0.9400000000000001 =============
new_qn: 0.8460000000000001
best_Eta: 1.3273403386954499
sum(new_qn_list): 2.023995303562388
new_xn: 0.9400000000000001
Lambda: 1
Rho: 1
============= xn0: 0.9500000000000001 =============
new_qn: 0.8550000000000001
best_Eta: 1.3273403386954499
sum(new_qn_list): 2.032995303562388
new_xn: 0.9500000000000001
Lambda: 1
Rho: 1
============= xn0: 0.9600000000000001 =============
new_qn: 0.8640000000000001
best_Eta: 1.3273403386954499
sum(new_qn_list): 2.0419953035623877
new_xn: 0.9600000000000001
Lambda: 1
Rho: 1
============= xn0: 0.97 =============
new_qn: 0.873
best_Eta: 1.3273403386954499
sum(new_qn_list): 2.0509953035623876
new_xn: 0.97
Lambda: 1
Rho: 1
============= xn0: 0.98 =============
new_qn: 0.882
best_Eta: 1.3273403386954499
sum(new_qn_list): 2.0599953035623875
new_xn: 0.98
Lambda: 1
Rho: 1
============= xn0: 0.99 =============
new_qn: 0.891
best_Eta: 1.3273403386954499
sum(new_qn_list): 2.0689953035623874
new_xn: 0.99
Lambda: 1
Rho: 1
============= xn0: 1.0 =============
new_qn: 0.9
best_Eta: 1.3273403386954499
sum(new_qn_list): 2.0779953035623877
new_xn: 1.0
Lambda: 1
Rho: 1
DONE
最终的列表：
[6.411989365649046e-05, -2.322749482954259e-05, -0.00025864827951866537, -0.000638849217277429, -0.0011606340046355912, -0.0018208997369569754, -0.0026166335216840364, -0.003544909237890956, -0.004602884434823706, -0.0057877973625216905, -0.007096964128004993, -0.008527775970875381, -0.010077696652522311, -0.01174425995344458, -0.013525067273501196, -0.015417785330184675, -0.017420143950278993, -0.019529933950509892, -0.021745005103033066, -0.02406326418182292, -0.02648267308623506, -0.029001247038207556, -0.031617052849752525, -0.03432820725756072, -0.037132875321705616, -0.0400292688855875, -0.04301564509440106, -0.04609030496955033, -0.049251592036559655, -0.05249789100415381, -0.055827626492295446, -0.05923926180707484, -0.06273129776045128, -0.06630227153294205, -0.06995075557744707, -0.07367535656248303, -0.0774747143531841, -0.08134750102850458, -0.08529241993313103, -0.08930820476268209, -0.09339361868084134, -0.09754745346712868, -0.10176852869407893, -0.10605569093264822, -0.11040781298472702, -0.11482379314168528, -0.11930255446792698, -0.12384304410847341, -0.12844423261964277, -0.13310511332192992, -0.13782470167423283, -0.14260203466860893, -0.14743617024478023, -0.1523261867236379, -0.15727118225903242, -0.16227027430716406, -0.16732259911291725, -0.17242731121251165, -0.17758358295186838, -0.182790604020115, -0.18804758099767616, -0.19335373691842273, -0.19870831084536905, -0.20411055745943446, -0.2095597466607998, -0.21505516318241297, -0.2205961062152127, -0.2261818890446588, -0.23181183869817196, -0.23748529560310433, -0.24320161325487433, -0.24896015789491627, -0.2547603081981068, -0.260601454969345, -0.2664830008489748, -0.27240436002675034, -0.2783649579640568, -0.2843642311241102, -0.2904016267098697, -0.2964766024094062, -0.3025886261484825, -0.30873717585010485, -0.31492173920082167, -0.32114181342354553, -0.3273969050566904, -0.33368652973941815, -0.3400102120027987, -0.3463674850666951, -0.35275789064219065, -0.3591809787393826, -0.3656363074803737, -0.37212344291729704, -0.3786419588552189, -0.3851914366797654, -0.3917714651893278, -0.3983816404317061, -0.4050215655450523, -0.4116908506029846, -0.4183891124637419, -0.42511597462326056]
**** log-parameter_analysis 运行时间： 2025-01-18 00:18:45 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.02784850326365579
DataOwner2: noise random: 0.06629090374061021
DataOwner3: noise random: 0.08276295014073162
DataOwner4: noise random: 0.08906825438273014
DataOwner5: noise random: 0.0850626027297014
DataOwner6: noise random: 0.037112498415870325
DataOwner7: noise random: 0.013223230663772158
DataOwner8: noise random: 0.030552238669615796
DataOwner9: noise random: 0.015201265796472197
DataOwner10: noise random: 0.09195623362875928
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9996861425865886, 0.9982218453159829, 0.9972287400907122, 0.9968014650309558, 0.9970694251847845, 0.999440235129353, 0.9999293431665475, 0.9996223425617536, 0.99990633953505, 0.9965845841479302]
归一化后的数据质量列表avg_f_list: [0.9927289057715316, 0.9489500486863047, 0.9192586652490226, 0.9064842005602928, 0.9144955446462849, 0.9853768826252638, 1.0, 0.9908214432464334, 0.9993122484648553, 0.9]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3354
DataOwner1的最优x_1 = 0.1560
DataOwner2的最优x_2 = 0.1150
DataOwner3的最优x_3 = 0.0822
DataOwner4的最优x_4 = 0.0667
DataOwner5的最优x_5 = 0.0766
DataOwner6的最优x_6 = 0.1496
DataOwner7的最优x_7 = 0.1621
DataOwner8的最优x_8 = 0.1544
DataOwner9的最优x_9 = 0.1615
DataOwner10的最优x_10 = 0.0585
每个DataOwner应该贡献数据比例 xn_list = [0.15599495209799633, 0.11497572308425803, 0.08223454473181768, 0.06674290310425791, 0.07656280385233347, 0.14964928708103348, 0.16207197763123637, 0.15436828051260218, 0.1615054529648605, 0.05852906742201197]
ModelOwner的最大效用 U(Eta) = 0.5744
DONE
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.335385497747093
DataOwner1的分配到的支付 ： 0.1804
DataOwner2的分配到的支付 ： 0.1271
DataOwner3的分配到的支付 ： 0.0880
DataOwner4的分配到的支付 ： 0.0705
DataOwner5的分配到的支付 ： 0.0815
DataOwner6的分配到的支付 ： 0.1717
DataOwner7的分配到的支付 ： 0.1888
DataOwner8的分配到的支付 ： 0.1781
DataOwner9的分配到的支付 ： 0.1880
DataOwner10的分配到的支付 ： 0.0613
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC1', 'DataOwner2': 'CPC8', 'DataOwner4': 'CPC2', 'DataOwner3': 'CPC9', 'DataOwner5': 'CPC3', 'DataOwner6': 'CPC4', 'DataOwner8': 'CPC5', 'DataOwner9': 'CPC6', 'DataOwner7': 'CPC10', 'DataOwner10': 'CPC7'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC8
DataOwner4 把数据交给 CPC2
DataOwner3 把数据交给 CPC9
DataOwner5 把数据交给 CPC3
DataOwner6 把数据交给 CPC4
DataOwner8 把数据交给 CPC5
DataOwner9 把数据交给 CPC6
DataOwner7 把数据交给 CPC10
DataOwner10 把数据交给 CPC7
DONE
----- literation 1: 模型训练 -----
CPC1调整模型中, 本轮训练的数据量为：918.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 26.82%
Epoch 1/5, Loss: 2.2957
Epoch 2/5, Loss: 2.2922
Epoch 3/5, Loss: 2.2857
Epoch 4/5, Loss: 2.2797
Epoch 5/5, Loss: 2.2726
新模型评估：
Accuracy: 39.91%
Model saved to ../../../data/model/mnist_cnn_model
CPC8调整模型中, 本轮训练的数据量为：676.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 39.91%
Epoch 1/5, Loss: 2.2795
Epoch 2/5, Loss: 2.2734
Epoch 3/5, Loss: 2.2665
Epoch 4/5, Loss: 2.2609
Epoch 5/5, Loss: 2.2508
新模型评估：
Accuracy: 43.10%
Model saved to ../../../data/model/mnist_cnn_model
CPC2调整模型中, 本轮训练的数据量为：78.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 43.10%
Epoch 1/5, Loss: 2.2514
Epoch 2/5, Loss: 2.2810
Epoch 3/5, Loss: 2.2355
Epoch 4/5, Loss: 2.2382
Epoch 5/5, Loss: 2.2484
新模型评估：
Accuracy: 44.28%
Model saved to ../../../data/model/mnist_cnn_model
CPC9调整模型中, 本轮训练的数据量为：193.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 44.28%
Epoch 1/5, Loss: 2.2218
Epoch 2/5, Loss: 2.2399
Epoch 3/5, Loss: 2.2451
Epoch 4/5, Loss: 2.2204
Epoch 5/5, Loss: 2.2249
新模型评估：
Accuracy: 45.97%
Model saved to ../../../data/model/mnist_cnn_model
CPC3调整模型中, 本轮训练的数据量为：540.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 45.97%
Epoch 1/5, Loss: 2.2161
Epoch 2/5, Loss: 2.2065
Epoch 3/5, Loss: 2.2005
Epoch 4/5, Loss: 2.1888
Epoch 5/5, Loss: 2.1759
新模型评估：
Accuracy: 41.09%
CPC4调整模型中, 本轮训练的数据量为：175.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 45.97%
Epoch 1/5, Loss: 2.2153
Epoch 2/5, Loss: 2.2092
Epoch 3/5, Loss: 2.2093
Epoch 4/5, Loss: 2.2067
Epoch 5/5, Loss: 2.1974
新模型评估：
Accuracy: 48.19%
Model saved to ../../../data/model/mnist_cnn_model
CPC5调整模型中, 本轮训练的数据量为：544.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 48.19%
Epoch 1/5, Loss: 2.1981
Epoch 2/5, Loss: 2.1842
Epoch 3/5, Loss: 2.1774
Epoch 4/5, Loss: 2.1627
Epoch 5/5, Loss: 2.1535
新模型评估：
Accuracy: 50.22%
Model saved to ../../../data/model/mnist_cnn_model
CPC6调整模型中, 本轮训练的数据量为：2090.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 50.22%
Epoch 1/5, Loss: 2.1391
Epoch 2/5, Loss: 2.0928
Epoch 3/5, Loss: 2.0380
Epoch 4/5, Loss: 1.9698
Epoch 5/5, Loss: 1.8894
新模型评估：
Accuracy: 59.26%
Model saved to ../../../data/model/mnist_cnn_model
CPC10调整模型中, 本轮训练的数据量为：1334.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 59.26%
Epoch 1/5, Loss: 1.8170
Epoch 2/5, Loss: 1.7685
Epoch 3/5, Loss: 1.7198
Epoch 4/5, Loss: 1.6698
Epoch 5/5, Loss: 1.6190
新模型评估：
Accuracy: 63.85%
Model saved to ../../../data/model/mnist_cnn_model
CPC7调整模型中, 本轮训练的数据量为：688.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 63.85%
Epoch 1/5, Loss: 1.6167
Epoch 2/5, Loss: 1.5877
Epoch 3/5, Loss: 1.5628
Epoch 4/5, Loss: 1.5358
Epoch 5/5, Loss: 1.5117
新模型评估：
Accuracy: 65.29%
Model saved to ../../../data/model/mnist_cnn_model
DONE
========================= literation: 2 =========================
----- literation 2: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3354
DataOwner1的最优x_1 = 0.1560
DataOwner2的最优x_2 = 0.1150
DataOwner3的最优x_3 = 0.0822
DataOwner4的最优x_4 = 0.0667
DataOwner5的最优x_5 = 0.0766
DataOwner6的最优x_6 = 0.1496
DataOwner7的最优x_7 = 0.1621
DataOwner8的最优x_8 = 0.1544
DataOwner9的最优x_9 = 0.1615
DataOwner10的最优x_10 = 0.0585
每个DataOwner应该贡献数据比例 xn_list = [0.15599495209799633, 0.11497572308425803, 0.08223454473181768, 0.06674290310425791, 0.07656280385233347, 0.14964928708103348, 0.16207197763123637, 0.15436828051260218, 0.1615054529648605, 0.05852906742201197]
ModelOwner的最大效用 U(Eta) = 0.5744
DONE
----- literation 2: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.335385497747093
DataOwner1的分配到的支付 ： 0.1804
DataOwner2的分配到的支付 ： 0.1271
DataOwner3的分配到的支付 ： 0.0880
DataOwner4的分配到的支付 ： 0.0705
DataOwner5的分配到的支付 ： 0.0815
DataOwner6的分配到的支付 ： 0.1717
DataOwner7的分配到的支付 ： 0.1888
DataOwner8的分配到的支付 ： 0.1781
DataOwner9的分配到的支付 ： 0.1880
DataOwner10的分配到的支付 ： 0.0613
DONE
----- literation 2: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC8
DataOwner4 把数据交给 CPC2
DataOwner3 把数据交给 CPC9
DataOwner5 把数据交给 CPC3
DataOwner6 把数据交给 CPC4
DataOwner8 把数据交给 CPC5
DataOwner9 把数据交给 CPC6
DataOwner7 把数据交给 CPC10
DataOwner10 把数据交给 CPC7
DONE
----- literation 2: 模型训练 -----
重新调整fn，进而调整xn、Eta
正在评估DataOwner1的数据质量, 本轮评估的样本数据量为：918.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 65.29%
Epoch 1/5, Loss: 1.4579
Epoch 2/5, Loss: 1.4133
Epoch 3/5, Loss: 1.3857
Epoch 4/5, Loss: 1.3510
Epoch 5/5, Loss: 1.3126
新模型评估：
Accuracy: 68.58%
loss差为：
0.14524223009745274
单位数据loss差为：
0.0001582159369253298
正在评估DataOwner2的数据质量, 本轮评估的样本数据量为：676.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 65.29%
Epoch 1/5, Loss: 1.5257
Epoch 2/5, Loss: 1.5042
Epoch 3/5, Loss: 1.4732
Epoch 4/5, Loss: 1.4454
Epoch 5/5, Loss: 1.4273
新模型评估：
Accuracy: 69.49%
loss差为：
0.09843862056732178
单位数据loss差为：
0.00014561926119426298
正在评估DataOwner4的数据质量, 本轮评估的样本数据量为：78.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 65.29%
Epoch 1/5, Loss: 1.4073
Epoch 2/5, Loss: 1.3786
Epoch 3/5, Loss: 1.4111
Epoch 4/5, Loss: 1.4209
Epoch 5/5, Loss: 1.3300
新模型评估：
Accuracy: 62.88%
loss差为：
0.07731783390045166
单位数据loss差为：
0.0009912542807750213
正在评估DataOwner3的数据质量, 本轮评估的样本数据量为：193.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 65.29%
Epoch 1/5, Loss: 1.4975
Epoch 2/5, Loss: 1.7071
Epoch 3/5, Loss: 1.5980
Epoch 4/5, Loss: 1.4481
Epoch 5/5, Loss: 1.4999
新模型评估：
Accuracy: 66.67%
loss差为：
-0.0023751556873321533
单位数据loss差为：
-1.2306506151980069e-05
正在评估DataOwner5的数据质量, 本轮评估的样本数据量为：540.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 65.29%
Epoch 1/5, Loss: 1.4762
Epoch 2/5, Loss: 1.4592
Epoch 3/5, Loss: 1.4256
Epoch 4/5, Loss: 1.4088
Epoch 5/5, Loss: 1.3875
新模型评估：
Accuracy: 65.78%
loss差为：
0.08864871660868334
单位数据loss差为：
0.00016416429001608025
正在评估DataOwner6的数据质量, 本轮评估的样本数据量为：175.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 65.29%
Epoch 1/5, Loss: 1.4504
Epoch 2/5, Loss: 1.4403
Epoch 3/5, Loss: 1.4208
Epoch 4/5, Loss: 1.4242
Epoch 5/5, Loss: 1.4223
新模型评估：
Accuracy: 66.20%
loss差为：
0.028066913286844963
单位数据loss差为：
0.00016038236163911406
正在评估DataOwner8的数据质量, 本轮评估的样本数据量为：544.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 65.29%
Epoch 1/5, Loss: 1.4443
Epoch 2/5, Loss: 1.4242
Epoch 3/5, Loss: 1.3954
Epoch 4/5, Loss: 1.3768
Epoch 5/5, Loss: 1.3461
新模型评估：
Accuracy: 67.47%
loss差为：
0.09818904929690886
单位数据loss差为：
0.00018049457591343541
正在评估DataOwner9的数据质量, 本轮评估的样本数据量为：2090.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 65.29%
Epoch 1/5, Loss: 1.4373
Epoch 2/5, Loss: 1.3596
Epoch 3/5, Loss: 1.2850
Epoch 4/5, Loss: 1.2103
Epoch 5/5, Loss: 1.1388
新模型评估：
Accuracy: 72.98%
loss差为：
0.2985326412952307
单位数据loss差为：
0.00014283858435178504
正在评估DataOwner7的数据质量, 本轮评估的样本数据量为：1334.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 65.29%
Epoch 1/5, Loss: 1.4620
Epoch 2/5, Loss: 1.4068
Epoch 3/5, Loss: 1.3558
Epoch 4/5, Loss: 1.3060
Epoch 5/5, Loss: 1.2555
新模型评估：
Accuracy: 70.96%
loss差为：
0.2064365602674938
单位数据loss差为：
0.00015475004517803134
正在评估DataOwner10的数据质量, 本轮评估的样本数据量为：688.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 65.29%
Epoch 1/5, Loss: 1.4912
Epoch 2/5, Loss: 1.4573
Epoch 3/5, Loss: 1.4321
Epoch 4/5, Loss: 1.4059
Epoch 5/5, Loss: 1.3768
新模型评估：
Accuracy: 66.49%
loss差为：
0.11432087421417236
单位数据loss差为：
0.00016616406135780868
经过服务器调节后的真实数据质量：
数据质量列表avg_f_list: [0.0001582159369253298, 0.00014561926119426298, -1.2306506151980069e-05, 0.0009912542807750213, 0.00016416429001608025, 0.00016038236163911406, 0.00015475004517803134, 0.00018049457591343541, 0.00014283858435178504, 0.00016616406135780868]
归一化后的数据质量列表avg_f_list:[0.9169917403408583, 0.9157365422606664, 0.9, 1.0, 0.9175844650834187, 0.9172076141316645, 0.9166463809174484, 0.919211699438336, 0.9154594612030263, 0.9177837326681808]
CPC1调整模型中, 本轮训练的数据量为：918.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 65.29%
Epoch 1/5, Loss: 1.4594
Epoch 2/5, Loss: 1.4207
Epoch 3/5, Loss: 1.3796
Epoch 4/5, Loss: 1.3521
Epoch 5/5, Loss: 1.3271
新模型评估：
Accuracy: 68.77%
Model saved to ../../../data/model/mnist_cnn_model
CPC8调整模型中, 本轮训练的数据量为：676.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 68.77%
Epoch 1/5, Loss: 1.3615
Epoch 2/5, Loss: 1.3456
Epoch 3/5, Loss: 1.3127
Epoch 4/5, Loss: 1.2884
Epoch 5/5, Loss: 1.2627
新模型评估：
Accuracy: 72.51%
Model saved to ../../../data/model/mnist_cnn_model
CPC2调整模型中, 本轮训练的数据量为：78.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 72.51%
Epoch 1/5, Loss: 1.0456
Epoch 2/5, Loss: 1.1548
Epoch 3/5, Loss: 1.1003
Epoch 4/5, Loss: 1.0377
Epoch 5/5, Loss: 1.0148
新模型评估：
Accuracy: 68.70%
CPC9调整模型中, 本轮训练的数据量为：193.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 72.51%
Epoch 1/5, Loss: 1.2494
Epoch 2/5, Loss: 1.1597
Epoch 3/5, Loss: 1.2309
Epoch 4/5, Loss: 1.2119
Epoch 5/5, Loss: 1.3607
新模型评估：
Accuracy: 72.03%
CPC3调整模型中, 本轮训练的数据量为：540.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 72.51%
Epoch 1/5, Loss: 1.1864
Epoch 2/5, Loss: 1.1837
Epoch 3/5, Loss: 1.1487
Epoch 4/5, Loss: 1.1420
Epoch 5/5, Loss: 1.1180
新模型评估：
Accuracy: 71.06%
CPC4调整模型中, 本轮训练的数据量为：175.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 72.51%
Epoch 1/5, Loss: 1.1799
Epoch 2/5, Loss: 1.1602
Epoch 3/5, Loss: 1.1570
Epoch 4/5, Loss: 1.1497
Epoch 5/5, Loss: 1.1388
新模型评估：
Accuracy: 72.84%
Model saved to ../../../data/model/mnist_cnn_model
CPC5调整模型中, 本轮训练的数据量为：544.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 72.84%
Epoch 1/5, Loss: 1.1199
Epoch 2/5, Loss: 1.0980
Epoch 3/5, Loss: 1.0734
Epoch 4/5, Loss: 1.0482
Epoch 5/5, Loss: 1.0421
新模型评估：
Accuracy: 73.77%
Model saved to ../../../data/model/mnist_cnn_model
CPC6调整模型中, 本轮训练的数据量为：2090.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 73.77%
Epoch 1/5, Loss: 1.0412
Epoch 2/5, Loss: 0.9762
Epoch 3/5, Loss: 0.9189
Epoch 4/5, Loss: 0.8656
Epoch 5/5, Loss: 0.8167
新模型评估：
Accuracy: 76.70%
Model saved to ../../../data/model/mnist_cnn_model
CPC10调整模型中, 本轮训练的数据量为：1334.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 76.70%
Epoch 1/5, Loss: 0.7798
Epoch 2/5, Loss: 0.7488
Epoch 3/5, Loss: 0.7204
Epoch 4/5, Loss: 0.6942
Epoch 5/5, Loss: 0.6688
新模型评估：
Accuracy: 76.52%
CPC7调整模型中, 本轮训练的数据量为：688.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 76.70%
Epoch 1/5, Loss: 0.8114
Epoch 2/5, Loss: 0.7852
Epoch 3/5, Loss: 0.7683
Epoch 4/5, Loss: 0.7529
Epoch 5/5, Loss: 0.7358
新模型评估：
Accuracy: 75.71%
DONE
========================= literation: 3 =========================
----- literation 3: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.2962
DataOwner1的最优x_1 = 0.1105
DataOwner2的最优x_2 = 0.1092
DataOwner3的最优x_3 = 0.0921
DataOwner4的最优x_4 = 0.1823
DataOwner5的最优x_5 = 0.1111
DataOwner6的最优x_6 = 0.1107
DataOwner7的最优x_7 = 0.1102
DataOwner8的最优x_8 = 0.1128
DataOwner9的最优x_9 = 0.1089
DataOwner10的最优x_10 = 0.1113
每个DataOwner应该贡献数据比例 xn_list = [0.11051712151933754, 0.10920866671235967, 0.0921431271183382, 0.18230603817484353, 0.11113236500981065, 0.11074139162433874, 0.11015786451367077, 0.11281279747184338, 0.10891880636308894, 0.11133882550614851]
ModelOwner的最大效用 U(Eta) = 0.5308
xn开始变化：
new_xn_list: [0.15599495209799633, 0.11497572308425803, 0.0921431271183382, 0.18230603817484353, 0.11113236500981065, 0.14964928708103348, 0.16207197763123637, 0.15436828051260218, 0.1615054529648605, 0.11133882550614851]
avg_f_list: [0.9169917403408583, 0.9157365422606664, 0.9, 1.0, 0.9175844650834187, 0.9172076141316645, 0.9166463809174484, 0.919211699438336, 0.9154594612030263, 0.9177837326681808]
============= xn0: -1.01 =============
new_qn: -0.9261616577442668
best_Eta: 1.296166836280543
sum(new_qn_list): 0.22408994222891637
============= xn0: -1.0 =============
new_qn: -0.9169917403408583
best_Eta: 1.296166836280543
sum(new_qn_list): 0.23325985963232493
============= xn0: -0.99 =============
new_qn: -0.9078218229374497
best_Eta: 1.296166836280543
sum(new_qn_list): 0.24242977703573348
============= xn0: -0.98 =============
new_qn: -0.8986519055340411
best_Eta: 1.296166836280543
sum(new_qn_list): 0.25159969443914226
============= xn0: -0.97 =============
new_qn: -0.8894819881306325
best_Eta: 1.296166836280543
sum(new_qn_list): 0.2607696118425508
============= xn0: -0.96 =============
new_qn: -0.880312070727224
best_Eta: 1.296166836280543
sum(new_qn_list): 0.2699395292459594
============= xn0: -0.95 =============
new_qn: -0.8711421533238153
best_Eta: 1.296166836280543
sum(new_qn_list): 0.27910944664936793
============= xn0: -0.94 =============
new_qn: -0.8619722359204067
best_Eta: 1.296166836280543
sum(new_qn_list): 0.2882793640527765
============= xn0: -0.9299999999999999 =============
new_qn: -0.8528023185169982
best_Eta: 1.296166836280543
sum(new_qn_list): 0.2974492814561851
============= xn0: -0.9199999999999999 =============
new_qn: -0.8436324011135895
best_Eta: 1.296166836280543
sum(new_qn_list): 0.30661919885959377
============= xn0: -0.9099999999999999 =============
new_qn: -0.834462483710181
best_Eta: 1.296166836280543
sum(new_qn_list): 0.3157891162630023
============= xn0: -0.8999999999999999 =============
new_qn: -0.8252925663067724
best_Eta: 1.296166836280543
sum(new_qn_list): 0.3249590336664109
============= xn0: -0.8899999999999999 =============
new_qn: -0.8161226489033638
best_Eta: 1.296166836280543
sum(new_qn_list): 0.33412895106981944
============= xn0: -0.8799999999999999 =============
new_qn: -0.8069527314999552
best_Eta: 1.296166836280543
sum(new_qn_list): 0.3432988684732281
============= xn0: -0.8699999999999999 =============
new_qn: -0.7977828140965466
best_Eta: 1.296166836280543
sum(new_qn_list): 0.35246878587663666
============= xn0: -0.8599999999999999 =============
new_qn: -0.788612896693138
best_Eta: 1.296166836280543
sum(new_qn_list): 0.3616387032800452
============= xn0: -0.8499999999999999 =============
new_qn: -0.7794429792897294
best_Eta: 1.296166836280543
sum(new_qn_list): 0.3708086206834539
============= xn0: -0.8399999999999999 =============
new_qn: -0.7702730618863208
best_Eta: 1.296166836280543
sum(new_qn_list): 0.37997853808686244
============= xn0: -0.8299999999999998 =============
new_qn: -0.7611031444829123
best_Eta: 1.296166836280543
sum(new_qn_list): 0.389148455490271
============= xn0: -0.8199999999999998 =============
new_qn: -0.7519332270795036
best_Eta: 1.296166836280543
sum(new_qn_list): 0.39831837289367966
============= xn0: -0.8099999999999998 =============
new_qn: -0.742763309676095
best_Eta: 1.296166836280543
sum(new_qn_list): 0.4074882902970882
============= xn0: -0.7999999999999998 =============
new_qn: -0.7335933922726865
best_Eta: 1.296166836280543
sum(new_qn_list): 0.4166582077004968
============= xn0: -0.7899999999999998 =============
new_qn: -0.7244234748692778
best_Eta: 1.296166836280543
sum(new_qn_list): 0.42582812510390544
============= xn0: -0.7799999999999998 =============
new_qn: -0.7152535574658693
best_Eta: 1.296166836280543
sum(new_qn_list): 0.434998042507314
============= xn0: -0.7699999999999998 =============
new_qn: -0.7060836400624607
best_Eta: 1.296166836280543
sum(new_qn_list): 0.44416795991072255
============= xn0: -0.7599999999999998 =============
new_qn: -0.696913722659052
best_Eta: 1.296166836280543
sum(new_qn_list): 0.4533378773141312
============= xn0: -0.7499999999999998 =============
new_qn: -0.6877438052556435
best_Eta: 1.296166836280543
sum(new_qn_list): 0.46250779471753983
============= xn0: -0.7399999999999998 =============
new_qn: -0.6785738878522349
best_Eta: 1.296166836280543
sum(new_qn_list): 0.4716777121209484
============= xn0: -0.7299999999999998 =============
new_qn: -0.6694039704488264
best_Eta: 1.296166836280543
sum(new_qn_list): 0.48084762952435695
============= xn0: -0.7199999999999998 =============
new_qn: -0.6602340530454177
best_Eta: 1.296166836280543
sum(new_qn_list): 0.4900175469277656
============= xn0: -0.7099999999999997 =============
new_qn: -0.6510641356420092
best_Eta: 1.296166836280543
sum(new_qn_list): 0.49918746433117417
============= xn0: -0.6999999999999997 =============
new_qn: -0.6418942182386006
best_Eta: 1.296166836280543
sum(new_qn_list): 0.5083573817345827
============= xn0: -0.6899999999999997 =============
new_qn: -0.6327243008351919
best_Eta: 1.296166836280543
sum(new_qn_list): 0.5175272991379913
============= xn0: -0.6799999999999997 =============
new_qn: -0.6235543834317834
best_Eta: 1.296166836280543
sum(new_qn_list): 0.5266972165413999
============= xn0: -0.6699999999999997 =============
new_qn: -0.6143844660283748
best_Eta: 1.296166836280543
sum(new_qn_list): 0.5358671339448084
============= xn0: -0.6599999999999997 =============
new_qn: -0.6052145486249662
best_Eta: 1.296166836280543
sum(new_qn_list): 0.5450370513482171
============= xn0: -0.6499999999999997 =============
new_qn: -0.5960446312215576
best_Eta: 1.296166836280543
sum(new_qn_list): 0.5542069687516257
============= xn0: -0.6399999999999997 =============
new_qn: -0.586874713818149
best_Eta: 1.296166836280543
sum(new_qn_list): 0.5633768861550342
============= xn0: -0.6299999999999997 =============
new_qn: -0.5777047964147404
best_Eta: 1.296166836280543
sum(new_qn_list): 0.5725468035584429
============= xn0: -0.6199999999999997 =============
new_qn: -0.5685348790113318
best_Eta: 1.296166836280543
sum(new_qn_list): 0.5817167209618515
============= xn0: -0.6099999999999997 =============
new_qn: -0.5593649616079233
best_Eta: 1.296166836280543
sum(new_qn_list): 0.59088663836526
============= xn0: -0.5999999999999996 =============
new_qn: -0.5501950442045146
best_Eta: 1.296166836280543
sum(new_qn_list): 0.6000565557686687
============= xn0: -0.5899999999999996 =============
new_qn: -0.541025126801106
best_Eta: 1.296166836280543
sum(new_qn_list): 0.6092264731720772
============= xn0: -0.5799999999999996 =============
new_qn: -0.5318552093976975
best_Eta: 1.296166836280543
sum(new_qn_list): 0.6183963905754858
============= xn0: -0.5699999999999996 =============
new_qn: -0.5226852919942889
best_Eta: 1.296166836280543
sum(new_qn_list): 0.6275663079788943
============= xn0: -0.5599999999999996 =============
new_qn: -0.5135153745908803
best_Eta: 1.296166836280543
sum(new_qn_list): 0.636736225382303
============= xn0: -0.5499999999999996 =============
new_qn: -0.5043454571874717
best_Eta: 1.296166836280543
sum(new_qn_list): 0.6459061427857116
============= xn0: -0.5399999999999996 =============
new_qn: -0.4951755397840631
best_Eta: 1.296166836280543
sum(new_qn_list): 0.6550760601891201
============= xn0: -0.5299999999999996 =============
new_qn: -0.48600562238065453
best_Eta: 1.296166836280543
sum(new_qn_list): 0.6642459775925287
============= xn0: -0.5199999999999996 =============
new_qn: -0.4768357049772459
best_Eta: 1.296166836280543
sum(new_qn_list): 0.6734158949959373
============= xn0: -0.5099999999999996 =============
new_qn: -0.4676657875738373
best_Eta: 1.296166836280543
sum(new_qn_list): 0.6825858123993459
============= xn0: -0.49999999999999956 =============
new_qn: -0.45849587017042875
best_Eta: 1.296166836280543
sum(new_qn_list): 0.6917557298027545
============= xn0: -0.48999999999999955 =============
new_qn: -0.44932595276702014
best_Eta: 1.296166836280543
sum(new_qn_list): 0.7009256472061631
============= xn0: -0.47999999999999954 =============
new_qn: -0.44015603536361153
best_Eta: 1.296166836280543
sum(new_qn_list): 0.7100955646095717
============= xn0: -0.46999999999999953 =============
new_qn: -0.430986117960203
best_Eta: 1.296166836280543
sum(new_qn_list): 0.7192654820129802
============= xn0: -0.4599999999999995 =============
new_qn: -0.42181620055679436
best_Eta: 1.296166836280543
sum(new_qn_list): 0.7284353994163889
============= xn0: -0.4499999999999995 =============
new_qn: -0.4126462831533858
best_Eta: 1.296166836280543
sum(new_qn_list): 0.7376053168197975
============= xn0: -0.4399999999999995 =============
new_qn: -0.4034763657499772
best_Eta: 1.296166836280543
sum(new_qn_list): 0.746775234223206
============= xn0: -0.4299999999999995 =============
new_qn: -0.3943064483465686
best_Eta: 1.296166836280543
sum(new_qn_list): 0.7559451516266147
============= xn0: -0.4199999999999995 =============
new_qn: -0.38513653094316
best_Eta: 1.296166836280543
sum(new_qn_list): 0.7651150690300232
============= xn0: -0.4099999999999995 =============
new_qn: -0.3759666135397514
best_Eta: 1.296166836280543
sum(new_qn_list): 0.7742849864334318
============= xn0: -0.39999999999999947 =============
new_qn: -0.3667966961363428
best_Eta: 1.296166836280543
sum(new_qn_list): 0.7834549038368405
============= xn0: -0.38999999999999946 =============
new_qn: -0.35762677873293425
best_Eta: 1.296166836280543
sum(new_qn_list): 0.792624821240249
============= xn0: -0.37999999999999945 =============
new_qn: -0.34845686132952564
best_Eta: 1.296166836280543
sum(new_qn_list): 0.8017947386436576
============= xn0: -0.36999999999999944 =============
new_qn: -0.3392869439261171
best_Eta: 1.296166836280543
sum(new_qn_list): 0.8109646560470661
============= xn0: -0.35999999999999943 =============
new_qn: -0.33011702652270847
best_Eta: 1.296166836280543
sum(new_qn_list): 0.8201345734504747
============= xn0: -0.3499999999999994 =============
new_qn: -0.32094710911929986
best_Eta: 1.296166836280543
sum(new_qn_list): 0.8293044908538834
============= xn0: -0.3399999999999994 =============
new_qn: -0.3117771917158913
best_Eta: 1.296166836280543
sum(new_qn_list): 0.8384744082572919
============= xn0: -0.3299999999999994 =============
new_qn: -0.3026072743124827
best_Eta: 1.296166836280543
sum(new_qn_list): 0.8476443256607005
============= xn0: -0.3199999999999994 =============
new_qn: -0.2934373569090741
best_Eta: 1.296166836280543
sum(new_qn_list): 0.8568142430641091
============= xn0: -0.3099999999999994 =============
new_qn: -0.2842674395056655
best_Eta: 1.296166836280543
sum(new_qn_list): 0.8659841604675177
============= xn0: -0.2999999999999994 =============
new_qn: -0.2750975221022569
best_Eta: 1.296166836280543
sum(new_qn_list): 0.8751540778709263
============= xn0: -0.28999999999999937 =============
new_qn: -0.26592760469884835
best_Eta: 1.296166836280543
sum(new_qn_list): 0.8843239952743348
============= xn0: -0.27999999999999936 =============
new_qn: -0.25675768729543974
best_Eta: 1.296166836280543
sum(new_qn_list): 0.8934939126777436
============= xn0: -0.26999999999999935 =============
new_qn: -0.24758776989203113
best_Eta: 1.296166836280543
sum(new_qn_list): 0.9026638300811521
============= xn0: -0.25999999999999934 =============
new_qn: -0.23841785248862254
best_Eta: 1.296166836280543
sum(new_qn_list): 0.9118337474845607
============= xn0: -0.24999999999999933 =============
new_qn: -0.22924793508521396
best_Eta: 1.296166836280543
sum(new_qn_list): 0.9210036648879693
============= xn0: -0.23999999999999932 =============
new_qn: -0.22007801768180538
best_Eta: 1.296166836280543
sum(new_qn_list): 0.9301735822913778
============= xn0: -0.22999999999999932 =============
new_qn: -0.21090810027839677
best_Eta: 1.296166836280543
sum(new_qn_list): 0.9393434996947866
============= xn0: -0.2199999999999993 =============
new_qn: -0.20173818287498818
best_Eta: 1.296166836280543
sum(new_qn_list): 0.9485134170981951
============= xn0: -0.2099999999999993 =============
new_qn: -0.1925682654715796
best_Eta: 1.296166836280543
sum(new_qn_list): 0.9576833345016037
============= xn0: -0.1999999999999993 =============
new_qn: -0.183398348068171
best_Eta: 1.296166836280543
sum(new_qn_list): 0.9668532519050123
============= xn0: -0.18999999999999928 =============
new_qn: -0.1742284306647624
best_Eta: 1.296166836280543
sum(new_qn_list): 0.9760231693084208
============= xn0: -0.17999999999999927 =============
new_qn: -0.16505851326135382
best_Eta: 1.296166836280543
sum(new_qn_list): 0.9851930867118294
============= xn0: -0.16999999999999926 =============
new_qn: -0.15588859585794523
best_Eta: 1.296166836280543
sum(new_qn_list): 0.9943630041152379
============= xn0: -0.15999999999999925 =============
new_qn: -0.14671867845453665
best_Eta: 1.296166836280543
sum(new_qn_list): 1.0035329215186468
============= xn0: -0.14999999999999925 =============
new_qn: -0.13754876105112804
best_Eta: 1.296166836280543
sum(new_qn_list): 1.0127028389220554
============= xn0: -0.13999999999999924 =============
new_qn: -0.12837884364771945
best_Eta: 1.296166836280543
sum(new_qn_list): 1.021872756325464
============= xn0: -0.12999999999999923 =============
new_qn: -0.11920892624431087
best_Eta: 1.296166836280543
sum(new_qn_list): 1.0310426737288725
============= xn0: -0.11999999999999922 =============
new_qn: -0.11003900884090227
best_Eta: 1.296166836280543
sum(new_qn_list): 1.040212591132281
============= xn0: -0.10999999999999921 =============
new_qn: -0.10086909143749369
best_Eta: 1.296166836280543
sum(new_qn_list): 1.0493825085356896
============= xn0: -0.0999999999999992 =============
new_qn: -0.09169917403408509
best_Eta: 1.296166836280543
sum(new_qn_list): 1.0585524259390984
============= xn0: -0.08999999999999919 =============
new_qn: -0.0825292566306765
best_Eta: 1.296166836280543
sum(new_qn_list): 1.067722343342507
============= xn0: -0.07999999999999918 =============
new_qn: -0.07335933922726791
best_Eta: 1.296166836280543
sum(new_qn_list): 1.0768922607459155
============= xn0: -0.06999999999999917 =============
new_qn: -0.06418942182385932
best_Eta: 1.296166836280543
sum(new_qn_list): 1.086062178149324
============= xn0: -0.059999999999999165 =============
new_qn: -0.055019504420450734
best_Eta: 1.296166836280543
sum(new_qn_list): 1.0952320955527326
============= xn0: -0.049999999999999156 =============
new_qn: -0.04584958701704214
best_Eta: 1.296166836280543
sum(new_qn_list): 1.1044020129561412
============= xn0: -0.03999999999999915 =============
new_qn: -0.03667966961363355
best_Eta: 1.296166836280543
sum(new_qn_list): 1.1135719303595495
============= xn0: -0.02999999999999914 =============
new_qn: -0.027509752210224957
best_Eta: 1.296166836280543
sum(new_qn_list): 1.1227418477629583
============= xn0: -0.01999999999999913 =============
new_qn: -0.018339834806816366
best_Eta: 1.296166836280543
sum(new_qn_list): 1.131911765166367
============= xn0: -0.00999999999999912 =============
new_qn: -0.009169917403407777
best_Eta: 1.296166836280543
sum(new_qn_list): 1.1410816825697754
============= xn0: 8.881784197001252e-16 =============
new_qn: 8.144522748140111e-16
best_Eta: 1.296166836280543
sum(new_qn_list): 1.1502515999731842
============= xn0: 0.010000000000000897 =============
new_qn: 0.009169917403409406
best_Eta: 1.296166836280543
sum(new_qn_list): 1.1594215173765925
============= xn0: 0.020000000000000906 =============
new_qn: 0.018339834806817997
best_Eta: 1.296166836280543
sum(new_qn_list): 1.1685914347800013
============= xn0: 0.030000000000000915 =============
new_qn: 0.027509752210226588
best_Eta: 1.296166836280543
sum(new_qn_list): 1.17776135218341
============= xn0: 0.040000000000000924 =============
new_qn: 0.036679669613635175
best_Eta: 1.296166836280543
sum(new_qn_list): 1.1869312695868184
============= xn0: 0.05000000000000093 =============
new_qn: 0.045849587017043766
best_Eta: 1.296166836280543
sum(new_qn_list): 1.1961011869902272
============= xn0: 0.06000000000000094 =============
new_qn: 0.05501950442045236
best_Eta: 1.296166836280543
sum(new_qn_list): 1.2052711043936355
============= xn0: 0.07000000000000095 =============
new_qn: 0.06418942182386095
best_Eta: 1.296166836280543
sum(new_qn_list): 1.2144410217970443
============= xn0: 0.08000000000000096 =============
new_qn: 0.07335933922726955
best_Eta: 1.296166836280543
sum(new_qn_list): 1.223610939200453
============= xn0: 0.09000000000000097 =============
new_qn: 0.08252925663067813
best_Eta: 1.296166836280543
sum(new_qn_list): 1.2327808566038614
============= xn0: 0.10000000000000098 =============
new_qn: 0.09169917403408673
best_Eta: 1.296166836280543
sum(new_qn_list): 1.2419507740072702
============= xn0: 0.11000000000000099 =============
new_qn: 0.10086909143749531
best_Eta: 1.296166836280543
sum(new_qn_list): 1.2511206914106785
============= xn0: 0.120000000000001 =============
new_qn: 0.11003900884090391
best_Eta: 1.296166836280543
sum(new_qn_list): 1.260290608814087
============= xn0: 0.130000000000001 =============
new_qn: 0.1192089262443125
best_Eta: 1.296166836280543
sum(new_qn_list): 1.2694605262174956
============= xn0: 0.140000000000001 =============
new_qn: 0.1283788436477211
best_Eta: 1.296166836280543
sum(new_qn_list): 1.2786304436209044
============= xn0: 0.15000000000000102 =============
new_qn: 0.13754876105112968
best_Eta: 1.296166836280543
sum(new_qn_list): 1.2878003610243127
============= xn0: 0.16000000000000103 =============
new_qn: 0.14671867845453826
best_Eta: 1.296166836280543
sum(new_qn_list): 1.2969702784277215
============= xn0: 0.17000000000000104 =============
new_qn: 0.15588859585794687
best_Eta: 1.296166836280543
sum(new_qn_list): 1.30614019583113
============= xn0: 0.18000000000000105 =============
new_qn: 0.16505851326135546
best_Eta: 1.296166836280543
sum(new_qn_list): 1.3153101132345386
============= xn0: 0.19000000000000106 =============
new_qn: 0.17422843066476404
best_Eta: 1.296166836280543
sum(new_qn_list): 1.3244800306379472
============= xn0: 0.20000000000000107 =============
new_qn: 0.18339834806817262
best_Eta: 1.296166836280543
sum(new_qn_list): 1.3336499480413557
============= xn0: 0.21000000000000107 =============
new_qn: 0.19256826547158123
best_Eta: 1.296166836280543
sum(new_qn_list): 1.3428198654447643
============= xn0: 0.22000000000000108 =============
new_qn: 0.20173818287498982
best_Eta: 1.296166836280543
sum(new_qn_list): 1.351989782848173
============= xn0: 0.2300000000000011 =============
new_qn: 0.2109081002783984
best_Eta: 1.296166836280543
sum(new_qn_list): 1.3611597002515816
============= xn0: 0.2400000000000011 =============
new_qn: 0.220078017681807
best_Eta: 1.296166836280543
sum(new_qn_list): 1.3703296176549902
============= xn0: 0.2500000000000011 =============
new_qn: 0.2292479350852156
best_Eta: 1.296166836280543
sum(new_qn_list): 1.3794995350583987
============= xn0: 0.2600000000000011 =============
new_qn: 0.23841785248862418
best_Eta: 1.296166836280543
sum(new_qn_list): 1.3886694524618073
============= xn0: 0.27000000000000113 =============
new_qn: 0.24758776989203277
best_Eta: 1.296166836280543
sum(new_qn_list): 1.3978393698652158
============= xn0: 0.28000000000000114 =============
new_qn: 0.25675768729544135
best_Eta: 1.296166836280543
sum(new_qn_list): 1.4070092872686244
============= xn0: 0.29000000000000115 =============
new_qn: 0.26592760469884996
best_Eta: 1.296166836280543
sum(new_qn_list): 1.4161792046720332
============= xn0: 0.30000000000000115 =============
new_qn: 0.2750975221022585
best_Eta: 1.296166836280543
sum(new_qn_list): 1.4253491220754417
============= xn0: 0.31000000000000116 =============
new_qn: 0.28426743950566713
best_Eta: 1.296166836280543
sum(new_qn_list): 1.4345190394788503
============= xn0: 0.3200000000000012 =============
new_qn: 0.29343735690907574
best_Eta: 1.296166836280543
sum(new_qn_list): 1.4436889568822588
============= xn0: 0.3300000000000012 =============
new_qn: 0.3026072743124843
best_Eta: 1.296166836280543
sum(new_qn_list): 1.4528588742856674
============= xn0: 0.3400000000000012 =============
new_qn: 0.3117771917158929
best_Eta: 1.296166836280543
sum(new_qn_list): 1.462028791689076
============= xn0: 0.3500000000000012 =============
new_qn: 0.3209471091193015
best_Eta: 1.296166836280543
sum(new_qn_list): 1.4711987090924845
============= xn0: 0.3600000000000012 =============
new_qn: 0.3301170265227101
best_Eta: 1.296166836280543
sum(new_qn_list): 1.480368626495893
============= xn0: 0.3700000000000012 =============
new_qn: 0.3392869439261187
best_Eta: 1.296166836280543
sum(new_qn_list): 1.4895385438993018
============= xn0: 0.3800000000000012 =============
new_qn: 0.34845686132952725
best_Eta: 1.296166836280543
sum(new_qn_list): 1.4987084613027104
============= xn0: 0.39000000000000123 =============
new_qn: 0.35762677873293586
best_Eta: 1.296166836280543
sum(new_qn_list): 1.507878378706119
============= xn0: 0.40000000000000124 =============
new_qn: 0.36679669613634447
best_Eta: 1.296166836280543
sum(new_qn_list): 1.5170482961095275
============= xn0: 0.41000000000000125 =============
new_qn: 0.375966613539753
best_Eta: 1.296166836280543
sum(new_qn_list): 1.526218213512936
============= xn0: 0.42000000000000126 =============
new_qn: 0.38513653094316164
best_Eta: 1.296166836280543
sum(new_qn_list): 1.5353881309163446
============= xn0: 0.43000000000000127 =============
new_qn: 0.39430644834657025
best_Eta: 1.296166836280543
sum(new_qn_list): 1.5445580483197534
============= xn0: 0.4400000000000013 =============
new_qn: 0.4034763657499788
best_Eta: 1.296166836280543
sum(new_qn_list): 1.553727965723162
============= xn0: 0.4500000000000013 =============
new_qn: 0.4126462831533874
best_Eta: 1.296166836280543
sum(new_qn_list): 1.5628978831265705
============= xn0: 0.4600000000000013 =============
new_qn: 0.42181620055679603
best_Eta: 1.296166836280543
sum(new_qn_list): 1.572067800529979
============= xn0: 0.4700000000000013 =============
new_qn: 0.4309861179602046
best_Eta: 1.296166836280543
sum(new_qn_list): 1.5812377179333876
============= xn0: 0.4800000000000013 =============
new_qn: 0.4401560353636132
best_Eta: 1.296166836280543
sum(new_qn_list): 1.5904076353367962
============= xn0: 0.4900000000000013 =============
new_qn: 0.44932595276702175
best_Eta: 1.296166836280543
sum(new_qn_list): 1.5995775527402047
============= xn0: 0.5000000000000013 =============
new_qn: 0.45849587017043036
best_Eta: 1.296166836280543
sum(new_qn_list): 1.6087474701436135
============= xn0: 0.5100000000000013 =============
new_qn: 0.467665787573839
best_Eta: 1.296166836280543
sum(new_qn_list): 1.617917387547022
============= xn0: 0.5200000000000014 =============
new_qn: 0.47683570497724753
best_Eta: 1.296166836280543
sum(new_qn_list): 1.6270873049504306
============= xn0: 0.5300000000000014 =============
new_qn: 0.48600562238065614
best_Eta: 1.296166836280543
sum(new_qn_list): 1.6362572223538392
============= xn0: 0.5400000000000014 =============
new_qn: 0.49517553978406476
best_Eta: 1.296166836280543
sum(new_qn_list): 1.6454271397572477
============= xn0: 0.5500000000000014 =============
new_qn: 0.5043454571874734
best_Eta: 1.296166836280543
sum(new_qn_list): 1.6545970571606565
============= xn0: 0.5600000000000014 =============
new_qn: 0.5135153745908819
best_Eta: 1.296166836280543
sum(new_qn_list): 1.663766974564065
============= xn0: 0.5700000000000014 =============
new_qn: 0.5226852919942905
best_Eta: 1.296166836280543
sum(new_qn_list): 1.6729368919674736
============= xn0: 0.5800000000000014 =============
new_qn: 0.5318552093976991
best_Eta: 1.296166836280543
sum(new_qn_list): 1.6821068093708822
============= xn0: 0.5900000000000014 =============
new_qn: 0.5410251268011077
best_Eta: 1.296166836280543
sum(new_qn_list): 1.6912767267742908
============= xn0: 0.6000000000000014 =============
new_qn: 0.5501950442045163
best_Eta: 1.296166836280543
sum(new_qn_list): 1.7004466441776993
============= xn0: 0.6100000000000014 =============
new_qn: 0.5593649616079248
best_Eta: 1.296166836280543
sum(new_qn_list): 1.7096165615811079
============= xn0: 0.6200000000000014 =============
new_qn: 0.5685348790113335
best_Eta: 1.296166836280543
sum(new_qn_list): 1.7187864789845166
============= xn0: 0.6300000000000014 =============
new_qn: 0.577704796414742
best_Eta: 1.296166836280543
sum(new_qn_list): 1.7279563963879252
============= xn0: 0.6400000000000015 =============
new_qn: 0.5868747138181506
best_Eta: 1.296166836280543
sum(new_qn_list): 1.7371263137913338
============= xn0: 0.6500000000000015 =============
new_qn: 0.5960446312215593
best_Eta: 1.296166836280543
sum(new_qn_list): 1.7462962311947423
============= xn0: 0.6600000000000015 =============
new_qn: 0.6052145486249678
best_Eta: 1.296166836280543
sum(new_qn_list): 1.7554661485981509
============= xn0: 0.6700000000000015 =============
new_qn: 0.6143844660283764
best_Eta: 1.296166836280543
sum(new_qn_list): 1.7646360660015594
============= xn0: 0.6800000000000015 =============
new_qn: 0.623554383431785
best_Eta: 1.296166836280543
sum(new_qn_list): 1.7738059834049682
============= xn0: 0.6900000000000015 =============
new_qn: 0.6327243008351936
best_Eta: 1.296166836280543
sum(new_qn_list): 1.7829759008083768
============= xn0: 0.7000000000000015 =============
new_qn: 0.6418942182386022
best_Eta: 1.296166836280543
sum(new_qn_list): 1.7921458182117853
============= xn0: 0.7100000000000015 =============
new_qn: 0.6510641356420108
best_Eta: 1.296166836280543
sum(new_qn_list): 1.8013157356151939
============= xn0: 0.7200000000000015 =============
new_qn: 0.6602340530454194
best_Eta: 1.296166836280543
sum(new_qn_list): 1.8104856530186024
============= xn0: 0.7300000000000015 =============
new_qn: 0.6694039704488279
best_Eta: 1.296166836280543
sum(new_qn_list): 1.819655570422011
============= xn0: 0.7400000000000015 =============
new_qn: 0.6785738878522366
best_Eta: 1.296166836280543
sum(new_qn_list): 1.8288254878254198
============= xn0: 0.7500000000000016 =============
new_qn: 0.6877438052556452
best_Eta: 1.296166836280543
sum(new_qn_list): 1.8379954052288283
============= xn0: 0.7600000000000016 =============
new_qn: 0.6969137226590537
best_Eta: 1.296166836280543
sum(new_qn_list): 1.8471653226322369
============= xn0: 0.7700000000000016 =============
new_qn: 0.7060836400624623
best_Eta: 1.296166836280543
sum(new_qn_list): 1.8563352400356454
============= xn0: 0.7800000000000016 =============
new_qn: 0.7152535574658709
best_Eta: 1.296166836280543
sum(new_qn_list): 1.865505157439054
============= xn0: 0.7900000000000016 =============
new_qn: 0.7244234748692795
best_Eta: 1.296166836280543
sum(new_qn_list): 1.8746750748424625
============= xn0: 0.8000000000000016 =============
new_qn: 0.733593392272688
best_Eta: 1.296166836280543
sum(new_qn_list): 1.883844992245871
============= xn0: 0.8100000000000016 =============
new_qn: 0.7427633096760967
best_Eta: 1.296166836280543
sum(new_qn_list): 1.8930149096492799
============= xn0: 0.8200000000000016 =============
new_qn: 0.7519332270795053
best_Eta: 1.296166836280543
sum(new_qn_list): 1.9021848270526884
============= xn0: 0.8300000000000016 =============
new_qn: 0.7611031444829138
best_Eta: 1.296166836280543
sum(new_qn_list): 1.911354744456097
============= xn0: 0.8400000000000016 =============
new_qn: 0.7702730618863225
best_Eta: 1.296166836280543
sum(new_qn_list): 1.9205246618595055
============= xn0: 0.8500000000000016 =============
new_qn: 0.779442979289731
best_Eta: 1.296166836280543
sum(new_qn_list): 1.929694579262914
============= xn0: 0.8600000000000017 =============
new_qn: 0.7886128966931396
best_Eta: 1.296166836280543
sum(new_qn_list): 1.9388644966663227
============= xn0: 0.8700000000000017 =============
new_qn: 0.7977828140965483
best_Eta: 1.296166836280543
sum(new_qn_list): 1.9480344140697314
============= xn0: 0.8800000000000017 =============
new_qn: 0.8069527314999568
best_Eta: 1.296166836280543
sum(new_qn_list): 1.95720433147314
============= xn0: 0.8900000000000017 =============
new_qn: 0.8161226489033654
best_Eta: 1.296166836280543
sum(new_qn_list): 1.9663742488765485
============= xn0: 0.9000000000000017 =============
new_qn: 0.8252925663067741
best_Eta: 1.296166836280543
sum(new_qn_list): 1.9755441662799573
============= xn0: 0.9100000000000017 =============
new_qn: 0.8344624837101826
best_Eta: 1.296166836280543
sum(new_qn_list): 1.9847140836833659
============= xn0: 0.9200000000000017 =============
new_qn: 0.8436324011135912
best_Eta: 1.296166836280543
sum(new_qn_list): 1.9938840010867744
============= xn0: 0.9300000000000017 =============
new_qn: 0.8528023185169997
best_Eta: 1.296166836280543
sum(new_qn_list): 2.003053918490183
============= xn0: 0.9400000000000017 =============
new_qn: 0.8619722359204084
best_Eta: 1.296166836280543
sum(new_qn_list): 2.0122238358935918
============= xn0: 0.9500000000000017 =============
new_qn: 0.871142153323817
best_Eta: 1.296166836280543
sum(new_qn_list): 2.021393753297
============= xn0: 0.9600000000000017 =============
new_qn: 0.8803120707272255
best_Eta: 1.296166836280543
sum(new_qn_list): 2.030563670700409
============= xn0: 0.9700000000000017 =============
new_qn: 0.8894819881306342
best_Eta: 1.296166836280543
sum(new_qn_list): 2.0397335881038177
============= xn0: 0.9800000000000018 =============
new_qn: 0.8986519055340427
best_Eta: 1.296166836280543
sum(new_qn_list): 2.048903505507226
============= xn0: 0.9900000000000018 =============
new_qn: 0.9078218229374513
best_Eta: 1.296166836280543
sum(new_qn_list): 2.0580734229106343
============= xn0: 1.0000000000000016 =============
new_qn: 0.9169917403408597
best_Eta: 1.296166836280543
sum(new_qn_list): 2.067243340314043
DONE
最终的列表：
[-4.347045541010555, -4.095494290558546, -3.863729416126257, -3.6495874878150363, -3.4512093822493597, -3.2669885957103073, -3.0955297460527795, -2.935614994731562, -2.7861766797803256, -2.6462748595070416, -2.5150787687205334, -2.3918514146427, -2.2759367093422727, -2.1667486644157306, -2.0637622723517657, -1.9665057751997028, -1.8745540803904246, -1.7875231299223704, -1.7050650656565518, -1.6268640624267934, -1.552632723767888, -1.482108953586001, -1.4150532320277163, -1.3512462359031225, -1.2904867538693343, -1.2325898546384746, -1.1773852730939571, -1.124715984660486, -1.0744369427973626, -1.0264139582469085, -0.9805227018100198, -0.9366478150512902, -0.8946821155471008, -0.854525885154569, -0.8160862313566126, -0.7792765130769039, -0.7440158234976781, -0.7102285233856538, -0.6778438192635114, -0.6467953814784462, -0.6170209978336787, -0.5884622589786819, -0.5610642722119488, -0.5347754007470873, -0.5095470258377766, -0.4853333294571789, -0.4620910954891304, -0.43977952761718, -0.4183600822978849, -0.3977963153805393, -0.37805374109004897, -0.3590997022257453, -0.34090325054900994, -0.32343503643868443, -0.30666720698718064, -0.2905733117934993, -0.27512821578334623, -0.26030801845231866, -0.2460899789867712, -0.23245244676924648, -0.21937479682208183, -0.20683736978460587, -0.19482141605677372, -0.1833090437756919, -0.1722831703216502, -0.16172747707741686, -0.15162636718899136, -0.14196492609803535, -0.13272888463609356, -0.12390458448868397, -0.1154789458535947, -0.10743943713245124, -0.09977404650796695, -0.09247125527140654, -0.08552001277580651, -0.0789097129005002, -0.07263017192162219, -0.06667160769156566, -0.061024620037945954, -0.05568017229954403, -0.0506295739230197, -0.04586446404997335, -0.04137679602922245, -0.03715882279401189, -0.033203083048324145, -0.0295023882105358, -0.026049810066417628, -0.022838669086918756, -0.019862523369349572, -0.01711515816349321, -0.014590575946867163, -0.012282987015834454, -0.010186800561553092, -0.008296616201864102, -0.006607215942171021, -0.005113556540168318, -0.003810762250946731, -0.002694117930549189, -0.001759062477484273, -0.001001182593031915, -0.0004162068424081976, 2.959124771157755e-17, 0.00025144233705764844, 0.0003419988817008751, 0.0002754275499754835, 5.537012726113122e-05, -0.00031464328014622206, -0.0008311899966623598, -0.0014909507215129097, -0.0022907056552042976, -0.0032273307988721495, -0.004297794417571554, -0.0054991536590984075, -0.006828551320420179, -0.008283212754252284, -0.00986044290874466, -0.011557623493643449, -0.013372210266668932, -0.01530173043420091, -0.01734378016069385, -0.01949602218155211, -0.021756183514485217, -0.024122053264636623, -0.02659148051903265, -0.029162372326140246, -0.03183269175654685, -0.03460045604098691, -0.0374637347821411, -0.04042064823681904, -0.04346936566531587, -0.046608103744897156, -0.049835125044524586, -0.05314873655808183, -0.056547288293499975, -0.060029171915311874, -0.06359281943829109, -0.06723670196994558, -0.07095932849974751, -0.07475924473308587, -0.07863503196802657, -0.08258530601305691, -0.08660871614408089, -0.09070394409901406, -0.09486970310840692, -0.09910473696059857, -0.10340781909997443, -0.10777775175696874, -0.11221336510851526, -0.11671351646771028, -0.1212770895015084, -0.12590299347532802, -0.13059016252348982, -0.1353375549444661, -0.14014415251996065, -0.14500895985688478, -0.14993100375133644, -0.15490933257372813, -0.15994301567424862, -0.16503114280787756, -0.1701728235782075, -0.17536718689935865, -0.18061338047530395, -0.18591057029595126, -0.19125794014935504, -0.1966546911494616, -0.20210004127880954, -0.2075932249456401, -0.21313349255488734, -0.21872011009254472, -0.22435235872292353, -0.23002953439833967, -0.2357509474807828, -0.2415159223751409, -0.2473237971735721, -0.2531739233106281, -0.25906566522875346, -0.26499840005379854, -0.2709715172801968, -0.2769844184654736, -0.28303651693376436, -0.28912723748803526, -0.29525601613070696, -0.3014222997924, -0.307625546068525, -0.31386522296345587, -0.32014080864203154, -0.32645179118814327, -0.332797668370172, -0.3391779474130515, -0.3455921447767385, -0.3520397859408806, -0.3585204051954831, -0.3650335454373774, -0.37157875797230544, -0.3781556023224417, -0.38476364603917756, -0.391402464521001, -0.3980716408363111, -0.4047707655510121, -0.4114994365607376, -0.4182572589275587, -0.4250438447210394]
**** log-parameter_analysis 运行时间： 2025-01-19 17:07:38 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.08490037573755677
DataOwner2: noise random: 0.0014809403931679843
DataOwner3: noise random: 0.0886004948189753
DataOwner4: noise random: 0.03527401286726519
DataOwner5: noise random: 0.08311964205127076
DataOwner6: noise random: 0.005148114543310923
DataOwner7: noise random: 0.07455536087139293
DataOwner8: noise random: 0.0494763307910655
DataOwner9: noise random: 0.020053704766235192
DataOwner10: noise random: 0.040329045555689697
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9970826722205202, 0.9999991135169196, 0.9968239918920992, 0.9994966263748225, 0.9971901007437024, 0.9999892950415576, 0.9977442069230315, 0.999005989736478, 0.9998372681629639, 0.9993418562888727]
归一化后的数据质量列表avg_f_list: [0.9081470998275711, 1.0, 0.9, 0.9841742395576568, 0.9115305457511054, 0.9996907685272522, 0.9289820403646512, 0.9687217090306652, 0.9949027037991073, 0.9792997779074348]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3377
DataOwner1的最优x_1 = 0.0667
DataOwner2的最优x_2 = 0.1607
DataOwner3的最优x_3 = 0.0563
DataOwner4的最优x_4 = 0.1471
DataOwner5的最优x_5 = 0.0709
DataOwner6的最优x_6 = 0.1605
DataOwner7的最优x_7 = 0.0915
DataOwner8的最优x_8 = 0.1329
DataOwner9的最优x_9 = 0.1564
DataOwner10的最优x_10 = 0.1427
每个DataOwner应该贡献数据比例 xn_list = [0.06668189453625943, 0.16070881246443253, 0.056318615975122915, 0.14711437697537558, 0.07087438159011955, 0.1604521340519029, 0.09150987653176934, 0.13289376340188125, 0.15643300307546212, 0.14273281610877922]
ModelOwner的最大效用 U(Eta) = 0.5770
xn开始变化：
new_xn_list: [0.06668189453625943, 0.16070881246443253, 0.056318615975122915, 0.14711437697537558, 0.07087438159011955, 0.1604521340519029, 0.09150987653176934, 0.13289376340188125, 0.15643300307546212, 0.14273281610877922]
avg_f_list: [0.9081470998275711, 1.0, 0.9, 0.9841742395576568, 0.9115305457511054, 0.9996907685272522, 0.9289820403646512, 0.9687217090306652, 0.9949027037991073, 0.9792997779074348]
============= xn0: 0.0 =============
new_qn: 0.0
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.0903503661173894
============= xn0: 0.001 =============
new_qn: 0.0009081470998275711
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.091258513217217
============= xn0: 0.002 =============
new_qn: 0.0018162941996551422
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.0921666603170446
============= xn0: 0.003 =============
new_qn: 0.0027244412994827136
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.0930748074168721
============= xn0: 0.004 =============
new_qn: 0.0036325883993102845
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.0939829545166997
============= xn0: 0.005 =============
new_qn: 0.004540735499137856
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.0948911016165273
============= xn0: 0.006 =============
new_qn: 0.005448882598965427
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.0957992487163548
============= xn0: 0.007 =============
new_qn: 0.0063570296987929984
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.0967073958161824
============= xn0: 0.008 =============
new_qn: 0.007265176798620569
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.09761554291601
============= xn0: 0.009000000000000001 =============
new_qn: 0.008173323898448142
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.0985236900158375
============= xn0: 0.01 =============
new_qn: 0.009081470998275712
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.099431837115665
============= xn0: 0.011 =============
new_qn: 0.009989618098103281
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1003399842154926
============= xn0: 0.012 =============
new_qn: 0.010897765197930854
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1012481313153202
============= xn0: 0.013000000000000001 =============
new_qn: 0.011805912297758426
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1021562784151477
============= xn0: 0.014 =============
new_qn: 0.012714059397585997
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1030644255149753
============= xn0: 0.015 =============
new_qn: 0.013622206497413566
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1039725726148029
============= xn0: 0.016 =============
new_qn: 0.014530353597241138
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1048807197146304
============= xn0: 0.017 =============
new_qn: 0.015438500697068711
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.105788866814458
============= xn0: 0.018000000000000002 =============
new_qn: 0.016346647796896284
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1066970139142858
============= xn0: 0.019 =============
new_qn: 0.017254794896723852
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1076051610141133
============= xn0: 0.02 =============
new_qn: 0.018162941996551423
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1085133081139409
============= xn0: 0.021 =============
new_qn: 0.019071089096378994
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1094214552137684
============= xn0: 0.022 =============
new_qn: 0.019979236196206562
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.110329602313596
============= xn0: 0.023 =============
new_qn: 0.020887383296034137
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1112377494134236
============= xn0: 0.024 =============
new_qn: 0.02179553039586171
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1121458965132511
============= xn0: 0.025 =============
new_qn: 0.02270367749568928
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1130540436130787
============= xn0: 0.026000000000000002 =============
new_qn: 0.02361182459551685
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1139621907129063
============= xn0: 0.027 =============
new_qn: 0.02451997169534442
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1148703378127338
============= xn0: 0.028 =============
new_qn: 0.025428118795171994
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1157784849125614
============= xn0: 0.029 =============
new_qn: 0.026336265894999565
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.116686632012389
============= xn0: 0.03 =============
new_qn: 0.027244412994827133
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1175947791122165
============= xn0: 0.031 =============
new_qn: 0.028152560094654704
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.118502926212044
============= xn0: 0.032 =============
new_qn: 0.029060707194482276
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1194110733118716
============= xn0: 0.033 =============
new_qn: 0.02996885429430985
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1203192204116992
============= xn0: 0.034 =============
new_qn: 0.030877001394137422
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1212273675115267
============= xn0: 0.035 =============
new_qn: 0.03178514849396499
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1221355146113543
============= xn0: 0.036000000000000004 =============
new_qn: 0.03269329559379257
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1230436617111819
============= xn0: 0.037 =============
new_qn: 0.03360144269362013
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1239518088110094
============= xn0: 0.038 =============
new_qn: 0.034509589793447704
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.124859955910837
============= xn0: 0.039 =============
new_qn: 0.03541773689327527
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1257681030106645
============= xn0: 0.04 =============
new_qn: 0.036325883993102846
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1266762501104923
============= xn0: 0.041 =============
new_qn: 0.03723403109293042
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1275843972103199
============= xn0: 0.042 =============
new_qn: 0.03814217819275799
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1284925443101474
============= xn0: 0.043000000000000003 =============
new_qn: 0.039050325292585564
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.129400691409975
============= xn0: 0.044 =============
new_qn: 0.039958472392413125
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1303088385098026
============= xn0: 0.045 =============
new_qn: 0.0408666194922407
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1312169856096301
============= xn0: 0.046 =============
new_qn: 0.041774766592068274
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1321251327094577
============= xn0: 0.047 =============
new_qn: 0.04268291369189584
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1330332798092853
============= xn0: 0.048 =============
new_qn: 0.04359106079172342
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1339414269091128
============= xn0: 0.049 =============
new_qn: 0.044499207891550985
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1348495740089404
============= xn0: 0.05 =============
new_qn: 0.04540735499137856
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.135757721108768
============= xn0: 0.051000000000000004 =============
new_qn: 0.046315502091206134
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1366658682085955
============= xn0: 0.052000000000000005 =============
new_qn: 0.0472236491910337
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.137574015308423
============= xn0: 0.053 =============
new_qn: 0.04813179629086127
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1384821624082506
============= xn0: 0.054 =============
new_qn: 0.04903994339068884
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1393903095080782
============= xn0: 0.055 =============
new_qn: 0.04994809049051641
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.140298456607906
============= xn0: 0.056 =============
new_qn: 0.05085623759034399
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1412066037077333
============= xn0: 0.057 =============
new_qn: 0.051764384690171555
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.142114750807561
============= xn0: 0.058 =============
new_qn: 0.05267253178999913
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1430228979073884
============= xn0: 0.059000000000000004 =============
new_qn: 0.0535806788898267
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1439310450072162
============= xn0: 0.06 =============
new_qn: 0.054488825989654266
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1448391921070435
============= xn0: 0.061 =============
new_qn: 0.05539697308948184
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1457473392068713
============= xn0: 0.062 =============
new_qn: 0.05630512018930941
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1466554863066991
============= xn0: 0.063 =============
new_qn: 0.05721326728913698
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1475636334065265
============= xn0: 0.064 =============
new_qn: 0.05812141438896455
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1484717805063542
============= xn0: 0.065 =============
new_qn: 0.059029561488792126
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1493799276061816
============= xn0: 0.066 =============
new_qn: 0.0599377085886197
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1502880747060094
============= xn0: 0.067 =============
new_qn: 0.06084585568844727
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1511962218058367
============= xn0: 0.068 =============
new_qn: 0.061754002788274844
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1521043689056645
============= xn0: 0.069 =============
new_qn: 0.06266214988810241
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1530125160054918
============= xn0: 0.07 =============
new_qn: 0.06357029698792999
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1539206631053196
============= xn0: 0.07100000000000001 =============
new_qn: 0.06447844408775756
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.154828810205147
============= xn0: 0.07200000000000001 =============
new_qn: 0.06538659118758514
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1557369573049747
============= xn0: 0.073 =============
new_qn: 0.06629473828741268
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.156645104404802
============= xn0: 0.074 =============
new_qn: 0.06720288538724026
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1575532515046298
============= xn0: 0.075 =============
new_qn: 0.06811103248706783
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1584613986044572
============= xn0: 0.076 =============
new_qn: 0.06901917958689541
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.159369545704285
============= xn0: 0.077 =============
new_qn: 0.06992732668672298
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1602776928041123
============= xn0: 0.078 =============
new_qn: 0.07083547378655054
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.16118583990394
============= xn0: 0.079 =============
new_qn: 0.07174362088637812
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1620939870037674
============= xn0: 0.08 =============
new_qn: 0.07265176798620569
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1630021341035952
============= xn0: 0.081 =============
new_qn: 0.07355991508603327
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1639102812034225
============= xn0: 0.082 =============
new_qn: 0.07446806218586084
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1648184283032503
============= xn0: 0.083 =============
new_qn: 0.0753762092856884
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1657265754030777
============= xn0: 0.084 =============
new_qn: 0.07628435638551598
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1666347225029055
============= xn0: 0.085 =============
new_qn: 0.07719250348534355
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1675428696027332
============= xn0: 0.08600000000000001 =============
new_qn: 0.07810065058517113
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1684510167025606
============= xn0: 0.08700000000000001 =============
new_qn: 0.0790087976849987
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1693591638023884
============= xn0: 0.088 =============
new_qn: 0.07991694478482625
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1702673109022157
============= xn0: 0.089 =============
new_qn: 0.08082509188465382
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.171175458002043
============= xn0: 0.09 =============
new_qn: 0.0817332389844814
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1720836051018708
============= xn0: 0.091 =============
new_qn: 0.08264138608430897
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1729917522016982
============= xn0: 0.092 =============
new_qn: 0.08354953318413655
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.173899899301526
============= xn0: 0.093 =============
new_qn: 0.08445768028396411
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1748080464013533
============= xn0: 0.094 =============
new_qn: 0.08536582738379168
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.175716193501181
============= xn0: 0.095 =============
new_qn: 0.08627397448361926
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1766243406010088
============= xn0: 0.096 =============
new_qn: 0.08718212158344683
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1775324877008362
============= xn0: 0.097 =============
new_qn: 0.08809026868327441
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.178440634800664
============= xn0: 0.098 =============
new_qn: 0.08899841578310197
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1793487819004913
============= xn0: 0.099 =============
new_qn: 0.08990656288292954
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.180256929000319
============= xn0: 0.1 =============
new_qn: 0.09081470998275712
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1811650761001464
============= xn0: 0.101 =============
new_qn: 0.0917228570825847
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1820732231999742
============= xn0: 0.10200000000000001 =============
new_qn: 0.09263100418241227
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1829813702998015
============= xn0: 0.10300000000000001 =============
new_qn: 0.09353915128223983
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1838895173996293
============= xn0: 0.10400000000000001 =============
new_qn: 0.0944472983820674
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1847976644994567
============= xn0: 0.105 =============
new_qn: 0.09535544548189497
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1857058115992845
============= xn0: 0.106 =============
new_qn: 0.09626359258172254
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1866139586991118
============= xn0: 0.107 =============
new_qn: 0.09717173968155011
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1875221057989396
============= xn0: 0.108 =============
new_qn: 0.09807988678137768
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.188430252898767
============= xn0: 0.109 =============
new_qn: 0.09898803388120525
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1893383999985947
============= xn0: 0.11 =============
new_qn: 0.09989618098103283
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.190246547098422
============= xn0: 0.111 =============
new_qn: 0.1008043280808604
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1911546941982498
============= xn0: 0.112 =============
new_qn: 0.10171247518068798
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1920628412980772
============= xn0: 0.113 =============
new_qn: 0.10262062228051554
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.192970988397905
============= xn0: 0.114 =============
new_qn: 0.10352876938034311
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1938791354977323
============= xn0: 0.115 =============
new_qn: 0.10443691648017069
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.19478728259756
============= xn0: 0.116 =============
new_qn: 0.10534506357999826
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1956954296973874
============= xn0: 0.117 =============
new_qn: 0.10625321067982584
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1966035767972152
============= xn0: 0.11800000000000001 =============
new_qn: 0.1071613577796534
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.197511723897043
============= xn0: 0.11900000000000001 =============
new_qn: 0.10806950487948097
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1984198709968703
============= xn0: 0.12 =============
new_qn: 0.10897765197930853
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.199328018096698
============= xn0: 0.121 =============
new_qn: 0.1098857990791361
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2002361651965254
============= xn0: 0.122 =============
new_qn: 0.11079394617896368
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2011443122963532
============= xn0: 0.123 =============
new_qn: 0.11170209327879124
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2020524593961806
============= xn0: 0.124 =============
new_qn: 0.11261024037861882
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2029606064960083
============= xn0: 0.125 =============
new_qn: 0.11351838747844639
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2038687535958357
============= xn0: 0.126 =============
new_qn: 0.11442653457827397
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2047769006956635
============= xn0: 0.127 =============
new_qn: 0.11533468167810154
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2056850477954908
============= xn0: 0.128 =============
new_qn: 0.1162428287779291
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2065931948953186
============= xn0: 0.129 =============
new_qn: 0.11715097587775668
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.207501341995146
============= xn0: 0.13 =============
new_qn: 0.11805912297758425
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2084094890949737
============= xn0: 0.131 =============
new_qn: 0.11896727007741183
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.209317636194801
============= xn0: 0.132 =============
new_qn: 0.1198754171772394
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2102257832946288
============= xn0: 0.133 =============
new_qn: 0.12078356427706696
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2111339303944562
============= xn0: 0.134 =============
new_qn: 0.12169171137689454
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.212042077494284
============= xn0: 0.135 =============
new_qn: 0.12259985847672211
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2129502245941113
============= xn0: 0.136 =============
new_qn: 0.12350800557654969
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.213858371693939
============= xn0: 0.137 =============
new_qn: 0.12441615267637726
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2147665187937664
============= xn0: 0.138 =============
new_qn: 0.12532429977620482
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2156746658935942
============= xn0: 0.139 =============
new_qn: 0.1262324468760324
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.216582812993422
============= xn0: 0.14 =============
new_qn: 0.12714059397585997
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2174909600932493
============= xn0: 0.14100000000000001 =============
new_qn: 0.12804874107568753
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.218399107193077
============= xn0: 0.14200000000000002 =============
new_qn: 0.12895688817551512
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2193072542929044
============= xn0: 0.14300000000000002 =============
new_qn: 0.12986503527534268
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2202154013927322
============= xn0: 0.14400000000000002 =============
new_qn: 0.13077318237517027
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2211235484925596
============= xn0: 0.145 =============
new_qn: 0.1316813294749978
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2220316955923873
============= xn0: 0.146 =============
new_qn: 0.13258947657482537
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2229398426922147
============= xn0: 0.147 =============
new_qn: 0.13349762367465295
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2238479897920425
============= xn0: 0.148 =============
new_qn: 0.13440577077448052
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2247561368918698
============= xn0: 0.149 =============
new_qn: 0.1353139178743081
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2256642839916976
============= xn0: 0.15 =============
new_qn: 0.13622206497413566
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.226572431091525
============= xn0: 0.151 =============
new_qn: 0.13713021207396323
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2274805781913527
============= xn0: 0.152 =============
new_qn: 0.13803835917379081
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.22838872529118
============= xn0: 0.153 =============
new_qn: 0.13894650627361838
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2292968723910078
============= xn0: 0.154 =============
new_qn: 0.13985465337344596
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2302050194908352
============= xn0: 0.155 =============
new_qn: 0.14076280047327352
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.231113166590663
============= xn0: 0.156 =============
new_qn: 0.14167094757310109
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2320213136904903
============= xn0: 0.157 =============
new_qn: 0.14257909467292867
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.232929460790318
============= xn0: 0.158 =============
new_qn: 0.14348724177275624
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2338376078901454
============= xn0: 0.159 =============
new_qn: 0.14439538887258382
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2347457549899732
============= xn0: 0.16 =============
new_qn: 0.14530353597241139
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2356539020898005
============= xn0: 0.161 =============
new_qn: 0.14621168307223895
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2365620491896283
============= xn0: 0.162 =============
new_qn: 0.14711983017206653
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.237470196289456
============= xn0: 0.163 =============
new_qn: 0.1480279772718941
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2383783433892834
============= xn0: 0.164 =============
new_qn: 0.14893612437172168
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2392864904891112
============= xn0: 0.165 =============
new_qn: 0.14984427147154925
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2401946375889386
============= xn0: 0.166 =============
new_qn: 0.1507524185713768
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2411027846887663
============= xn0: 0.167 =============
new_qn: 0.1516605656712044
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2420109317885937
============= xn0: 0.168 =============
new_qn: 0.15256871277103196
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2429190788884215
============= xn0: 0.169 =============
new_qn: 0.15347685987085954
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2438272259882488
============= xn0: 0.17 =============
new_qn: 0.1543850069706871
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2447353730880766
============= xn0: 0.171 =============
new_qn: 0.15529315407051467
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.245643520187904
============= xn0: 0.17200000000000001 =============
new_qn: 0.15620130117034225
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2465516672877317
============= xn0: 0.17300000000000001 =============
new_qn: 0.15710944827016982
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.247459814387559
============= xn0: 0.17400000000000002 =============
new_qn: 0.1580175953699974
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2483679614873868
============= xn0: 0.17500000000000002 =============
new_qn: 0.15892574246982497
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2492761085872142
============= xn0: 0.176 =============
new_qn: 0.1598338895696525
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.250184255687042
============= xn0: 0.177 =============
new_qn: 0.1607420366694801
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2510924027868693
============= xn0: 0.178 =============
new_qn: 0.16165018376930765
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.252000549886697
============= xn0: 0.179 =============
new_qn: 0.16255833086913524
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2529086969865244
============= xn0: 0.18 =============
new_qn: 0.1634664779689628
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2538168440863522
============= xn0: 0.181 =============
new_qn: 0.16437462506879036
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2547249911861795
============= xn0: 0.182 =============
new_qn: 0.16528277216861795
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2556331382860073
============= xn0: 0.183 =============
new_qn: 0.1661909192684455
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2565412853858346
============= xn0: 0.184 =============
new_qn: 0.1670990663682731
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2574494324856624
============= xn0: 0.185 =============
new_qn: 0.16800721346810066
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2583575795854902
============= xn0: 0.186 =============
new_qn: 0.16891536056792822
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2592657266853176
============= xn0: 0.187 =============
new_qn: 0.1698235076677558
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2601738737851453
============= xn0: 0.188 =============
new_qn: 0.17073165476758337
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2610820208849727
============= xn0: 0.189 =============
new_qn: 0.17163980186741096
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2619901679848005
============= xn0: 0.19 =============
new_qn: 0.17254794896723852
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2628983150846278
============= xn0: 0.191 =============
new_qn: 0.17345609606706608
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2638064621844556
============= xn0: 0.192 =============
new_qn: 0.17436424316689367
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.264714609284283
============= xn0: 0.193 =============
new_qn: 0.17527239026672123
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2656227563841107
============= xn0: 0.194 =============
new_qn: 0.17618053736654882
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.266530903483938
============= xn0: 0.195 =============
new_qn: 0.17708868446637638
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2674390505837658
============= xn0: 0.196 =============
new_qn: 0.17799683156620394
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2683471976835932
============= xn0: 0.197 =============
new_qn: 0.17890497866603153
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.269255344783421
============= xn0: 0.198 =============
new_qn: 0.1798131257658591
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2701634918832483
============= xn0: 0.199 =============
new_qn: 0.18072127286568668
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.271071638983076
============= xn0: 0.2 =============
new_qn: 0.18162941996551424
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2719797860829034
============= xn0: 0.201 =============
new_qn: 0.1825375670653418
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2728879331827312
============= xn0: 0.202 =============
new_qn: 0.1834457141651694
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2737960802825585
============= xn0: 0.203 =============
new_qn: 0.18435386126499695
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2747042273823863
============= xn0: 0.20400000000000001 =============
new_qn: 0.18526200836482454
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.275612374482214
============= xn0: 0.20500000000000002 =============
new_qn: 0.1861701554646521
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2765205215820414
============= xn0: 0.20600000000000002 =============
new_qn: 0.18707830256447966
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2774286686818692
============= xn0: 0.20700000000000002 =============
new_qn: 0.18798644966430725
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2783368157816966
============= xn0: 0.20800000000000002 =============
new_qn: 0.1888945967641348
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2792449628815243
============= xn0: 0.209 =============
new_qn: 0.18980274386396237
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2801531099813517
============= xn0: 0.21 =============
new_qn: 0.19071089096378993
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2810612570811795
============= xn0: 0.211 =============
new_qn: 0.1916190380636175
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2819694041810068
============= xn0: 0.212 =============
new_qn: 0.19252718516344508
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2828775512808346
============= xn0: 0.213 =============
new_qn: 0.19343533226327264
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.283785698380662
============= xn0: 0.214 =============
new_qn: 0.19434347936310023
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2846938454804897
============= xn0: 0.215 =============
new_qn: 0.1952516264629278
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.285601992580317
============= xn0: 0.216 =============
new_qn: 0.19615977356275535
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2865101396801448
============= xn0: 0.217 =============
new_qn: 0.19706792066258294
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2874182867799722
============= xn0: 0.218 =============
new_qn: 0.1979760677624105
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2883264338798
============= xn0: 0.219 =============
new_qn: 0.1988842148622381
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2892345809796273
============= xn0: 0.22 =============
new_qn: 0.19979236196206565
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.290142728079455
============= xn0: 0.221 =============
new_qn: 0.2007005090618932
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2910508751792824
============= xn0: 0.222 =============
new_qn: 0.2016086561617208
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2919590222791102
============= xn0: 0.223 =============
new_qn: 0.20251680326154836
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2928671693789375
============= xn0: 0.224 =============
new_qn: 0.20342495036137595
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2937753164787653
============= xn0: 0.225 =============
new_qn: 0.2043330974612035
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2946834635785927
============= xn0: 0.226 =============
new_qn: 0.20524124456103107
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2955916106784204
============= xn0: 0.227 =============
new_qn: 0.20614939166085866
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2964997577782482
============= xn0: 0.228 =============
new_qn: 0.20705753876068622
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2974079048780756
============= xn0: 0.229 =============
new_qn: 0.2079656858605138
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2983160519779033
============= xn0: 0.23 =============
new_qn: 0.20887383296034137
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2992241990777307
============= xn0: 0.231 =============
new_qn: 0.20978198006016893
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3001323461775585
============= xn0: 0.232 =============
new_qn: 0.21069012715999652
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3010404932773858
============= xn0: 0.233 =============
new_qn: 0.21159827425982408
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3019486403772136
============= xn0: 0.234 =============
new_qn: 0.21250642135965167
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.302856787477041
============= xn0: 0.23500000000000001 =============
new_qn: 0.21341456845947923
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3037649345768687
============= xn0: 0.23600000000000002 =============
new_qn: 0.2143227155593068
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.304673081676696
============= xn0: 0.23700000000000002 =============
new_qn: 0.21523086265913438
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3055812287765238
============= xn0: 0.23800000000000002 =============
new_qn: 0.21613900975896194
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3064893758763512
============= xn0: 0.23900000000000002 =============
new_qn: 0.21704715685878953
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.307397522976179
============= xn0: 0.24 =============
new_qn: 0.21795530395861706
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3083056700760063
============= xn0: 0.241 =============
new_qn: 0.21886345105844462
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.309213817175834
============= xn0: 0.242 =============
new_qn: 0.2197715981582722
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3101219642756614
============= xn0: 0.243 =============
new_qn: 0.22067974525809977
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3110301113754892
============= xn0: 0.244 =============
new_qn: 0.22158789235792736
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3119382584753165
============= xn0: 0.245 =============
new_qn: 0.22249603945775492
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3128464055751443
============= xn0: 0.246 =============
new_qn: 0.22340418655758248
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3137545526749717
============= xn0: 0.247 =============
new_qn: 0.22431233365741007
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3146626997747994
============= xn0: 0.248 =============
new_qn: 0.22522048075723763
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3155708468746268
============= xn0: 0.249 =============
new_qn: 0.22612862785706522
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3164789939744546
============= xn0: 0.25 =============
new_qn: 0.22703677495689278
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3173871410742823
============= xn0: 0.251 =============
new_qn: 0.22794492205672034
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3182952881741097
============= xn0: 0.252 =============
new_qn: 0.22885306915654793
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3192034352739375
============= xn0: 0.253 =============
new_qn: 0.2297612162563755
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3201115823737648
============= xn0: 0.254 =============
new_qn: 0.23066936335620308
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3210197294735926
============= xn0: 0.255 =============
new_qn: 0.23157751045603064
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.32192787657342
============= xn0: 0.256 =============
new_qn: 0.2324856575558582
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3228360236732477
============= xn0: 0.257 =============
new_qn: 0.2333938046556858
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.323744170773075
============= xn0: 0.258 =============
new_qn: 0.23430195175551335
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3246523178729028
============= xn0: 0.259 =============
new_qn: 0.23521009885534094
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3255604649727302
============= xn0: 0.26 =============
new_qn: 0.2361182459551685
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.326468612072558
============= xn0: 0.261 =============
new_qn: 0.23702639305499607
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3273767591723853
============= xn0: 0.262 =============
new_qn: 0.23793454015482365
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.328284906272213
============= xn0: 0.263 =============
new_qn: 0.23884268725465121
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3291930533720404
============= xn0: 0.264 =============
new_qn: 0.2397508343544788
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3301012004718682
============= xn0: 0.265 =============
new_qn: 0.24065898145430636
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3310093475716955
============= xn0: 0.266 =============
new_qn: 0.24156712855413393
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3319174946715233
============= xn0: 0.267 =============
new_qn: 0.24247527565396151
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3328256417713507
============= xn0: 0.268 =============
new_qn: 0.24338342275378907
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3337337888711784
============= xn0: 0.269 =============
new_qn: 0.24429156985361666
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3346419359710062
============= xn0: 0.27 =============
new_qn: 0.24519971695344422
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3355500830708336
============= xn0: 0.271 =============
new_qn: 0.24610786405327179
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3364582301706613
============= xn0: 0.272 =============
new_qn: 0.24701601115309937
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3373663772704887
============= xn0: 0.273 =============
new_qn: 0.24792415825292693
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3382745243703165
============= xn0: 0.274 =============
new_qn: 0.24883230535275452
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3391826714701438
============= xn0: 0.275 =============
new_qn: 0.24974045245258208
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3400908185699716
============= xn0: 0.276 =============
new_qn: 0.25064859955240965
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.340998965669799
============= xn0: 0.277 =============
new_qn: 0.2515567466522372
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3419071127696267
============= xn0: 0.278 =============
new_qn: 0.2524648937520648
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.342815259869454
============= xn0: 0.279 =============
new_qn: 0.2533730408518924
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3437234069692818
============= xn0: 0.28 =============
new_qn: 0.25428118795171994
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3446315540691092
============= xn0: 0.281 =============
new_qn: 0.2551893350515475
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.345539701168937
============= xn0: 0.28200000000000003 =============
new_qn: 0.25609748215137507
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3464478482687643
============= xn0: 0.28300000000000003 =============
new_qn: 0.2570056292512027
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.347355995368592
============= xn0: 0.28400000000000003 =============
new_qn: 0.25791377635103024
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3482641424684194
============= xn0: 0.28500000000000003 =============
new_qn: 0.2588219234508578
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3491722895682472
============= xn0: 0.28600000000000003 =============
new_qn: 0.25973007055068537
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3500804366680745
============= xn0: 0.28700000000000003 =============
new_qn: 0.2606382176505129
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3509885837679023
============= xn0: 0.28800000000000003 =============
new_qn: 0.26154636475034054
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.35189673086773
============= xn0: 0.289 =============
new_qn: 0.26245451185016805
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3528048779675574
============= xn0: 0.29 =============
new_qn: 0.2633626589499956
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3537130250673848
============= xn0: 0.291 =============
new_qn: 0.26427080604982317
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3546211721672126
============= xn0: 0.292 =============
new_qn: 0.26517895314965073
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.35552931926704
============= xn0: 0.293 =============
new_qn: 0.26608710024947835
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3564374663668677
============= xn0: 0.294 =============
new_qn: 0.2669952473493059
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3573456134666955
============= xn0: 0.295 =============
new_qn: 0.26790339444913347
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3582537605665228
============= xn0: 0.296 =============
new_qn: 0.26881154154896103
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3591619076663506
============= xn0: 0.297 =============
new_qn: 0.2697196886487886
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.360070054766178
============= xn0: 0.298 =============
new_qn: 0.2706278357486162
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3609782018660057
============= xn0: 0.299 =============
new_qn: 0.27153598284844377
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.361886348965833
============= xn0: 0.3 =============
new_qn: 0.27244412994827133
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3627944960656608
============= xn0: 0.301 =============
new_qn: 0.2733522770480989
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3637026431654882
============= xn0: 0.302 =============
new_qn: 0.27426042414792645
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.364610790265316
============= xn0: 0.303 =============
new_qn: 0.27516857124775407
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3655189373651433
============= xn0: 0.304 =============
new_qn: 0.27607671834758163
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.366427084464971
============= xn0: 0.305 =============
new_qn: 0.2769848654474092
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3673352315647984
============= xn0: 0.306 =============
new_qn: 0.27789301254723675
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3682433786646262
============= xn0: 0.307 =============
new_qn: 0.2788011596470643
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3691515257644535
============= xn0: 0.308 =============
new_qn: 0.27970930674689193
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3700596728642813
============= xn0: 0.309 =============
new_qn: 0.2806174538467195
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3709678199641087
============= xn0: 0.31 =============
new_qn: 0.28152560094654705
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3718759670639364
============= xn0: 0.311 =============
new_qn: 0.2824337480463746
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3727841141637638
============= xn0: 0.312 =============
new_qn: 0.28334189514620217
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3736922612635916
============= xn0: 0.313 =============
new_qn: 0.2842500422460298
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3746004083634193
============= xn0: 0.314 =============
new_qn: 0.28515818934585735
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3755085554632467
============= xn0: 0.315 =============
new_qn: 0.2860663364456849
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3764167025630745
============= xn0: 0.316 =============
new_qn: 0.28697448354551247
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3773248496629018
============= xn0: 0.317 =============
new_qn: 0.28788263064534003
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3782329967627296
============= xn0: 0.318 =============
new_qn: 0.28879077774516765
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.379141143862557
============= xn0: 0.319 =============
new_qn: 0.2896989248449952
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3800492909623847
============= xn0: 0.32 =============
new_qn: 0.29060707194482277
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.380957438062212
============= xn0: 0.321 =============
new_qn: 0.29151521904465033
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3818655851620398
============= xn0: 0.322 =============
new_qn: 0.2924233661444779
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3827737322618672
============= xn0: 0.323 =============
new_qn: 0.2933315132443055
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.383681879361695
============= xn0: 0.324 =============
new_qn: 0.29423966034413307
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3845900264615223
============= xn0: 0.325 =============
new_qn: 0.29514780744396063
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.38549817356135
============= xn0: 0.326 =============
new_qn: 0.2960559545437882
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3864063206611774
============= xn0: 0.327 =============
new_qn: 0.29696410164361575
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3873144677610052
============= xn0: 0.328 =============
new_qn: 0.29787224874344337
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3882226148608325
============= xn0: 0.329 =============
new_qn: 0.29878039584327093
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3891307619606603
============= xn0: 0.33 =============
new_qn: 0.2996885429430985
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3900389090604877
============= xn0: 0.331 =============
new_qn: 0.30059669004292605
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3909470561603154
============= xn0: 0.332 =============
new_qn: 0.3015048371427536
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3918552032601428
============= xn0: 0.333 =============
new_qn: 0.30241298424258123
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3927633503599706
============= xn0: 0.334 =============
new_qn: 0.3033211313424088
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3936714974597983
============= xn0: 0.335 =============
new_qn: 0.30422927844223635
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3945796445596257
============= xn0: 0.336 =============
new_qn: 0.3051374255420639
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3954877916594535
============= xn0: 0.337 =============
new_qn: 0.3060455726418915
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3963959387592808
============= xn0: 0.338 =============
new_qn: 0.3069537197417191
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3973040858591086
============= xn0: 0.339 =============
new_qn: 0.30786186684154665
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.398212232958936
============= xn0: 0.34 =============
new_qn: 0.3087700139413742
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3991203800587637
============= xn0: 0.341 =============
new_qn: 0.30967816104120177
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.400028527158591
============= xn0: 0.342 =============
new_qn: 0.31058630814102933
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4009366742584188
============= xn0: 0.343 =============
new_qn: 0.31149445524085695
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4018448213582462
============= xn0: 0.34400000000000003 =============
new_qn: 0.3124026023406845
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.402752968458074
============= xn0: 0.34500000000000003 =============
new_qn: 0.31331074944051207
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4036611155579013
============= xn0: 0.34600000000000003 =============
new_qn: 0.31421889654033963
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.404569262657729
============= xn0: 0.34700000000000003 =============
new_qn: 0.3151270436401672
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4054774097575564
============= xn0: 0.34800000000000003 =============
new_qn: 0.3160351907399948
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4063855568573842
============= xn0: 0.34900000000000003 =============
new_qn: 0.31694333783982237
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4072937039572115
============= xn0: 0.35000000000000003 =============
new_qn: 0.31785148493964993
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4082018510570393
============= xn0: 0.35100000000000003 =============
new_qn: 0.3187596320394775
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4091099981568667
============= xn0: 0.352 =============
new_qn: 0.319667779139305
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4100181452566944
============= xn0: 0.353 =============
new_qn: 0.3205759262391326
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4109262923565218
============= xn0: 0.354 =============
new_qn: 0.3214840733389602
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4118344394563496
============= xn0: 0.355 =============
new_qn: 0.32239222043878774
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.412742586556177
============= xn0: 0.356 =============
new_qn: 0.3233003675386153
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4136507336560047
============= xn0: 0.357 =============
new_qn: 0.32420851463844286
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.414558880755832
============= xn0: 0.358 =============
new_qn: 0.3251166617382705
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4154670278556598
============= xn0: 0.359 =============
new_qn: 0.32602480883809803
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4163751749554876
============= xn0: 0.36 =============
new_qn: 0.3269329559379256
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.417283322055315
============= xn0: 0.361 =============
new_qn: 0.32784110303775316
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4181914691551427
============= xn0: 0.362 =============
new_qn: 0.3287492501375807
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.41909961625497
============= xn0: 0.363 =============
new_qn: 0.32965739723740833
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4200077633547978
============= xn0: 0.364 =============
new_qn: 0.3305655443372359
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4209159104546252
============= xn0: 0.365 =============
new_qn: 0.33147369143706346
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.421824057554453
============= xn0: 0.366 =============
new_qn: 0.332381838536891
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4227322046542803
============= xn0: 0.367 =============
new_qn: 0.3332899856367186
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.423640351754108
============= xn0: 0.368 =============
new_qn: 0.3341981327365462
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4245484988539359
============= xn0: 0.369 =============
new_qn: 0.33510627983637375
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4254566459537632
============= xn0: 0.37 =============
new_qn: 0.3360144269362013
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.426364793053591
============= xn0: 0.371 =============
new_qn: 0.3369225740360289
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4272729401534183
============= xn0: 0.372 =============
new_qn: 0.33783072113585644
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.428181087253246
============= xn0: 0.373 =============
new_qn: 0.33873886823568405
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4290892343530734
============= xn0: 0.374 =============
new_qn: 0.3396470153355116
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4299973814529012
============= xn0: 0.375 =============
new_qn: 0.3405551624353392
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4309055285527286
============= xn0: 0.376 =============
new_qn: 0.34146330953516674
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4318136756525564
============= xn0: 0.377 =============
new_qn: 0.3423714566349943
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4327218227523837
============= xn0: 0.378 =============
new_qn: 0.3432796037348219
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4336299698522115
============= xn0: 0.379 =============
new_qn: 0.3441877508346495
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4345381169520393
============= xn0: 0.38 =============
new_qn: 0.34509589793447704
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4354462640518666
============= xn0: 0.381 =============
new_qn: 0.3460040450343046
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4363544111516944
============= xn0: 0.382 =============
new_qn: 0.34691219213413216
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4372625582515217
============= xn0: 0.383 =============
new_qn: 0.3478203392339598
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4381707053513495
============= xn0: 0.384 =============
new_qn: 0.34872848633378734
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4390788524511768
============= xn0: 0.385 =============
new_qn: 0.3496366334336149
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4399869995510046
============= xn0: 0.386 =============
new_qn: 0.35054478053344246
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.440895146650832
============= xn0: 0.387 =============
new_qn: 0.35145292763327
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4418032937506597
============= xn0: 0.388 =============
new_qn: 0.35236107473309763
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.442711440850487
============= xn0: 0.389 =============
new_qn: 0.3532692218329252
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4436195879503149
============= xn0: 0.39 =============
new_qn: 0.35417736893275276
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4445277350501422
============= xn0: 0.391 =============
new_qn: 0.3550855160325803
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.44543588214997
============= xn0: 0.392 =============
new_qn: 0.3559936631324079
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4463440292497973
============= xn0: 0.393 =============
new_qn: 0.3569018102322355
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.447252176349625
============= xn0: 0.394 =============
new_qn: 0.35780995733206306
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4481603234494524
============= xn0: 0.395 =============
new_qn: 0.3587181044318906
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4490684705492802
============= xn0: 0.396 =============
new_qn: 0.3596262515317182
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4499766176491076
============= xn0: 0.397 =============
new_qn: 0.36053439863154574
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4508847647489354
============= xn0: 0.398 =============
new_qn: 0.36144254573137335
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4517929118487631
============= xn0: 0.399 =============
new_qn: 0.3623506928312009
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4527010589485905
============= xn0: 0.4 =============
new_qn: 0.3632588399310285
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4536092060484183
============= xn0: 0.401 =============
new_qn: 0.36416698703085604
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4545173531482456
============= xn0: 0.402 =============
new_qn: 0.3650751341306836
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4554255002480734
============= xn0: 0.403 =============
new_qn: 0.3659832812305112
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4563336473479007
============= xn0: 0.404 =============
new_qn: 0.3668914283303388
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4572417944477285
============= xn0: 0.405 =============
new_qn: 0.36779957543016634
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4581499415475558
============= xn0: 0.406 =============
new_qn: 0.3687077225299939
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4590580886473836
============= xn0: 0.40700000000000003 =============
new_qn: 0.36961586962982146
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.459966235747211
============= xn0: 0.40800000000000003 =============
new_qn: 0.3705240167296491
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4608743828470387
============= xn0: 0.40900000000000003 =============
new_qn: 0.37143216382947664
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.461782529946866
============= xn0: 0.41000000000000003 =============
new_qn: 0.3723403109293042
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4626906770466939
============= xn0: 0.41100000000000003 =============
new_qn: 0.37324845802913176
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4635988241465212
============= xn0: 0.41200000000000003 =============
new_qn: 0.3741566051289593
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.464506971246349
============= xn0: 0.41300000000000003 =============
new_qn: 0.37506475222878694
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4654151183461763
============= xn0: 0.41400000000000003 =============
new_qn: 0.3759728993286145
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.466323265446004
============= xn0: 0.41500000000000004 =============
new_qn: 0.37688104642844206
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4672314125458314
============= xn0: 0.41600000000000004 =============
new_qn: 0.3777891935282696
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4681395596456592
============= xn0: 0.417 =============
new_qn: 0.3786973406280971
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4690477067454866
============= xn0: 0.418 =============
new_qn: 0.37960548772792474
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4699558538453144
============= xn0: 0.419 =============
new_qn: 0.3805136348277523
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4708640009451417
============= xn0: 0.42 =============
new_qn: 0.38142178192757986
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4717721480449695
============= xn0: 0.421 =============
new_qn: 0.3823299290274074
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4726802951447968
============= xn0: 0.422 =============
new_qn: 0.383238076127235
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4735884422446246
============= xn0: 0.423 =============
new_qn: 0.3841462232270626
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4744965893444524
============= xn0: 0.424 =============
new_qn: 0.38505437032689016
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4754047364442797
============= xn0: 0.425 =============
new_qn: 0.3859625174267177
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4763128835441075
============= xn0: 0.426 =============
new_qn: 0.3868706645265453
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4772210306439348
============= xn0: 0.427 =============
new_qn: 0.38777881162637284
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4781291777437626
============= xn0: 0.428 =============
new_qn: 0.38868695872620046
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.47903732484359
============= xn0: 0.429 =============
new_qn: 0.389595105826028
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4799454719434177
============= xn0: 0.43 =============
new_qn: 0.3905032529258556
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.480853619043245
============= xn0: 0.431 =============
new_qn: 0.39141140002568314
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4817617661430729
============= xn0: 0.432 =============
new_qn: 0.3923195471255107
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4826699132429002
============= xn0: 0.433 =============
new_qn: 0.3932276942253383
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.483578060342728
============= xn0: 0.434 =============
new_qn: 0.3941358413251659
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4844862074425553
============= xn0: 0.435 =============
new_qn: 0.39504398842499344
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.485394354542383
============= xn0: 0.436 =============
new_qn: 0.395952135524821
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4863025016422104
============= xn0: 0.437 =============
new_qn: 0.39686028262464856
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4872106487420382
============= xn0: 0.438 =============
new_qn: 0.3977684297244762
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4881187958418656
============= xn0: 0.439 =============
new_qn: 0.39867657682430374
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4890269429416934
============= xn0: 0.44 =============
new_qn: 0.3995847239241313
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4899350900415207
============= xn0: 0.441 =============
new_qn: 0.40049287102395886
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4908432371413485
============= xn0: 0.442 =============
new_qn: 0.4014010181237864
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4917513842411758
============= xn0: 0.443 =============
new_qn: 0.40230916522361404
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4926595313410036
============= xn0: 0.444 =============
new_qn: 0.4032173123234416
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4935676784408314
============= xn0: 0.445 =============
new_qn: 0.40412545942326916
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4944758255406587
============= xn0: 0.446 =============
new_qn: 0.4050336065230967
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4953839726404865
============= xn0: 0.447 =============
new_qn: 0.4059417536229243
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4962921197403138
============= xn0: 0.448 =============
new_qn: 0.4068499007227519
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4972002668401416
============= xn0: 0.449 =============
new_qn: 0.40775804782257946
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.498108413939969
============= xn0: 0.45 =============
new_qn: 0.408666194922407
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4990165610397967
============= xn0: 0.451 =============
new_qn: 0.4095743420222346
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.499924708139624
============= xn0: 0.452 =============
new_qn: 0.41048248912206214
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5008328552394519
============= xn0: 0.453 =============
new_qn: 0.41139063622188976
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5017410023392792
============= xn0: 0.454 =============
new_qn: 0.4122987833217173
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.502649149439107
============= xn0: 0.455 =============
new_qn: 0.4132069304215449
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5035572965389343
============= xn0: 0.456 =============
new_qn: 0.41411507752137244
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5044654436387621
============= xn0: 0.457 =============
new_qn: 0.4150232246212
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5053735907385895
============= xn0: 0.458 =============
new_qn: 0.4159313717210276
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5062817378384172
============= xn0: 0.459 =============
new_qn: 0.4168395188208552
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5071898849382446
============= xn0: 0.46 =============
new_qn: 0.41774766592068274
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5080980320380724
============= xn0: 0.461 =============
new_qn: 0.4186558130205103
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5090061791378997
============= xn0: 0.462 =============
new_qn: 0.41956396012033786
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5099143262377275
============= xn0: 0.463 =============
new_qn: 0.4204721072201655
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5108224733375553
============= xn0: 0.464 =============
new_qn: 0.42138025431999304
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5117306204373826
============= xn0: 0.465 =============
new_qn: 0.4222884014198206
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5126387675372104
============= xn0: 0.466 =============
new_qn: 0.42319654851964816
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5135469146370377
============= xn0: 0.467 =============
new_qn: 0.4241046956194757
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5144550617368655
============= xn0: 0.468 =============
new_qn: 0.42501284271930334
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5153632088366928
============= xn0: 0.46900000000000003 =============
new_qn: 0.4259209898191309
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5162713559365206
============= xn0: 0.47000000000000003 =============
new_qn: 0.42682913691895846
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.517179503036348
============= xn0: 0.47100000000000003 =============
new_qn: 0.427737284018786
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5180876501361757
============= xn0: 0.47200000000000003 =============
new_qn: 0.4286454311186136
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.518995797236003
============= xn0: 0.47300000000000003 =============
new_qn: 0.4295535782184412
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5199039443358309
============= xn0: 0.47400000000000003 =============
new_qn: 0.43046172531826876
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5208120914356582
============= xn0: 0.47500000000000003 =============
new_qn: 0.4313698724180963
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.521720238535486
============= xn0: 0.47600000000000003 =============
new_qn: 0.4322780195179239
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5226283856353133
============= xn0: 0.47700000000000004 =============
new_qn: 0.43318616661775144
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5235365327351411
============= xn0: 0.47800000000000004 =============
new_qn: 0.43409431371757906
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5244446798349685
============= xn0: 0.47900000000000004 =============
new_qn: 0.4350024608174066
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5253528269347962
============= xn0: 0.48 =============
new_qn: 0.4359106079172341
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5262609740346236
============= xn0: 0.481 =============
new_qn: 0.4368187550170617
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5271691211344514
============= xn0: 0.482 =============
new_qn: 0.43772690211688925
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5280772682342787
============= xn0: 0.483 =============
new_qn: 0.43863504921671687
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5289854153341065
============= xn0: 0.484 =============
new_qn: 0.4395431963165444
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5298935624339338
============= xn0: 0.485 =============
new_qn: 0.440451343416372
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5308017095337616
============= xn0: 0.486 =============
new_qn: 0.44135949051619955
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.531709856633589
============= xn0: 0.487 =============
new_qn: 0.4422676376160271
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5326180037334167
============= xn0: 0.488 =============
new_qn: 0.4431757847158547
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5335261508332445
============= xn0: 0.489 =============
new_qn: 0.4440839318156823
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5344342979330718
============= xn0: 0.49 =============
new_qn: 0.44499207891550985
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5353424450328996
============= xn0: 0.491 =============
new_qn: 0.4459002260153374
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.536250592132727
============= xn0: 0.492 =============
new_qn: 0.44680837311516497
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5371587392325547
============= xn0: 0.493 =============
new_qn: 0.4477165202149926
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.538066886332382
============= xn0: 0.494 =============
new_qn: 0.44862466731482015
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5389750334322099
============= xn0: 0.495 =============
new_qn: 0.4495328144146477
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5398831805320372
============= xn0: 0.496 =============
new_qn: 0.45044096151447527
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.540791327631865
============= xn0: 0.497 =============
new_qn: 0.45134910861430283
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5416994747316923
============= xn0: 0.498 =============
new_qn: 0.45225725571413045
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5426076218315201
============= xn0: 0.499 =============
new_qn: 0.453165402813958
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5435157689313475
============= xn0: 0.5 =============
new_qn: 0.45407354991378557
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5444239160311752
============= xn0: 0.501 =============
new_qn: 0.45498169701361313
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5453320631310026
============= xn0: 0.502 =============
new_qn: 0.4558898441134407
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5462402102308304
============= xn0: 0.503 =============
new_qn: 0.4567979912132683
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5471483573306577
============= xn0: 0.504 =============
new_qn: 0.45770613831309587
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5480565044304855
============= xn0: 0.505 =============
new_qn: 0.45861428541292343
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5489646515303128
============= xn0: 0.506 =============
new_qn: 0.459522432512751
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5498727986301406
============= xn0: 0.507 =============
new_qn: 0.46043057961257855
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.550780945729968
============= xn0: 0.508 =============
new_qn: 0.46133872671240617
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5516890928297957
============= xn0: 0.509 =============
new_qn: 0.4622468738122337
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5525972399296235
============= xn0: 0.51 =============
new_qn: 0.4631550209120613
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5535053870294508
============= xn0: 0.511 =============
new_qn: 0.46406316801188885
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5544135341292786
============= xn0: 0.512 =============
new_qn: 0.4649713151117164
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.555321681229106
============= xn0: 0.513 =============
new_qn: 0.465879462211544
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5562298283289338
============= xn0: 0.514 =============
new_qn: 0.4667876093113716
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.557137975428761
============= xn0: 0.515 =============
new_qn: 0.46769575641119915
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5580461225285889
============= xn0: 0.516 =============
new_qn: 0.4686039035110267
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5589542696284162
============= xn0: 0.517 =============
new_qn: 0.46951205061085427
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.559862416728244
============= xn0: 0.518 =============
new_qn: 0.4704201977106819
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5607705638280713
============= xn0: 0.519 =============
new_qn: 0.47132834481050945
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5616787109278991
============= xn0: 0.52 =============
new_qn: 0.472236491910337
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5625868580277265
============= xn0: 0.521 =============
new_qn: 0.47314463901016457
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5634950051275542
============= xn0: 0.522 =============
new_qn: 0.47405278610999213
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5644031522273816
============= xn0: 0.523 =============
new_qn: 0.47496093320981975
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5653112993272094
============= xn0: 0.524 =============
new_qn: 0.4758690803096473
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5662194464270367
============= xn0: 0.525 =============
new_qn: 0.47677722740947487
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5671275935268645
============= xn0: 0.526 =============
new_qn: 0.47768537450930243
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5680357406266918
============= xn0: 0.527 =============
new_qn: 0.47859352160913
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5689438877265196
============= xn0: 0.528 =============
new_qn: 0.4795016687089576
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5698520348263474
============= xn0: 0.529 =============
new_qn: 0.48040981580878517
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5707601819261747
============= xn0: 0.53 =============
new_qn: 0.48131796290861273
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5716683290260025
============= xn0: 0.531 =============
new_qn: 0.4822261100084403
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5725764761258298
============= xn0: 0.532 =============
new_qn: 0.48313425710826785
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5734846232256576
============= xn0: 0.533 =============
new_qn: 0.48404240420809547
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.574392770325485
============= xn0: 0.534 =============
new_qn: 0.48495055130792303
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5753009174253128
============= xn0: 0.535 =============
new_qn: 0.4858586984077506
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.57620906452514
============= xn0: 0.536 =============
new_qn: 0.48676684550757815
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5771172116249679
============= xn0: 0.537 =============
new_qn: 0.4876749926074057
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5780253587247952
============= xn0: 0.538 =============
new_qn: 0.4885831397072333
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.578933505824623
============= xn0: 0.539 =============
new_qn: 0.4894912868070609
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5798416529244503
============= xn0: 0.54 =============
new_qn: 0.49039943390688845
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5807498000242781
============= xn0: 0.541 =============
new_qn: 0.491307581006716
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5816579471241055
============= xn0: 0.542 =============
new_qn: 0.49221572810654357
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5825660942239332
============= xn0: 0.543 =============
new_qn: 0.4931238752063712
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5834742413237606
============= xn0: 0.544 =============
new_qn: 0.49403202230619875
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5843823884235884
============= xn0: 0.545 =============
new_qn: 0.4949401694060263
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5852905355234157
============= xn0: 0.546 =============
new_qn: 0.49584831650585387
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5861986826232435
============= xn0: 0.547 =============
new_qn: 0.49675646360568143
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5871068297230708
============= xn0: 0.548 =============
new_qn: 0.49766461070550905
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5880149768228986
============= xn0: 0.549 =============
new_qn: 0.4985727578053366
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5889231239227264
============= xn0: 0.55 =============
new_qn: 0.49948090490516417
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5898312710225537
============= xn0: 0.551 =============
new_qn: 0.5003890520049917
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5907394181223815
============= xn0: 0.552 =============
new_qn: 0.5012971991048193
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5916475652222088
============= xn0: 0.553 =============
new_qn: 0.5022053462046469
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5925557123220366
============= xn0: 0.554 =============
new_qn: 0.5031134933044744
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.593463859421864
============= xn0: 0.555 =============
new_qn: 0.504021640404302
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5943720065216918
============= xn0: 0.556 =============
new_qn: 0.5049297875041296
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.595280153621519
============= xn0: 0.557 =============
new_qn: 0.5058379346039572
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5961883007213469
============= xn0: 0.558 =============
new_qn: 0.5067460817037848
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5970964478211742
============= xn0: 0.559 =============
new_qn: 0.5076542288036123
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.598004594921002
============= xn0: 0.56 =============
new_qn: 0.5085623759034399
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5989127420208293
============= xn0: 0.561 =============
new_qn: 0.5094705230032675
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5998208891206571
============= xn0: 0.562 =============
new_qn: 0.510378670103095
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6007290362204845
============= xn0: 0.5630000000000001 =============
new_qn: 0.5112868172029226
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6016371833203122
============= xn0: 0.5640000000000001 =============
new_qn: 0.5121949643027501
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6025453304201396
============= xn0: 0.5650000000000001 =============
new_qn: 0.5131031114025777
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6034534775199674
============= xn0: 0.5660000000000001 =============
new_qn: 0.5140112585024054
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6043616246197951
============= xn0: 0.5670000000000001 =============
new_qn: 0.5149194056022329
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6052697717196225
============= xn0: 0.5680000000000001 =============
new_qn: 0.5158275527020605
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6061779188194503
============= xn0: 0.5690000000000001 =============
new_qn: 0.516735699801888
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6070860659192776
============= xn0: 0.5700000000000001 =============
new_qn: 0.5176438469017156
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6079942130191054
============= xn0: 0.5710000000000001 =============
new_qn: 0.5185519940015432
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6089023601189327
============= xn0: 0.5720000000000001 =============
new_qn: 0.5194601411013707
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6098105072187605
============= xn0: 0.5730000000000001 =============
new_qn: 0.5203682882011983
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6107186543185879
============= xn0: 0.5740000000000001 =============
new_qn: 0.5212764353010259
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6116268014184156
============= xn0: 0.5750000000000001 =============
new_qn: 0.5221845824008534
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.612534948518243
============= xn0: 0.5760000000000001 =============
new_qn: 0.5230927295006811
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6134430956180708
============= xn0: 0.577 =============
new_qn: 0.5240008766005085
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.614351242717898
============= xn0: 0.578 =============
new_qn: 0.5249090237003361
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6152593898177259
============= xn0: 0.579 =============
new_qn: 0.5258171708001637
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6161675369175532
============= xn0: 0.58 =============
new_qn: 0.5267253178999912
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.617075684017381
============= xn0: 0.581 =============
new_qn: 0.5276334649998188
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6179838311172083
============= xn0: 0.582 =============
new_qn: 0.5285416120996463
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6188919782170361
============= xn0: 0.583 =============
new_qn: 0.5294497591994739
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6198001253168635
============= xn0: 0.584 =============
new_qn: 0.5303579062993015
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6207082724166912
============= xn0: 0.585 =============
new_qn: 0.5312660533991291
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6216164195165186
============= xn0: 0.586 =============
new_qn: 0.5321742004989567
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6225245666163464
============= xn0: 0.587 =============
new_qn: 0.5330823475987843
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6234327137161737
============= xn0: 0.588 =============
new_qn: 0.5339904946986118
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6243408608160015
============= xn0: 0.589 =============
new_qn: 0.5348986417984394
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6252490079158288
============= xn0: 0.59 =============
new_qn: 0.5358067888982669
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6261571550156566
============= xn0: 0.591 =============
new_qn: 0.5367149359980945
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.627065302115484
============= xn0: 0.592 =============
new_qn: 0.5376230830979221
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6279734492153117
============= xn0: 0.593 =============
new_qn: 0.5385312301977496
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.628881596315139
============= xn0: 0.594 =============
new_qn: 0.5394393772975772
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6297897434149669
============= xn0: 0.595 =============
new_qn: 0.5403475243974049
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6306978905147946
============= xn0: 0.596 =============
new_qn: 0.5412556714972324
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.631606037614622
============= xn0: 0.597 =============
new_qn: 0.54216381859706
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6325141847144498
============= xn0: 0.598 =============
new_qn: 0.5430719656968875
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.633422331814277
============= xn0: 0.599 =============
new_qn: 0.5439801127967151
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6343304789141049
============= xn0: 0.6 =============
new_qn: 0.5448882598965427
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6352386260139322
============= xn0: 0.601 =============
new_qn: 0.5457964069963702
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.63614677311376
============= xn0: 0.602 =============
new_qn: 0.5467045540961978
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6370549202135873
============= xn0: 0.603 =============
new_qn: 0.5476127011960253
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6379630673134151
============= xn0: 0.604 =============
new_qn: 0.5485208482958529
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6388712144132425
============= xn0: 0.605 =============
new_qn: 0.5494289953956806
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6397793615130702
============= xn0: 0.606 =============
new_qn: 0.5503371424955081
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6406875086128976
============= xn0: 0.607 =============
new_qn: 0.5512452895953357
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6415956557127254
============= xn0: 0.608 =============
new_qn: 0.5521534366951633
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6425038028125527
============= xn0: 0.609 =============
new_qn: 0.5530615837949908
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6434119499123805
============= xn0: 0.61 =============
new_qn: 0.5539697308948184
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6443200970122078
============= xn0: 0.611 =============
new_qn: 0.5548778779946459
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6452282441120356
============= xn0: 0.612 =============
new_qn: 0.5557860250944735
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.646136391211863
============= xn0: 0.613 =============
new_qn: 0.5566941721943011
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6470445383116907
============= xn0: 0.614 =============
new_qn: 0.5576023192941286
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.647952685411518
============= xn0: 0.615 =============
new_qn: 0.5585104663939563
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6488608325113459
============= xn0: 0.616 =============
new_qn: 0.5594186134937839
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6497689796111736
============= xn0: 0.617 =============
new_qn: 0.5603267605936114
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.650677126711001
============= xn0: 0.618 =============
new_qn: 0.561234907693439
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6515852738108288
============= xn0: 0.619 =============
new_qn: 0.5621430547932665
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.652493420910656
============= xn0: 0.62 =============
new_qn: 0.5630512018930941
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6534015680104839
============= xn0: 0.621 =============
new_qn: 0.5639593489929217
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6543097151103112
============= xn0: 0.622 =============
new_qn: 0.5648674960927492
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.655217862210139
============= xn0: 0.623 =============
new_qn: 0.5657756431925768
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6561260093099663
============= xn0: 0.624 =============
new_qn: 0.5666837902924043
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6570341564097941
============= xn0: 0.625 =============
new_qn: 0.567591937392232
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6579423035096215
============= xn0: 0.626 =============
new_qn: 0.5685000844920596
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6588504506094492
============= xn0: 0.627 =============
new_qn: 0.5694082315918871
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6597585977092766
============= xn0: 0.628 =============
new_qn: 0.5703163786917147
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6606667448091044
============= xn0: 0.629 =============
new_qn: 0.5712245257915423
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6615748919089317
============= xn0: 0.63 =============
new_qn: 0.5721326728913698
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6624830390087595
============= xn0: 0.631 =============
new_qn: 0.5730408199911974
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6633911861085868
============= xn0: 0.632 =============
new_qn: 0.5739489670910249
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6642993332084146
============= xn0: 0.633 =============
new_qn: 0.5748571141908525
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.665207480308242
============= xn0: 0.634 =============
new_qn: 0.5757652612906801
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6661156274080697
============= xn0: 0.635 =============
new_qn: 0.5766734083905076
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.667023774507897
============= xn0: 0.636 =============
new_qn: 0.5775815554903353
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6679319216077249
============= xn0: 0.637 =============
new_qn: 0.5784897025901629
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6688400687075526
============= xn0: 0.638 =============
new_qn: 0.5793978496899904
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.66974821580738
============= xn0: 0.639 =============
new_qn: 0.580305996789818
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6706563629072078
============= xn0: 0.64 =============
new_qn: 0.5812141438896455
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.671564510007035
============= xn0: 0.641 =============
new_qn: 0.5821222909894731
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6724726571068629
============= xn0: 0.642 =============
new_qn: 0.5830304380893007
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6733808042066902
============= xn0: 0.643 =============
new_qn: 0.5839385851891282
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.674288951306518
============= xn0: 0.644 =============
new_qn: 0.5848467322889558
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6751970984063453
============= xn0: 0.645 =============
new_qn: 0.5857548793887833
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6761052455061731
============= xn0: 0.646 =============
new_qn: 0.586663026488611
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6770133926060005
============= xn0: 0.647 =============
new_qn: 0.5875711735884386
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6779215397058282
============= xn0: 0.648 =============
new_qn: 0.5884793206882661
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6788296868056556
============= xn0: 0.649 =============
new_qn: 0.5893874677880937
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6797378339054834
============= xn0: 0.65 =============
new_qn: 0.5902956148879213
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6806459810053107
============= xn0: 0.651 =============
new_qn: 0.5912037619877488
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6815541281051385
============= xn0: 0.652 =============
new_qn: 0.5921119090875764
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6824622752049658
============= xn0: 0.653 =============
new_qn: 0.5930200561874039
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6833704223047936
============= xn0: 0.654 =============
new_qn: 0.5939282032872315
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.684278569404621
============= xn0: 0.655 =============
new_qn: 0.5948363503870591
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6851867165044487
============= xn0: 0.656 =============
new_qn: 0.5957444974868867
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6860948636042765
============= xn0: 0.657 =============
new_qn: 0.5966526445867143
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6870030107041039
============= xn0: 0.658 =============
new_qn: 0.5975607916865419
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6879111578039316
============= xn0: 0.659 =============
new_qn: 0.5984689387863694
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.688819304903759
============= xn0: 0.66 =============
new_qn: 0.599377085886197
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6897274520035868
============= xn0: 0.661 =============
new_qn: 0.6002852329860245
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.690635599103414
============= xn0: 0.662 =============
new_qn: 0.6011933800858521
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6915437462032419
============= xn0: 0.663 =============
new_qn: 0.6021015271856797
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6924518933030692
============= xn0: 0.664 =============
new_qn: 0.6030096742855072
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.693360040402897
============= xn0: 0.665 =============
new_qn: 0.6039178213853348
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6942681875027243
============= xn0: 0.666 =============
new_qn: 0.6048259684851625
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6951763346025521
============= xn0: 0.667 =============
new_qn: 0.60573411558499
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6960844817023795
============= xn0: 0.668 =============
new_qn: 0.6066422626848176
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6969926288022072
============= xn0: 0.669 =============
new_qn: 0.6075504097846451
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6979007759020346
============= xn0: 0.67 =============
new_qn: 0.6084585568844727
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6988089230018624
============= xn0: 0.671 =============
new_qn: 0.6093667039843003
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6997170701016897
============= xn0: 0.672 =============
new_qn: 0.6102748510841278
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7006252172015175
============= xn0: 0.673 =============
new_qn: 0.6111829981839554
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7015333643013448
============= xn0: 0.674 =============
new_qn: 0.612091145283783
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7024415114011726
============= xn0: 0.675 =============
new_qn: 0.6129992923836105
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.703349658501
============= xn0: 0.676 =============
new_qn: 0.6139074394834382
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7042578056008277
============= xn0: 0.677 =============
new_qn: 0.6148155865832657
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7051659527006555
============= xn0: 0.678 =============
new_qn: 0.6157237336830933
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7060740998004829
============= xn0: 0.679 =============
new_qn: 0.6166318807829209
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7069822469003106
============= xn0: 0.68 =============
new_qn: 0.6175400278827484
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.707890394000138
============= xn0: 0.681 =============
new_qn: 0.618448174982576
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7087985410999658
============= xn0: 0.682 =============
new_qn: 0.6193563220824035
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.709706688199793
============= xn0: 0.683 =============
new_qn: 0.6202644691822311
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7106148352996209
============= xn0: 0.684 =============
new_qn: 0.6211726162820587
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7115229823994482
============= xn0: 0.685 =============
new_qn: 0.6220807633818862
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.712431129499276
============= xn0: 0.686 =============
new_qn: 0.6229889104817139
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7133392765991033
============= xn0: 0.687 =============
new_qn: 0.6238970575815415
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7142474236989311
============= xn0: 0.6880000000000001 =============
new_qn: 0.624805204681369
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7151555707987585
============= xn0: 0.6890000000000001 =============
new_qn: 0.6257133517811966
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7160637178985862
============= xn0: 0.6900000000000001 =============
new_qn: 0.6266214988810241
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7169718649984136
============= xn0: 0.6910000000000001 =============
new_qn: 0.6275296459808517
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7178800120982414
============= xn0: 0.6920000000000001 =============
new_qn: 0.6284377930806793
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7187881591980687
============= xn0: 0.6930000000000001 =============
new_qn: 0.6293459401805068
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7196963062978965
============= xn0: 0.6940000000000001 =============
new_qn: 0.6302540872803344
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7206044533977238
============= xn0: 0.6950000000000001 =============
new_qn: 0.631162234380162
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7215126004975516
============= xn0: 0.6960000000000001 =============
new_qn: 0.6320703814799896
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7224207475973794
============= xn0: 0.6970000000000001 =============
new_qn: 0.6329785285798172
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7233288946972067
============= xn0: 0.6980000000000001 =============
new_qn: 0.6338866756796447
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7242370417970345
============= xn0: 0.6990000000000001 =============
new_qn: 0.6347948227794723
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7251451888968619
============= xn0: 0.7000000000000001 =============
new_qn: 0.6357029698792999
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7260533359966896
============= xn0: 0.7010000000000001 =============
new_qn: 0.6366111169791274
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.726961483096517
============= xn0: 0.7020000000000001 =============
new_qn: 0.637519264078955
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7278696301963448
============= xn0: 0.7030000000000001 =============
new_qn: 0.6384274111787825
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.728777777296172
============= xn0: 0.704 =============
new_qn: 0.63933555827861
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7296859243959994
============= xn0: 0.705 =============
new_qn: 0.6402437053784377
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7305940714958272
============= xn0: 0.706 =============
new_qn: 0.6411518524782652
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.731502218595655
============= xn0: 0.707 =============
new_qn: 0.6420599995780928
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7324103656954823
============= xn0: 0.708 =============
new_qn: 0.6429681466779203
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7333185127953101
============= xn0: 0.709 =============
new_qn: 0.6438762937777479
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.734226659895138
============= xn0: 0.71 =============
new_qn: 0.6447844408775755
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7351348069949653
============= xn0: 0.711 =============
new_qn: 0.645692587977403
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.736042954094793
============= xn0: 0.712 =============
new_qn: 0.6466007350772306
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7369511011946204
============= xn0: 0.713 =============
new_qn: 0.6475088821770582
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7378592482944482
============= xn0: 0.714 =============
new_qn: 0.6484170292768857
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7387673953942755
============= xn0: 0.715 =============
new_qn: 0.6493251763767134
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7396755424941033
============= xn0: 0.716 =============
new_qn: 0.650233323476541
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7405836895939306
============= xn0: 0.717 =============
new_qn: 0.6511414705763685
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7414918366937584
============= xn0: 0.718 =============
new_qn: 0.6520496176761961
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7423999837935857
============= xn0: 0.719 =============
new_qn: 0.6529577647760236
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7433081308934135
============= xn0: 0.72 =============
new_qn: 0.6538659118758512
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7442162779932409
============= xn0: 0.721 =============
new_qn: 0.6547740589756788
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7451244250930686
============= xn0: 0.722 =============
new_qn: 0.6556822060755063
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.746032572192896
============= xn0: 0.723 =============
new_qn: 0.6565903531753339
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7469407192927238
============= xn0: 0.724 =============
new_qn: 0.6574985002751614
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.747848866392551
============= xn0: 0.725 =============
new_qn: 0.6584066473749891
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7487570134923789
============= xn0: 0.726 =============
new_qn: 0.6593147944748167
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7496651605922067
============= xn0: 0.727 =============
new_qn: 0.6602229415746442
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.750573307692034
============= xn0: 0.728 =============
new_qn: 0.6611310886744718
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7514814547918618
============= xn0: 0.729 =============
new_qn: 0.6620392357742994
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7523896018916891
============= xn0: 0.73 =============
new_qn: 0.6629473828741269
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.753297748991517
============= xn0: 0.731 =============
new_qn: 0.6638555299739545
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7542058960913443
============= xn0: 0.732 =============
new_qn: 0.664763677073782
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.755114043191172
============= xn0: 0.733 =============
new_qn: 0.6656718241736096
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7560221902909994
============= xn0: 0.734 =============
new_qn: 0.6665799712734372
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7569303373908272
============= xn0: 0.735 =============
new_qn: 0.6674881183732648
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7578384844906545
============= xn0: 0.736 =============
new_qn: 0.6683962654730924
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7587466315904823
============= xn0: 0.737 =============
new_qn: 0.66930441257292
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7596547786903096
============= xn0: 0.738 =============
new_qn: 0.6702125596727475
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7605629257901374
============= xn0: 0.739 =============
new_qn: 0.6711207067725751
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7614710728899647
============= xn0: 0.74 =============
new_qn: 0.6720288538724026
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7623792199897925
============= xn0: 0.741 =============
new_qn: 0.6729370009722302
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7632873670896199
============= xn0: 0.742 =============
new_qn: 0.6738451480720578
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7641955141894476
============= xn0: 0.743 =============
new_qn: 0.6747532951718853
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.765103661289275
============= xn0: 0.744 =============
new_qn: 0.6756614422717129
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7660118083891028
============= xn0: 0.745 =============
new_qn: 0.6765695893715405
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7669199554889305
============= xn0: 0.746 =============
new_qn: 0.6774777364713681
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7678281025887579
============= xn0: 0.747 =============
new_qn: 0.6783858835711957
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7687362496885857
============= xn0: 0.748 =============
new_qn: 0.6792940306710232
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.769644396788413
============= xn0: 0.749 =============
new_qn: 0.6802021777708508
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7705525438882408
============= xn0: 0.75 =============
new_qn: 0.6811103248706784
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7714606909880681
============= xn0: 0.751 =============
new_qn: 0.6820184719705059
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.772368838087896
============= xn0: 0.752 =============
new_qn: 0.6829266190703335
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7732769851877233
============= xn0: 0.753 =============
new_qn: 0.683834766170161
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.774185132287551
============= xn0: 0.754 =============
new_qn: 0.6847429132699886
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7750932793873784
============= xn0: 0.755 =============
new_qn: 0.6856510603698162
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7760014264872062
============= xn0: 0.756 =============
new_qn: 0.6865592074696438
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7769095735870335
============= xn0: 0.757 =============
new_qn: 0.6874673545694714
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7778177206868613
============= xn0: 0.758 =============
new_qn: 0.688375501669299
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7787258677866886
============= xn0: 0.759 =============
new_qn: 0.6892836487691265
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7796340148865164
============= xn0: 0.76 =============
new_qn: 0.6901917958689541
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7805421619863437
============= xn0: 0.761 =============
new_qn: 0.6910999429687816
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7814503090861715
============= xn0: 0.762 =============
new_qn: 0.6920080900686092
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7823584561859989
============= xn0: 0.763 =============
new_qn: 0.6929162371684368
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7832666032858266
============= xn0: 0.764 =============
new_qn: 0.6938243842682643
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.784174750385654
============= xn0: 0.765 =============
new_qn: 0.6947325313680919
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7850828974854818
============= xn0: 0.766 =============
new_qn: 0.6956406784679195
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7859910445853096
============= xn0: 0.767 =============
new_qn: 0.6965488255677471
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.786899191685137
============= xn0: 0.768 =============
new_qn: 0.6974569726675747
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7878073387849647
============= xn0: 0.769 =============
new_qn: 0.6983651197674022
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.788715485884792
============= xn0: 0.77 =============
new_qn: 0.6992732668672298
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7896236329846198
============= xn0: 0.771 =============
new_qn: 0.7001814139670574
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7905317800844471
============= xn0: 0.772 =============
new_qn: 0.7010895610668849
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.791439927184275
============= xn0: 0.773 =============
new_qn: 0.7019977081667125
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7923480742841023
============= xn0: 0.774 =============
new_qn: 0.70290585526654
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.79325622138393
============= xn0: 0.775 =============
new_qn: 0.7038140023663676
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7941643684837574
============= xn0: 0.776 =============
new_qn: 0.7047221494661953
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7950725155835852
============= xn0: 0.777 =============
new_qn: 0.7056302965660228
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7959806626834125
============= xn0: 0.778 =============
new_qn: 0.7065384436658504
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7968888097832403
============= xn0: 0.779 =============
new_qn: 0.707446590765678
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7977969568830676
============= xn0: 0.78 =============
new_qn: 0.7083547378655055
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7987051039828954
============= xn0: 0.781 =============
new_qn: 0.7092628849653331
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7996132510827227
============= xn0: 0.782 =============
new_qn: 0.7101710320651606
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8005213981825505
============= xn0: 0.783 =============
new_qn: 0.7110791791649882
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8014295452823779
============= xn0: 0.784 =============
new_qn: 0.7119873262648158
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8023376923822056
============= xn0: 0.785 =============
new_qn: 0.7128954733646433
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.803245839482033
============= xn0: 0.786 =============
new_qn: 0.713803620464471
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8041539865818608
============= xn0: 0.787 =============
new_qn: 0.7147117675642986
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8050621336816886
============= xn0: 0.788 =============
new_qn: 0.7156199146641261
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.805970280781516
============= xn0: 0.789 =============
new_qn: 0.7165280617639537
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8068784278813437
============= xn0: 0.79 =============
new_qn: 0.7174362088637812
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.807786574981171
============= xn0: 0.791 =============
new_qn: 0.7183443559636088
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8086947220809988
============= xn0: 0.792 =============
new_qn: 0.7192525030634364
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8096028691808261
============= xn0: 0.793 =============
new_qn: 0.7201606501632639
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.810511016280654
============= xn0: 0.794 =============
new_qn: 0.7210687972630915
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8114191633804813
============= xn0: 0.795 =============
new_qn: 0.721976944362919
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.812327310480309
============= xn0: 0.796 =============
new_qn: 0.7228850914627467
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8132354575801364
============= xn0: 0.797 =============
new_qn: 0.7237932385625743
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8141436046799642
============= xn0: 0.798 =============
new_qn: 0.7247013856624018
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8150517517797915
============= xn0: 0.799 =============
new_qn: 0.7256095327622294
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8159598988796193
============= xn0: 0.8 =============
new_qn: 0.726517679862057
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8168680459794466
============= xn0: 0.801 =============
new_qn: 0.7274258269618845
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8177761930792744
============= xn0: 0.802 =============
new_qn: 0.7283339740617121
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8186843401791017
============= xn0: 0.803 =============
new_qn: 0.7292421211615396
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8195924872789295
============= xn0: 0.804 =============
new_qn: 0.7301502682613672
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8205006343787569
============= xn0: 0.805 =============
new_qn: 0.7310584153611948
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8214087814785846
============= xn0: 0.806 =============
new_qn: 0.7319665624610224
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8223169285784124
============= xn0: 0.807 =============
new_qn: 0.73287470956085
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8232250756782398
============= xn0: 0.808 =============
new_qn: 0.7337828566606776
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8241332227780676
============= xn0: 0.809 =============
new_qn: 0.7346910037605051
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.825041369877895
============= xn0: 0.81 =============
new_qn: 0.7355991508603327
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8259495169777227
============= xn0: 0.811 =============
new_qn: 0.7365072979601602
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.82685766407755
============= xn0: 0.812 =============
new_qn: 0.7374154450599878
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8277658111773778
============= xn0: 0.8130000000000001 =============
new_qn: 0.7383235921598154
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8286739582772051
============= xn0: 0.8140000000000001 =============
new_qn: 0.7392317392596429
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.829582105377033
============= xn0: 0.8150000000000001 =============
new_qn: 0.7401398863594705
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8304902524768603
============= xn0: 0.8160000000000001 =============
new_qn: 0.7410480334592981
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.831398399576688
============= xn0: 0.8170000000000001 =============
new_qn: 0.7419561805591257
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8323065466765154
============= xn0: 0.8180000000000001 =============
new_qn: 0.7428643276589533
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8332146937763432
============= xn0: 0.8190000000000001 =============
new_qn: 0.7437724747587808
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8341228408761705
============= xn0: 0.8200000000000001 =============
new_qn: 0.7446806218586084
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8350309879759983
============= xn0: 0.8210000000000001 =============
new_qn: 0.745588768958436
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8359391350758256
============= xn0: 0.8220000000000001 =============
new_qn: 0.7464969160582635
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8368472821756534
============= xn0: 0.8230000000000001 =============
new_qn: 0.7474050631580911
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8377554292754807
============= xn0: 0.8240000000000001 =============
new_qn: 0.7483132102579186
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8386635763753085
============= xn0: 0.8250000000000001 =============
new_qn: 0.7492213573577462
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8395717234751359
============= xn0: 0.8260000000000001 =============
new_qn: 0.7501295044575739
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8404798705749637
============= xn0: 0.8270000000000001 =============
new_qn: 0.7510376515574014
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8413880176747914
============= xn0: 0.8280000000000001 =============
new_qn: 0.751945798657229
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8422961647746188
============= xn0: 0.8290000000000001 =============
new_qn: 0.7528539457570566
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8432043118744466
============= xn0: 0.8300000000000001 =============
new_qn: 0.7537620928568841
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.844112458974274
============= xn0: 0.8310000000000001 =============
new_qn: 0.7546702399567117
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8450206060741017
============= xn0: 0.8320000000000001 =============
new_qn: 0.7555783870565392
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.845928753173929
============= xn0: 0.833 =============
new_qn: 0.7564865341563667
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8468369002737564
============= xn0: 0.834 =============
new_qn: 0.7573946812561942
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8477450473735841
============= xn0: 0.835 =============
new_qn: 0.7583028283560219
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.848653194473412
============= xn0: 0.836 =============
new_qn: 0.7592109754558495
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8495613415732393
============= xn0: 0.837 =============
new_qn: 0.760119122555677
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.850469488673067
============= xn0: 0.838 =============
new_qn: 0.7610272696555046
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8513776357728944
============= xn0: 0.839 =============
new_qn: 0.7619354167553322
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8522857828727222
============= xn0: 0.84 =============
new_qn: 0.7628435638551597
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8531939299725495
============= xn0: 0.841 =============
new_qn: 0.7637517109549873
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8541020770723773
============= xn0: 0.842 =============
new_qn: 0.7646598580548148
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8550102241722046
============= xn0: 0.843 =============
new_qn: 0.7655680051546424
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8559183712720324
============= xn0: 0.844 =============
new_qn: 0.76647615225447
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8568265183718597
============= xn0: 0.845 =============
new_qn: 0.7673842993542976
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8577346654716875
============= xn0: 0.846 =============
new_qn: 0.7682924464541252
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8586428125715149
============= xn0: 0.847 =============
new_qn: 0.7692005935539528
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8595509596713427
============= xn0: 0.848 =============
new_qn: 0.7701087406537803
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.86045910677117
============= xn0: 0.849 =============
new_qn: 0.7710168877536079
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8613672538709978
============= xn0: 0.85 =============
new_qn: 0.7719250348534354
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8622754009708251
============= xn0: 0.851 =============
new_qn: 0.772833181953263
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.863183548070653
============= xn0: 0.852 =============
new_qn: 0.7737413290530906
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8640916951704802
============= xn0: 0.853 =============
new_qn: 0.7746494761529181
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.864999842270308
============= xn0: 0.854 =============
new_qn: 0.7755576232527457
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8659079893701354
============= xn0: 0.855 =============
new_qn: 0.7764657703525734
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8668161364699631
============= xn0: 0.856 =============
new_qn: 0.7773739174524009
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.867724283569791
============= xn0: 0.857 =============
new_qn: 0.7782820645522285
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8686324306696183
============= xn0: 0.858 =============
new_qn: 0.779190211652056
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.869540577769446
============= xn0: 0.859 =============
new_qn: 0.7800983587518836
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8704487248692734
============= xn0: 0.86 =============
new_qn: 0.7810065058517112
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8713568719691012
============= xn0: 0.861 =============
new_qn: 0.7819146529515387
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8722650190689285
============= xn0: 0.862 =============
new_qn: 0.7828228000513663
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8731731661687563
============= xn0: 0.863 =============
new_qn: 0.7837309471511938
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8740813132685836
============= xn0: 0.864 =============
new_qn: 0.7846390942510214
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8749894603684114
============= xn0: 0.865 =============
new_qn: 0.7855472413508491
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8758976074682387
============= xn0: 0.866 =============
new_qn: 0.7864553884506766
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8768057545680665
============= xn0: 0.867 =============
new_qn: 0.7873635355505042
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8777139016678939
============= xn0: 0.868 =============
new_qn: 0.7882716826503318
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8786220487677217
============= xn0: 0.869 =============
new_qn: 0.7891798297501593
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.879530195867549
============= xn0: 0.87 =============
new_qn: 0.7900879768499869
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8804383429673768
============= xn0: 0.871 =============
new_qn: 0.7909961239498144
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8813464900672041
============= xn0: 0.872 =============
new_qn: 0.791904271049642
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.882254637167032
============= xn0: 0.873 =============
new_qn: 0.7928124181494696
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8831627842668592
============= xn0: 0.874 =============
new_qn: 0.7937205652492971
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.884070931366687
============= xn0: 0.875 =============
new_qn: 0.7946287123491247
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8849790784665144
============= xn0: 0.876 =============
new_qn: 0.7955368594489524
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8858872255663421
============= xn0: 0.877 =============
new_qn: 0.7964450065487799
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.88679537266617
============= xn0: 0.878 =============
new_qn: 0.7973531536486075
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8877035197659973
============= xn0: 0.879 =============
new_qn: 0.798261300748435
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.888611666865825
============= xn0: 0.88 =============
new_qn: 0.7991694478482626
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8895198139656524
============= xn0: 0.881 =============
new_qn: 0.8000775949480902
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8904279610654802
============= xn0: 0.882 =============
new_qn: 0.8009857420479177
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8913361081653075
============= xn0: 0.883 =============
new_qn: 0.8018938891477453
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8922442552651353
============= xn0: 0.884 =============
new_qn: 0.8028020362475728
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8931524023649626
============= xn0: 0.885 =============
new_qn: 0.8037101833474004
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8940605494647904
============= xn0: 0.886 =============
new_qn: 0.8046183304472281
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8949686965646177
============= xn0: 0.887 =============
new_qn: 0.8055264775470556
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8958768436644455
============= xn0: 0.888 =============
new_qn: 0.8064346246468832
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8967849907642729
============= xn0: 0.889 =============
new_qn: 0.8073427717467108
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8976931378641007
============= xn0: 0.89 =============
new_qn: 0.8082509188465383
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.898601284963928
============= xn0: 0.891 =============
new_qn: 0.8091590659463659
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8995094320637558
============= xn0: 0.892 =============
new_qn: 0.8100672130461934
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9004175791635831
============= xn0: 0.893 =============
new_qn: 0.810975360146021
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.901325726263411
============= xn0: 0.894 =============
new_qn: 0.8118835072458486
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9022338733632382
============= xn0: 0.895 =============
new_qn: 0.8127916543456761
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.903142020463066
============= xn0: 0.896 =============
new_qn: 0.8136998014455038
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9040501675628938
============= xn0: 0.897 =============
new_qn: 0.8146079485453314
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9049583146627211
============= xn0: 0.898 =============
new_qn: 0.8155160956451589
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.905866461762549
============= xn0: 0.899 =============
new_qn: 0.8164242427449865
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9067746088623763
============= xn0: 0.9 =============
new_qn: 0.817332389844814
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.907682755962204
============= xn0: 0.901 =============
new_qn: 0.8182405369446416
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9085909030620314
============= xn0: 0.902 =============
new_qn: 0.8191486840444692
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9094990501618592
============= xn0: 0.903 =============
new_qn: 0.8200568311442967
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9104071972616865
============= xn0: 0.904 =============
new_qn: 0.8209649782441243
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9113153443615143
============= xn0: 0.905 =============
new_qn: 0.8218731253439518
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9122234914613416
============= xn0: 0.906 =============
new_qn: 0.8227812724437795
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9131316385611694
============= xn0: 0.907 =============
new_qn: 0.8236894195436071
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9140397856609968
============= xn0: 0.908 =============
new_qn: 0.8245975666434346
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9149479327608245
============= xn0: 0.909 =============
new_qn: 0.8255057137432622
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9158560798606519
============= xn0: 0.91 =============
new_qn: 0.8264138608430898
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9167642269604797
============= xn0: 0.911 =============
new_qn: 0.8273220079429173
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.917672374060307
============= xn0: 0.912 =============
new_qn: 0.8282301550427449
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9185805211601348
============= xn0: 0.913 =============
new_qn: 0.8291383021425724
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9194886682599621
============= xn0: 0.914 =============
new_qn: 0.8300464492424
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.92039681535979
============= xn0: 0.915 =============
new_qn: 0.8309545963422276
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9213049624596172
============= xn0: 0.916 =============
new_qn: 0.8318627434420552
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.922213109559445
============= xn0: 0.917 =============
new_qn: 0.8327708905418828
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9231212566592728
============= xn0: 0.918 =============
new_qn: 0.8336790376417104
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9240294037591001
============= xn0: 0.919 =============
new_qn: 0.8345871847415379
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.924937550858928
============= xn0: 0.92 =============
new_qn: 0.8354953318413655
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9258456979587553
============= xn0: 0.921 =============
new_qn: 0.836403478941193
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.926753845058583
============= xn0: 0.922 =============
new_qn: 0.8373116260410206
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9276619921584104
============= xn0: 0.923 =============
new_qn: 0.8382197731408482
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9285701392582382
============= xn0: 0.924 =============
new_qn: 0.8391279202406757
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9294782863580655
============= xn0: 0.925 =============
new_qn: 0.8400360673405033
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9303864334578933
============= xn0: 0.926 =============
new_qn: 0.840944214440331
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9312945805577206
============= xn0: 0.927 =============
new_qn: 0.8418523615401585
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9322027276575484
============= xn0: 0.928 =============
new_qn: 0.8427605086399861
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9331108747573758
============= xn0: 0.929 =============
new_qn: 0.8436686557398136
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9340190218572035
============= xn0: 0.93 =============
new_qn: 0.8445768028396412
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9349271689570309
============= xn0: 0.931 =============
new_qn: 0.8454849499394688
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9358353160568587
============= xn0: 0.932 =============
new_qn: 0.8463930970392963
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.936743463156686
============= xn0: 0.933 =============
new_qn: 0.8473012441391239
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9376516102565138
============= xn0: 0.934 =============
new_qn: 0.8482093912389514
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9385597573563411
============= xn0: 0.935 =============
new_qn: 0.849117538338779
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.939467904456169
============= xn0: 0.936 =============
new_qn: 0.8500256854386067
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9403760515559967
============= xn0: 0.937 =============
new_qn: 0.8509338325384342
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.941284198655824
============= xn0: 0.9380000000000001 =============
new_qn: 0.8518419796382618
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9421923457556518
============= xn0: 0.9390000000000001 =============
new_qn: 0.8527501267380894
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9431004928554791
============= xn0: 0.9400000000000001 =============
new_qn: 0.8536582738379169
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.944008639955307
============= xn0: 0.9410000000000001 =============
new_qn: 0.8545664209377445
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9449167870551343
============= xn0: 0.9420000000000001 =============
new_qn: 0.855474568037572
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.945824934154962
============= xn0: 0.9430000000000001 =============
new_qn: 0.8563827151373996
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9467330812547894
============= xn0: 0.9440000000000001 =============
new_qn: 0.8572908622372272
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9476412283546172
============= xn0: 0.9450000000000001 =============
new_qn: 0.8581990093370547
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9485493754544445
============= xn0: 0.9460000000000001 =============
new_qn: 0.8591071564368824
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9494575225542723
============= xn0: 0.9470000000000001 =============
new_qn: 0.86001530353671
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9503656696540996
============= xn0: 0.9480000000000001 =============
new_qn: 0.8609234506365375
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9512738167539274
============= xn0: 0.9490000000000001 =============
new_qn: 0.8618315977363651
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9521819638537548
============= xn0: 0.9500000000000001 =============
new_qn: 0.8627397448361926
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9530901109535825
============= xn0: 0.9510000000000001 =============
new_qn: 0.8636478919360202
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9539982580534099
============= xn0: 0.9520000000000001 =============
new_qn: 0.8645560390358478
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9549064051532377
============= xn0: 0.9530000000000001 =============
new_qn: 0.8654641861356753
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.955814552253065
============= xn0: 0.9540000000000001 =============
new_qn: 0.8663723332355029
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9567226993528928
============= xn0: 0.9550000000000001 =============
new_qn: 0.8672804803353305
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9576308464527201
============= xn0: 0.9560000000000001 =============
new_qn: 0.8681886274351581
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.958538993552548
============= xn0: 0.9570000000000001 =============
new_qn: 0.8690967745349857
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9594471406523757
============= xn0: 0.9580000000000001 =============
new_qn: 0.8700049216348132
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.960355287752203
============= xn0: 0.9590000000000001 =============
new_qn: 0.8709130687346408
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9612634348520308
============= xn0: 0.96 =============
new_qn: 0.8718212158344683
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9621715819518581
============= xn0: 0.961 =============
new_qn: 0.8727293629342958
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9630797290516855
============= xn0: 0.962 =============
new_qn: 0.8736375100341234
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9639878761515133
============= xn0: 0.963 =============
new_qn: 0.8745456571339509
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9648960232513406
============= xn0: 0.964 =============
new_qn: 0.8754538042337785
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9658041703511684
============= xn0: 0.965 =============
new_qn: 0.8763619513336062
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9667123174509962
============= xn0: 0.966 =============
new_qn: 0.8772700984334337
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9676204645508235
============= xn0: 0.967 =============
new_qn: 0.8781782455332613
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9685286116506513
============= xn0: 0.968 =============
new_qn: 0.8790863926330889
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9694367587504786
============= xn0: 0.969 =============
new_qn: 0.8799945397329164
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9703449058503064
============= xn0: 0.97 =============
new_qn: 0.880902686832744
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9712530529501338
============= xn0: 0.971 =============
new_qn: 0.8818108339325715
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9721612000499615
============= xn0: 0.972 =============
new_qn: 0.8827189810323991
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9730693471497889
============= xn0: 0.973 =============
new_qn: 0.8836271281322267
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9739774942496167
============= xn0: 0.974 =============
new_qn: 0.8845352752320542
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.974885641349444
============= xn0: 0.975 =============
new_qn: 0.8854434223318819
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9757937884492718
============= xn0: 0.976 =============
new_qn: 0.8863515694317095
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9767019355490991
============= xn0: 0.977 =============
new_qn: 0.887259716531537
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.977610082648927
============= xn0: 0.978 =============
new_qn: 0.8881678636313646
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9785182297487542
============= xn0: 0.979 =============
new_qn: 0.8890760107311921
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.979426376848582
============= xn0: 0.98 =============
new_qn: 0.8899841578310197
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9803345239484094
============= xn0: 0.981 =============
new_qn: 0.8908923049308473
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9812426710482371
============= xn0: 0.982 =============
new_qn: 0.8918004520306748
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9821508181480645
============= xn0: 0.983 =============
new_qn: 0.8927085991305024
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9830589652478923
============= xn0: 0.984 =============
new_qn: 0.8936167462303299
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9839671123477196
============= xn0: 0.985 =============
new_qn: 0.8945248933301576
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9848752594475474
============= xn0: 0.986 =============
new_qn: 0.8954330404299852
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9857834065473752
============= xn0: 0.987 =============
new_qn: 0.8963411875298127
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9866915536472025
============= xn0: 0.988 =============
new_qn: 0.8972493346296403
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9875997007470303
============= xn0: 0.989 =============
new_qn: 0.8981574817294679
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9885078478468576
============= xn0: 0.99 =============
new_qn: 0.8990656288292954
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9894159949466854
============= xn0: 0.991 =============
new_qn: 0.899973775929123
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9903241420465128
============= xn0: 0.992 =============
new_qn: 0.9008819230289505
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9912322891463405
============= xn0: 0.993 =============
new_qn: 0.9017900701287781
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9921404362461679
============= xn0: 0.994 =============
new_qn: 0.9026982172286057
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9930485833459957
============= xn0: 0.995 =============
new_qn: 0.9036063643284333
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.993956730445823
============= xn0: 0.996 =============
new_qn: 0.9045145114282609
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9948648775456508
============= xn0: 0.997 =============
new_qn: 0.9054226585280885
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9957730246454781
============= xn0: 0.998 =============
new_qn: 0.906330805627916
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.996681171745306
============= xn0: 0.999 =============
new_qn: 0.9072389527277436
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9975893188451332
============= xn0: 1.0 =============
new_qn: 0.9081470998275711
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.998497465944961
DONE
最终的列表：
[0.0, 0.00011323537291038415, 0.00022461941394612446, 0.00033415673747120124, 0.00044185194252752136, 0.0005477096128984667, 0.000651734317172116, 0.0007539306088041632, 0.0008543030261805185, 0.0009528560926795968, 0.0010495943167342977, 0.001144522191893687, 0.0012376441968843643, 0.0013289647956715289, 0.0014184884375197382, 0.0015062195570533857, 0.001592162574316859, 0.0016763218948344163, 0.0017587019096697437, 0.0018393069954852695, 0.0019181415146011273, 0.001995209815053873, 0.0020705162306548855, 0.002144065081048512, 0.002215860671769883, 0.0022859072943025027, 0.0023542092261354987, 0.002420770730820636, 0.002485596058029043, 0.002548689443607619, 0.0026100551096352315, 0.002669697264478625, 0.002727620102847987, 0.0027838278058523597, 0.0028383245410546695, 0.0028911144625265867, 0.0029422017109030432, 0.0029915904134365165, 0.00303928468405109, 0.0030852886233961596, 0.003129606318899962, 0.003172241844822815, 0.0032131992623100902, 0.0032524826194449685, 0.0032900959513008643, 0.0033260432799937026, 0.00336032861473385, 0.0033929559518778435, 0.003423929274979892, 0.0034532525548430434, 0.003480929749570233, 0.0035069648046149454, 0.0035313616528317437, 0.003554124214526555, 0.003575256397506589, 0.003594762097130194, 0.003612645196356387, 0.0036289095657940224, 0.0036435590637510645, 0.003656597536283189, 0.0036680288172426107, 0.003677856728326212, 0.0036860850791238797, 0.0036927176671663064, 0.0036977582779726126, 0.0037012106850980303, 0.0037030786501807827, 0.003703365922989449, 0.0037020762414694403, 0.0036992133317898385, 0.0036947809083894556, 0.0036887826740232, 0.003681222319807803, 0.003672103525267645, 0.003661429958380133, 0.0036492052756210286, 0.003635433122009396, 0.003620117131152581, 0.0036032609252906467, 0.003584868115341061, 0.003564942300942564, 0.003543487070499507, 0.0035205060012253597, 0.003496002659186509, 0.003469980599345529, 0.0034424433656044112, 0.003413394490847585, 0.0033828374969845504, 0.0033507758949926497, 0.003317213184959339, 0.0032821528561243485, 0.003245598386921858, 0.0032075532450220046, 0.0031680208873728077, 0.0031270047602412893, 0.003084508299254912, 0.0030405349294425066, 0.0029950880652750284, 0.0029481711107064296, 0.002899787459213876, 0.002849940493838257, 0.0027986335872240142, 0.002745870101659359, 0.002691653389115617, 0.0026359867912872093, 0.0025788736396305384, 0.002520317255403595, 0.0024603209497046086, 0.0023988880235111254, 0.002336021767718452, 0.0022717254631783174, 0.002206002380736885, 0.002138855781273194, 0.0020702889157367815, 0.0020003050251857363, 0.0019289073408240015, 0.001856099084039109, 0.0017818834664390798, 0.0017062636898898392, 0.0016292429465520064, 0.0015508244189174347, 0.0014710112798461822, 0.0013898066926024966, 0.0013072138108915082, 0.0012232357788948545, 0.0011378757313070254, 0.001051136793370766, 0.0009630220809129786, 0.0008735347003798188, 0.0007826777488723902, 0.0006904543141814379, 0.0005968674748226266, 0.0005019203000710681, 0.00040561584999629363, 0.0003079571754963928, 0.00020894731833268065, 0.00010858931116347614, 6.886177578629971e-06, -9.615906786711315e-05, -0.00020054341962003064, -0.00030626388109389335, -0.0004133174646368809, -0.0005217011914977199, -0.0006314120917932375, -0.0007424472044747221, -0.0008548035772957541, -0.0009684782667788716, -0.0010834683381835408, -0.001199770865473515, -0.001317382931284694, -0.0014363016268928996, -0.001556524052182151, -0.0016780473156123854, -0.0018008685341884267, -0.0019249848334277897, -0.0020503933473299263, -0.0021770912183442515, -0.0023050755973396952, -0.002434343643573228, -0.0025648925246594123, -0.0026967194165392616, -0.002829821503450153, -0.0029641959778947957, -0.003099840040611479, -0.00323675090054365, -0.003374925774809856, -0.003514361888673906, -0.0036550564755148673, -0.0037970067767979787, -0.00394021004204434, -0.004084663528802324, -0.004230364502617628, -0.004377310237004689, -0.0045254980134172285, -0.004674925121219781, -0.004825588857658547, -0.004977486527833169, -0.005130615444667808, -0.005284972928883219, -0.005440556308967975, -0.0055973629211509, -0.005755390109372566, -0.0059146352252577306, -0.0060750956280873625, -0.006236768684771188, -0.006399651769819992, -0.006563742265318223, -0.006729037560896878, -0.006895535053706131, -0.00706323214838836, -0.007232126257051247, -0.007402214799240886, -0.007573495201914998, -0.00774596489941673, -0.007919621333447535, -0.008094461953041504, -0.008270484214538493, -0.00844768558155845, -0.008626063524975164, -0.008805615522890414, -0.008986339060608056, -0.009168231630608675, -0.009351290732523332, -0.009535513873108997, -0.00972089856622238, -0.00990744233279503, -0.01009514270080833, -0.010283997205268014, -0.010474003388179831, -0.010665158798524005, -0.01085746099223131, -0.011050907532157594, -0.011245495988060072, -0.01144122393657232, -0.01163808896118032, -0.011836088652198123, -0.012035220606743807, -0.012235482428715194, -0.012436871728766646, -0.012639386124284308, -0.0128430232393634, -0.01304778070478374, -0.01325365615798707, -0.013460647243052792, -0.0136687516106756, -0.013877966918141332, -0.014088290829304795, -0.014299721014565975, -0.014512255150847586, -0.014725890921572171, -0.014940626016639258, -0.015156458132402989, -0.015373384971649112, -0.015591404243573304, -0.015810513663757964, -0.016030710954150845, -0.01625199384304199, -0.016474360065042548, -0.016697807361062134, -0.016922333478287366, -0.017147936170159833, -0.017374613196354582, -0.017602362322758358, -0.017831181321448286, -0.018061067970670114, -0.01829202005481731, -0.018524035364409414, -0.01875711169607122, -0.018991246852511356, -0.019226438642501648, -0.019462684880856035, -0.01969998338840956, -0.01993833199199818, -0.020177728524437555, -0.02041817082450295, -0.02065965673690842, -0.02090218411228667, -0.021145750807168462, -0.021390354683962887, -0.02163599361093657, -0.02188266546219439, -0.022130368117658644, -0.022379099463049945, -0.02262885738986692, -0.022879639795366724, -0.023131444582545208, -0.023384269660117668, -0.023638112942498818, -0.023892972349784103, -0.02414884580772969, -0.024405731247733625, -0.024663626606816674, -0.024922529827602874, -0.025182438858301043, -0.025443351652685325, -0.025705266170076763, -0.025968180375324224, -0.026232092238785865, -0.0264969997363102, -0.02676290084921823, -0.027029793564284144, -0.027297675873717453, -0.02756654577514439, -0.027836401271589872, -0.02810724037145912, -0.02837906108851984, -0.028651861441883586, -0.02892563945598864, -0.02920039316058126, -0.02947612059069865, -0.02975281978665062, -0.030030488794002386, -0.03030912566355637, -0.030588728451335145, -0.03086929521856413, -0.031150824031653424, -0.031433312962181326, -0.031716760086876516, -0.03200116348760129, -0.03228652125133402, -0.03257283147015255, -0.03286009224121672, -0.03314830166675198, -0.033437457854031905, -0.03372755891536244, -0.03401860296806386, -0.034310588134455366, -0.03460351254183752, -0.03489737432247658, -0.03519217161358734, -0.035487902557317264, -0.035784565300729654, -0.03608215799578829, -0.036380678799340094, -0.03668012587309971, -0.03698049738363368, -0.037281791502343564, -0.037584006405450954, -0.03788714027398088, -0.03819119129374654, -0.03849615765533315, -0.038802037554082625, -0.039108829190077465, -0.039416530768125846, -0.03972514049774556, -0.040034656593148976, -0.040345077273227226, -0.04065640076153548, -0.04096862528627698, -0.0412817490802887, -0.0415957703810253, -0.041910687430544824, -0.042226498475493035, -0.042543201767089034, -0.042860795561110165, -0.04317927811787664, -0.043498647702237825, -0.04381890258355642, -0.04414004103569452, -0.044462061336998815, -0.04478496177028596, -0.04510874062282788, -0.04543339618633785, -0.04575892675695559, -0.0460853306352334, -0.04641260612612136, -0.046740751538953684, -0.047069765187434, -0.047399645389621714, -0.04773039046791766, -0.048061998749050316, -0.04839446856406149, -0.048727798248292986, -0.04906198614137208, -0.04939703058719863, -0.0497329299339303, -0.05006968253396982, -0.05040728674395062, -0.05074574092472389, -0.05108504344134457, -0.05142519266305795, -0.05176618696328683, -0.052108024719617174, -0.05245070431378568, -0.052794224131665846, -0.05313858256325554, -0.05348377800266285, -0.05382980884809385, -0.05417667350183886, -0.054524370370260034, -0.05487289786377808, -0.05522225439685918, -0.05557243838800263, -0.05592344825972728, -0.05627528243855989, -0.05662793935502103, -0.056981417443613736, -0.05733571514280966, -0.05769083089503768, -0.05804676314667062, -0.058403510348012666, -0.05876107095328781, -0.059119443420626194, -0.05947862621205324, -0.059838617793475934, -0.060199416634671765, -0.06056102120927548, -0.060923429994768075, -0.06128664147246349, -0.06165065412749765, -0.06201546644881545, -0.06238107692915984, -0.06274748406505881, -0.06311468635681444, -0.06348268230849036, -0.06385147042790074, -0.06422104922659749, -0.06459141721985967, -0.0649625729266809, -0.0653345148697585, -0.06570724157548152, -0.06608075157391907, -0.06645504339880937, -0.06683011558754759, -0.0672059666811754, -0.06758259522436855, -0.0679599997654265, -0.06833817885626026, -0.06871713105238209, -0.06909685491289369, -0.0694773490004752, -0.06985861188137404, -0.07024064212539421, -0.07062343830588463, -0.07100699899972895, -0.07139132278733373, -0.07177640825261844, -0.07216225398300369, -0.07254885856940141, -0.07293622060620308, -0.07332433869126992, -0.07371321142592119, -0.07410283741492474, -0.07449321526648506, -0.07488434359223384, -0.07527622100721876, -0.07566884612989333, -0.07606221758210613, -0.07645633398909052, -0.07685119397945467, -0.07724679618517033, -0.07764313924156335, -0.07804022178730285, -0.07843804246439129, -0.07883659991815439, -0.07923589279723048, -0.07963591975356088, -0.08003667944237952, -0.08043817052220287, -0.08084039165482038, -0.0812433415052839, -0.08164701874189811, -0.08205142203621035, -0.08245655006300134, -0.08286240150027457, -0.08326897502924685, -0.08367626933433886, -0.08408428310316496, -0.08449301502652379, -0.08490246379838828, -0.08531262811589668, -0.08572350667934225, -0.08613509819216414, -0.08654740136093769, -0.08696041489536532, -0.08737413750826645, -0.0877885679155686, -0.08820370483629791, -0.08861954699256963, -0.08903609310957905, -0.08945334191559212, -0.08987129214193601, -0.09028994252299039, -0.09070929179617765, -0.09112933870195417, -0.09155008198380138, -0.09197152038821593, -0.09239365266470162, -0.0928164775657595, -0.09323999384687975, -0.09366420026653172, -0.09408909558615614, -0.09451467857015522, -0.09494094798588465, -0.09536790260364403, -0.09579554119666878, -0.09622386254112092, -0.09665286541608037, -0.09708254860353649, -0.09751291088837938, -0.09794395105839104, -0.09837566790423691, -0.09880806021945726, -0.09924112680045893, -0.09967486644650625, -0.10010927795971314, -0.10054436014503415, -0.10098011181025679, -0.10141653176599214, -0.10185361882566707, -0.10229137180551628, -0.10272978952457323, -0.10316887080466225, -0.10360861447039027, -0.10404901934913885, -0.10449008427105544, -0.10493180806904573, -0.10537418957876526, -0.1058172276386114, -0.10626092108971547, -0.10670526877593434, -0.10715026954384266, -0.1075959222427248, -0.1080422257245669, -0.10848917884404924, -0.10893678045853772, -0.10938502942807649, -0.10983392461537989, -0.11028346488582486, -0.11073364910744299, -0.11118447615091243, -0.11163594488955109, -0.11208805419930767, -0.11254080295875524, -0.11299419004908262, -0.11344821435408758, -0.11390287476016808, -0.11435817015631622, -0.11481409943410925, -0.11527066148770332, -0.11572785521382456, -0.11618567951176334, -0.11664413328336515, -0.11710321543302443, -0.11756292486767628, -0.11802326049679007, -0.11848422123236102, -0.1189458059889037, -0.11940801368344434, -0.11987084323551367, -0.12033429356714004, -0.12079836360284119, -0.12126305226961853, -0.12172835849694846, -0.12219428121677672, -0.12266081936350981, -0.1231279718740092, -0.12359573768758314, -0.12406411574598075, -0.12453310499338388, -0.12500270437640126, -0.1254729128440602, -0.12594372934780118, -0.12641515284146937, -0.12688718228130935, -0.12735981662595647, -0.12783305483643181, -0.12830689587613375, -0.12878133871083247, -0.1292563823086622, -0.129732025640115, -0.13020826767803412, -0.13068510739760653, -0.13116254377635755, -0.1316405757941425, -0.13211920243314196, -0.1325984226778531, -0.1330782355150853, -0.1335586399339515, -0.1340396349258633, -0.13452121948452295, -0.13500339260591854, -0.13548615328831565, -0.1359695005322527, -0.13645343334053295, -0.1369379507182193, -0.13742305167262703, -0.1379087352133181, -0.13839500035209418, -0.1388818461029908, -0.1393692714822708, -0.1398572755084181, -0.14034585720213155, -0.14083501558631828, -0.14132474968608827, -0.1418150585287471, -0.14230594114379075, -0.1427973965628988, -0.14328942381992865, -0.1437820219509091, -0.1442751899940345, -0.14476892698965854, -0.14526323198028823, -0.14575810401057793, -0.14625354212732322, -0.14674954537945478, -0.14724611281803313, -0.14774324349624163, -0.1482409364693812, -0.1487391907948642, -0.14923800553220873, -0.14973737974303247, -0.1502373124910471, -0.15073780284205207, -0.15123884986392955, -0.15174045262663743, -0.1522426102022047, -0.15274532166472538, -0.15324858609035197, -0.15375240255729117, -0.15425677014579658, -0.15476168793816464, -0.1552671550187275, -0.15577317047384848, -0.15627973339191564, -0.15678684286333677, -0.15729449798053347, -0.15780269783793582, -0.15831144153197624, -0.15882072816108495, -0.15933055682568342, -0.15984092662817967, -0.16035183667296232, -0.16086328606639533, -0.16137527391681228, -0.16188779933451175, -0.1624008614317507, -0.1629144593227399, -0.16342859212363875, -0.1639432589525489, -0.16445845892951028, -0.1649741911764943, -0.16549045481740005, -0.1660072489780477, -0.16652457278617422, -0.16704242537142738, -0.16756080586536143, -0.16807971340143052, -0.16859914711498508, -0.1691191061432652, -0.16963958962539677, -0.17016059670238515, -0.17068212651711095, -0.17120417821432438, -0.17172675094064033, -0.17224984384453335, -0.17277345607633243, -0.17329758678821633, -0.1738222351342078, -0.17434740027016965, -0.17487308135379842, -0.17539927754462087, -0.1759259880039875, -0.17645321189506918, -0.1769809483828506, -0.17750919663412684, -0.1780379558174972, -0.1785672251033617, -0.1790970036639144, -0.17962729067314054, -0.18015808530680988, -0.18068938674247353, -0.18122119415945753, -0.1817535067388596, -0.1822863236635432, -0.18281964411813334, -0.1833534672890117, -0.1838877923643119, -0.18442261853391484, -0.18495794498944385, -0.18549377092426028, -0.18603009553345834, -0.18656691801386144, -0.1871042375640159, -0.18764205338418838, -0.18818036467635918, -0.18871917064421956, -0.1892584704931658, -0.1897982634302952, -0.1903385486644013, -0.19087932540596997, -0.19142059286717367, -0.19196235026186848, -0.19250459680558807, -0.19304733171554062, -0.19359055421060312, -0.1941342635113179, -0.19467845883988788, -0.1952231394201716, -0.19576830447767973, -0.19631395323957002, -0.19686008493464324, -0.1974066987933386, -0.19795379404772967, -0.19850136993151962, -0.19904942568003775, -0.19959796053023365, -0.20014697372067491, -0.20069646449154105, -0.20124643208462034, -0.20179687574330496, -0.20234779471258724, -0.2028991882390549, -0.20345105557088755, -0.20400339595785144, -0.2045562086512967, -0.20510949290415154, -0.2056632479709194, -0.20621747310767446, -0.2067721675720568, -0.2073273306232694, -0.20788296152207308, -0.20843905953078312, -0.20899562391326454, -0.20955265393492845, -0.21011014886272816, -0.21066810796515473, -0.21122653051223267, -0.21178541577551702, -0.21234476302808808, -0.21290457154454817, -0.21346484060101772, -0.21402556947513052, -0.21458675744603106, -0.21514840379436895, -0.21571050780229656, -0.21627306875346408, -0.21683608593301607, -0.2173995586275872, -0.21796348612529892, -0.21852786771575494, -0.21909270269003822, -0.21965799034070588, -0.22022372996178685, -0.22078992084877647, -0.22135656229863443, -0.2219236536097795, -0.22249119408208606, -0.2230591830168812, -0.2236276197169399, -0.2241965034864818, -0.2247658336311672, -0.22533560945809383, -0.22590583027579236, -0.22647649539422354, -0.22704760412477354, -0.22761915578025171, -0.22819114967488507, -0.22876358512431627, -0.22933646144559883, -0.22990977795719436, -0.23048353397896792, -0.2310577288321859, -0.23163236183951064, -0.2322074323249984, -0.23278293961409446, -0.23335888303363073, -0.23393526191182168, -0.23451207557826015, -0.2350893233639152, -0.23566700460112733, -0.23624511862360553, -0.2368236647664238, -0.2374026423660175, -0.23798205076017998, -0.238561889288059, -0.2391421572901531, -0.23972285410830907, -0.24030397908571688, -0.24088553156690784, -0.24146751089775031, -0.2420499164254465, -0.24263274749852892, -0.24321600346685746, -0.24379968368161553, -0.24438378749530687, -0.24496831426175214, -0.24555326333608563, -0.24613863407475223, -0.24672442583550303, -0.24731063797739372, -0.2478972698607793, -0.24848432084731298, -0.24907179029994053, -0.2496596775828991, -0.2502479820617125, -0.25083670310318884, -0.2514258400754167, -0.2520153923477624, -0.2526053592908659, -0.25319574027663916, -0.25378653467826107, -0.2543777418701756, -0.2549693612280881, -0.255561392128962, -0.2561538339510159, -0.25674668607372064, -0.25733994787779546, -0.25793361874520504, -0.25852769805915754, -0.25912218520409913, -0.25971707956571366, -0.260312380530917, -0.2609080874878559, -0.26150419982590345, -0.2621007169356574, -0.26269763820893566, -0.26329496303877464, -0.2638926908194247, -0.2644908209463487, -0.2650893528162176, -0.2656882858269085, -0.2662876193775008, -0.2668873528682737, -0.26748748570070313, -0.2680880172774587, -0.26868894700240054, -0.26929027428057684, -0.2698919985182203, -0.27049411912274557, -0.2710966355027463, -0.27169954706799193, -0.27230285322942527, -0.2729065533991586, -0.27351064699047245, -0.2741151334178106, -0.274720012096779, -0.275325282444142, -0.2759309438778198, -0.27653699581688496, -0.2771434376815606, -0.2777502688932165, -0.2783574888743673, -0.27896509704866845, -0.2795730928409147, -0.2801814756770361, -0.2807902449840963, -0.281399400190289, -0.2820089407249349, -0.2826188660184802, -0.2832291755024924, -0.2838398686096586, -0.28445094477378174, -0.28506240342977907, -0.28567424401367847, -0.286286465962616, -0.2868990687148333, -0.28751205170967475, -0.2881254143875843, -0.2887391561901045, -0.2893532765598712, -0.2899677749406131, -0.29058265077714773, -0.29119790351538, -0.2918135326022978, -0.2924295374859712, -0.29304591761554843, -0.29366267244125466, -0.29427980141438725, -0.2948973039873155, -0.29551517961347595, -0.29613342774737184, -0.29675204784456855, -0.2973710393616923, -0.2979904017564271, -0.29861013448751206, -0.2992302370147395, -0.29985070879895115, -0.3004715493020369, -0.3010927579869316, -0.30171433431761263, -0.3023362777590969, -0.3029585877774398, -0.3035812638397306, -0.30420430541409194, -0.3048277119696754, -0.30545148297666114, -0.3060756179062536, -0.30670011623068005, -0.3073249774231871, -0.30795020095804015, -0.30857578631051863, -0.3092017329569152, -0.30982804037453227, -0.31045470804168085, -0.31108173543767637, -0.31170912204283785, -0.31233686733848454, -0.31296497080693386, -0.31359343193149913, -0.31422225019648675, -0.3148514250871941, -0.3154809560899072, -0.3161108426918984, -0.31674108438142334, -0.31737168064772003, -0.31800263098100456, -0.3186339348724707, -0.31926559181428593, -0.3198976012995902, -0.32052996282249335, -0.32116267587807246, -0.32179573996236954, -0.32242915457238996, -0.3230629192060993, -0.3236970333624215, -0.3243314965412363, -0.324966308243377, -0.32560146797062917, -0.3262369752257265, -0.3268728295123501, -0.32750903033512546, -0.3281455771996209, -0.32878246961234425, -0.32941970708074175, -0.3300572891131951, -0.33069521521901957, -0.3313334849084617, -0.331972097692697, -0.3326110530838279, -0.3332503505948814, -0.33388998973980677, -0.33452997003347407, -0.33517029099167095, -0.33581095213110146, -0.3364519529693828, -0.3370932930250443, -0.33773497181752465, -0.3383769888671696, -0.33901934369523035, -0.3396620358238608, -0.3403050647761162, -0.3409484300759501, -0.3415921312482132, -0.3422361678186503, -0.34288053931389906, -0.34352524526148687, -0.3441702851898304, -0.34481565862823116, -0.3454613651068763, -0.3461074041568336, -0.34675377531005147, -0.34740047809935615, -0.34804751205844975, -0.3486948767219077, -0.349342571625178, -0.3499905963045775, -0.3506389502972912, -0.3512876331413697, -0.3519366443757266, -0.35258598354013837, -0.3532356501752396, -0.3538856438225235, -0.3545359640243384, -0.3551866103238863, -0.35583758226522066, -0.3564888793932448, -0.3571405012537092, -0.3577924473932108, -0.35844471735918937, -0.3590973106999269, -0.35975022696454506, -0.3604034657030033, -0.36105702646609683, -0.361710908805455, -0.36236511227353885, -0.36301963642364, -0.36367448080987796, -0.364329644987198, -0.36498512851137055, -0.36564093093898786, -0.36629705182746275, -0.36695349073502703, -0.36761024722072877, -0.36826732084443103, -0.3689247111668099, -0.3695824177493525, -0.3702404401543552, -0.3708987779449213, -0.3715574306849603, -0.3722163979391847, -0.3728756792731093, -0.373535274253048, -0.37419518244611394, -0.3748554034202154, -0.3755159367440557, -0.3761767819871308, -0.37683793871972693, -0.3774994065129199, -0.3781611849385724, -0.378823273569332, -0.3794856719786308, -0.3801483797406817, -0.38081139643047834, -0.3814747216237917, -0.38213835489716996, -0.38280229582793557, -0.38346654399418356, -0.3841310989747806, -0.38479596034936225, -0.3854611276983321, -0.38612660060285875, -0.3867923786448756, -0.3874584614070782, -0.3881248484729227, -0.3887915394266237, -0.38945853385315365, -0.39012583133823986, -0.39079343146836376, -0.391461333830758, -0.39212953801340644]
