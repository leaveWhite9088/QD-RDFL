Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Accuracy: 26.82%
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Accuracy: 26.82%
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Accuracy: 26.82%
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Accuracy: 26.82%
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Accuracy: 26.82%
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
**** log-parameter_analysis 运行时间： 2025-01-16 21:47:54 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
**** log-parameter_analysis 运行时间： 2025-01-16 21:48:33 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
初始化模型的准确率：
Accuracy: 26.82%
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.06243327744641907
DataOwner2: noise random: 0.054989255909617854
DataOwner3: noise random: 0.007607090840630016
DataOwner4: noise random: 0.022890580097662674
DataOwner5: noise random: 0.07195974312594888
DataOwner6: noise random: 0.05885538592512627
DataOwner7: noise random: 0.06603582725656529
DataOwner8: noise random: 0.058337104397914
DataOwner9: noise random: 0.0601634506853087
DataOwner10: noise random: 0.08721991535424461
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9984262044307662, 0.9987761276361904, 0.999976569346265, 0.9997876932064483, 0.997901748388904, 0.9985958394269219, 0.9982362565490936, 0.9986235914271477, 0.9985339222502236, 0.9969222546414809]
归一化后的数据质量列表avg_f_list: [0.9492401711889628, 0.9606968558873683, 1.0, 0.9938160877947247, 0.9320691821929448, 0.9547941174110048, 0.9430211695459734, 0.9557027336771151, 0.9527669138421895, 0.9]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3346
DataOwner1的最优x_1 = 0.1158
DataOwner2的最优x_2 = 0.1273
DataOwner3的最优x_3 = 0.1625
DataOwner4的最优x_4 = 0.1574
DataOwner5的最优x_5 = 0.0975
DataOwner6的最优x_6 = 0.1215
DataOwner7的最优x_7 = 0.1094
DataOwner8的最优x_8 = 0.1224
DataOwner9的最优x_9 = 0.1194
DataOwner10的最优x_10 = 0.0592
每个DataOwner应该贡献数据比例 xn_list = [0.11584016358304243, 0.12731003046941367, 0.1625111617628011, 0.15736934839796288, 0.09751165787892696, 0.12147362886365241, 0.10936409767128098, 0.12238205125342426, 0.11943355177036506, 0.059242445418123485]
ModelOwner的最大效用 U(Eta) = 0.5735
DONE
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.334632781236934
DataOwner1的分配到的支付 ： 0.1281
DataOwner2的分配到的支付 ： 0.1425
DataOwner3的分配到的支付 ： 0.1894
DataOwner4的分配到的支付 ： 0.1823
DataOwner5的分配到的支付 ： 0.1059
DataOwner6的分配到的支付 ： 0.1352
DataOwner7的分配到的支付 ： 0.1202
DataOwner8的分配到的支付 ： 0.1363
DataOwner9的分配到的支付 ： 0.1326
DataOwner10的分配到的支付 ： 0.0621
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC2', 'DataOwner7': 'CPC1', 'DataOwner2': 'CPC3', 'DataOwner3': 'CPC10', 'DataOwner4': 'CPC4', 'DataOwner5': 'CPC5', 'DataOwner6': 'CPC6', 'DataOwner8': 'CPC7', 'DataOwner9': 'CPC8', 'DataOwner10': 'CPC9'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC2
DataOwner7 把数据交给 CPC1
DataOwner2 把数据交给 CPC3
DataOwner3 把数据交给 CPC10
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner8 把数据交给 CPC7
DataOwner9 把数据交给 CPC8
DataOwner10 把数据交给 CPC9
DONE
----- literation 1: 模型训练 -----
CPC2调整模型中, 本轮训练的数据量为：818.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 26.82%
Epoch 1/5, Loss: 2.2901
Epoch 2/5, Loss: 2.2867
Epoch 3/5, Loss: 2.2827
Epoch 4/5, Loss: 2.2773
Epoch 5/5, Loss: 2.2700
新模型评估：
Accuracy: 35.19%
Model saved to ../../data/model/mnist_cnn_model
CPC1调整模型中, 本轮训练的数据量为：128.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 35.19%
Epoch 1/5, Loss: 2.2410
Epoch 2/5, Loss: 2.2393
Epoch 3/5, Loss: 2.2374
Epoch 4/5, Loss: 2.2357
Epoch 5/5, Loss: 2.2338
新模型评估：
Accuracy: 35.80%
Model saved to ../../data/model/mnist_cnn_model
CPC3调整模型中, 本轮训练的数据量为：598.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 35.80%
Epoch 1/5, Loss: 2.2711
Epoch 2/5, Loss: 2.2654
Epoch 3/5, Loss: 2.2584
Epoch 4/5, Loss: 2.2500
Epoch 5/5, Loss: 2.2481
新模型评估：
Accuracy: 40.67%
Model saved to ../../data/model/mnist_cnn_model
CPC10调整模型中, 本轮训练的数据量为：764.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 40.67%
Epoch 1/5, Loss: 2.2356
Epoch 2/5, Loss: 2.2253
Epoch 3/5, Loss: 2.2135
Epoch 4/5, Loss: 2.2009
Epoch 5/5, Loss: 2.1868
新模型评估：
Accuracy: 39.95%
CPC4调整模型中, 本轮训练的数据量为：370.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 40.67%
Epoch 1/5, Loss: 2.2215
Epoch 2/5, Loss: 2.2164
Epoch 3/5, Loss: 2.2083
Epoch 4/5, Loss: 2.2031
Epoch 5/5, Loss: 2.1969
新模型评估：
Accuracy: 42.21%
Model saved to ../../data/model/mnist_cnn_model
CPC5调整模型中, 本轮训练的数据量为：573.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 42.21%
Epoch 1/5, Loss: 2.2052
Epoch 2/5, Loss: 2.1951
Epoch 3/5, Loss: 2.1847
Epoch 4/5, Loss: 2.1742
Epoch 5/5, Loss: 2.1627
新模型评估：
Accuracy: 42.97%
Model saved to ../../data/model/mnist_cnn_model
CPC6调整模型中, 本轮训练的数据量为：571.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 42.97%
Epoch 1/5, Loss: 2.1676
Epoch 2/5, Loss: 2.1550
Epoch 3/5, Loss: 2.1433
Epoch 4/5, Loss: 2.1314
Epoch 5/5, Loss: 2.1178
新模型评估：
Accuracy: 46.62%
Model saved to ../../data/model/mnist_cnn_model
CPC7调整模型中, 本轮训练的数据量为：2591.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 46.62%
Epoch 1/5, Loss: 2.0734
Epoch 2/5, Loss: 2.0015
Epoch 3/5, Loss: 1.9179
Epoch 4/5, Loss: 1.8150
Epoch 5/5, Loss: 1.7044
新模型评估：
Accuracy: 59.97%
Model saved to ../../data/model/mnist_cnn_model
CPC8调整模型中, 本轮训练的数据量为：140.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 59.97%
Epoch 1/5, Loss: 1.7086
Epoch 2/5, Loss: 1.5711
Epoch 3/5, Loss: 1.6475
Epoch 4/5, Loss: 1.6114
Epoch 5/5, Loss: 1.6912
新模型评估：
Accuracy: 59.67%
CPC9调整模型中, 本轮训练的数据量为：418.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 59.97%
Epoch 1/5, Loss: 1.6904
Epoch 2/5, Loss: 1.6807
Epoch 3/5, Loss: 1.6615
Epoch 4/5, Loss: 1.6554
Epoch 5/5, Loss: 1.6353
新模型评估：
Accuracy: 62.02%
Model saved to ../../data/model/mnist_cnn_model
DONE
========================= literation: 2 =========================
----- literation 2: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3346
DataOwner1的最优x_1 = 0.1158
DataOwner2的最优x_2 = 0.1273
DataOwner3的最优x_3 = 0.1625
DataOwner4的最优x_4 = 0.1574
DataOwner5的最优x_5 = 0.0975
DataOwner6的最优x_6 = 0.1215
DataOwner7的最优x_7 = 0.1094
DataOwner8的最优x_8 = 0.1224
DataOwner9的最优x_9 = 0.1194
DataOwner10的最优x_10 = 0.0592
每个DataOwner应该贡献数据比例 xn_list = [0.11584016358304243, 0.12731003046941367, 0.1625111617628011, 0.15736934839796288, 0.09751165787892696, 0.12147362886365241, 0.10936409767128098, 0.12238205125342426, 0.11943355177036506, 0.059242445418123485]
ModelOwner的最大效用 U(Eta) = 0.5735
DONE
----- literation 2: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.334632781236934
DataOwner1的分配到的支付 ： 0.1281
DataOwner2的分配到的支付 ： 0.1425
DataOwner3的分配到的支付 ： 0.1894
DataOwner4的分配到的支付 ： 0.1823
DataOwner5的分配到的支付 ： 0.1059
DataOwner6的分配到的支付 ： 0.1352
DataOwner7的分配到的支付 ： 0.1202
DataOwner8的分配到的支付 ： 0.1363
DataOwner9的分配到的支付 ： 0.1326
DataOwner10的分配到的支付 ： 0.0621
DONE
----- literation 2: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC2
DataOwner7 把数据交给 CPC1
DataOwner2 把数据交给 CPC3
DataOwner3 把数据交给 CPC10
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner8 把数据交给 CPC7
DataOwner9 把数据交给 CPC8
DataOwner10 把数据交给 CPC9
DONE
----- literation 2: 模型训练 -----
重新调整fn，进而调整xn、Eta
正在评估DataOwner1的数据质量, 本轮评估的样本数据量为：818.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 62.02%
Epoch 1/5, Loss: 1.6299
Epoch 2/5, Loss: 1.5942
Epoch 3/5, Loss: 1.5626
Epoch 4/5, Loss: 1.5304
Epoch 5/5, Loss: 1.4972
新模型评估：
Accuracy: 64.92%
loss差为：
0.13263187958643985
单位数据loss差为：
0.00016214166208611229
正在评估DataOwner7的数据质量, 本轮评估的样本数据量为：128.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 62.02%
Epoch 1/5, Loss: 1.5747
Epoch 2/5, Loss: 1.5654
Epoch 3/5, Loss: 1.5567
Epoch 4/5, Loss: 1.5490
Epoch 5/5, Loss: 1.5408
新模型评估：
Accuracy: 61.76%
loss差为：
0.03392672538757324
单位数据loss差为：
0.00026505254209041595
正在评估DataOwner2的数据质量, 本轮评估的样本数据量为：598.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 62.02%
Epoch 1/5, Loss: 1.6138
Epoch 2/5, Loss: 1.5840
Epoch 3/5, Loss: 1.5669
Epoch 4/5, Loss: 1.5377
Epoch 5/5, Loss: 1.5095
新模型评估：
Accuracy: 62.53%
loss差为：
0.10427180528640756
单位数据loss差为：
0.00017436756736857452
正在评估DataOwner3的数据质量, 本轮评估的样本数据量为：764.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 62.02%
Epoch 1/5, Loss: 1.6010
Epoch 2/5, Loss: 1.5687
Epoch 3/5, Loss: 1.5379
Epoch 4/5, Loss: 1.5082
Epoch 5/5, Loss: 1.4788
新模型评估：
Accuracy: 63.66%
loss差为：
0.12212305267651868
单位数据loss差为：
0.00015984692758706635
正在评估DataOwner4的数据质量, 本轮评估的样本数据量为：370.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 62.02%
Epoch 1/5, Loss: 1.5337
Epoch 2/5, Loss: 1.5153
Epoch 3/5, Loss: 1.4963
Epoch 4/5, Loss: 1.4825
Epoch 5/5, Loss: 1.4663
新模型评估：
Accuracy: 63.38%
loss差为：
0.0673686067263286
单位数据loss差为：
0.0001820773154765638
正在评估DataOwner5的数据质量, 本轮评估的样本数据量为：573.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 62.02%
Epoch 1/5, Loss: 1.5873
Epoch 2/5, Loss: 1.5626
Epoch 3/5, Loss: 1.5400
Epoch 4/5, Loss: 1.5183
Epoch 5/5, Loss: 1.4974
新模型评估：
Accuracy: 63.43%
loss差为：
0.08990650706821013
单位数据loss差为：
0.00015690489889739987
正在评估DataOwner6的数据质量, 本轮评估的样本数据量为：571.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 62.02%
Epoch 1/5, Loss: 1.6395
Epoch 2/5, Loss: 1.6138
Epoch 3/5, Loss: 1.5897
Epoch 4/5, Loss: 1.5650
Epoch 5/5, Loss: 1.5419
新模型评估：
Accuracy: 65.45%
loss差为：
0.09758251243167448
单位数据loss差为：
0.00017089756993288
正在评估DataOwner8的数据质量, 本轮评估的样本数据量为：2591.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 62.02%
Epoch 1/5, Loss: 1.5622
Epoch 2/5, Loss: 1.4604
Epoch 3/5, Loss: 1.3590
Epoch 4/5, Loss: 1.2621
Epoch 5/5, Loss: 1.1627
新模型评估：
Accuracy: 73.76%
loss差为：
0.3994588735626967
单位数据loss差为：
0.00015417169956105623
正在评估DataOwner9的数据质量, 本轮评估的样本数据量为：140.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 62.02%
Epoch 1/5, Loss: 1.5613
Epoch 2/5, Loss: 1.6712
Epoch 3/5, Loss: 1.5361
Epoch 4/5, Loss: 1.6175
Epoch 5/5, Loss: 1.5694
新模型评估：
Accuracy: 63.68%
loss差为：
-0.008124669392903572
单位数据loss差为：
-5.803335280645409e-05
正在评估DataOwner10的数据质量, 本轮评估的样本数据量为：418.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 62.02%
Epoch 1/5, Loss: 1.5740
Epoch 2/5, Loss: 1.5403
Epoch 3/5, Loss: 1.5210
Epoch 4/5, Loss: 1.5060
Epoch 5/5, Loss: 1.5046
新模型评估：
Accuracy: 63.85%
loss差为：
0.06942813737051834
单位数据loss差为：
0.000166096022417508
经过服务器调节后的真实数据质量：
数据质量列表avg_f_list: [0.00016214166208611229, 0.00017436756736857452, 0.00015984692758706635, 0.0001820773154765638, 0.00015690489889739987, 0.00017089756993288, 0.00026505254209041595, 0.00015417169956105623, -5.803335280645409e-05, 0.000166096022417508]
归一化后的数据质量列表avg_f_list:[0.9681475169204917, 0.9719316206141441, 0.9674372616802317, 0.9743179049520754, 0.9665266590398393, 0.970857603614175, 1.0, 0.965680692261495, 0.9, 0.9693714516059282]
CPC2调整模型中, 本轮训练的数据量为：818.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 62.02%
Epoch 1/5, Loss: 1.6280
Epoch 2/5, Loss: 1.5906
Epoch 3/5, Loss: 1.5593
Epoch 4/5, Loss: 1.5269
Epoch 5/5, Loss: 1.4933
新模型评估：
Accuracy: 64.71%
Model saved to ../../data/model/mnist_cnn_model
CPC1调整模型中, 本轮训练的数据量为：128.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 64.71%
Epoch 1/5, Loss: 1.4318
Epoch 2/5, Loss: 1.4225
Epoch 3/5, Loss: 1.4135
Epoch 4/5, Loss: 1.4055
Epoch 5/5, Loss: 1.3968
新模型评估：
Accuracy: 64.62%
CPC3调整模型中, 本轮训练的数据量为：598.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 64.71%
Epoch 1/5, Loss: 1.4612
Epoch 2/5, Loss: 1.4307
Epoch 3/5, Loss: 1.4162
Epoch 4/5, Loss: 1.3944
Epoch 5/5, Loss: 1.3598
新模型评估：
Accuracy: 66.03%
Model saved to ../../data/model/mnist_cnn_model
CPC10调整模型中, 本轮训练的数据量为：764.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 66.03%
Epoch 1/5, Loss: 1.3385
Epoch 2/5, Loss: 1.3087
Epoch 3/5, Loss: 1.2796
Epoch 4/5, Loss: 1.2521
Epoch 5/5, Loss: 1.2244
新模型评估：
Accuracy: 70.17%
Model saved to ../../data/model/mnist_cnn_model
CPC4调整模型中, 本轮训练的数据量为：370.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 70.17%
Epoch 1/5, Loss: 1.1372
Epoch 2/5, Loss: 1.1184
Epoch 3/5, Loss: 1.1041
Epoch 4/5, Loss: 1.0880
Epoch 5/5, Loss: 1.0735
新模型评估：
Accuracy: 72.07%
Model saved to ../../data/model/mnist_cnn_model
CPC5调整模型中, 本轮训练的数据量为：573.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 72.07%
Epoch 1/5, Loss: 1.1436
Epoch 2/5, Loss: 1.1197
Epoch 3/5, Loss: 1.1013
Epoch 4/5, Loss: 1.0818
Epoch 5/5, Loss: 1.0631
新模型评估：
Accuracy: 73.32%
Model saved to ../../data/model/mnist_cnn_model
CPC6调整模型中, 本轮训练的数据量为：571.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 73.32%
Epoch 1/5, Loss: 1.0731
Epoch 2/5, Loss: 1.0488
Epoch 3/5, Loss: 1.0285
Epoch 4/5, Loss: 1.0070
Epoch 5/5, Loss: 0.9886
新模型评估：
Accuracy: 74.82%
Model saved to ../../data/model/mnist_cnn_model
CPC7调整模型中, 本轮训练的数据量为：2591.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 74.82%
Epoch 1/5, Loss: 0.9291
Epoch 2/5, Loss: 0.8585
Epoch 3/5, Loss: 0.7943
Epoch 4/5, Loss: 0.7429
Epoch 5/5, Loss: 0.6969
新模型评估：
Accuracy: 76.62%
Model saved to ../../data/model/mnist_cnn_model
CPC8调整模型中, 本轮训练的数据量为：140.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 76.62%
Epoch 1/5, Loss: 0.7203
Epoch 2/5, Loss: 0.7209
Epoch 3/5, Loss: 0.6678
Epoch 4/5, Loss: 0.6812
Epoch 5/5, Loss: 0.7740
新模型评估：
Accuracy: 78.52%
Model saved to ../../data/model/mnist_cnn_model
CPC9调整模型中, 本轮训练的数据量为：418.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.52%
Epoch 1/5, Loss: 0.6735
Epoch 2/5, Loss: 0.6646
Epoch 3/5, Loss: 0.6526
Epoch 4/5, Loss: 0.6466
Epoch 5/5, Loss: 0.6330
新模型评估：
Accuracy: 76.71%
DONE
========================= literation: 3 =========================
----- literation 3: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3484
DataOwner1的最优x_1 = 0.1247
DataOwner2的最优x_2 = 0.1284
DataOwner3的最优x_3 = 0.1240
DataOwner4的最优x_4 = 0.1307
DataOwner5的最优x_5 = 0.1231
DataOwner6的最优x_6 = 0.1274
DataOwner7的最优x_7 = 0.1542
DataOwner8的最优x_8 = 0.1222
DataOwner9的最优x_9 = 0.0458
DataOwner10的最优x_10 = 0.1259
每个DataOwner应该贡献数据比例 xn_list = [0.12468285297287045, 0.1284040027468734, 0.12397748412642012, 0.1307189683338194, 0.12306991466579897, 0.12735413817233376, 0.154160281839489, 0.12222349949728369, 0.045774413425866384, 0.1258932073561056]
ModelOwner的最大效用 U(Eta) = 0.5894
Eta开始变化：
eta:0.0
**** log-parameter_analysis 运行时间： 2025-01-16 22:13:53 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
**** log-parameter_analysis 运行时间： 2025-01-16 22:15:55 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
初始化模型的准确率：
Accuracy: 26.82%
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.0632795930733255
DataOwner2: noise random: 0.04503076473182004
DataOwner3: noise random: 0.041237930190075926
DataOwner4: noise random: 0.08227730029454687
DataOwner5: noise random: 0.09925296604154038
DataOwner6: noise random: 0.05915015035531762
DataOwner7: noise random: 0.015312677037508783
DataOwner8: noise random: 0.06284724133808836
DataOwner9: noise random: 0.002469761536688731
DataOwner10: noise random: 0.042656650366639064
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9983798162637667, 0.9991779612945522, 0.9993133871780431, 0.9972548076093326, 0.996016253915969, 0.9985843270555544, 0.9999051171141704, 0.9984014594426085, 0.9999975288579281, 0.9992634863584059]
归一化后的数据质量列表avg_f_list: [0.9593669711902542, 0.9794144444851456, 0.982816015224801, 0.9311094740107072, 0.9, 0.9645037877821547, 0.9976788404442055, 0.9599105955105363, 1.0, 0.9815626272934308]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3483
DataOwner1的最优x_1 = 0.1159
DataOwner2的最优x_2 = 0.1357
DataOwner3的最优x_3 = 0.1388
DataOwner4的最优x_4 = 0.0849
DataOwner5的最优x_5 = 0.0459
DataOwner6的最优x_6 = 0.1211
DataOwner7的最优x_7 = 0.1522
DataOwner8的最优x_8 = 0.1164
DataOwner9的最优x_9 = 0.1542
DataOwner10的最优x_10 = 0.1377
每个DataOwner应该贡献数据比例 xn_list = [0.11588634048351755, 0.1356559105882501, 0.13884096897889717, 0.08487180747731937, 0.0458803464328502, 0.12111932394271689, 0.1522150172100344, 0.11644576867381826, 0.15422643383762558, 0.1376728794129774]
ModelOwner的最大效用 U(Eta) = 0.5893
DONE
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3482618027644884
DataOwner1的分配到的支付 ： 0.1280
DataOwner2的分配到的支付 ： 0.1530
DataOwner3的分配到的支付 ： 0.1572
DataOwner4的分配到的支付 ： 0.0910
DataOwner5的分配到的支付 ： 0.0476
DataOwner6的分配到的支付 ： 0.1345
DataOwner7的分配到的支付 ： 0.1749
DataOwner8的分配到的支付 ： 0.1287
DataOwner9的分配到的支付 ： 0.1776
DataOwner10的分配到的支付 ： 0.1556
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC1', 'DataOwner2': 'CPC2', 'DataOwner3': 'CPC3', 'DataOwner4': 'CPC4', 'DataOwner5': 'CPC5', 'DataOwner6': 'CPC6', 'DataOwner7': 'CPC7', 'DataOwner8': 'CPC8', 'DataOwner9': 'CPC9', 'DataOwner10': 'CPC10'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC2
DataOwner3 把数据交给 CPC3
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC8
DataOwner9 把数据交给 CPC9
DataOwner10 把数据交给 CPC10
DONE
----- literation 1: 模型训练 -----
CPC1调整模型中, 本轮训练的数据量为：1837.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 26.82%
Epoch 1/5, Loss: 2.2939
Epoch 2/5, Loss: 2.2849
Epoch 3/5, Loss: 2.2706
Epoch 4/5, Loss: 2.2483
Epoch 5/5, Loss: 2.2157
新模型评估：
Accuracy: 50.42%
Model saved to ../../data/model/mnist_cnn_model
CPC2调整模型中, 本轮训练的数据量为：307.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 50.42%
Epoch 1/5, Loss: 2.1984
Epoch 2/5, Loss: 2.1911
Epoch 3/5, Loss: 2.1853
Epoch 4/5, Loss: 2.1799
Epoch 5/5, Loss: 2.1713
新模型评估：
Accuracy: 50.05%
CPC3调整模型中, 本轮训练的数据量为：314.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 50.42%
Epoch 1/5, Loss: 2.2024
Epoch 2/5, Loss: 2.1971
Epoch 3/5, Loss: 2.1908
Epoch 4/5, Loss: 2.1847
Epoch 5/5, Loss: 2.1783
新模型评估：
Accuracy: 49.75%
CPC4调整模型中, 本轮训练的数据量为：288.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 50.42%
Epoch 1/5, Loss: 2.1854
Epoch 2/5, Loss: 2.1718
Epoch 3/5, Loss: 2.1704
Epoch 4/5, Loss: 2.1698
Epoch 5/5, Loss: 2.1550
新模型评估：
Accuracy: 52.57%
Model saved to ../../data/model/mnist_cnn_model
CPC5调整模型中, 本轮训练的数据量为：363.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 52.57%
Epoch 1/5, Loss: 2.1732
Epoch 2/5, Loss: 2.1585
Epoch 3/5, Loss: 2.1550
Epoch 4/5, Loss: 2.1446
Epoch 5/5, Loss: 2.1395
新模型评估：
Accuracy: 51.08%
CPC6调整模型中, 本轮训练的数据量为：822.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 52.57%
Epoch 1/5, Loss: 2.1799
Epoch 2/5, Loss: 2.1641
Epoch 3/5, Loss: 2.1482
Epoch 4/5, Loss: 2.1296
Epoch 5/5, Loss: 2.1092
新模型评估：
Accuracy: 54.33%
Model saved to ../../data/model/mnist_cnn_model
CPC7调整模型中, 本轮训练的数据量为：689.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 54.33%
Epoch 1/5, Loss: 2.0644
Epoch 2/5, Loss: 2.0435
Epoch 3/5, Loss: 2.0249
Epoch 4/5, Loss: 2.0035
Epoch 5/5, Loss: 1.9830
新模型评估：
Accuracy: 54.61%
Model saved to ../../data/model/mnist_cnn_model
CPC8调整模型中, 本轮训练的数据量为：1581.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 54.61%
Epoch 1/5, Loss: 1.9699
Epoch 2/5, Loss: 1.9198
Epoch 3/5, Loss: 1.8682
Epoch 4/5, Loss: 1.8095
Epoch 5/5, Loss: 1.7497
新模型评估：
Accuracy: 59.06%
Model saved to ../../data/model/mnist_cnn_model
CPC9调整模型中, 本轮训练的数据量为：349.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 59.06%
Epoch 1/5, Loss: 1.6865
Epoch 2/5, Loss: 1.6619
Epoch 3/5, Loss: 1.6601
Epoch 4/5, Loss: 1.6345
Epoch 5/5, Loss: 1.6234
新模型评估：
Accuracy: 63.24%
Model saved to ../../data/model/mnist_cnn_model
CPC10调整模型中, 本轮训练的数据量为：155.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 63.24%
Epoch 1/5, Loss: 1.5716
Epoch 2/5, Loss: 1.5849
Epoch 3/5, Loss: 1.5739
Epoch 4/5, Loss: 1.5501
Epoch 5/5, Loss: 1.5552
新模型评估：
Accuracy: 62.97%
DONE
========================= literation: 2 =========================
----- literation 2: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3483
DataOwner1的最优x_1 = 0.1159
DataOwner2的最优x_2 = 0.1357
DataOwner3的最优x_3 = 0.1388
DataOwner4的最优x_4 = 0.0849
DataOwner5的最优x_5 = 0.0459
DataOwner6的最优x_6 = 0.1211
DataOwner7的最优x_7 = 0.1522
DataOwner8的最优x_8 = 0.1164
DataOwner9的最优x_9 = 0.1542
DataOwner10的最优x_10 = 0.1377
每个DataOwner应该贡献数据比例 xn_list = [0.11588634048351755, 0.1356559105882501, 0.13884096897889717, 0.08487180747731937, 0.0458803464328502, 0.12111932394271689, 0.1522150172100344, 0.11644576867381826, 0.15422643383762558, 0.1376728794129774]
ModelOwner的最大效用 U(Eta) = 0.5893
DONE
----- literation 2: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3482618027644884
DataOwner1的分配到的支付 ： 0.1280
DataOwner2的分配到的支付 ： 0.1530
DataOwner3的分配到的支付 ： 0.1572
DataOwner4的分配到的支付 ： 0.0910
DataOwner5的分配到的支付 ： 0.0476
DataOwner6的分配到的支付 ： 0.1345
DataOwner7的分配到的支付 ： 0.1749
DataOwner8的分配到的支付 ： 0.1287
DataOwner9的分配到的支付 ： 0.1776
DataOwner10的分配到的支付 ： 0.1556
DONE
----- literation 2: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC2
DataOwner3 把数据交给 CPC3
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC8
DataOwner9 把数据交给 CPC9
DataOwner10 把数据交给 CPC10
DONE
----- literation 2: 模型训练 -----
重新调整fn，进而调整xn、Eta
正在评估DataOwner1的数据质量, 本轮评估的样本数据量为：1837.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 63.24%
Epoch 1/5, Loss: 1.6254
Epoch 2/5, Loss: 1.5526
Epoch 3/5, Loss: 1.4871
Epoch 4/5, Loss: 1.4163
Epoch 5/5, Loss: 1.3453
新模型评估：
Accuracy: 69.53%
loss差为：
0.28014404198219034
单位数据loss差为：
0.00015250083940238996
正在评估DataOwner2的数据质量, 本轮评估的样本数据量为：307.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 63.24%
Epoch 1/5, Loss: 1.6402
Epoch 2/5, Loss: 1.6274
Epoch 3/5, Loss: 1.6112
Epoch 4/5, Loss: 1.6039
Epoch 5/5, Loss: 1.5845
新模型评估：
Accuracy: 64.26%
loss差为：
0.0557025671005249
单位数据loss差为：
0.00018144158664666092
正在评估DataOwner3的数据质量, 本轮评估的样本数据量为：314.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 63.24%
Epoch 1/5, Loss: 1.6740
Epoch 2/5, Loss: 1.6565
Epoch 3/5, Loss: 1.6429
Epoch 4/5, Loss: 1.6289
Epoch 5/5, Loss: 1.6153
新模型评估：
Accuracy: 65.12%
loss差为：
0.05869555473327637
单位数据loss差为：
0.00018692851825884193
正在评估DataOwner4的数据质量, 本轮评估的样本数据量为：288.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 63.24%
Epoch 1/5, Loss: 1.7087
Epoch 2/5, Loss: 1.7082
Epoch 3/5, Loss: 1.6919
Epoch 4/5, Loss: 1.6691
Epoch 5/5, Loss: 1.6632
新模型评估：
Accuracy: 62.53%
loss差为：
0.04546785354614258
单位数据loss差为：
0.00015787449147966172
正在评估DataOwner5的数据质量, 本轮评估的样本数据量为：363.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 63.24%
Epoch 1/5, Loss: 1.6323
Epoch 2/5, Loss: 1.6127
Epoch 3/5, Loss: 1.5997
Epoch 4/5, Loss: 1.5867
Epoch 5/5, Loss: 1.5651
新模型评估：
Accuracy: 63.10%
loss差为：
0.067196786403656
单位数据loss差为：
0.0001851151140596584
正在评估DataOwner6的数据质量, 本轮评估的样本数据量为：822.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 63.24%
Epoch 1/5, Loss: 1.6427
Epoch 2/5, Loss: 1.6119
Epoch 3/5, Loss: 1.5799
Epoch 4/5, Loss: 1.5488
Epoch 5/5, Loss: 1.5196
新模型评估：
Accuracy: 64.70%
loss差为：
0.12308200506063605
单位数据loss差为：
0.00014973479934383946
正在评估DataOwner7的数据质量, 本轮评估的样本数据量为：689.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 63.24%
Epoch 1/5, Loss: 1.6401
Epoch 2/5, Loss: 1.6127
Epoch 3/5, Loss: 1.5856
Epoch 4/5, Loss: 1.5624
Epoch 5/5, Loss: 1.5368
新模型评估：
Accuracy: 63.96%
loss差为：
0.10334283655340037
单位数据loss差为：
0.00014998960312539967
正在评估DataOwner8的数据质量, 本轮评估的样本数据量为：1581.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 63.24%
Epoch 1/5, Loss: 1.6251
Epoch 2/5, Loss: 1.5657
Epoch 3/5, Loss: 1.5059
Epoch 4/5, Loss: 1.4450
Epoch 5/5, Loss: 1.3855
新模型评估：
Accuracy: 68.56%
loss差为：
0.23957962989807124
单位数据loss差为：
0.00015153676780396664
正在评估DataOwner9的数据质量, 本轮评估的样本数据量为：349.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 63.24%
Epoch 1/5, Loss: 1.6217
Epoch 2/5, Loss: 1.6038
Epoch 3/5, Loss: 1.5979
Epoch 4/5, Loss: 1.5784
Epoch 5/5, Loss: 1.5674
新模型评估：
Accuracy: 62.39%
loss差为：
0.05430680513381958
单位数据loss差为：
0.0001556068915009157
正在评估DataOwner10的数据质量, 本轮评估的样本数据量为：155.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 63.24%
Epoch 1/5, Loss: 1.6603
Epoch 2/5, Loss: 1.6464
Epoch 3/5, Loss: 1.6486
Epoch 4/5, Loss: 1.6433
Epoch 5/5, Loss: 1.5951
新模型评估：
Accuracy: 62.90%
loss差为：
0.06519889831542969
单位数据loss差为：
0.00042063805364793346
经过服务器调节后的真实数据质量：
数据质量列表avg_f_list: [0.00015250083940238996, 0.00018144158664666092, 0.00018692851825884193, 0.00015787449147966172, 0.0001851151140596584, 0.00014973479934383946, 0.00014998960312539967, 0.00015153676780396664, 0.0001556068915009157, 0.00042063805364793346]
归一化后的数据质量列表avg_f_list:[0.9010210434960096, 0.9117040998212705, 0.9137295208987235, 0.9030046490791452, 0.9130601290880411, 0.9, 0.900094057113568, 0.9006651704737753, 0.9021675974960732, 1.0]
CPC1调整模型中, 本轮训练的数据量为：1837.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 63.24%
Epoch 1/5, Loss: 1.6259
Epoch 2/5, Loss: 1.5549
Epoch 3/5, Loss: 1.4870
Epoch 4/5, Loss: 1.4169
Epoch 5/5, Loss: 1.3473
新模型评估：
Accuracy: 69.82%
Model saved to ../../data/model/mnist_cnn_model
CPC2调整模型中, 本轮训练的数据量为：307.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 69.82%
Epoch 1/5, Loss: 1.3011
Epoch 2/5, Loss: 1.2877
Epoch 3/5, Loss: 1.2691
Epoch 4/5, Loss: 1.2574
Epoch 5/5, Loss: 1.2363
新模型评估：
Accuracy: 71.63%
Model saved to ../../data/model/mnist_cnn_model
CPC3调整模型中, 本轮训练的数据量为：314.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 71.63%
Epoch 1/5, Loss: 1.2775
Epoch 2/5, Loss: 1.2585
Epoch 3/5, Loss: 1.2437
Epoch 4/5, Loss: 1.2302
Epoch 5/5, Loss: 1.2142
新模型评估：
Accuracy: 73.06%
Model saved to ../../data/model/mnist_cnn_model
CPC4调整模型中, 本轮训练的数据量为：288.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 73.06%
Epoch 1/5, Loss: 1.2780
Epoch 2/5, Loss: 1.2410
Epoch 3/5, Loss: 1.2243
Epoch 4/5, Loss: 1.2062
Epoch 5/5, Loss: 1.2006
新模型评估：
Accuracy: 70.66%
CPC5调整模型中, 本轮训练的数据量为：363.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 73.06%
Epoch 1/5, Loss: 1.1970
Epoch 2/5, Loss: 1.1731
Epoch 3/5, Loss: 1.1647
Epoch 4/5, Loss: 1.1478
Epoch 5/5, Loss: 1.1288
新模型评估：
Accuracy: 72.11%
CPC6调整模型中, 本轮训练的数据量为：822.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 73.06%
Epoch 1/5, Loss: 1.2109
Epoch 2/5, Loss: 1.1803
Epoch 3/5, Loss: 1.1513
Epoch 4/5, Loss: 1.1269
Epoch 5/5, Loss: 1.1012
新模型评估：
Accuracy: 73.58%
Model saved to ../../data/model/mnist_cnn_model
CPC7调整模型中, 本轮训练的数据量为：689.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 73.58%
Epoch 1/5, Loss: 1.1024
Epoch 2/5, Loss: 1.0728
Epoch 3/5, Loss: 1.0491
Epoch 4/5, Loss: 1.0295
Epoch 5/5, Loss: 1.0043
新模型评估：
Accuracy: 74.89%
Model saved to ../../data/model/mnist_cnn_model
CPC8调整模型中, 本轮训练的数据量为：1581.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 74.89%
Epoch 1/5, Loss: 0.9562
Epoch 2/5, Loss: 0.9082
Epoch 3/5, Loss: 0.8670
Epoch 4/5, Loss: 0.8265
Epoch 5/5, Loss: 0.7914
新模型评估：
Accuracy: 75.67%
Model saved to ../../data/model/mnist_cnn_model
CPC9调整模型中, 本轮训练的数据量为：349.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 75.67%
Epoch 1/5, Loss: 0.7694
Epoch 2/5, Loss: 0.7880
Epoch 3/5, Loss: 0.7706
Epoch 4/5, Loss: 0.7506
Epoch 5/5, Loss: 0.7577
新模型评估：
Accuracy: 75.56%
CPC10调整模型中, 本轮训练的数据量为：155.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 75.67%
Epoch 1/5, Loss: 0.7816
Epoch 2/5, Loss: 0.7943
Epoch 3/5, Loss: 0.7680
Epoch 4/5, Loss: 0.7425
Epoch 5/5, Loss: 0.7239
新模型评估：
Accuracy: 75.88%
Model saved to ../../data/model/mnist_cnn_model
DONE
========================= literation: 3 =========================
----- literation 3: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.2839
DataOwner1的最优x_1 = 0.1023
DataOwner2的最优x_2 = 0.1135
DataOwner3的最优x_3 = 0.1156
DataOwner4的最优x_4 = 0.1045
DataOwner5的最优x_5 = 0.1149
DataOwner6的最优x_6 = 0.1012
DataOwner7的最优x_7 = 0.1013
DataOwner8的最优x_8 = 0.1020
DataOwner9的最优x_9 = 0.1036
DataOwner10的最优x_10 = 0.1876
每个DataOwner应该贡献数据比例 xn_list = [0.10234620356980426, 0.11353153471311027, 0.11559010360714898, 0.1044654456868464, 0.1149119017907497, 0.1012476588903034, 0.1013490747488754, 0.10196391360056359, 0.10357355235319035, 0.18759158624393596]
ModelOwner的最大效用 U(Eta) = 0.5177
Eta开始变化：
eta:0.01
new: DataOwner1的最优x_1 = 0.0126
new: DataOwner2的最优x_2 = 0.0140
new: DataOwner3的最优x_3 = 0.0143
new: DataOwner4的最优x_4 = 0.0129
new: DataOwner5的最优x_5 = 0.0142
new: DataOwner6的最优x_6 = 0.0125
new: DataOwner7的最优x_7 = 0.0125
new: DataOwner8的最优x_8 = 0.0126
new: DataOwner9的最优x_9 = 0.0128
new: DataOwner10的最优x_10 = 0.0232
eta:0.02
new: DataOwner1的最优x_1 = 0.0130
new: DataOwner2的最优x_2 = 0.0144
new: DataOwner3的最优x_3 = 0.0147
new: DataOwner4的最优x_4 = 0.0132
new: DataOwner5的最优x_5 = 0.0146
new: DataOwner6的最优x_6 = 0.0128
new: DataOwner7的最优x_7 = 0.0129
new: DataOwner8的最优x_8 = 0.0129
new: DataOwner9的最优x_9 = 0.0131
new: DataOwner10的最优x_10 = 0.0238
eta:0.03
new: DataOwner1的最优x_1 = 0.0133
new: DataOwner2的最优x_2 = 0.0147
new: DataOwner3的最优x_3 = 0.0150
new: DataOwner4的最优x_4 = 0.0136
new: DataOwner5的最优x_5 = 0.0149
new: DataOwner6的最优x_6 = 0.0132
new: DataOwner7的最优x_7 = 0.0132
new: DataOwner8的最优x_8 = 0.0132
new: DataOwner9的最优x_9 = 0.0135
new: DataOwner10的最优x_10 = 0.0244
eta:0.04
new: DataOwner1的最优x_1 = 0.0133
new: DataOwner2的最优x_2 = 0.0148
new: DataOwner3的最优x_3 = 0.0150
new: DataOwner4的最优x_4 = 0.0136
new: DataOwner5的最优x_5 = 0.0150
new: DataOwner6的最优x_6 = 0.0132
new: DataOwner7的最优x_7 = 0.0132
new: DataOwner8的最优x_8 = 0.0133
new: DataOwner9的最优x_9 = 0.0135
new: DataOwner10的最优x_10 = 0.0244
eta:0.05
new: DataOwner1的最优x_1 = 0.0125
new: DataOwner2的最优x_2 = 0.0139
new: DataOwner3的最优x_3 = 0.0141
new: DataOwner4的最优x_4 = 0.0128
new: DataOwner5的最优x_5 = 0.0140
new: DataOwner6的最优x_6 = 0.0124
new: DataOwner7的最优x_7 = 0.0124
new: DataOwner8的最优x_8 = 0.0125
new: DataOwner9的最优x_9 = 0.0127
new: DataOwner10的最优x_10 = 0.0229
eta:0.060000000000000005
new: DataOwner1的最优x_1 = 0.0165
new: DataOwner2的最优x_2 = 0.0183
new: DataOwner3的最优x_3 = 0.0186
new: DataOwner4的最优x_4 = 0.0169
new: DataOwner5的最优x_5 = 0.0185
new: DataOwner6的最优x_6 = 0.0163
new: DataOwner7的最优x_7 = 0.0164
new: DataOwner8的最优x_8 = 0.0164
new: DataOwner9的最优x_9 = 0.0167
new: DataOwner10的最优x_10 = 0.0303
eta:0.06999999999999999
new: DataOwner1的最优x_1 = 0.0132
new: DataOwner2的最优x_2 = 0.0146
new: DataOwner3的最优x_3 = 0.0149
new: DataOwner4的最优x_4 = 0.0134
new: DataOwner5的最优x_5 = 0.0148
new: DataOwner6的最优x_6 = 0.0130
new: DataOwner7的最优x_7 = 0.0130
new: DataOwner8的最优x_8 = 0.0131
new: DataOwner9的最优x_9 = 0.0133
new: DataOwner10的最优x_10 = 0.0241
eta:0.08
new: DataOwner1的最优x_1 = 0.0113
new: DataOwner2的最优x_2 = 0.0125
new: DataOwner3的最优x_3 = 0.0128
new: DataOwner4的最优x_4 = 0.0115
new: DataOwner5的最优x_5 = 0.0127
new: DataOwner6的最优x_6 = 0.0112
new: DataOwner7的最优x_7 = 0.0112
new: DataOwner8的最优x_8 = 0.0113
new: DataOwner9的最优x_9 = 0.0114
new: DataOwner10的最优x_10 = 0.0207
eta:0.09
new: DataOwner1的最优x_1 = 0.0127
new: DataOwner2的最优x_2 = 0.0141
new: DataOwner3的最优x_3 = 0.0144
new: DataOwner4的最优x_4 = 0.0130
new: DataOwner5的最优x_5 = 0.0143
new: DataOwner6的最优x_6 = 0.0126
new: DataOwner7的最优x_7 = 0.0126
new: DataOwner8的最优x_8 = 0.0127
new: DataOwner9的最优x_9 = 0.0129
new: DataOwner10的最优x_10 = 0.0233
eta:0.09999999999999999
new: DataOwner1的最优x_1 = 0.0088
new: DataOwner2的最优x_2 = 0.0097
new: DataOwner3的最优x_3 = 0.0099
new: DataOwner4的最优x_4 = 0.0089
new: DataOwner5的最优x_5 = 0.0098
new: DataOwner6的最优x_6 = 0.0087
new: DataOwner7的最优x_7 = 0.0087
new: DataOwner8的最优x_8 = 0.0087
new: DataOwner9的最优x_9 = 0.0089
new: DataOwner10的最优x_10 = 0.0161
eta:0.11
new: DataOwner1的最优x_1 = 0.0088
new: DataOwner2的最优x_2 = 0.0097
new: DataOwner3的最优x_3 = 0.0099
new: DataOwner4的最优x_4 = 0.0089
new: DataOwner5的最优x_5 = 0.0098
new: DataOwner6的最优x_6 = 0.0087
new: DataOwner7的最优x_7 = 0.0087
new: DataOwner8的最优x_8 = 0.0087
new: DataOwner9的最优x_9 = 0.0089
new: DataOwner10的最优x_10 = 0.0161
eta:0.12
new: DataOwner1的最优x_1 = 0.0127
new: DataOwner2的最优x_2 = 0.0141
new: DataOwner3的最优x_3 = 0.0144
new: DataOwner4的最优x_4 = 0.0130
new: DataOwner5的最优x_5 = 0.0143
new: DataOwner6的最优x_6 = 0.0126
new: DataOwner7的最优x_7 = 0.0126
new: DataOwner8的最优x_8 = 0.0127
new: DataOwner9的最优x_9 = 0.0129
new: DataOwner10的最优x_10 = 0.0233
eta:0.13
new: DataOwner1的最优x_1 = 0.0125
new: DataOwner2的最优x_2 = 0.0139
new: DataOwner3的最优x_3 = 0.0142
new: DataOwner4的最优x_4 = 0.0128
new: DataOwner5的最优x_5 = 0.0141
new: DataOwner6的最优x_6 = 0.0124
new: DataOwner7的最优x_7 = 0.0124
new: DataOwner8的最优x_8 = 0.0125
new: DataOwner9的最优x_9 = 0.0127
new: DataOwner10的最优x_10 = 0.0230
eta:0.14
new: DataOwner1的最优x_1 = 0.0149
new: DataOwner2的最优x_2 = 0.0165
new: DataOwner3的最优x_3 = 0.0168
new: DataOwner4的最优x_4 = 0.0152
new: DataOwner5的最优x_5 = 0.0167
new: DataOwner6的最优x_6 = 0.0147
new: DataOwner7的最优x_7 = 0.0147
new: DataOwner8的最优x_8 = 0.0148
new: DataOwner9的最优x_9 = 0.0150
new: DataOwner10的最优x_10 = 0.0272
eta:0.15000000000000002
new: DataOwner1的最优x_1 = 0.0132
new: DataOwner2的最优x_2 = 0.0146
new: DataOwner3的最优x_3 = 0.0149
new: DataOwner4的最优x_4 = 0.0134
new: DataOwner5的最优x_5 = 0.0148
new: DataOwner6的最优x_6 = 0.0130
new: DataOwner7的最优x_7 = 0.0130
new: DataOwner8的最优x_8 = 0.0131
new: DataOwner9的最优x_9 = 0.0133
new: DataOwner10的最优x_10 = 0.0241
eta:0.16
new: DataOwner1的最优x_1 = 0.0128
new: DataOwner2的最优x_2 = 0.0141
new: DataOwner3的最优x_3 = 0.0144
new: DataOwner4的最优x_4 = 0.0130
new: DataOwner5的最优x_5 = 0.0143
new: DataOwner6的最优x_6 = 0.0126
new: DataOwner7的最优x_7 = 0.0126
new: DataOwner8的最优x_8 = 0.0127
new: DataOwner9的最优x_9 = 0.0129
new: DataOwner10的最优x_10 = 0.0234
eta:0.17
new: DataOwner1的最优x_1 = 0.0164
new: DataOwner2的最优x_2 = 0.0182
new: DataOwner3的最优x_3 = 0.0185
new: DataOwner4的最优x_4 = 0.0167
new: DataOwner5的最优x_5 = 0.0184
new: DataOwner6的最优x_6 = 0.0162
new: DataOwner7的最优x_7 = 0.0162
new: DataOwner8的最优x_8 = 0.0163
new: DataOwner9的最优x_9 = 0.0166
new: DataOwner10的最优x_10 = 0.0301
eta:0.18000000000000002
new: DataOwner1的最优x_1 = 0.0143
new: DataOwner2的最优x_2 = 0.0159
new: DataOwner3的最优x_3 = 0.0162
new: DataOwner4的最优x_4 = 0.0146
new: DataOwner5的最优x_5 = 0.0161
new: DataOwner6的最优x_6 = 0.0142
new: DataOwner7的最优x_7 = 0.0142
new: DataOwner8的最优x_8 = 0.0143
new: DataOwner9的最优x_9 = 0.0145
new: DataOwner10的最优x_10 = 0.0263
eta:0.19
new: DataOwner1的最优x_1 = 0.0183
new: DataOwner2的最优x_2 = 0.0203
new: DataOwner3的最优x_3 = 0.0207
new: DataOwner4的最优x_4 = 0.0187
new: DataOwner5的最优x_5 = 0.0206
new: DataOwner6的最优x_6 = 0.0181
new: DataOwner7的最优x_7 = 0.0181
new: DataOwner8的最优x_8 = 0.0183
new: DataOwner9的最优x_9 = 0.0185
new: DataOwner10的最优x_10 = 0.0336
eta:0.2
new: DataOwner1的最优x_1 = 0.0193
new: DataOwner2的最优x_2 = 0.0214
new: DataOwner3的最优x_3 = 0.0218
new: DataOwner4的最优x_4 = 0.0197
new: DataOwner5的最优x_5 = 0.0217
new: DataOwner6的最优x_6 = 0.0191
new: DataOwner7的最优x_7 = 0.0191
new: DataOwner8的最优x_8 = 0.0192
new: DataOwner9的最优x_9 = 0.0195
new: DataOwner10的最优x_10 = 0.0354
eta:0.21000000000000002
new: DataOwner1的最优x_1 = 0.0167
new: DataOwner2的最优x_2 = 0.0186
new: DataOwner3的最优x_3 = 0.0189
new: DataOwner4的最优x_4 = 0.0171
new: DataOwner5的最优x_5 = 0.0188
new: DataOwner6的最优x_6 = 0.0166
new: DataOwner7的最优x_7 = 0.0166
new: DataOwner8的最优x_8 = 0.0167
new: DataOwner9的最优x_9 = 0.0169
new: DataOwner10的最优x_10 = 0.0307
eta:0.22
new: DataOwner1的最优x_1 = 0.0193
new: DataOwner2的最优x_2 = 0.0214
new: DataOwner3的最优x_3 = 0.0218
new: DataOwner4的最优x_4 = 0.0197
new: DataOwner5的最优x_5 = 0.0217
new: DataOwner6的最优x_6 = 0.0191
new: DataOwner7的最优x_7 = 0.0191
new: DataOwner8的最优x_8 = 0.0192
new: DataOwner9的最优x_9 = 0.0195
new: DataOwner10的最优x_10 = 0.0354
eta:0.23
new: DataOwner1的最优x_1 = 0.0183
new: DataOwner2的最优x_2 = 0.0203
new: DataOwner3的最优x_3 = 0.0207
new: DataOwner4的最优x_4 = 0.0187
new: DataOwner5的最优x_5 = 0.0206
new: DataOwner6的最优x_6 = 0.0181
new: DataOwner7的最优x_7 = 0.0182
new: DataOwner8的最优x_8 = 0.0183
new: DataOwner9的最优x_9 = 0.0186
new: DataOwner10的最优x_10 = 0.0336
eta:0.24000000000000002
new: DataOwner1的最优x_1 = 0.0210
new: DataOwner2的最优x_2 = 0.0233
new: DataOwner3的最优x_3 = 0.0238
new: DataOwner4的最优x_4 = 0.0215
new: DataOwner5的最优x_5 = 0.0236
new: DataOwner6的最优x_6 = 0.0208
new: DataOwner7的最优x_7 = 0.0208
new: DataOwner8的最优x_8 = 0.0210
new: DataOwner9的最优x_9 = 0.0213
new: DataOwner10的最优x_10 = 0.0386
eta:0.25
new: DataOwner1的最优x_1 = 0.0199
new: DataOwner2的最优x_2 = 0.0221
new: DataOwner3的最优x_3 = 0.0225
new: DataOwner4的最优x_4 = 0.0203
new: DataOwner5的最优x_5 = 0.0224
new: DataOwner6的最优x_6 = 0.0197
new: DataOwner7的最优x_7 = 0.0197
new: DataOwner8的最优x_8 = 0.0199
new: DataOwner9的最优x_9 = 0.0202
new: DataOwner10的最优x_10 = 0.0365
eta:0.26
new: DataOwner1的最优x_1 = 0.0228
new: DataOwner2的最优x_2 = 0.0253
new: DataOwner3的最优x_3 = 0.0257
new: DataOwner4的最优x_4 = 0.0233
new: DataOwner5的最优x_5 = 0.0256
new: DataOwner6的最优x_6 = 0.0226
new: DataOwner7的最优x_7 = 0.0226
new: DataOwner8的最优x_8 = 0.0227
new: DataOwner9的最优x_9 = 0.0231
new: DataOwner10的最优x_10 = 0.0418
eta:0.27
new: DataOwner1的最优x_1 = 0.0215
new: DataOwner2的最优x_2 = 0.0239
new: DataOwner3的最优x_3 = 0.0243
new: DataOwner4的最优x_4 = 0.0220
new: DataOwner5的最优x_5 = 0.0242
new: DataOwner6的最优x_6 = 0.0213
new: DataOwner7的最优x_7 = 0.0213
new: DataOwner8的最优x_8 = 0.0214
new: DataOwner9的最优x_9 = 0.0218
new: DataOwner10的最优x_10 = 0.0394
eta:0.28
new: DataOwner1的最优x_1 = 0.0223
new: DataOwner2的最优x_2 = 0.0248
new: DataOwner3的最优x_3 = 0.0252
new: DataOwner4的最优x_4 = 0.0228
new: DataOwner5的最优x_5 = 0.0251
new: DataOwner6的最优x_6 = 0.0221
new: DataOwner7的最优x_7 = 0.0221
new: DataOwner8的最优x_8 = 0.0222
new: DataOwner9的最优x_9 = 0.0226
new: DataOwner10的最优x_10 = 0.0409
eta:0.29000000000000004
new: DataOwner1的最优x_1 = 0.0231
new: DataOwner2的最优x_2 = 0.0256
new: DataOwner3的最优x_3 = 0.0261
new: DataOwner4的最优x_4 = 0.0236
new: DataOwner5的最优x_5 = 0.0260
new: DataOwner6的最优x_6 = 0.0229
new: DataOwner7的最优x_7 = 0.0229
new: DataOwner8的最优x_8 = 0.0230
new: DataOwner9的最优x_9 = 0.0234
new: DataOwner10的最优x_10 = 0.0424
eta:0.3
new: DataOwner1的最优x_1 = 0.0239
new: DataOwner2的最优x_2 = 0.0265
new: DataOwner3的最优x_3 = 0.0270
new: DataOwner4的最优x_4 = 0.0244
new: DataOwner5的最优x_5 = 0.0268
new: DataOwner6的最优x_6 = 0.0237
new: DataOwner7的最优x_7 = 0.0237
new: DataOwner8的最优x_8 = 0.0238
new: DataOwner9的最优x_9 = 0.0242
new: DataOwner10的最优x_10 = 0.0438
eta:0.31
new: DataOwner1的最优x_1 = 0.0247
new: DataOwner2的最优x_2 = 0.0274
new: DataOwner3的最优x_3 = 0.0279
new: DataOwner4的最优x_4 = 0.0252
new: DataOwner5的最优x_5 = 0.0277
new: DataOwner6的最优x_6 = 0.0244
new: DataOwner7的最优x_7 = 0.0245
new: DataOwner8的最优x_8 = 0.0246
new: DataOwner9的最优x_9 = 0.0250
new: DataOwner10的最优x_10 = 0.0453
eta:0.32
new: DataOwner1的最优x_1 = 0.0255
new: DataOwner2的最优x_2 = 0.0283
new: DataOwner3的最优x_3 = 0.0288
new: DataOwner4的最优x_4 = 0.0260
new: DataOwner5的最优x_5 = 0.0286
new: DataOwner6的最优x_6 = 0.0252
new: DataOwner7的最优x_7 = 0.0253
new: DataOwner8的最优x_8 = 0.0254
new: DataOwner9的最优x_9 = 0.0258
new: DataOwner10的最优x_10 = 0.0468
eta:0.33
new: DataOwner1的最优x_1 = 0.0263
new: DataOwner2的最优x_2 = 0.0292
new: DataOwner3的最优x_3 = 0.0297
new: DataOwner4的最优x_4 = 0.0268
new: DataOwner5的最优x_5 = 0.0295
new: DataOwner6的最优x_6 = 0.0260
new: DataOwner7的最优x_7 = 0.0260
new: DataOwner8的最优x_8 = 0.0262
new: DataOwner9的最优x_9 = 0.0266
new: DataOwner10的最优x_10 = 0.0482
eta:0.34
new: DataOwner1的最优x_1 = 0.0271
new: DataOwner2的最优x_2 = 0.0301
new: DataOwner3的最优x_3 = 0.0306
new: DataOwner4的最优x_4 = 0.0277
new: DataOwner5的最优x_5 = 0.0304
new: DataOwner6的最优x_6 = 0.0268
new: DataOwner7的最优x_7 = 0.0268
new: DataOwner8的最优x_8 = 0.0270
new: DataOwner9的最优x_9 = 0.0274
new: DataOwner10的最优x_10 = 0.0497
eta:0.35000000000000003
new: DataOwner1的最优x_1 = 0.0279
new: DataOwner2的最优x_2 = 0.0309
new: DataOwner3的最优x_3 = 0.0315
new: DataOwner4的最优x_4 = 0.0285
new: DataOwner5的最优x_5 = 0.0313
new: DataOwner6的最优x_6 = 0.0276
new: DataOwner7的最优x_7 = 0.0276
new: DataOwner8的最优x_8 = 0.0278
new: DataOwner9的最优x_9 = 0.0282
new: DataOwner10的最优x_10 = 0.0511
eta:0.36000000000000004
new: DataOwner1的最优x_1 = 0.0287
new: DataOwner2的最优x_2 = 0.0318
new: DataOwner3的最优x_3 = 0.0324
new: DataOwner4的最优x_4 = 0.0293
new: DataOwner5的最优x_5 = 0.0322
new: DataOwner6的最优x_6 = 0.0284
new: DataOwner7的最优x_7 = 0.0284
new: DataOwner8的最优x_8 = 0.0286
new: DataOwner9的最优x_9 = 0.0290
new: DataOwner10的最优x_10 = 0.0526
eta:0.37
new: DataOwner1的最优x_1 = 0.0295
new: DataOwner2的最优x_2 = 0.0327
new: DataOwner3的最优x_3 = 0.0333
new: DataOwner4的最优x_4 = 0.0301
new: DataOwner5的最优x_5 = 0.0331
new: DataOwner6的最优x_6 = 0.0292
new: DataOwner7的最优x_7 = 0.0292
new: DataOwner8的最优x_8 = 0.0294
new: DataOwner9的最优x_9 = 0.0298
new: DataOwner10的最优x_10 = 0.0541
eta:0.38
new: DataOwner1的最优x_1 = 0.0303
new: DataOwner2的最优x_2 = 0.0336
new: DataOwner3的最优x_3 = 0.0342
new: DataOwner4的最优x_4 = 0.0309
new: DataOwner5的最优x_5 = 0.0340
new: DataOwner6的最优x_6 = 0.0300
new: DataOwner7的最优x_7 = 0.0300
new: DataOwner8的最优x_8 = 0.0302
new: DataOwner9的最优x_9 = 0.0307
new: DataOwner10的最优x_10 = 0.0555
eta:0.39
new: DataOwner1的最优x_1 = 0.0311
new: DataOwner2的最优x_2 = 0.0345
new: DataOwner3的最优x_3 = 0.0351
new: DataOwner4的最优x_4 = 0.0317
new: DataOwner5的最优x_5 = 0.0349
new: DataOwner6的最优x_6 = 0.0308
new: DataOwner7的最优x_7 = 0.0308
new: DataOwner8的最优x_8 = 0.0310
new: DataOwner9的最优x_9 = 0.0315
new: DataOwner10的最优x_10 = 0.0570
eta:0.4
new: DataOwner1的最优x_1 = 0.0351
new: DataOwner2的最优x_2 = 0.0389
new: DataOwner3的最优x_3 = 0.0396
new: DataOwner4的最优x_4 = 0.0358
new: DataOwner5的最优x_5 = 0.0394
new: DataOwner6的最优x_6 = 0.0347
new: DataOwner7的最优x_7 = 0.0347
new: DataOwner8的最优x_8 = 0.0349
new: DataOwner9的最优x_9 = 0.0355
new: DataOwner10的最优x_10 = 0.0643
eta:0.41000000000000003
new: DataOwner1的最优x_1 = 0.0327
new: DataOwner2的最优x_2 = 0.0363
new: DataOwner3的最优x_3 = 0.0369
new: DataOwner4的最优x_4 = 0.0334
new: DataOwner5的最优x_5 = 0.0367
new: DataOwner6的最优x_6 = 0.0323
new: DataOwner7的最优x_7 = 0.0324
new: DataOwner8的最优x_8 = 0.0326
new: DataOwner9的最优x_9 = 0.0331
new: DataOwner10的最优x_10 = 0.0599
eta:0.42000000000000004
new: DataOwner1的最优x_1 = 0.0405
new: DataOwner2的最优x_2 = 0.0449
new: DataOwner3的最优x_3 = 0.0458
new: DataOwner4的最优x_4 = 0.0413
new: DataOwner5的最优x_5 = 0.0455
new: DataOwner6的最优x_6 = 0.0401
new: DataOwner7的最优x_7 = 0.0401
new: DataOwner8的最优x_8 = 0.0404
new: DataOwner9的最优x_9 = 0.0410
new: DataOwner10的最优x_10 = 0.0743
eta:0.43
new: DataOwner1的最优x_1 = 0.0343
new: DataOwner2的最优x_2 = 0.0380
new: DataOwner3的最优x_3 = 0.0387
new: DataOwner4的最优x_4 = 0.0350
new: DataOwner5的最优x_5 = 0.0385
new: DataOwner6的最优x_6 = 0.0339
new: DataOwner7的最优x_7 = 0.0339
new: DataOwner8的最优x_8 = 0.0341
new: DataOwner9的最优x_9 = 0.0347
new: DataOwner10的最优x_10 = 0.0628
eta:0.44
new: DataOwner1的最优x_1 = 0.0351
new: DataOwner2的最优x_2 = 0.0389
new: DataOwner3的最优x_3 = 0.0396
new: DataOwner4的最优x_4 = 0.0358
new: DataOwner5的最优x_5 = 0.0394
new: DataOwner6的最优x_6 = 0.0347
new: DataOwner7的最优x_7 = 0.0347
new: DataOwner8的最优x_8 = 0.0349
new: DataOwner9的最优x_9 = 0.0355
new: DataOwner10的最优x_10 = 0.0643
eta:0.45
new: DataOwner1的最优x_1 = 0.0359
new: DataOwner2的最优x_2 = 0.0398
new: DataOwner3的最优x_3 = 0.0405
new: DataOwner4的最优x_4 = 0.0366
new: DataOwner5的最优x_5 = 0.0403
new: DataOwner6的最优x_6 = 0.0355
new: DataOwner7的最优x_7 = 0.0355
new: DataOwner8的最优x_8 = 0.0357
new: DataOwner9的最优x_9 = 0.0363
new: DataOwner10的最优x_10 = 0.0657
eta:0.46
new: DataOwner1的最优x_1 = 0.0367
new: DataOwner2的最优x_2 = 0.0407
new: DataOwner3的最优x_3 = 0.0414
new: DataOwner4的最优x_4 = 0.0374
new: DataOwner5的最优x_5 = 0.0412
new: DataOwner6的最优x_6 = 0.0363
new: DataOwner7的最优x_7 = 0.0363
new: DataOwner8的最优x_8 = 0.0365
new: DataOwner9的最优x_9 = 0.0371
new: DataOwner10的最优x_10 = 0.0672
eta:0.47000000000000003
new: DataOwner1的最优x_1 = 0.0375
new: DataOwner2的最优x_2 = 0.0416
new: DataOwner3的最优x_3 = 0.0423
new: DataOwner4的最优x_4 = 0.0382
new: DataOwner5的最优x_5 = 0.0421
new: DataOwner6的最优x_6 = 0.0371
new: DataOwner7的最优x_7 = 0.0371
new: DataOwner8的最优x_8 = 0.0373
new: DataOwner9的最优x_9 = 0.0379
new: DataOwner10的最优x_10 = 0.0687
eta:0.48000000000000004
new: DataOwner1的最优x_1 = 0.0383
new: DataOwner2的最优x_2 = 0.0424
new: DataOwner3的最优x_3 = 0.0432
new: DataOwner4的最优x_4 = 0.0391
new: DataOwner5的最优x_5 = 0.0430
new: DataOwner6的最优x_6 = 0.0379
new: DataOwner7的最优x_7 = 0.0379
new: DataOwner8的最优x_8 = 0.0381
new: DataOwner9的最优x_9 = 0.0387
new: DataOwner10的最优x_10 = 0.0701
eta:0.49
new: DataOwner1的最优x_1 = 0.0391
new: DataOwner2的最优x_2 = 0.0433
new: DataOwner3的最优x_3 = 0.0441
new: DataOwner4的最优x_4 = 0.0399
new: DataOwner5的最优x_5 = 0.0439
new: DataOwner6的最优x_6 = 0.0386
new: DataOwner7的最优x_7 = 0.0387
new: DataOwner8的最优x_8 = 0.0389
new: DataOwner9的最优x_9 = 0.0395
new: DataOwner10的最优x_10 = 0.0716
eta:0.5
new: DataOwner1的最优x_1 = 0.0530
new: DataOwner2的最优x_2 = 0.0588
new: DataOwner3的最优x_3 = 0.0599
new: DataOwner4的最优x_4 = 0.0541
new: DataOwner5的最优x_5 = 0.0596
new: DataOwner6的最优x_6 = 0.0525
new: DataOwner7的最优x_7 = 0.0525
new: DataOwner8的最优x_8 = 0.0529
new: DataOwner9的最优x_9 = 0.0537
new: DataOwner10的最优x_10 = 0.0972
eta:0.51
new: DataOwner1的最优x_1 = 0.0407
new: DataOwner2的最优x_2 = 0.0451
new: DataOwner3的最优x_3 = 0.0459
new: DataOwner4的最优x_4 = 0.0415
new: DataOwner5的最优x_5 = 0.0456
new: DataOwner6的最优x_6 = 0.0402
new: DataOwner7的最优x_7 = 0.0403
new: DataOwner8的最优x_8 = 0.0405
new: DataOwner9的最优x_9 = 0.0411
new: DataOwner10的最优x_10 = 0.0745
eta:0.52
new: DataOwner1的最优x_1 = 0.0456
new: DataOwner2的最优x_2 = 0.0506
new: DataOwner3的最优x_3 = 0.0515
new: DataOwner4的最优x_4 = 0.0465
new: DataOwner5的最优x_5 = 0.0512
new: DataOwner6的最优x_6 = 0.0451
new: DataOwner7的最优x_7 = 0.0452
new: DataOwner8的最优x_8 = 0.0454
new: DataOwner9的最优x_9 = 0.0461
new: DataOwner10的最优x_10 = 0.0836
eta:0.53
new: DataOwner1的最优x_1 = 0.0465
new: DataOwner2的最优x_2 = 0.0516
new: DataOwner3的最优x_3 = 0.0525
new: DataOwner4的最优x_4 = 0.0474
new: DataOwner5的最优x_5 = 0.0522
new: DataOwner6的最优x_6 = 0.0460
new: DataOwner7的最优x_7 = 0.0460
new: DataOwner8的最优x_8 = 0.0463
new: DataOwner9的最优x_9 = 0.0470
new: DataOwner10的最优x_10 = 0.0852
eta:0.54
new: DataOwner1的最优x_1 = 0.0473
new: DataOwner2的最优x_2 = 0.0525
new: DataOwner3的最优x_3 = 0.0535
new: DataOwner4的最优x_4 = 0.0483
new: DataOwner5的最优x_5 = 0.0532
new: DataOwner6的最优x_6 = 0.0468
new: DataOwner7的最优x_7 = 0.0469
new: DataOwner8的最优x_8 = 0.0472
new: DataOwner9的最优x_9 = 0.0479
new: DataOwner10的最优x_10 = 0.0868
eta:0.55
new: DataOwner1的最优x_1 = 0.0530
new: DataOwner2的最优x_2 = 0.0588
new: DataOwner3的最优x_3 = 0.0599
new: DataOwner4的最优x_4 = 0.0541
new: DataOwner5的最优x_5 = 0.0596
new: DataOwner6的最优x_6 = 0.0525
new: DataOwner7的最优x_7 = 0.0525
new: DataOwner8的最优x_8 = 0.0529
new: DataOwner9的最优x_9 = 0.0537
new: DataOwner10的最优x_10 = 0.0972
eta:0.56
new: DataOwner1的最优x_1 = 0.0446
new: DataOwner2的最优x_2 = 0.0495
new: DataOwner3的最优x_3 = 0.0504
new: DataOwner4的最优x_4 = 0.0456
new: DataOwner5的最优x_5 = 0.0501
new: DataOwner6的最优x_6 = 0.0442
new: DataOwner7的最优x_7 = 0.0442
new: DataOwner8的最优x_8 = 0.0445
new: DataOwner9的最优x_9 = 0.0452
new: DataOwner10的最优x_10 = 0.0818
eta:0.5700000000000001
new: DataOwner1的最优x_1 = 0.0454
new: DataOwner2的最优x_2 = 0.0504
new: DataOwner3的最优x_3 = 0.0513
new: DataOwner4的最优x_4 = 0.0464
new: DataOwner5的最优x_5 = 0.0510
new: DataOwner6的最优x_6 = 0.0449
new: DataOwner7的最优x_7 = 0.0450
new: DataOwner8的最优x_8 = 0.0453
new: DataOwner9的最优x_9 = 0.0460
new: DataOwner10的最优x_10 = 0.0833
eta:0.5800000000000001
new: DataOwner1的最优x_1 = 0.0462
new: DataOwner2的最优x_2 = 0.0513
new: DataOwner3的最优x_3 = 0.0522
new: DataOwner4的最优x_4 = 0.0472
new: DataOwner5的最优x_5 = 0.0519
new: DataOwner6的最优x_6 = 0.0457
new: DataOwner7的最优x_7 = 0.0458
new: DataOwner8的最优x_8 = 0.0461
new: DataOwner9的最优x_9 = 0.0468
new: DataOwner10的最优x_10 = 0.0847
eta:0.59
new: DataOwner1的最优x_1 = 0.0470
new: DataOwner2的最优x_2 = 0.0522
new: DataOwner3的最优x_3 = 0.0531
new: DataOwner4的最优x_4 = 0.0480
new: DataOwner5的最优x_5 = 0.0528
new: DataOwner6的最优x_6 = 0.0465
new: DataOwner7的最优x_7 = 0.0466
new: DataOwner8的最优x_8 = 0.0469
new: DataOwner9的最优x_9 = 0.0476
new: DataOwner10的最优x_10 = 0.0862
eta:0.6
new: DataOwner1的最优x_1 = 0.0526
new: DataOwner2的最优x_2 = 0.0584
new: DataOwner3的最优x_3 = 0.0594
new: DataOwner4的最优x_4 = 0.0537
new: DataOwner5的最优x_5 = 0.0591
new: DataOwner6的最优x_6 = 0.0520
new: DataOwner7的最优x_7 = 0.0521
new: DataOwner8的最优x_8 = 0.0524
new: DataOwner9的最优x_9 = 0.0532
new: DataOwner10的最优x_10 = 0.0964
eta:0.61
new: DataOwner1的最优x_1 = 0.0486
new: DataOwner2的最优x_2 = 0.0539
new: DataOwner3的最优x_3 = 0.0549
new: DataOwner4的最优x_4 = 0.0496
new: DataOwner5的最优x_5 = 0.0546
new: DataOwner6的最优x_6 = 0.0481
new: DataOwner7的最优x_7 = 0.0482
new: DataOwner8的最优x_8 = 0.0484
new: DataOwner9的最优x_9 = 0.0492
new: DataOwner10的最优x_10 = 0.0891
eta:0.62
new: DataOwner1的最优x_1 = 0.0494
new: DataOwner2的最优x_2 = 0.0548
new: DataOwner3的最优x_3 = 0.0558
new: DataOwner4的最优x_4 = 0.0504
new: DataOwner5的最优x_5 = 0.0555
new: DataOwner6的最优x_6 = 0.0489
new: DataOwner7的最优x_7 = 0.0489
new: DataOwner8的最优x_8 = 0.0492
new: DataOwner9的最优x_9 = 0.0500
new: DataOwner10的最优x_10 = 0.0906
eta:0.63
new: DataOwner1的最优x_1 = 0.0502
new: DataOwner2的最优x_2 = 0.0557
new: DataOwner3的最优x_3 = 0.0567
new: DataOwner4的最优x_4 = 0.0513
new: DataOwner5的最优x_5 = 0.0564
new: DataOwner6的最优x_6 = 0.0497
new: DataOwner7的最优x_7 = 0.0497
new: DataOwner8的最优x_8 = 0.0500
new: DataOwner9的最优x_9 = 0.0508
new: DataOwner10的最优x_10 = 0.0920
eta:0.64
new: DataOwner1的最优x_1 = 0.0510
new: DataOwner2的最优x_2 = 0.0566
new: DataOwner3的最优x_3 = 0.0576
new: DataOwner4的最优x_4 = 0.0521
new: DataOwner5的最优x_5 = 0.0573
new: DataOwner6的最优x_6 = 0.0505
new: DataOwner7的最优x_7 = 0.0505
new: DataOwner8的最优x_8 = 0.0508
new: DataOwner9的最优x_9 = 0.0516
new: DataOwner10的最优x_10 = 0.0935
eta:0.65
new: DataOwner1的最优x_1 = 0.0518
new: DataOwner2的最优x_2 = 0.0575
new: DataOwner3的最优x_3 = 0.0585
new: DataOwner4的最优x_4 = 0.0529
new: DataOwner5的最优x_5 = 0.0582
new: DataOwner6的最优x_6 = 0.0513
new: DataOwner7的最优x_7 = 0.0513
new: DataOwner8的最优x_8 = 0.0516
new: DataOwner9的最优x_9 = 0.0524
new: DataOwner10的最优x_10 = 0.0950
eta:0.66
new: DataOwner1的最优x_1 = 0.0526
new: DataOwner2的最优x_2 = 0.0584
new: DataOwner3的最优x_3 = 0.0594
new: DataOwner4的最优x_4 = 0.0537
new: DataOwner5的最优x_5 = 0.0591
new: DataOwner6的最优x_6 = 0.0520
new: DataOwner7的最优x_7 = 0.0521
new: DataOwner8的最优x_8 = 0.0524
new: DataOwner9的最优x_9 = 0.0532
new: DataOwner10的最优x_10 = 0.0964
eta:0.67
new: DataOwner1的最优x_1 = 0.0534
new: DataOwner2的最优x_2 = 0.0592
new: DataOwner3的最优x_3 = 0.0603
new: DataOwner4的最优x_4 = 0.0545
new: DataOwner5的最优x_5 = 0.0600
new: DataOwner6的最优x_6 = 0.0528
new: DataOwner7的最优x_7 = 0.0529
new: DataOwner8的最优x_8 = 0.0532
new: DataOwner9的最优x_9 = 0.0540
new: DataOwner10的最优x_10 = 0.0979
eta:0.68
new: DataOwner1的最优x_1 = 0.0542
new: DataOwner2的最优x_2 = 0.0601
new: DataOwner3的最优x_3 = 0.0612
new: DataOwner4的最优x_4 = 0.0553
new: DataOwner5的最优x_5 = 0.0609
new: DataOwner6的最优x_6 = 0.0536
new: DataOwner7的最优x_7 = 0.0537
new: DataOwner8的最优x_8 = 0.0540
new: DataOwner9的最优x_9 = 0.0549
new: DataOwner10的最优x_10 = 0.0994
eta:0.6900000000000001
new: DataOwner1的最优x_1 = 0.0550
new: DataOwner2的最优x_2 = 0.0610
new: DataOwner3的最优x_3 = 0.0621
new: DataOwner4的最优x_4 = 0.0561
new: DataOwner5的最优x_5 = 0.0618
new: DataOwner6的最优x_6 = 0.0544
new: DataOwner7的最优x_7 = 0.0545
new: DataOwner8的最优x_8 = 0.0548
new: DataOwner9的最优x_9 = 0.0557
new: DataOwner10的最优x_10 = 0.1008
eta:0.7000000000000001
new: DataOwner1的最优x_1 = 0.0558
new: DataOwner2的最优x_2 = 0.0619
new: DataOwner3的最优x_3 = 0.0630
new: DataOwner4的最优x_4 = 0.0570
new: DataOwner5的最优x_5 = 0.0626
new: DataOwner6的最优x_6 = 0.0552
new: DataOwner7的最优x_7 = 0.0553
new: DataOwner8的最优x_8 = 0.0556
new: DataOwner9的最优x_9 = 0.0565
new: DataOwner10的最优x_10 = 0.1023
eta:0.7100000000000001
new: DataOwner1的最优x_1 = 0.0566
new: DataOwner2的最优x_2 = 0.0628
new: DataOwner3的最优x_3 = 0.0639
new: DataOwner4的最优x_4 = 0.0578
new: DataOwner5的最优x_5 = 0.0635
new: DataOwner6的最优x_6 = 0.0560
new: DataOwner7的最优x_7 = 0.0560
new: DataOwner8的最优x_8 = 0.0564
new: DataOwner9的最优x_9 = 0.0573
new: DataOwner10的最优x_10 = 0.1037
eta:0.72
new: DataOwner1的最优x_1 = 0.0574
new: DataOwner2的最优x_2 = 0.0637
new: DataOwner3的最优x_3 = 0.0648
new: DataOwner4的最优x_4 = 0.0586
new: DataOwner5的最优x_5 = 0.0644
new: DataOwner6的最优x_6 = 0.0568
new: DataOwner7的最优x_7 = 0.0568
new: DataOwner8的最优x_8 = 0.0572
new: DataOwner9的最优x_9 = 0.0581
new: DataOwner10的最优x_10 = 0.1052
eta:0.73
new: DataOwner1的最优x_1 = 0.0582
new: DataOwner2的最优x_2 = 0.0646
new: DataOwner3的最优x_3 = 0.0657
new: DataOwner4的最优x_4 = 0.0594
new: DataOwner5的最优x_5 = 0.0653
new: DataOwner6的最优x_6 = 0.0576
new: DataOwner7的最优x_7 = 0.0576
new: DataOwner8的最优x_8 = 0.0580
new: DataOwner9的最优x_9 = 0.0589
new: DataOwner10的最优x_10 = 0.1067
eta:0.74
new: DataOwner1的最优x_1 = 0.0590
new: DataOwner2的最优x_2 = 0.0654
new: DataOwner3的最优x_3 = 0.0666
new: DataOwner4的最优x_4 = 0.0602
new: DataOwner5的最优x_5 = 0.0662
new: DataOwner6的最优x_6 = 0.0584
new: DataOwner7的最优x_7 = 0.0584
new: DataOwner8的最优x_8 = 0.0588
new: DataOwner9的最优x_9 = 0.0597
new: DataOwner10的最优x_10 = 0.1081
eta:0.75
new: DataOwner1的最优x_1 = 0.0598
new: DataOwner2的最优x_2 = 0.0663
new: DataOwner3的最优x_3 = 0.0675
new: DataOwner4的最优x_4 = 0.0610
new: DataOwner5的最优x_5 = 0.0671
new: DataOwner6的最优x_6 = 0.0591
new: DataOwner7的最优x_7 = 0.0592
new: DataOwner8的最优x_8 = 0.0596
new: DataOwner9的最优x_9 = 0.0605
new: DataOwner10的最优x_10 = 0.1096
eta:0.76
new: DataOwner1的最优x_1 = 0.0606
new: DataOwner2的最优x_2 = 0.0672
new: DataOwner3的最优x_3 = 0.0684
new: DataOwner4的最优x_4 = 0.0618
new: DataOwner5的最优x_5 = 0.0680
new: DataOwner6的最优x_6 = 0.0599
new: DataOwner7的最优x_7 = 0.0600
new: DataOwner8的最优x_8 = 0.0604
new: DataOwner9的最优x_9 = 0.0613
new: DataOwner10的最优x_10 = 0.1110
eta:0.77
new: DataOwner1的最优x_1 = 0.0614
new: DataOwner2的最优x_2 = 0.0681
new: DataOwner3的最优x_3 = 0.0693
new: DataOwner4的最优x_4 = 0.0626
new: DataOwner5的最优x_5 = 0.0689
new: DataOwner6的最优x_6 = 0.0607
new: DataOwner7的最优x_7 = 0.0608
new: DataOwner8的最优x_8 = 0.0611
new: DataOwner9的最优x_9 = 0.0621
new: DataOwner10的最优x_10 = 0.1125
eta:0.78
new: DataOwner1的最优x_1 = 0.0622
new: DataOwner2的最优x_2 = 0.0690
new: DataOwner3的最优x_3 = 0.0702
new: DataOwner4的最优x_4 = 0.0635
new: DataOwner5的最优x_5 = 0.0698
new: DataOwner6的最优x_6 = 0.0615
new: DataOwner7的最优x_7 = 0.0616
new: DataOwner8的最优x_8 = 0.0619
new: DataOwner9的最优x_9 = 0.0629
new: DataOwner10的最优x_10 = 0.1140
eta:0.79
new: DataOwner1的最优x_1 = 0.0630
new: DataOwner2的最优x_2 = 0.0699
new: DataOwner3的最优x_3 = 0.0711
new: DataOwner4的最优x_4 = 0.0643
new: DataOwner5的最优x_5 = 0.0707
new: DataOwner6的最优x_6 = 0.0623
new: DataOwner7的最优x_7 = 0.0624
new: DataOwner8的最优x_8 = 0.0627
new: DataOwner9的最优x_9 = 0.0637
new: DataOwner10的最优x_10 = 0.1154
eta:0.8
new: DataOwner1的最优x_1 = 0.0638
new: DataOwner2的最优x_2 = 0.0707
new: DataOwner3的最优x_3 = 0.0720
new: DataOwner4的最优x_4 = 0.0651
new: DataOwner5的最优x_5 = 0.0716
new: DataOwner6的最优x_6 = 0.0631
new: DataOwner7的最优x_7 = 0.0631
new: DataOwner8的最优x_8 = 0.0635
new: DataOwner9的最优x_9 = 0.0645
new: DataOwner10的最优x_10 = 0.1169
eta:0.81
new: DataOwner1的最优x_1 = 0.0646
new: DataOwner2的最优x_2 = 0.0716
new: DataOwner3的最优x_3 = 0.0729
new: DataOwner4的最优x_4 = 0.0659
new: DataOwner5的最优x_5 = 0.0725
new: DataOwner6的最优x_6 = 0.0639
new: DataOwner7的最优x_7 = 0.0639
new: DataOwner8的最优x_8 = 0.0643
new: DataOwner9的最优x_9 = 0.0653
new: DataOwner10的最优x_10 = 0.1183
eta:0.8200000000000001
new: DataOwner1的最优x_1 = 0.0654
new: DataOwner2的最优x_2 = 0.0725
new: DataOwner3的最优x_3 = 0.0738
new: DataOwner4的最优x_4 = 0.0667
new: DataOwner5的最优x_5 = 0.0734
new: DataOwner6的最优x_6 = 0.0647
new: DataOwner7的最优x_7 = 0.0647
new: DataOwner8的最优x_8 = 0.0651
new: DataOwner9的最优x_9 = 0.0661
new: DataOwner10的最优x_10 = 0.1198
eta:0.8300000000000001
new: DataOwner1的最优x_1 = 0.0662
new: DataOwner2的最优x_2 = 0.0734
new: DataOwner3的最优x_3 = 0.0747
new: DataOwner4的最优x_4 = 0.0675
new: DataOwner5的最优x_5 = 0.0743
new: DataOwner6的最优x_6 = 0.0655
new: DataOwner7的最优x_7 = 0.0655
new: DataOwner8的最优x_8 = 0.0659
new: DataOwner9的最优x_9 = 0.0670
new: DataOwner10的最优x_10 = 0.1213
eta:0.8400000000000001
new: DataOwner1的最优x_1 = 0.0670
new: DataOwner2的最优x_2 = 0.0743
new: DataOwner3的最优x_3 = 0.0756
new: DataOwner4的最优x_4 = 0.0683
new: DataOwner5的最优x_5 = 0.0752
new: DataOwner6的最优x_6 = 0.0662
new: DataOwner7的最优x_7 = 0.0663
new: DataOwner8的最优x_8 = 0.0667
new: DataOwner9的最优x_9 = 0.0678
new: DataOwner10的最优x_10 = 0.1227
eta:0.85
new: DataOwner1的最优x_1 = 0.0678
new: DataOwner2的最优x_2 = 0.0752
new: DataOwner3的最优x_3 = 0.0765
new: DataOwner4的最优x_4 = 0.0692
new: DataOwner5的最优x_5 = 0.0761
new: DataOwner6的最优x_6 = 0.0670
new: DataOwner7的最优x_7 = 0.0671
new: DataOwner8的最优x_8 = 0.0675
new: DataOwner9的最优x_9 = 0.0686
new: DataOwner10的最优x_10 = 0.1242
eta:0.86
new: DataOwner1的最优x_1 = 0.0686
new: DataOwner2的最优x_2 = 0.0760
new: DataOwner3的最优x_3 = 0.0774
new: DataOwner4的最优x_4 = 0.0700
new: DataOwner5的最优x_5 = 0.0770
new: DataOwner6的最优x_6 = 0.0678
new: DataOwner7的最优x_7 = 0.0679
new: DataOwner8的最优x_8 = 0.0683
new: DataOwner9的最优x_9 = 0.0694
new: DataOwner10的最优x_10 = 0.1257
eta:0.87
new: DataOwner1的最优x_1 = 0.0694
new: DataOwner2的最优x_2 = 0.0769
new: DataOwner3的最优x_3 = 0.0783
new: DataOwner4的最优x_4 = 0.0708
new: DataOwner5的最优x_5 = 0.0779
new: DataOwner6的最优x_6 = 0.0686
new: DataOwner7的最优x_7 = 0.0687
new: DataOwner8的最优x_8 = 0.0691
new: DataOwner9的最优x_9 = 0.0702
new: DataOwner10的最优x_10 = 0.1271
eta:0.88
new: DataOwner1的最优x_1 = 0.0701
new: DataOwner2的最优x_2 = 0.0778
new: DataOwner3的最优x_3 = 0.0792
new: DataOwner4的最优x_4 = 0.0716
new: DataOwner5的最优x_5 = 0.0788
new: DataOwner6的最优x_6 = 0.0694
new: DataOwner7的最优x_7 = 0.0695
new: DataOwner8的最优x_8 = 0.0699
new: DataOwner9的最优x_9 = 0.0710
new: DataOwner10的最优x_10 = 0.1286
eta:0.89
new: DataOwner1的最优x_1 = 0.0709
new: DataOwner2的最优x_2 = 0.0787
new: DataOwner3的最优x_3 = 0.0801
new: DataOwner4的最优x_4 = 0.0724
new: DataOwner5的最优x_5 = 0.0797
new: DataOwner6的最优x_6 = 0.0702
new: DataOwner7的最优x_7 = 0.0703
new: DataOwner8的最优x_8 = 0.0707
new: DataOwner9的最优x_9 = 0.0718
new: DataOwner10的最优x_10 = 0.1300
eta:0.9
new: DataOwner1的最优x_1 = 0.0717
new: DataOwner2的最优x_2 = 0.0796
new: DataOwner3的最优x_3 = 0.0810
new: DataOwner4的最优x_4 = 0.0732
new: DataOwner5的最优x_5 = 0.0805
new: DataOwner6的最优x_6 = 0.0710
new: DataOwner7的最优x_7 = 0.0710
new: DataOwner8的最优x_8 = 0.0715
new: DataOwner9的最优x_9 = 0.0726
new: DataOwner10的最优x_10 = 0.1315
eta:0.91
new: DataOwner1的最优x_1 = 0.0725
new: DataOwner2的最优x_2 = 0.0805
new: DataOwner3的最优x_3 = 0.0819
new: DataOwner4的最优x_4 = 0.0740
new: DataOwner5的最优x_5 = 0.0814
new: DataOwner6的最优x_6 = 0.0718
new: DataOwner7的最优x_7 = 0.0718
new: DataOwner8的最优x_8 = 0.0723
new: DataOwner9的最优x_9 = 0.0734
new: DataOwner10的最优x_10 = 0.1330
eta:0.92
new: DataOwner1的最优x_1 = 0.0733
new: DataOwner2的最优x_2 = 0.0814
new: DataOwner3的最优x_3 = 0.0828
new: DataOwner4的最优x_4 = 0.0749
new: DataOwner5的最优x_5 = 0.0823
new: DataOwner6的最优x_6 = 0.0725
new: DataOwner7的最优x_7 = 0.0726
new: DataOwner8的最优x_8 = 0.0731
new: DataOwner9的最优x_9 = 0.0742
new: DataOwner10的最优x_10 = 0.1344
eta:0.93
new: DataOwner1的最优x_1 = 0.0741
new: DataOwner2的最优x_2 = 0.0822
new: DataOwner3的最优x_3 = 0.0837
new: DataOwner4的最优x_4 = 0.0757
new: DataOwner5的最优x_5 = 0.0832
new: DataOwner6的最优x_6 = 0.0733
new: DataOwner7的最优x_7 = 0.0734
new: DataOwner8的最优x_8 = 0.0739
new: DataOwner9的最优x_9 = 0.0750
new: DataOwner10的最优x_10 = 0.1359
eta:0.9400000000000001
new: DataOwner1的最优x_1 = 0.0749
new: DataOwner2的最优x_2 = 0.0831
new: DataOwner3的最优x_3 = 0.0846
new: DataOwner4的最优x_4 = 0.0765
new: DataOwner5的最优x_5 = 0.0841
new: DataOwner6的最优x_6 = 0.0741
new: DataOwner7的最优x_7 = 0.0742
new: DataOwner8的最优x_8 = 0.0747
new: DataOwner9的最优x_9 = 0.0758
new: DataOwner10的最优x_10 = 0.1373
eta:0.9500000000000001
new: DataOwner1的最优x_1 = 0.0757
new: DataOwner2的最优x_2 = 0.0840
new: DataOwner3的最优x_3 = 0.0855
new: DataOwner4的最优x_4 = 0.0773
new: DataOwner5的最优x_5 = 0.0850
new: DataOwner6的最优x_6 = 0.0749
new: DataOwner7的最优x_7 = 0.0750
new: DataOwner8的最优x_8 = 0.0754
new: DataOwner9的最优x_9 = 0.0766
new: DataOwner10的最优x_10 = 0.1388
eta:0.9600000000000001
new: DataOwner1的最优x_1 = 0.0765
new: DataOwner2的最优x_2 = 0.0849
new: DataOwner3的最优x_3 = 0.0864
new: DataOwner4的最优x_4 = 0.0781
new: DataOwner5的最优x_5 = 0.0859
new: DataOwner6的最优x_6 = 0.0757
new: DataOwner7的最优x_7 = 0.0758
new: DataOwner8的最优x_8 = 0.0762
new: DataOwner9的最优x_9 = 0.0774
new: DataOwner10的最优x_10 = 0.1403
eta:0.97
new: DataOwner1的最优x_1 = 0.0773
new: DataOwner2的最优x_2 = 0.0858
new: DataOwner3的最优x_3 = 0.0873
new: DataOwner4的最优x_4 = 0.0789
new: DataOwner5的最优x_5 = 0.0868
new: DataOwner6的最优x_6 = 0.0765
new: DataOwner7的最优x_7 = 0.0766
new: DataOwner8的最优x_8 = 0.0770
new: DataOwner9的最优x_9 = 0.0782
new: DataOwner10的最优x_10 = 0.1417
eta:0.98
new: DataOwner1的最优x_1 = 0.0781
new: DataOwner2的最优x_2 = 0.0867
new: DataOwner3的最优x_3 = 0.0882
new: DataOwner4的最优x_4 = 0.0797
new: DataOwner5的最优x_5 = 0.0877
new: DataOwner6的最优x_6 = 0.0773
new: DataOwner7的最优x_7 = 0.0774
new: DataOwner8的最优x_8 = 0.0778
new: DataOwner9的最优x_9 = 0.0791
new: DataOwner10的最优x_10 = 0.1432
eta:0.99
new: DataOwner1的最优x_1 = 0.0789
new: DataOwner2的最优x_2 = 0.0875
new: DataOwner3的最优x_3 = 0.0891
new: DataOwner4的最优x_4 = 0.0805
new: DataOwner5的最优x_5 = 0.0886
new: DataOwner6的最优x_6 = 0.0781
new: DataOwner7的最优x_7 = 0.0781
new: DataOwner8的最优x_8 = 0.0786
new: DataOwner9的最优x_9 = 0.0799
new: DataOwner10的最优x_10 = 0.1446
eta:1.0
new: DataOwner1的最优x_1 = 0.0797
new: DataOwner2的最优x_2 = 0.0884
new: DataOwner3的最优x_3 = 0.0900
new: DataOwner4的最优x_4 = 0.0814
new: DataOwner5的最优x_5 = 0.0895
new: DataOwner6的最优x_6 = 0.0789
new: DataOwner7的最优x_7 = 0.0789
new: DataOwner8的最优x_8 = 0.0794
new: DataOwner9的最优x_9 = 0.0807
new: DataOwner10的最优x_10 = 0.1461
DONE
----- literation 3: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.2839333904312875
DataOwner1的分配到的支付 ： 0.1088
DataOwner2的分配到的支付 ： 0.1289
DataOwner3的分配到的支付 ： 0.1322
DataOwner4的分配到的支付 ： 0.0983
DataOwner5的分配到的支付 ： 0.1094
DataOwner6的分配到的支付 ： 0.1136
DataOwner7的分配到的支付 ： 0.1428
DataOwner8的分配到的支付 ： 0.1093
DataOwner9的分配到的支付 ： 0.1450
DataOwner10的分配到的支付 ： 0.1955
DONE
----- literation 3: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC2
DataOwner3 把数据交给 CPC3
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC8
DataOwner9 把数据交给 CPC9
DataOwner10 把数据交给 CPC10
DONE
----- literation 3: 模型训练 -----
CPC1调整模型中, 本轮训练的数据量为：1837.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 75.88%
Epoch 1/5, Loss: 0.7682
Epoch 2/5, Loss: 0.7336
Epoch 3/5, Loss: 0.7038
Epoch 4/5, Loss: 0.6740
Epoch 5/5, Loss: 0.6487
新模型评估：
Accuracy: 76.99%
Model saved to ../../data/model/mnist_cnn_model
CPC2调整模型中, 本轮训练的数据量为：307.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 76.99%
Epoch 1/5, Loss: 0.6290
Epoch 2/5, Loss: 0.6237
Epoch 3/5, Loss: 0.6170
Epoch 4/5, Loss: 0.6156
Epoch 5/5, Loss: 0.5982
新模型评估：
Accuracy: 78.18%
Model saved to ../../data/model/mnist_cnn_model
CPC3调整模型中, 本轮训练的数据量为：314.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.18%
Epoch 1/5, Loss: 0.6428
Epoch 2/5, Loss: 0.6269
Epoch 3/5, Loss: 0.6149
Epoch 4/5, Loss: 0.6080
Epoch 5/5, Loss: 0.6023
新模型评估：
Accuracy: 75.74%
CPC4调整模型中, 本轮训练的数据量为：354.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.18%
Epoch 1/5, Loss: 0.6019
Epoch 2/5, Loss: 0.5859
Epoch 3/5, Loss: 0.5875
Epoch 4/5, Loss: 0.5759
Epoch 5/5, Loss: 0.5716
新模型评估：
Accuracy: 76.37%
CPC5调整模型中, 本轮训练的数据量为：910.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.18%
Epoch 1/5, Loss: 0.6485
Epoch 2/5, Loss: 0.6260
Epoch 3/5, Loss: 0.6168
Epoch 4/5, Loss: 0.5924
Epoch 5/5, Loss: 0.5780
新模型评估：
Accuracy: 77.05%
CPC6调整模型中, 本轮训练的数据量为：822.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.18%
Epoch 1/5, Loss: 0.6450
Epoch 2/5, Loss: 0.6277
Epoch 3/5, Loss: 0.6185
Epoch 4/5, Loss: 0.6093
Epoch 5/5, Loss: 0.5973
新模型评估：
Accuracy: 76.53%
CPC7调整模型中, 本轮训练的数据量为：689.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.18%
Epoch 1/5, Loss: 0.6397
Epoch 2/5, Loss: 0.6286
Epoch 3/5, Loss: 0.6109
Epoch 4/5, Loss: 0.6020
Epoch 5/5, Loss: 0.5970
新模型评估：
Accuracy: 76.07%
CPC8调整模型中, 本轮训练的数据量为：1581.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.18%
Epoch 1/5, Loss: 0.6127
Epoch 2/5, Loss: 0.5881
Epoch 3/5, Loss: 0.5679
Epoch 4/5, Loss: 0.5533
Epoch 5/5, Loss: 0.5354
新模型评估：
Accuracy: 76.01%
CPC9调整模型中, 本轮训练的数据量为：349.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.18%
Epoch 1/5, Loss: 0.6430
Epoch 2/5, Loss: 0.6043
Epoch 3/5, Loss: 0.5870
Epoch 4/5, Loss: 0.5974
Epoch 5/5, Loss: 0.5688
新模型评估：
Accuracy: 74.54%
CPC10调整模型中, 本轮训练的数据量为：212.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.18%
Epoch 1/5, Loss: 0.6673
Epoch 2/5, Loss: 0.5600
Epoch 3/5, Loss: 0.5867
Epoch 4/5, Loss: 0.5962
Epoch 5/5, Loss: 0.5745
新模型评估：
Accuracy: 76.77%
DONE
最终的列表：
[0.2965308180541614, 0.2941111185845482, 0.2913385068654928, 0.2818670779526195, 0.2534199943362924, 0.3332212849127076, 0.24818873946998898, 0.19559513084091534, 0.21800263806699263, 0.11648918283380015, 0.10648918283380014, 0.1885105996930746, 0.17410923818146434, 0.21642095578132658, 0.16808675799180056, 0.1490193483495381, 0.22069704941492194, 0.16509331821107945, 0.24289170202936605, 0.2537249405004975, 0.18824666302407717, 0.2337249405005086, 0.20306459096730764, 0.2511644189345281, 0.21740425688093978, 0.2680514743503707, 0.23127862182855063, 0.2380452238659514, 0.24470012703400357, 0.2512448097617808, 0.257680721318236, 0.2640092825751938, 0.2702318867434917, 0.27634990008698085, 0.2823646626126242, 0.28827748874956954, 0.29408966794513014, 0.29980246539398403, 0.3054171225403406, 0.37206016804674646, 0.3163568667590112, 0.4529515830079368, 0.3269183800450551, 0.3320601680467489, 0.3371107983906784, 0.34207136209365635, 0.3469429307320206, 0.3517265568047016, 0.3564232743246104, 0.591209092859339, 0.3655600293716357, 0.443784474349711, 0.4491171140816069, 0.4543562907997343, 0.5412090928593389, 0.3869500080877195, 0.39098659039930805, 0.39494480227022866, 0.39882551397498267, 0.4838906449471322, 0.40635784622313176, 0.41001113648700993, 0.4135902666483008, 0.41709603797945294, 0.4205292388394398, 0.42389064494713213, 0.4271810196485034, 0.43040111418158233, 0.43355166792788735, 0.43663340866130296, 0.4396470527282864, 0.44259330560601495, 0.44547286147438236, 0.44828640410070864, 0.4510346067307984, 0.4537181323591908, 0.45633763392781557, 0.4588937545977898, 0.46138712780173785, 0.46381837757928235, 0.46618811869441945, 0.46849695683536097, 0.47074548875452904, 0.47293430255066027, 0.4750639776797231, 0.47713508524937, 0.4791481881234857, 0.4811038410906442, 0.4830025910231891, 0.4848449770123816, 0.48663153053119823, 0.48836277556902796, 0.4900392287733045, 0.49166139958590793, 0.49322979037658554, 0.4947448965736475, 0.4962072067892074, 0.4976172029477155, 0.4989753604035203, 0.5002821480615223]
**** log-parameter_analysis 运行时间： 2025-01-16 22:26:05 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
初始化模型的准确率：
Accuracy: 26.82%
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.04151349600731256
DataOwner2: noise random: 0.046612983030370085
DataOwner3: noise random: 0.017411176263931672
DataOwner4: noise random: 0.03890017389637121
DataOwner5: noise random: 0.0497559243724908
DataOwner6: noise random: 0.035827308349533585
DataOwner7: noise random: 0.07952148308702177
DataOwner8: noise random: 0.05386855861964357
DataOwner9: noise random: 0.05328004969862751
DataOwner10: noise random: 0.09272632123950135
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9993034924313705, 0.9991162695788327, 0.9998775473031273, 0.9993880384901934, 0.998996536795742, 0.9994803941561424, 0.9974332535322054, 0.9988305012152284, 0.9988532625513901, 0.9965250591615107]
归一化后的数据质量列表avg_f_list: [0.98287675160933, 0.9772921575815783, 1.0, 0.9853986414789282, 0.9737206972800659, 0.988153480930932, 0.9270901590797793, 0.968768089739045, 0.9694470283422599, 0.9]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3502
DataOwner1的最优x_1 = 0.1375
DataOwner2的最优x_2 = 0.1322
DataOwner3的最优x_3 = 0.1530
DataOwner4的最优x_4 = 0.1399
DataOwner5的最优x_5 = 0.1288
DataOwner6的最优x_6 = 0.1424
DataOwner7的最优x_7 = 0.0784
DataOwner8的最优x_8 = 0.1239
DataOwner9的最优x_9 = 0.1246
DataOwner10的最优x_10 = 0.0439
每个DataOwner应该贡献数据比例 xn_list = [0.137534867745608, 0.13224468474016188, 0.15297362465600614, 0.1398817678393709, 0.12879302088048886, 0.14241606407869858, 0.07836296778214286, 0.12391614189817246, 0.12459098913363321, 0.04387605961272437]
ModelOwner的最大效用 U(Eta) = 0.5916
DONE
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.350225181027784
DataOwner1的分配到的支付 ： 0.1554
DataOwner2的分配到的支付 ： 0.1486
DataOwner3的分配到的支付 ： 0.1759
DataOwner4的分配到的支付 ： 0.1585
DataOwner5的分配到的支付 ： 0.1442
DataOwner6的分配到的支付 ： 0.1618
DataOwner7的分配到的支付 ： 0.0835
DataOwner8的分配到的支付 ： 0.1380
DataOwner9的分配到的支付 ： 0.1389
DataOwner10的分配到的支付 ： 0.0454
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC1', 'DataOwner2': 'CPC2', 'DataOwner3': 'CPC3', 'DataOwner4': 'CPC4', 'DataOwner5': 'CPC5', 'DataOwner6': 'CPC6', 'DataOwner7': 'CPC7', 'DataOwner8': 'CPC8', 'DataOwner9': 'CPC9', 'DataOwner10': 'CPC10'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC2
DataOwner3 把数据交给 CPC3
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC8
DataOwner9 把数据交给 CPC9
DataOwner10 把数据交给 CPC10
DONE
----- literation 1: 模型训练 -----
CPC1调整模型中, 本轮训练的数据量为：527.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 26.82%
Epoch 1/5, Loss: 2.2929
Epoch 2/5, Loss: 2.2877
Epoch 3/5, Loss: 2.2884
Epoch 4/5, Loss: 2.2836
Epoch 5/5, Loss: 2.2862
新模型评估：
Accuracy: 32.43%
Model saved to ../../data/model/mnist_cnn_model
CPC2调整模型中, 本轮训练的数据量为：168.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 32.43%
Epoch 1/5, Loss: 2.2873
Epoch 2/5, Loss: 2.2833
Epoch 3/5, Loss: 2.2782
Epoch 4/5, Loss: 2.2796
Epoch 5/5, Loss: 2.2780
新模型评估：
Accuracy: 33.06%
Model saved to ../../data/model/mnist_cnn_model
CPC3调整模型中, 本轮训练的数据量为：1952.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 33.06%
Epoch 1/5, Loss: 2.2721
Epoch 2/5, Loss: 2.2540
Epoch 3/5, Loss: 2.2265
Epoch 4/5, Loss: 2.1894
Epoch 5/5, Loss: 2.1357
新模型评估：
Accuracy: 50.57%
Model saved to ../../data/model/mnist_cnn_model
CPC4调整模型中, 本轮训练的数据量为：2142.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 50.57%
Epoch 1/5, Loss: 2.0872
Epoch 2/5, Loss: 2.0334
Epoch 3/5, Loss: 1.9603
Epoch 4/5, Loss: 1.8863
Epoch 5/5, Loss: 1.7998
新模型评估：
Accuracy: 60.41%
Model saved to ../../data/model/mnist_cnn_model
CPC5调整模型中, 本轮训练的数据量为：657.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 60.41%
Epoch 1/5, Loss: 1.7351
Epoch 2/5, Loss: 1.7072
Epoch 3/5, Loss: 1.6802
Epoch 4/5, Loss: 1.6538
Epoch 5/5, Loss: 1.6278
新模型评估：
Accuracy: 61.90%
Model saved to ../../data/model/mnist_cnn_model
CPC6调整模型中, 本轮训练的数据量为：545.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 61.90%
Epoch 1/5, Loss: 1.6304
Epoch 2/5, Loss: 1.6179
Epoch 3/5, Loss: 1.5827
Epoch 4/5, Loss: 1.5735
Epoch 5/5, Loss: 1.5396
新模型评估：
Accuracy: 66.08%
Model saved to ../../data/model/mnist_cnn_model
CPC7调整模型中, 本轮训练的数据量为：900.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 66.08%
Epoch 1/5, Loss: 1.5416
Epoch 2/5, Loss: 1.5035
Epoch 3/5, Loss: 1.4608
Epoch 4/5, Loss: 1.4679
Epoch 5/5, Loss: 1.4410
新模型评估：
Accuracy: 68.61%
Model saved to ../../data/model/mnist_cnn_model
CPC8调整模型中, 本轮训练的数据量为：158.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 68.61%
Epoch 1/5, Loss: 1.3626
Epoch 2/5, Loss: 1.3430
Epoch 3/5, Loss: 1.2929
Epoch 4/5, Loss: 1.3177
Epoch 5/5, Loss: 1.2947
新模型评估：
Accuracy: 68.12%
CPC9调整模型中, 本轮训练的数据量为：158.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 68.61%
Epoch 1/5, Loss: 1.3961
Epoch 2/5, Loss: 1.3819
Epoch 3/5, Loss: 1.3210
Epoch 4/5, Loss: 1.3636
Epoch 5/5, Loss: 1.3165
新模型评估：
Accuracy: 68.99%
Model saved to ../../data/model/mnist_cnn_model
CPC10调整模型中, 本轮训练的数据量为：168.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 68.99%
Epoch 1/5, Loss: 1.3605
Epoch 2/5, Loss: 1.3471
Epoch 3/5, Loss: 1.3327
Epoch 4/5, Loss: 1.3331
Epoch 5/5, Loss: 1.3271
新模型评估：
Accuracy: 68.73%
DONE
========================= literation: 2 =========================
----- literation 2: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3502
DataOwner1的最优x_1 = 0.1375
DataOwner2的最优x_2 = 0.1322
DataOwner3的最优x_3 = 0.1530
DataOwner4的最优x_4 = 0.1399
DataOwner5的最优x_5 = 0.1288
DataOwner6的最优x_6 = 0.1424
DataOwner7的最优x_7 = 0.0784
DataOwner8的最优x_8 = 0.1239
DataOwner9的最优x_9 = 0.1246
DataOwner10的最优x_10 = 0.0439
每个DataOwner应该贡献数据比例 xn_list = [0.137534867745608, 0.13224468474016188, 0.15297362465600614, 0.1398817678393709, 0.12879302088048886, 0.14241606407869858, 0.07836296778214286, 0.12391614189817246, 0.12459098913363321, 0.04387605961272437]
ModelOwner的最大效用 U(Eta) = 0.5916
DONE
----- literation 2: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.350225181027784
DataOwner1的分配到的支付 ： 0.1554
DataOwner2的分配到的支付 ： 0.1486
DataOwner3的分配到的支付 ： 0.1759
DataOwner4的分配到的支付 ： 0.1585
DataOwner5的分配到的支付 ： 0.1442
DataOwner6的分配到的支付 ： 0.1618
DataOwner7的分配到的支付 ： 0.0835
DataOwner8的分配到的支付 ： 0.1380
DataOwner9的分配到的支付 ： 0.1389
DataOwner10的分配到的支付 ： 0.0454
DONE
----- literation 2: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC2
DataOwner3 把数据交给 CPC3
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC8
DataOwner9 把数据交给 CPC9
DataOwner10 把数据交给 CPC10
DONE
----- literation 2: 模型训练 -----
重新调整fn，进而调整xn、Eta
正在评估DataOwner1的数据质量, 本轮评估的样本数据量为：527.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 68.99%
Epoch 1/5, Loss: 1.3326
Epoch 2/5, Loss: 1.2919
Epoch 3/5, Loss: 1.3019
Epoch 4/5, Loss: 1.2728
Epoch 5/5, Loss: 1.2489
新模型评估：
Accuracy: 70.96%
loss差为：
0.08372253841824007
单位数据loss差为：
0.00015886629680880469
正在评估DataOwner2的数据质量, 本轮评估的样本数据量为：168.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 68.99%
Epoch 1/5, Loss: 1.3102
Epoch 2/5, Loss: 1.3167
Epoch 3/5, Loss: 1.3092
Epoch 4/5, Loss: 1.2989
Epoch 5/5, Loss: 1.2653
新模型评估：
Accuracy: 68.16%
loss差为：
0.04494853814442945
单位数据loss差为：
0.00026755082228827054
正在评估DataOwner3的数据质量, 本轮评估的样本数据量为：1952.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 68.99%
Epoch 1/5, Loss: 1.2801
Epoch 2/5, Loss: 1.2070
Epoch 3/5, Loss: 1.1440
Epoch 4/5, Loss: 1.0710
Epoch 5/5, Loss: 1.0084
新模型评估：
Accuracy: 74.28%
loss差为：
0.2716945236729036
单位数据loss差为：
0.00013918776827505307
正在评估DataOwner4的数据质量, 本轮评估的样本数据量为：2142.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 68.99%
Epoch 1/5, Loss: 1.3128
Epoch 2/5, Loss: 1.2341
Epoch 3/5, Loss: 1.1610
Epoch 4/5, Loss: 1.0899
Epoch 5/5, Loss: 1.0240
新模型评估：
Accuracy: 75.17%
loss差为：
0.2887916354572071
单位数据loss差为：
0.00013482335922371946
正在评估DataOwner5的数据质量, 本轮评估的样本数据量为：657.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 68.99%
Epoch 1/5, Loss: 1.3092
Epoch 2/5, Loss: 1.2944
Epoch 3/5, Loss: 1.2571
Epoch 4/5, Loss: 1.2204
Epoch 5/5, Loss: 1.2227
新模型评估：
Accuracy: 70.99%
loss差为：
0.08650871840390284
单位数据loss差为：
0.00013167232633775166
正在评估DataOwner6的数据质量, 本轮评估的样本数据量为：545.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 68.99%
Epoch 1/5, Loss: 1.3121
Epoch 2/5, Loss: 1.2784
Epoch 3/5, Loss: 1.2663
Epoch 4/5, Loss: 1.2467
Epoch 5/5, Loss: 1.2238
新模型评估：
Accuracy: 69.86%
loss差为：
0.08823635843065047
单位数据loss差为：
0.00016190157510211097
正在评估DataOwner7的数据质量, 本轮评估的样本数据量为：900.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 68.99%
Epoch 1/5, Loss: 1.3363
Epoch 2/5, Loss: 1.2659
Epoch 3/5, Loss: 1.2441
Epoch 4/5, Loss: 1.2402
Epoch 5/5, Loss: 1.2246
新模型评估：
Accuracy: 70.95%
loss差为：
0.11170395215352391
单位数据loss差为：
0.00012411550239280433
正在评估DataOwner8的数据质量, 本轮评估的样本数据量为：158.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 68.99%
Epoch 1/5, Loss: 1.3612
Epoch 2/5, Loss: 1.3370
Epoch 3/5, Loss: 1.3670
Epoch 4/5, Loss: 1.3504
Epoch 5/5, Loss: 1.3066
新模型评估：
Accuracy: 69.92%
loss差为：
0.054616451263427734
单位数据loss差为：
0.00034567374217359326
正在评估DataOwner9的数据质量, 本轮评估的样本数据量为：158.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 68.99%
Epoch 1/5, Loss: 1.2993
Epoch 2/5, Loss: 1.3248
Epoch 3/5, Loss: 1.3409
Epoch 4/5, Loss: 1.2884
Epoch 5/5, Loss: 1.2854
新模型评估：
Accuracy: 69.04%
loss差为：
0.013891180356343513
单位数据loss差为：
8.791886301483236e-05
正在评估DataOwner10的数据质量, 本轮评估的样本数据量为：168.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 68.99%
Epoch 1/5, Loss: 1.3128
Epoch 2/5, Loss: 1.2965
Epoch 3/5, Loss: 1.2921
Epoch 4/5, Loss: 1.2750
Epoch 5/5, Loss: 1.2946
新模型评估：
Accuracy: 67.89%
loss差为：
0.018131693204243904
单位数据loss差为：
0.00010792674526335657
经过服务器调节后的真实数据质量：
数据质量列表avg_f_list: [0.00015886629680880469, 0.00026755082228827054, 0.00013918776827505307, 0.00013482335922371946, 0.00013167232633775166, 0.00016190157510211097, 0.00012411550239280433, 0.00034567374217359326, 8.791886301483236e-05, 0.00010792674526335657]
归一化后的数据质量列表avg_f_list:[0.9275251564686282, 0.9696910024980734, 0.9198905663503046, 0.9181973262201555, 0.916974834178006, 0.9287027397226145, 0.9140430472145116, 1.0, 0.9, 0.9077623679962236]
CPC1调整模型中, 本轮训练的数据量为：527.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 68.99%
Epoch 1/5, Loss: 1.3577
Epoch 2/5, Loss: 1.3192
Epoch 3/5, Loss: 1.2915
Epoch 4/5, Loss: 1.2610
Epoch 5/5, Loss: 1.2602
新模型评估：
Accuracy: 70.62%
Model saved to ../../data/model/mnist_cnn_model
CPC2调整模型中, 本轮训练的数据量为：168.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
**** log-parameter_analysis 运行时间： 2025-01-16 22:48:53 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
初始化模型的准确率：
Accuracy: 26.82%
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.04801723896813356
DataOwner2: noise random: 0.013828695794205382
DataOwner3: noise random: 0.006348097989480406
DataOwner4: noise random: 0.07557704579440738
DataOwner5: noise random: 0.024660978075070852
DataOwner6: noise random: 0.06827969205510441
DataOwner7: noise random: 0.03074177039705124
DataOwner8: noise random: 0.008345320849753524
DataOwner9: noise random: 0.057462311342606265
DataOwner10: noise random: 0.07192308523565658
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9990658090206056, 0.9999228321745915, 0.9999836761596198, 0.9976889748375081, 0.9997541291500119, 0.9981136563152806, 0.9996176509290464, 0.9999718053776145, 0.998663897434256, 0.997901697864157]
归一化后的数据质量列表avg_f_list: [0.9600005835108195, 0.9973485008945624, 1.0, 0.9, 0.9899966497863599, 0.9185070481147284, 0.9840491123159966, 0.9994826872721572, 0.9424858166661415, 0.9092701836443409]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3409
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1565
DataOwner3的最优x_3 = 0.1588
DataOwner4的最优x_4 = 0.0532
DataOwner5的最优x_5 = 0.1502
DataOwner6的最优x_6 = 0.0765
DataOwner7的最优x_7 = 0.1449
DataOwner8的最优x_8 = 0.1583
DataOwner9的最优x_9 = 0.1038
DataOwner10的最优x_10 = 0.0651
每个DataOwner应该贡献数据比例 xn_list = [0.12208055180731643, 0.156545163868101, 0.15878353465075998, 0.05320610334353326, 0.15020099597345887, 0.07646147022172775, 0.14491672602361286, 0.15834886257933178, 0.10384944730279615, 0.06509809931648429]
ModelOwner的最大效用 U(Eta) = 0.5807
DONE
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3409100954141866
DataOwner1的分配到的支付 ： 0.1358
DataOwner2的分配到的支付 ： 0.1810
DataOwner3的分配到的支付 ： 0.1840
DataOwner4的分配到的支付 ： 0.0555
DataOwner5的分配到的支付 ： 0.1724
DataOwner6的分配到的支付 ： 0.0814
DataOwner7的分配到的支付 ： 0.1653
DataOwner8的分配到的支付 ： 0.1834
DataOwner9的分配到的支付 ： 0.1134
DataOwner10的分配到的支付 ： 0.0686
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC1', 'DataOwner2': 'CPC2', 'DataOwner3': 'CPC3', 'DataOwner4': 'CPC4', 'DataOwner5': 'CPC5', 'DataOwner6': 'CPC6', 'DataOwner7': 'CPC7', 'DataOwner8': 'CPC10', 'DataOwner9': 'CPC8', 'DataOwner10': 'CPC9'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC2
DataOwner3 把数据交给 CPC3
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC10
DataOwner9 把数据交给 CPC8
DataOwner10 把数据交给 CPC9
DONE
----- literation 1: 模型训练 -----
CPC1调整模型中, 本轮训练的数据量为：575.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 26.82%
Epoch 1/5, Loss: 2.2912
Epoch 2/5, Loss: 2.2888
Epoch 3/5, Loss: 2.2860
Epoch 4/5, Loss: 2.2829
Epoch 5/5, Loss: 2.2791
新模型评估：
Accuracy: 33.84%
Model saved to ../../data/model/mnist_cnn_model
CPC2调整模型中, 本轮训练的数据量为：1473.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 33.84%
Epoch 1/5, Loss: 2.2735
Epoch 2/5, Loss: 2.2768
Epoch 3/5, Loss: 2.2422
Epoch 4/5, Loss: 2.2264
Epoch 5/5, Loss: 2.2093
新模型评估：
Accuracy: 45.02%
Model saved to ../../data/model/mnist_cnn_model
CPC3调整模型中, 本轮训练的数据量为：1494.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 45.02%
Epoch 1/5, Loss: 2.1851
Epoch 2/5, Loss: 2.1580
Epoch 3/5, Loss: 2.1277
Epoch 4/5, Loss: 2.0916
Epoch 5/5, Loss: 2.0425
新模型评估：
Accuracy: 51.12%
Model saved to ../../data/model/mnist_cnn_model
CPC4调整模型中, 本轮训练的数据量为：563.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 51.12%
Epoch 1/5, Loss: 2.0437
Epoch 2/5, Loss: 2.0247
Epoch 3/5, Loss: 2.0108
Epoch 4/5, Loss: 1.9913
Epoch 5/5, Loss: 1.9766
新模型评估：
Accuracy: 52.23%
Model saved to ../../data/model/mnist_cnn_model
CPC5调整模型中, 本轮训练的数据量为：176.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 52.23%
Epoch 1/5, Loss: 1.9645
Epoch 2/5, Loss: 1.9649
Epoch 3/5, Loss: 1.9519
Epoch 4/5, Loss: 1.9394
Epoch 5/5, Loss: 1.9302
新模型评估：
Accuracy: 53.30%
Model saved to ../../data/model/mnist_cnn_model
CPC6调整模型中, 本轮训练的数据量为：179.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 53.30%
Epoch 1/5, Loss: 1.9247
Epoch 2/5, Loss: 1.9131
Epoch 3/5, Loss: 1.9012
Epoch 4/5, Loss: 1.9007
Epoch 5/5, Loss: 1.8927
新模型评估：
Accuracy: 54.09%
Model saved to ../../data/model/mnist_cnn_model
CPC7调整模型中, 本轮训练的数据量为：1875.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 54.09%
Epoch 1/5, Loss: 1.8612
Epoch 2/5, Loss: 1.7958
Epoch 3/5, Loss: 1.7269
Epoch 4/5, Loss: 1.6486
Epoch 5/5, Loss: 1.5726
新模型评估：
Accuracy: 61.77%
Model saved to ../../data/model/mnist_cnn_model
CPC10调整模型中, 本轮训练的数据量为：372.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 61.77%
Epoch 1/5, Loss: 1.5527
Epoch 2/5, Loss: 1.5388
Epoch 3/5, Loss: 1.5216
Epoch 4/5, Loss: 1.5012
Epoch 5/5, Loss: 1.4857
新模型评估：
Accuracy: 63.91%
Model saved to ../../data/model/mnist_cnn_model
CPC8调整模型中, 本轮训练的数据量为：488.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 63.91%
Epoch 1/5, Loss: 1.5155
Epoch 2/5, Loss: 1.4852
Epoch 3/5, Loss: 1.4658
Epoch 4/5, Loss: 1.4453
Epoch 5/5, Loss: 1.4206
新模型评估：
Accuracy: 64.39%
Model saved to ../../data/model/mnist_cnn_model
CPC9调整模型中, 本轮训练的数据量为：153.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 64.39%
Epoch 1/5, Loss: 1.3611
Epoch 2/5, Loss: 1.3746
Epoch 3/5, Loss: 1.3459
Epoch 4/5, Loss: 1.3085
Epoch 5/5, Loss: 1.3196
新模型评估：
Accuracy: 65.63%
Model saved to ../../data/model/mnist_cnn_model
DONE
========================= literation: 2 =========================
----- literation 2: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3409
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1565
DataOwner3的最优x_3 = 0.1588
DataOwner4的最优x_4 = 0.0532
DataOwner5的最优x_5 = 0.1502
DataOwner6的最优x_6 = 0.0765
DataOwner7的最优x_7 = 0.1449
DataOwner8的最优x_8 = 0.1583
DataOwner9的最优x_9 = 0.1038
DataOwner10的最优x_10 = 0.0651
每个DataOwner应该贡献数据比例 xn_list = [0.12208055180731643, 0.156545163868101, 0.15878353465075998, 0.05320610334353326, 0.15020099597345887, 0.07646147022172775, 0.14491672602361286, 0.15834886257933178, 0.10384944730279615, 0.06509809931648429]
ModelOwner的最大效用 U(Eta) = 0.5807
DONE
----- literation 2: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3409100954141866
DataOwner1的分配到的支付 ： 0.1358
DataOwner2的分配到的支付 ： 0.1810
DataOwner3的分配到的支付 ： 0.1840
DataOwner4的分配到的支付 ： 0.0555
DataOwner5的分配到的支付 ： 0.1724
DataOwner6的分配到的支付 ： 0.0814
DataOwner7的分配到的支付 ： 0.1653
DataOwner8的分配到的支付 ： 0.1834
DataOwner9的分配到的支付 ： 0.1134
DataOwner10的分配到的支付 ： 0.0686
DONE
----- literation 2: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC2
DataOwner3 把数据交给 CPC3
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC10
DataOwner9 把数据交给 CPC8
DataOwner10 把数据交给 CPC9
DONE
----- literation 2: 模型训练 -----
重新调整fn，进而调整xn、Eta
正在评估DataOwner1的数据质量, 本轮评估的样本数据量为：575.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 65.63%
Epoch 1/5, Loss: 1.3276
Epoch 2/5, Loss: 1.3024
Epoch 3/5, Loss: 1.2802
Epoch 4/5, Loss: 1.2583
Epoch 5/5, Loss: 1.2370
新模型评估：
Accuracy: 68.55%
loss差为：
0.09060584174262143
单位数据loss差为：
0.00015757537694368945
正在评估DataOwner2的数据质量, 本轮评估的样本数据量为：1473.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 65.63%
Epoch 1/5, Loss: 1.3186
Epoch 2/5, Loss: 1.2970
Epoch 3/5, Loss: 1.2780
Epoch 4/5, Loss: 1.2066
Epoch 5/5, Loss: 1.1644
新模型评估：
Accuracy: 72.00%
loss差为：
0.15425249064962054
单位数据loss差为：
0.00010471995291895488
正在评估DataOwner3的数据质量, 本轮评估的样本数据量为：1494.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 65.63%
Epoch 1/5, Loss: 1.3278
Epoch 2/5, Loss: 1.2730
Epoch 3/5, Loss: 1.2125
Epoch 4/5, Loss: 1.1596
Epoch 5/5, Loss: 1.1159
新模型评估：
Accuracy: 71.10%
loss差为：
0.21189370254675555
单位数据loss差为：
0.00014182978751456194
正在评估DataOwner4的数据质量, 本轮评估的样本数据量为：563.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 65.63%
Epoch 1/5, Loss: 1.4256
Epoch 2/5, Loss: 1.4002
Epoch 3/5, Loss: 1.3748
Epoch 4/5, Loss: 1.3558
Epoch 5/5, Loss: 1.3358
新模型评估：
Accuracy: 67.90%
loss差为：
0.08988640043470597
单位数据loss差为：
0.00015965612865844755
正在评估DataOwner5的数据质量, 本轮评估的样本数据量为：176.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 65.63%
Epoch 1/5, Loss: 1.3800
Epoch 2/5, Loss: 1.3669
Epoch 3/5, Loss: 1.3529
Epoch 4/5, Loss: 1.3474
Epoch 5/5, Loss: 1.3455
新模型评估：
Accuracy: 67.83%
loss差为：
0.034468015034993416
单位数据loss差为：
0.00019584099451700805
正在评估DataOwner6的数据质量, 本轮评估的样本数据量为：179.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 65.63%
Epoch 1/5, Loss: 1.4484
Epoch 2/5, Loss: 1.4253
Epoch 3/5, Loss: 1.4166
Epoch 4/5, Loss: 1.4094
Epoch 5/5, Loss: 1.3986
新模型评估：
Accuracy: 66.09%
loss差为：
0.049703796704610115
单位数据loss差为：
0.00027767484192519617
正在评估DataOwner7的数据质量, 本轮评估的样本数据量为：1875.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 65.63%
Epoch 1/5, Loss: 1.3431
Epoch 2/5, Loss: 1.2712
Epoch 3/5, Loss: 1.2086
Epoch 4/5, Loss: 1.1468
Epoch 5/5, Loss: 1.0836
新模型评估：
Accuracy: 74.73%
loss差为：
0.25942529042561846
单位数据loss差为：
0.00013836015489366319
正在评估DataOwner8的数据质量, 本轮评估的样本数据量为：372.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 65.63%
Epoch 1/5, Loss: 1.3454
Epoch 2/5, Loss: 1.3270
Epoch 3/5, Loss: 1.3075
Epoch 4/5, Loss: 1.2851
Epoch 5/5, Loss: 1.2695
新模型评估：
Accuracy: 66.54%
loss差为：
0.07583886384963989
单位数据loss差为：
0.00020386791357430078
正在评估DataOwner9的数据质量, 本轮评估的样本数据量为：488.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 65.63%
Epoch 1/5, Loss: 1.3955
Epoch 2/5, Loss: 1.3819
Epoch 3/5, Loss: 1.3611
Epoch 4/5, Loss: 1.3433
Epoch 5/5, Loss: 1.3209
新模型评估：
Accuracy: 68.35%
loss差为：
0.07459424436092377
单位数据loss差为：
0.00015285705811664705
正在评估DataOwner10的数据质量, 本轮评估的样本数据量为：153.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 65.63%
Epoch 1/5, Loss: 1.3561
Epoch 2/5, Loss: 1.3415
Epoch 3/5, Loss: 1.3783
Epoch 4/5, Loss: 1.3576
Epoch 5/5, Loss: 1.3417
新模型评估：
Accuracy: 66.91%
loss差为：
0.014415979385375977
单位数据loss差为：
9.422208748611749e-05
经过服务器调节后的真实数据质量：
数据质量列表avg_f_list: [0.00015757537694368945, 0.00010471995291895488, 0.00014182978751456194, 0.00015965612865844755, 0.00019584099451700805, 0.00027767484192519617, 0.00013836015489366319, 0.00020386791357430078, 0.00015285705811664705, 9.422208748611749e-05]
归一化后的数据质量列表avg_f_list:[0.9345338447772451, 0.9057223809285042, 0.9259509322571955, 0.9356680614430674, 0.955392412799469, 1.0, 0.9240596373395981, 0.9597678821576892, 0.9319618916651378, 0.9]
CPC1调整模型中, 本轮训练的数据量为：575.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 65.63%
Epoch 1/5, Loss: 1.3275
Epoch 2/5, Loss: 1.3034
Epoch 3/5, Loss: 1.2813
Epoch 4/5, Loss: 1.2591
Epoch 5/5, Loss: 1.2382
新模型评估：
Accuracy: 68.61%
Model saved to ../../data/model/mnist_cnn_model
CPC2调整模型中, 本轮训练的数据量为：1473.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 68.61%
Epoch 1/5, Loss: 1.2725
Epoch 2/5, Loss: 1.2163
Epoch 3/5, Loss: 1.1345
Epoch 4/5, Loss: 1.1070
Epoch 5/5, Loss: 1.0665
新模型评估：
Accuracy: 72.18%
Model saved to ../../data/model/mnist_cnn_model
CPC3调整模型中, 本轮训练的数据量为：1494.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 72.18%
Epoch 1/5, Loss: 1.0169
Epoch 2/5, Loss: 0.9693
Epoch 3/5, Loss: 0.9217
Epoch 4/5, Loss: 0.8875
Epoch 5/5, Loss: 0.8441
新模型评估：
Accuracy: 74.98%
Model saved to ../../data/model/mnist_cnn_model
CPC4调整模型中, 本轮训练的数据量为：563.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 74.98%
Epoch 1/5, Loss: 0.9194
Epoch 2/5, Loss: 0.8943
Epoch 3/5, Loss: 0.8804
Epoch 4/5, Loss: 0.8652
Epoch 5/5, Loss: 0.8497
新模型评估：
Accuracy: 74.57%
CPC5调整模型中, 本轮训练的数据量为：176.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 74.98%
Epoch 1/5, Loss: 0.8500
Epoch 2/5, Loss: 0.8329
Epoch 3/5, Loss: 0.8298
Epoch 4/5, Loss: 0.8097
Epoch 5/5, Loss: 0.8084
新模型评估：
Accuracy: 76.70%
Model saved to ../../data/model/mnist_cnn_model
CPC6调整模型中, 本轮训练的数据量为：179.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 76.70%
Epoch 1/5, Loss: 0.8739
Epoch 2/5, Loss: 0.8692
Epoch 3/5, Loss: 0.8583
Epoch 4/5, Loss: 0.8371
Epoch 5/5, Loss: 0.8379
新模型评估：
Accuracy: 74.93%
CPC7调整模型中, 本轮训练的数据量为：1875.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 76.70%
Epoch 1/5, Loss: 0.8129
Epoch 2/5, Loss: 0.7779
Epoch 3/5, Loss: 0.7379
Epoch 4/5, Loss: 0.7047
Epoch 5/5, Loss: 0.6765
新模型评估：
Accuracy: 77.85%
Model saved to ../../data/model/mnist_cnn_model
CPC10调整模型中, 本轮训练的数据量为：372.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.85%
Epoch 1/5, Loss: 0.6299
Epoch 2/5, Loss: 0.6095
Epoch 3/5, Loss: 0.6017
Epoch 4/5, Loss: 0.5937
Epoch 5/5, Loss: 0.5825
新模型评估：
Accuracy: 75.98%
CPC8调整模型中, 本轮训练的数据量为：488.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.85%
Epoch 1/5, Loss: 0.7327
Epoch 2/5, Loss: 0.7161
Epoch 3/5, Loss: 0.7134
Epoch 4/5, Loss: 0.6957
Epoch 5/5, Loss: 0.6907
新模型评估：
Accuracy: 77.38%
CPC9调整模型中, 本轮训练的数据量为：153.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.85%
Epoch 1/5, Loss: 0.6532
Epoch 2/5, Loss: 0.6175
Epoch 3/5, Loss: 0.5831
Epoch 4/5, Loss: 0.6010
Epoch 5/5, Loss: 0.5761
新模型评估：
Accuracy: 75.29%
DONE
========================= literation: 3 =========================
----- literation 3: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3136
DataOwner1的最优x_1 = 0.1162
DataOwner2的最优x_2 = 0.0848
DataOwner3的最优x_3 = 0.1073
DataOwner4的最优x_4 = 0.1174
DataOwner5的最优x_5 = 0.1365
DataOwner6的最优x_6 = 0.1740
DataOwner7的最优x_7 = 0.1053
DataOwner8的最优x_8 = 0.1405
DataOwner9的最优x_9 = 0.1136
DataOwner10的最优x_10 = 0.0781
每个DataOwner应该贡献数据比例 xn_list = [0.11620255377803676, 0.08482707488724157, 0.10728327525586422, 0.11735540341075558, 0.13648503595001474, 0.17396867979660227, 0.10527056275068633, 0.14050251885918308, 0.11356623983823431, 0.07808734539011171]
ModelOwner的最大效用 U(Eta) = 0.5498
Eta开始变化：
eta:0.01
new: DataOwner1的最优x_1 = 0.0128
new: DataOwner2的最优x_2 = 0.0093
new: DataOwner3的最优x_3 = 0.0118
new: DataOwner4的最优x_4 = 0.0129
new: DataOwner5的最优x_5 = 0.0150
new: DataOwner6的最优x_6 = 0.0191
new: DataOwner7的最优x_7 = 0.0116
new: DataOwner8的最优x_8 = 0.0154
new: DataOwner9的最优x_9 = 0.0125
new: DataOwner10的最优x_10 = 0.0086
eta:0.02
new: DataOwner1的最优x_1 = 0.0144
new: DataOwner2的最优x_2 = 0.0105
new: DataOwner3的最优x_3 = 0.0133
new: DataOwner4的最优x_4 = 0.0145
new: DataOwner5的最优x_5 = 0.0169
new: DataOwner6的最优x_6 = 0.0216
new: DataOwner7的最优x_7 = 0.0130
new: DataOwner8的最优x_8 = 0.0174
new: DataOwner9的最优x_9 = 0.0141
new: DataOwner10的最优x_10 = 0.0097
eta:0.03
new: DataOwner1的最优x_1 = 0.0122
new: DataOwner2的最优x_2 = 0.0089
new: DataOwner3的最优x_3 = 0.0113
new: DataOwner4的最优x_4 = 0.0123
new: DataOwner5的最优x_5 = 0.0143
new: DataOwner6的最优x_6 = 0.0183
new: DataOwner7的最优x_7 = 0.0110
new: DataOwner8的最优x_8 = 0.0147
new: DataOwner9的最优x_9 = 0.0119
new: DataOwner10的最优x_10 = 0.0082
eta:0.04
new: DataOwner1的最优x_1 = 0.0134
new: DataOwner2的最优x_2 = 0.0098
new: DataOwner3的最优x_3 = 0.0124
new: DataOwner4的最优x_4 = 0.0136
new: DataOwner5的最优x_5 = 0.0158
new: DataOwner6的最优x_6 = 0.0201
new: DataOwner7的最优x_7 = 0.0122
new: DataOwner8的最优x_8 = 0.0162
new: DataOwner9的最优x_9 = 0.0131
new: DataOwner10的最优x_10 = 0.0090
eta:0.05
new: DataOwner1的最优x_1 = 0.0139
new: DataOwner2的最优x_2 = 0.0101
new: DataOwner3的最优x_3 = 0.0128
new: DataOwner4的最优x_4 = 0.0140
new: DataOwner5的最优x_5 = 0.0163
new: DataOwner6的最优x_6 = 0.0208
new: DataOwner7的最优x_7 = 0.0126
new: DataOwner8的最优x_8 = 0.0168
new: DataOwner9的最优x_9 = 0.0136
new: DataOwner10的最优x_10 = 0.0093
eta:0.060000000000000005
new: DataOwner1的最优x_1 = 0.0125
new: DataOwner2的最优x_2 = 0.0091
new: DataOwner3的最优x_3 = 0.0116
new: DataOwner4的最优x_4 = 0.0126
new: DataOwner5的最优x_5 = 0.0147
new: DataOwner6的最优x_6 = 0.0187
new: DataOwner7的最优x_7 = 0.0113
new: DataOwner8的最优x_8 = 0.0151
new: DataOwner9的最优x_9 = 0.0122
new: DataOwner10的最优x_10 = 0.0084
eta:0.06999999999999999
new: DataOwner1的最优x_1 = 0.0161
new: DataOwner2的最优x_2 = 0.0117
new: DataOwner3的最优x_3 = 0.0148
new: DataOwner4的最优x_4 = 0.0162
new: DataOwner5的最优x_5 = 0.0189
new: DataOwner6的最优x_6 = 0.0240
new: DataOwner7的最优x_7 = 0.0146
new: DataOwner8的最优x_8 = 0.0194
new: DataOwner9的最优x_9 = 0.0157
new: DataOwner10的最优x_10 = 0.0108
eta:0.08
new: DataOwner1的最优x_1 = 0.0138
new: DataOwner2的最优x_2 = 0.0101
new: DataOwner3的最优x_3 = 0.0127
new: DataOwner4的最优x_4 = 0.0139
new: DataOwner5的最优x_5 = 0.0162
new: DataOwner6的最优x_6 = 0.0206
new: DataOwner7的最优x_7 = 0.0125
new: DataOwner8的最优x_8 = 0.0167
new: DataOwner9的最优x_9 = 0.0135
new: DataOwner10的最优x_10 = 0.0093
eta:0.09
new: DataOwner1的最优x_1 = 0.0141
new: DataOwner2的最优x_2 = 0.0103
new: DataOwner3的最优x_3 = 0.0130
new: DataOwner4的最优x_4 = 0.0142
new: DataOwner5的最优x_5 = 0.0166
new: DataOwner6的最优x_6 = 0.0211
new: DataOwner7的最优x_7 = 0.0128
new: DataOwner8的最优x_8 = 0.0171
new: DataOwner9的最优x_9 = 0.0138
new: DataOwner10的最优x_10 = 0.0095
eta:0.09999999999999999
new: DataOwner1的最优x_1 = 0.0097
new: DataOwner2的最优x_2 = 0.0071
new: DataOwner3的最优x_3 = 0.0090
new: DataOwner4的最优x_4 = 0.0098
new: DataOwner5的最优x_5 = 0.0114
new: DataOwner6的最优x_6 = 0.0146
new: DataOwner7的最优x_7 = 0.0088
new: DataOwner8的最优x_8 = 0.0118
new: DataOwner9的最优x_9 = 0.0095
new: DataOwner10的最优x_10 = 0.0065
eta:0.11
new: DataOwner1的最优x_1 = 0.0097
new: DataOwner2的最优x_2 = 0.0071
new: DataOwner3的最优x_3 = 0.0090
new: DataOwner4的最优x_4 = 0.0098
new: DataOwner5的最优x_5 = 0.0114
new: DataOwner6的最优x_6 = 0.0146
new: DataOwner7的最优x_7 = 0.0088
new: DataOwner8的最优x_8 = 0.0118
new: DataOwner9的最优x_9 = 0.0095
new: DataOwner10的最优x_10 = 0.0065
eta:0.12
new: DataOwner1的最优x_1 = 0.0141
new: DataOwner2的最优x_2 = 0.0103
new: DataOwner3的最优x_3 = 0.0130
new: DataOwner4的最优x_4 = 0.0143
new: DataOwner5的最优x_5 = 0.0166
new: DataOwner6的最优x_6 = 0.0212
new: DataOwner7的最优x_7 = 0.0128
new: DataOwner8的最优x_8 = 0.0171
new: DataOwner9的最优x_9 = 0.0138
new: DataOwner10的最优x_10 = 0.0095
eta:0.13
new: DataOwner1的最优x_1 = 0.0127
new: DataOwner2的最优x_2 = 0.0092
new: DataOwner3的最优x_3 = 0.0117
new: DataOwner4的最优x_4 = 0.0128
new: DataOwner5的最优x_5 = 0.0149
new: DataOwner6的最优x_6 = 0.0189
new: DataOwner7的最优x_7 = 0.0115
new: DataOwner8的最优x_8 = 0.0153
new: DataOwner9的最优x_9 = 0.0124
new: DataOwner10的最优x_10 = 0.0085
eta:0.14
new: DataOwner1的最优x_1 = 0.0136
new: DataOwner2的最优x_2 = 0.0099
new: DataOwner3的最优x_3 = 0.0126
new: DataOwner4的最优x_4 = 0.0138
new: DataOwner5的最优x_5 = 0.0160
new: DataOwner6的最优x_6 = 0.0204
new: DataOwner7的最优x_7 = 0.0123
new: DataOwner8的最优x_8 = 0.0165
new: DataOwner9的最优x_9 = 0.0133
new: DataOwner10的最优x_10 = 0.0092
eta:0.15000000000000002
new: DataOwner1的最优x_1 = 0.0161
new: DataOwner2的最优x_2 = 0.0117
new: DataOwner3的最优x_3 = 0.0148
new: DataOwner4的最优x_4 = 0.0162
new: DataOwner5的最优x_5 = 0.0189
new: DataOwner6的最优x_6 = 0.0240
new: DataOwner7的最优x_7 = 0.0145
new: DataOwner8的最优x_8 = 0.0194
new: DataOwner9的最优x_9 = 0.0157
new: DataOwner10的最优x_10 = 0.0108
eta:0.16
new: DataOwner1的最优x_1 = 0.0142
new: DataOwner2的最优x_2 = 0.0103
new: DataOwner3的最优x_3 = 0.0131
new: DataOwner4的最优x_4 = 0.0143
new: DataOwner5的最优x_5 = 0.0166
new: DataOwner6的最优x_6 = 0.0212
new: DataOwner7的最优x_7 = 0.0128
new: DataOwner8的最优x_8 = 0.0171
new: DataOwner9的最优x_9 = 0.0138
new: DataOwner10的最优x_10 = 0.0095
eta:0.17
new: DataOwner1的最优x_1 = 0.0165
new: DataOwner2的最优x_2 = 0.0121
new: DataOwner3的最优x_3 = 0.0153
new: DataOwner4的最优x_4 = 0.0167
new: DataOwner5的最优x_5 = 0.0194
new: DataOwner6的最优x_6 = 0.0248
new: DataOwner7的最优x_7 = 0.0150
new: DataOwner8的最优x_8 = 0.0200
new: DataOwner9的最优x_9 = 0.0162
new: DataOwner10的最优x_10 = 0.0111
eta:0.18000000000000002
new: DataOwner1的最优x_1 = 0.0193
new: DataOwner2的最优x_2 = 0.0141
new: DataOwner3的最优x_3 = 0.0178
new: DataOwner4的最优x_4 = 0.0195
new: DataOwner5的最优x_5 = 0.0226
new: DataOwner6的最优x_6 = 0.0288
new: DataOwner7的最优x_7 = 0.0175
new: DataOwner8的最优x_8 = 0.0233
new: DataOwner9的最优x_9 = 0.0188
new: DataOwner10的最优x_10 = 0.0129
eta:0.19
new: DataOwner1的最优x_1 = 0.0203
new: DataOwner2的最优x_2 = 0.0148
new: DataOwner3的最优x_3 = 0.0188
new: DataOwner4的最优x_4 = 0.0205
new: DataOwner5的最优x_5 = 0.0239
new: DataOwner6的最优x_6 = 0.0304
new: DataOwner7的最优x_7 = 0.0184
new: DataOwner8的最优x_8 = 0.0246
new: DataOwner9的最优x_9 = 0.0199
new: DataOwner10的最优x_10 = 0.0137
eta:0.2
new: DataOwner1的最优x_1 = 0.0195
new: DataOwner2的最优x_2 = 0.0142
new: DataOwner3的最优x_3 = 0.0180
new: DataOwner4的最优x_4 = 0.0197
new: DataOwner5的最优x_5 = 0.0229
new: DataOwner6的最优x_6 = 0.0291
new: DataOwner7的最优x_7 = 0.0176
new: DataOwner8的最优x_8 = 0.0235
new: DataOwner9的最优x_9 = 0.0190
new: DataOwner10的最优x_10 = 0.0131
eta:0.21000000000000002
new: DataOwner1的最优x_1 = 0.0186
new: DataOwner2的最优x_2 = 0.0136
new: DataOwner3的最优x_3 = 0.0172
new: DataOwner4的最优x_4 = 0.0188
new: DataOwner5的最优x_5 = 0.0218
new: DataOwner6的最优x_6 = 0.0278
new: DataOwner7的最优x_7 = 0.0168
new: DataOwner8的最优x_8 = 0.0225
new: DataOwner9的最优x_9 = 0.0182
new: DataOwner10的最优x_10 = 0.0125
eta:0.22
new: DataOwner1的最优x_1 = 0.0195
new: DataOwner2的最优x_2 = 0.0142
new: DataOwner3的最优x_3 = 0.0180
new: DataOwner4的最优x_4 = 0.0197
new: DataOwner5的最优x_5 = 0.0229
new: DataOwner6的最优x_6 = 0.0291
new: DataOwner7的最优x_7 = 0.0176
new: DataOwner8的最优x_8 = 0.0235
new: DataOwner9的最优x_9 = 0.0190
new: DataOwner10的最优x_10 = 0.0131
eta:0.23
new: DataOwner1的最优x_1 = 0.0203
new: DataOwner2的最优x_2 = 0.0149
new: DataOwner3的最优x_3 = 0.0188
new: DataOwner4的最优x_4 = 0.0205
new: DataOwner5的最优x_5 = 0.0239
new: DataOwner6的最优x_6 = 0.0305
new: DataOwner7的最优x_7 = 0.0184
new: DataOwner8的最优x_8 = 0.0246
new: DataOwner9的最优x_9 = 0.0199
new: DataOwner10的最优x_10 = 0.0137
eta:0.24000000000000002
new: DataOwner1的最优x_1 = 0.0212
new: DataOwner2的最优x_2 = 0.0155
new: DataOwner3的最优x_3 = 0.0196
new: DataOwner4的最优x_4 = 0.0214
new: DataOwner5的最优x_5 = 0.0249
new: DataOwner6的最优x_6 = 0.0318
new: DataOwner7的最优x_7 = 0.0192
new: DataOwner8的最优x_8 = 0.0257
new: DataOwner9的最优x_9 = 0.0207
new: DataOwner10的最优x_10 = 0.0143
eta:0.25
new: DataOwner1的最优x_1 = 0.0243
new: DataOwner2的最优x_2 = 0.0178
new: DataOwner3的最优x_3 = 0.0225
new: DataOwner4的最优x_4 = 0.0246
new: DataOwner5的最优x_5 = 0.0286
new: DataOwner6的最优x_6 = 0.0364
new: DataOwner7的最优x_7 = 0.0220
new: DataOwner8的最优x_8 = 0.0294
new: DataOwner9的最优x_9 = 0.0238
new: DataOwner10的最优x_10 = 0.0163
eta:0.26
new: DataOwner1的最优x_1 = 0.0230
new: DataOwner2的最优x_2 = 0.0168
new: DataOwner3的最优x_3 = 0.0212
new: DataOwner4的最优x_4 = 0.0232
new: DataOwner5的最优x_5 = 0.0270
new: DataOwner6的最优x_6 = 0.0344
new: DataOwner7的最优x_7 = 0.0208
new: DataOwner8的最优x_8 = 0.0278
new: DataOwner9的最优x_9 = 0.0225
new: DataOwner10的最优x_10 = 0.0155
eta:0.27
new: DataOwner1的最优x_1 = 0.0239
new: DataOwner2的最优x_2 = 0.0174
new: DataOwner3的最优x_3 = 0.0221
new: DataOwner4的最优x_4 = 0.0241
new: DataOwner5的最优x_5 = 0.0281
new: DataOwner6的最优x_6 = 0.0358
new: DataOwner7的最优x_7 = 0.0216
new: DataOwner8的最优x_8 = 0.0289
new: DataOwner9的最优x_9 = 0.0233
new: DataOwner10的最优x_10 = 0.0161
eta:0.28
new: DataOwner1的最优x_1 = 0.0248
new: DataOwner2的最优x_2 = 0.0181
new: DataOwner3的最优x_3 = 0.0229
new: DataOwner4的最优x_4 = 0.0250
new: DataOwner5的最优x_5 = 0.0291
new: DataOwner6的最优x_6 = 0.0371
new: DataOwner7的最优x_7 = 0.0224
new: DataOwner8的最优x_8 = 0.0299
new: DataOwner9的最优x_9 = 0.0242
new: DataOwner10的最优x_10 = 0.0166
eta:0.29000000000000004
new: DataOwner1的最优x_1 = 0.0257
new: DataOwner2的最优x_2 = 0.0187
new: DataOwner3的最优x_3 = 0.0237
new: DataOwner4的最优x_4 = 0.0259
new: DataOwner5的最优x_5 = 0.0301
new: DataOwner6的最优x_6 = 0.0384
new: DataOwner7的最优x_7 = 0.0232
new: DataOwner8的最优x_8 = 0.0310
new: DataOwner9的最优x_9 = 0.0251
new: DataOwner10的最优x_10 = 0.0172
eta:0.3
new: DataOwner1的最优x_1 = 0.0265
new: DataOwner2的最优x_2 = 0.0194
new: DataOwner3的最优x_3 = 0.0245
new: DataOwner4的最优x_4 = 0.0268
new: DataOwner5的最优x_5 = 0.0312
new: DataOwner6的最优x_6 = 0.0397
new: DataOwner7的最优x_7 = 0.0240
new: DataOwner8的最优x_8 = 0.0321
new: DataOwner9的最优x_9 = 0.0259
new: DataOwner10的最优x_10 = 0.0178
eta:0.31
new: DataOwner1的最优x_1 = 0.0274
new: DataOwner2的最优x_2 = 0.0200
new: DataOwner3的最优x_3 = 0.0253
new: DataOwner4的最优x_4 = 0.0277
new: DataOwner5的最优x_5 = 0.0322
new: DataOwner6的最优x_6 = 0.0411
new: DataOwner7的最优x_7 = 0.0248
new: DataOwner8的最优x_8 = 0.0332
new: DataOwner9的最优x_9 = 0.0268
new: DataOwner10的最优x_10 = 0.0184
eta:0.32
new: DataOwner1的最优x_1 = 0.0311
new: DataOwner2的最优x_2 = 0.0227
new: DataOwner3的最优x_3 = 0.0287
new: DataOwner4的最优x_4 = 0.0314
new: DataOwner5的最优x_5 = 0.0366
new: DataOwner6的最优x_6 = 0.0466
new: DataOwner7的最优x_7 = 0.0282
new: DataOwner8的最优x_8 = 0.0377
new: DataOwner9的最优x_9 = 0.0304
new: DataOwner10的最优x_10 = 0.0209
eta:0.33
new: DataOwner1的最优x_1 = 0.0292
new: DataOwner2的最优x_2 = 0.0213
new: DataOwner3的最优x_3 = 0.0270
new: DataOwner4的最优x_4 = 0.0295
new: DataOwner5的最优x_5 = 0.0343
new: DataOwner6的最优x_6 = 0.0437
new: DataOwner7的最优x_7 = 0.0264
new: DataOwner8的最优x_8 = 0.0353
new: DataOwner9的最优x_9 = 0.0285
new: DataOwner10的最优x_10 = 0.0196
eta:0.34
new: DataOwner1的最优x_1 = 0.0400
new: DataOwner2的最优x_2 = 0.0292
new: DataOwner3的最优x_3 = 0.0370
new: DataOwner4的最优x_4 = 0.0404
new: DataOwner5的最优x_5 = 0.0470
new: DataOwner6的最优x_6 = 0.0599
new: DataOwner7的最优x_7 = 0.0363
new: DataOwner8的最优x_8 = 0.0484
new: DataOwner9的最优x_9 = 0.0391
new: DataOwner10的最优x_10 = 0.0269
eta:0.35000000000000003
new: DataOwner1的最优x_1 = 0.0310
new: DataOwner2的最优x_2 = 0.0226
new: DataOwner3的最优x_3 = 0.0286
new: DataOwner4的最优x_4 = 0.0313
new: DataOwner5的最优x_5 = 0.0364
new: DataOwner6的最优x_6 = 0.0464
new: DataOwner7的最优x_7 = 0.0280
new: DataOwner8的最优x_8 = 0.0374
new: DataOwner9的最优x_9 = 0.0303
new: DataOwner10的最优x_10 = 0.0208
eta:0.36000000000000004
new: DataOwner1的最优x_1 = 0.0318
new: DataOwner2的最优x_2 = 0.0232
new: DataOwner3的最优x_3 = 0.0294
new: DataOwner4的最优x_4 = 0.0322
new: DataOwner5的最优x_5 = 0.0374
new: DataOwner6的最优x_6 = 0.0477
new: DataOwner7的最优x_7 = 0.0289
new: DataOwner8的最优x_8 = 0.0385
new: DataOwner9的最优x_9 = 0.0311
new: DataOwner10的最优x_10 = 0.0214
eta:0.37
new: DataOwner1的最优x_1 = 0.0327
new: DataOwner2的最优x_2 = 0.0239
new: DataOwner3的最优x_3 = 0.0302
new: DataOwner4的最优x_4 = 0.0331
new: DataOwner5的最优x_5 = 0.0384
new: DataOwner6的最优x_6 = 0.0490
new: DataOwner7的最优x_7 = 0.0297
new: DataOwner8的最优x_8 = 0.0396
new: DataOwner9的最优x_9 = 0.0320
new: DataOwner10的最优x_10 = 0.0220
eta:0.38
new: DataOwner1的最优x_1 = 0.0370
new: DataOwner2的最优x_2 = 0.0270
new: DataOwner3的最优x_3 = 0.0341
new: DataOwner4的最优x_4 = 0.0373
new: DataOwner5的最优x_5 = 0.0434
new: DataOwner6的最优x_6 = 0.0554
new: DataOwner7的最优x_7 = 0.0335
new: DataOwner8的最优x_8 = 0.0447
new: DataOwner9的最优x_9 = 0.0361
new: DataOwner10的最优x_10 = 0.0248
eta:0.39
new: DataOwner1的最优x_1 = 0.0345
new: DataOwner2的最优x_2 = 0.0252
new: DataOwner3的最优x_3 = 0.0319
new: DataOwner4的最优x_4 = 0.0348
new: DataOwner5的最优x_5 = 0.0405
new: DataOwner6的最优x_6 = 0.0517
new: DataOwner7的最优x_7 = 0.0313
new: DataOwner8的最优x_8 = 0.0417
new: DataOwner9的最优x_9 = 0.0337
new: DataOwner10的最优x_10 = 0.0232
eta:0.4
new: DataOwner1的最优x_1 = 0.0354
new: DataOwner2的最优x_2 = 0.0258
new: DataOwner3的最优x_3 = 0.0327
new: DataOwner4的最优x_4 = 0.0357
new: DataOwner5的最优x_5 = 0.0416
new: DataOwner6的最优x_6 = 0.0530
new: DataOwner7的最优x_7 = 0.0321
new: DataOwner8的最优x_8 = 0.0428
new: DataOwner9的最优x_9 = 0.0346
new: DataOwner10的最优x_10 = 0.0238
eta:0.41000000000000003
new: DataOwner1的最优x_1 = 0.0399
new: DataOwner2的最优x_2 = 0.0291
new: DataOwner3的最优x_3 = 0.0368
new: DataOwner4的最优x_4 = 0.0403
new: DataOwner5的最优x_5 = 0.0469
new: DataOwner6的最优x_6 = 0.0597
new: DataOwner7的最优x_7 = 0.0361
new: DataOwner8的最优x_8 = 0.0482
new: DataOwner9的最优x_9 = 0.0390
new: DataOwner10的最优x_10 = 0.0268
eta:0.42000000000000004
new: DataOwner1的最优x_1 = 0.0372
new: DataOwner2的最优x_2 = 0.0271
new: DataOwner3的最优x_3 = 0.0343
new: DataOwner4的最优x_4 = 0.0375
new: DataOwner5的最优x_5 = 0.0436
new: DataOwner6的最优x_6 = 0.0556
new: DataOwner7的最优x_7 = 0.0337
new: DataOwner8的最优x_8 = 0.0449
new: DataOwner9的最优x_9 = 0.0363
new: DataOwner10的最优x_10 = 0.0250
eta:0.43
new: DataOwner1的最优x_1 = 0.0380
new: DataOwner2的最优x_2 = 0.0278
new: DataOwner3的最优x_3 = 0.0351
new: DataOwner4的最优x_4 = 0.0384
new: DataOwner5的最优x_5 = 0.0447
new: DataOwner6的最优x_6 = 0.0569
new: DataOwner7的最优x_7 = 0.0345
new: DataOwner8的最优x_8 = 0.0460
new: DataOwner9的最优x_9 = 0.0372
new: DataOwner10的最优x_10 = 0.0256
eta:0.44
new: DataOwner1的最优x_1 = 0.0389
new: DataOwner2的最优x_2 = 0.0284
new: DataOwner3的最优x_3 = 0.0359
new: DataOwner4的最优x_4 = 0.0393
new: DataOwner5的最优x_5 = 0.0457
new: DataOwner6的最优x_6 = 0.0583
new: DataOwner7的最优x_7 = 0.0353
new: DataOwner8的最优x_8 = 0.0471
new: DataOwner9的最优x_9 = 0.0380
new: DataOwner10的最优x_10 = 0.0262
eta:0.45
new: DataOwner1的最优x_1 = 0.0398
new: DataOwner2的最优x_2 = 0.0291
new: DataOwner3的最优x_3 = 0.0368
new: DataOwner4的最优x_4 = 0.0402
new: DataOwner5的最优x_5 = 0.0468
new: DataOwner6的最优x_6 = 0.0596
new: DataOwner7的最优x_7 = 0.0361
new: DataOwner8的最优x_8 = 0.0481
new: DataOwner9的最优x_9 = 0.0389
new: DataOwner10的最优x_10 = 0.0268
eta:0.46
new: DataOwner1的最优x_1 = 0.0407
new: DataOwner2的最优x_2 = 0.0297
new: DataOwner3的最优x_3 = 0.0376
new: DataOwner4的最优x_4 = 0.0411
new: DataOwner5的最优x_5 = 0.0478
new: DataOwner6的最优x_6 = 0.0609
new: DataOwner7的最优x_7 = 0.0369
new: DataOwner8的最优x_8 = 0.0492
new: DataOwner9的最优x_9 = 0.0398
new: DataOwner10的最优x_10 = 0.0273
eta:0.47000000000000003
new: DataOwner1的最优x_1 = 0.0553
new: DataOwner2的最优x_2 = 0.0404
new: DataOwner3的最优x_3 = 0.0511
new: DataOwner4的最优x_4 = 0.0559
new: DataOwner5的最优x_5 = 0.0650
new: DataOwner6的最优x_6 = 0.0828
new: DataOwner7的最优x_7 = 0.0501
new: DataOwner8的最优x_8 = 0.0669
new: DataOwner9的最优x_9 = 0.0541
new: DataOwner10的最优x_10 = 0.0372
eta:0.48000000000000004
new: DataOwner1的最优x_1 = 0.0425
new: DataOwner2的最优x_2 = 0.0310
new: DataOwner3的最优x_3 = 0.0392
new: DataOwner4的最优x_4 = 0.0429
new: DataOwner5的最优x_5 = 0.0499
new: DataOwner6的最优x_6 = 0.0636
new: DataOwner7的最优x_7 = 0.0385
new: DataOwner8的最优x_8 = 0.0513
new: DataOwner9的最优x_9 = 0.0415
new: DataOwner10的最优x_10 = 0.0285
eta:0.49
new: DataOwner1的最优x_1 = 0.0477
new: DataOwner2的最优x_2 = 0.0348
new: DataOwner3的最优x_3 = 0.0440
new: DataOwner4的最优x_4 = 0.0482
new: DataOwner5的最优x_5 = 0.0560
new: DataOwner6的最优x_6 = 0.0714
new: DataOwner7的最优x_7 = 0.0432
new: DataOwner8的最优x_8 = 0.0577
new: DataOwner9的最优x_9 = 0.0466
new: DataOwner10的最优x_10 = 0.0320
eta:0.5
new: DataOwner1的最优x_1 = 0.0487
new: DataOwner2的最优x_2 = 0.0355
new: DataOwner3的最优x_3 = 0.0449
new: DataOwner4的最优x_4 = 0.0491
new: DataOwner5的最优x_5 = 0.0571
new: DataOwner6的最优x_6 = 0.0728
new: DataOwner7的最优x_7 = 0.0441
new: DataOwner8的最优x_8 = 0.0588
new: DataOwner9的最优x_9 = 0.0476
new: DataOwner10的最优x_10 = 0.0327
eta:0.51
new: DataOwner1的最优x_1 = 0.0496
new: DataOwner2的最优x_2 = 0.0362
new: DataOwner3的最优x_3 = 0.0458
new: DataOwner4的最优x_4 = 0.0501
new: DataOwner5的最优x_5 = 0.0583
new: DataOwner6的最优x_6 = 0.0743
new: DataOwner7的最优x_7 = 0.0450
new: DataOwner8的最优x_8 = 0.0600
new: DataOwner9的最优x_9 = 0.0485
new: DataOwner10的最优x_10 = 0.0333
eta:0.52
new: DataOwner1的最优x_1 = 0.0506
new: DataOwner2的最优x_2 = 0.0369
new: DataOwner3的最优x_3 = 0.0467
new: DataOwner4的最优x_4 = 0.0511
new: DataOwner5的最优x_5 = 0.0594
new: DataOwner6的最优x_6 = 0.0758
new: DataOwner7的最优x_7 = 0.0458
new: DataOwner8的最优x_8 = 0.0612
new: DataOwner9的最优x_9 = 0.0495
new: DataOwner10的最优x_10 = 0.0340
eta:0.53
new: DataOwner1的最优x_1 = 0.0516
new: DataOwner2的最优x_2 = 0.0376
new: DataOwner3的最优x_3 = 0.0476
new: DataOwner4的最优x_4 = 0.0521
new: DataOwner5的最优x_5 = 0.0606
new: DataOwner6的最优x_6 = 0.0772
new: DataOwner7的最优x_7 = 0.0467
new: DataOwner8的最优x_8 = 0.0624
new: DataOwner9的最优x_9 = 0.0504
new: DataOwner10的最优x_10 = 0.0347
eta:0.54
new: DataOwner1的最优x_1 = 0.0478
new: DataOwner2的最优x_2 = 0.0349
new: DataOwner3的最优x_3 = 0.0441
new: DataOwner4的最优x_4 = 0.0482
new: DataOwner5的最优x_5 = 0.0561
new: DataOwner6的最优x_6 = 0.0715
new: DataOwner7的最优x_7 = 0.0433
new: DataOwner8的最优x_8 = 0.0578
new: DataOwner9的最优x_9 = 0.0467
new: DataOwner10的最优x_10 = 0.0321
eta:0.55
new: DataOwner1的最优x_1 = 0.0487
new: DataOwner2的最优x_2 = 0.0355
new: DataOwner3的最优x_3 = 0.0449
new: DataOwner4的最优x_4 = 0.0491
new: DataOwner5的最优x_5 = 0.0571
new: DataOwner6的最优x_6 = 0.0728
new: DataOwner7的最优x_7 = 0.0441
new: DataOwner8的最优x_8 = 0.0588
new: DataOwner9的最优x_9 = 0.0476
new: DataOwner10的最优x_10 = 0.0327
eta:0.56
new: DataOwner1的最优x_1 = 0.0545
new: DataOwner2的最优x_2 = 0.0398
new: DataOwner3的最优x_3 = 0.0503
new: DataOwner4的最优x_4 = 0.0550
new: DataOwner5的最优x_5 = 0.0640
new: DataOwner6的最优x_6 = 0.0816
new: DataOwner7的最优x_7 = 0.0494
new: DataOwner8的最优x_8 = 0.0659
new: DataOwner9的最优x_9 = 0.0533
new: DataOwner10的最优x_10 = 0.0366
eta:0.5700000000000001
new: DataOwner1的最优x_1 = 0.0504
new: DataOwner2的最优x_2 = 0.0368
new: DataOwner3的最优x_3 = 0.0466
new: DataOwner4的最优x_4 = 0.0509
new: DataOwner5的最优x_5 = 0.0592
new: DataOwner6的最优x_6 = 0.0755
new: DataOwner7的最优x_7 = 0.0457
new: DataOwner8的最优x_8 = 0.0610
new: DataOwner9的最优x_9 = 0.0493
new: DataOwner10的最优x_10 = 0.0339
eta:0.5800000000000001
new: DataOwner1的最优x_1 = 0.0564
new: DataOwner2的最优x_2 = 0.0412
new: DataOwner3的最优x_3 = 0.0521
new: DataOwner4的最优x_4 = 0.0570
new: DataOwner5的最优x_5 = 0.0663
new: DataOwner6的最优x_6 = 0.0845
new: DataOwner7的最优x_7 = 0.0511
new: DataOwner8的最优x_8 = 0.0682
new: DataOwner9的最优x_9 = 0.0552
new: DataOwner10的最优x_10 = 0.0379
eta:0.59
new: DataOwner1的最优x_1 = 0.0522
new: DataOwner2的最优x_2 = 0.0381
new: DataOwner3的最优x_3 = 0.0482
new: DataOwner4的最优x_4 = 0.0527
new: DataOwner5的最优x_5 = 0.0613
new: DataOwner6的最优x_6 = 0.0781
new: DataOwner7的最优x_7 = 0.0473
new: DataOwner8的最优x_8 = 0.0631
new: DataOwner9的最优x_9 = 0.0510
new: DataOwner10的最优x_10 = 0.0351
eta:0.6
new: DataOwner1的最优x_1 = 0.0531
new: DataOwner2的最优x_2 = 0.0387
new: DataOwner3的最优x_3 = 0.0490
new: DataOwner4的最优x_4 = 0.0536
new: DataOwner5的最优x_5 = 0.0623
new: DataOwner6的最优x_6 = 0.0795
new: DataOwner7的最优x_7 = 0.0481
new: DataOwner8的最优x_8 = 0.0642
new: DataOwner9的最优x_9 = 0.0519
new: DataOwner10的最优x_10 = 0.0357
eta:0.61
new: DataOwner1的最优x_1 = 0.0653
new: DataOwner2的最优x_2 = 0.0477
new: DataOwner3的最优x_3 = 0.0603
new: DataOwner4的最优x_4 = 0.0659
new: DataOwner5的最优x_5 = 0.0767
new: DataOwner6的最优x_6 = 0.0978
new: DataOwner7的最优x_7 = 0.0592
new: DataOwner8的最优x_8 = 0.0789
new: DataOwner9的最优x_9 = 0.0638
new: DataOwner10的最优x_10 = 0.0439
eta:0.62
new: DataOwner1的最优x_1 = 0.0548
new: DataOwner2的最优x_2 = 0.0400
new: DataOwner3的最优x_3 = 0.0506
new: DataOwner4的最优x_4 = 0.0554
new: DataOwner5的最优x_5 = 0.0644
new: DataOwner6的最优x_6 = 0.0821
new: DataOwner7的最优x_7 = 0.0497
new: DataOwner8的最优x_8 = 0.0663
new: DataOwner9的最优x_9 = 0.0536
new: DataOwner10的最优x_10 = 0.0369
eta:0.63
new: DataOwner1的最优x_1 = 0.0557
new: DataOwner2的最优x_2 = 0.0407
new: DataOwner3的最优x_3 = 0.0515
new: DataOwner4的最优x_4 = 0.0563
new: DataOwner5的最优x_5 = 0.0655
new: DataOwner6的最优x_6 = 0.0834
new: DataOwner7的最优x_7 = 0.0505
new: DataOwner8的最优x_8 = 0.0674
new: DataOwner9的最优x_9 = 0.0545
new: DataOwner10的最优x_10 = 0.0375
eta:0.64
new: DataOwner1的最优x_1 = 0.0566
new: DataOwner2的最优x_2 = 0.0413
new: DataOwner3的最优x_3 = 0.0523
new: DataOwner4的最优x_4 = 0.0572
new: DataOwner5的最优x_5 = 0.0665
new: DataOwner6的最优x_6 = 0.0848
new: DataOwner7的最优x_7 = 0.0513
new: DataOwner8的最优x_8 = 0.0685
new: DataOwner9的最优x_9 = 0.0553
new: DataOwner10的最优x_10 = 0.0380
eta:0.65
new: DataOwner1的最优x_1 = 0.0575
new: DataOwner2的最优x_2 = 0.0420
new: DataOwner3的最优x_3 = 0.0531
new: DataOwner4的最优x_4 = 0.0581
new: DataOwner5的最优x_5 = 0.0675
new: DataOwner6的最优x_6 = 0.0861
new: DataOwner7的最优x_7 = 0.0521
new: DataOwner8的最优x_8 = 0.0695
new: DataOwner9的最优x_9 = 0.0562
new: DataOwner10的最优x_10 = 0.0386
eta:0.66
new: DataOwner1的最优x_1 = 0.0584
new: DataOwner2的最优x_2 = 0.0426
new: DataOwner3的最优x_3 = 0.0539
new: DataOwner4的最优x_4 = 0.0590
new: DataOwner5的最优x_5 = 0.0686
new: DataOwner6的最优x_6 = 0.0874
new: DataOwner7的最优x_7 = 0.0529
new: DataOwner8的最优x_8 = 0.0706
new: DataOwner9的最优x_9 = 0.0571
new: DataOwner10的最优x_10 = 0.0392
eta:0.67
new: DataOwner1的最优x_1 = 0.0652
new: DataOwner2的最优x_2 = 0.0476
new: DataOwner3的最优x_3 = 0.0602
new: DataOwner4的最优x_4 = 0.0658
new: DataOwner5的最优x_5 = 0.0766
new: DataOwner6的最优x_6 = 0.0976
new: DataOwner7的最优x_7 = 0.0591
new: DataOwner8的最优x_8 = 0.0788
new: DataOwner9的最优x_9 = 0.0637
new: DataOwner10的最优x_10 = 0.0438
eta:0.68
new: DataOwner1的最优x_1 = 0.0662
new: DataOwner2的最优x_2 = 0.0483
new: DataOwner3的最优x_3 = 0.0611
new: DataOwner4的最优x_4 = 0.0668
new: DataOwner5的最优x_5 = 0.0777
new: DataOwner6的最优x_6 = 0.0991
new: DataOwner7的最优x_7 = 0.0599
new: DataOwner8的最优x_8 = 0.0800
new: DataOwner9的最优x_9 = 0.0647
new: DataOwner10的最优x_10 = 0.0445
eta:0.6900000000000001
new: DataOwner1的最优x_1 = 0.0610
new: DataOwner2的最优x_2 = 0.0446
new: DataOwner3的最优x_3 = 0.0564
new: DataOwner4的最优x_4 = 0.0616
new: DataOwner5的最优x_5 = 0.0717
new: DataOwner6的最优x_6 = 0.0914
new: DataOwner7的最优x_7 = 0.0553
new: DataOwner8的最优x_8 = 0.0738
new: DataOwner9的最优x_9 = 0.0597
new: DataOwner10的最优x_10 = 0.0410
eta:0.7000000000000001
new: DataOwner1的最优x_1 = 0.0619
new: DataOwner2的最优x_2 = 0.0452
new: DataOwner3的最优x_3 = 0.0572
new: DataOwner4的最优x_4 = 0.0625
new: DataOwner5的最优x_5 = 0.0727
new: DataOwner6的最优x_6 = 0.0927
new: DataOwner7的最优x_7 = 0.0561
new: DataOwner8的最优x_8 = 0.0749
new: DataOwner9的最优x_9 = 0.0605
new: DataOwner10的最优x_10 = 0.0416
eta:0.7100000000000001
new: DataOwner1的最优x_1 = 0.0628
new: DataOwner2的最优x_2 = 0.0458
new: DataOwner3的最优x_3 = 0.0580
new: DataOwner4的最优x_4 = 0.0634
new: DataOwner5的最优x_5 = 0.0738
new: DataOwner6的最优x_6 = 0.0940
new: DataOwner7的最优x_7 = 0.0569
new: DataOwner8的最优x_8 = 0.0759
new: DataOwner9的最优x_9 = 0.0614
new: DataOwner10的最优x_10 = 0.0422
eta:0.72
new: DataOwner1的最优x_1 = 0.0637
new: DataOwner2的最优x_2 = 0.0465
new: DataOwner3的最优x_3 = 0.0588
new: DataOwner4的最优x_4 = 0.0643
new: DataOwner5的最优x_5 = 0.0748
new: DataOwner6的最优x_6 = 0.0954
new: DataOwner7的最优x_7 = 0.0577
new: DataOwner8的最优x_8 = 0.0770
new: DataOwner9的最优x_9 = 0.0622
new: DataOwner10的最优x_10 = 0.0428
eta:0.73
new: DataOwner1的最优x_1 = 0.0646
new: DataOwner2的最优x_2 = 0.0471
new: DataOwner3的最优x_3 = 0.0596
new: DataOwner4的最优x_4 = 0.0652
new: DataOwner5的最优x_5 = 0.0758
new: DataOwner6的最优x_6 = 0.0967
new: DataOwner7的最优x_7 = 0.0585
new: DataOwner8的最优x_8 = 0.0781
new: DataOwner9的最优x_9 = 0.0631
new: DataOwner10的最优x_10 = 0.0434
eta:0.74
new: DataOwner1的最优x_1 = 0.0655
new: DataOwner2的最优x_2 = 0.0478
new: DataOwner3的最优x_3 = 0.0604
new: DataOwner4的最优x_4 = 0.0661
new: DataOwner5的最优x_5 = 0.0769
new: DataOwner6的最优x_6 = 0.0980
new: DataOwner7的最优x_7 = 0.0593
new: DataOwner8的最优x_8 = 0.0792
new: DataOwner9的最优x_9 = 0.0640
new: DataOwner10的最优x_10 = 0.0440
eta:0.75
new: DataOwner1的最优x_1 = 0.0663
new: DataOwner2的最优x_2 = 0.0484
new: DataOwner3的最优x_3 = 0.0613
new: DataOwner4的最优x_4 = 0.0670
new: DataOwner5的最优x_5 = 0.0779
new: DataOwner6的最优x_6 = 0.0993
new: DataOwner7的最优x_7 = 0.0601
new: DataOwner8的最优x_8 = 0.0802
new: DataOwner9的最优x_9 = 0.0648
new: DataOwner10的最优x_10 = 0.0446
eta:0.76
new: DataOwner1的最优x_1 = 0.0672
new: DataOwner2的最优x_2 = 0.0491
new: DataOwner3的最优x_3 = 0.0621
new: DataOwner4的最优x_4 = 0.0679
new: DataOwner5的最优x_5 = 0.0790
new: DataOwner6的最优x_6 = 0.1007
new: DataOwner7的最优x_7 = 0.0609
new: DataOwner8的最优x_8 = 0.0813
new: DataOwner9的最优x_9 = 0.0657
new: DataOwner10的最优x_10 = 0.0452
eta:0.77
new: DataOwner1的最优x_1 = 0.0681
new: DataOwner2的最优x_2 = 0.0497
new: DataOwner3的最优x_3 = 0.0629
new: DataOwner4的最优x_4 = 0.0688
new: DataOwner5的最优x_5 = 0.0800
new: DataOwner6的最优x_6 = 0.1020
new: DataOwner7的最优x_7 = 0.0617
new: DataOwner8的最优x_8 = 0.0824
new: DataOwner9的最优x_9 = 0.0666
new: DataOwner10的最优x_10 = 0.0458
eta:0.78
new: DataOwner1的最优x_1 = 0.0690
new: DataOwner2的最优x_2 = 0.0504
new: DataOwner3的最优x_3 = 0.0637
new: DataOwner4的最优x_4 = 0.0697
new: DataOwner5的最优x_5 = 0.0810
new: DataOwner6的最优x_6 = 0.1033
new: DataOwner7的最优x_7 = 0.0625
new: DataOwner8的最优x_8 = 0.0834
new: DataOwner9的最优x_9 = 0.0674
new: DataOwner10的最优x_10 = 0.0464
eta:0.79
new: DataOwner1的最优x_1 = 0.0699
new: DataOwner2的最优x_2 = 0.0510
new: DataOwner3的最优x_3 = 0.0645
new: DataOwner4的最优x_4 = 0.0706
new: DataOwner5的最优x_5 = 0.0821
new: DataOwner6的最优x_6 = 0.1046
new: DataOwner7的最优x_7 = 0.0633
new: DataOwner8的最优x_8 = 0.0845
new: DataOwner9的最优x_9 = 0.0683
new: DataOwner10的最优x_10 = 0.0470
eta:0.8
new: DataOwner1的最优x_1 = 0.0708
new: DataOwner2的最优x_2 = 0.0517
new: DataOwner3的最优x_3 = 0.0653
new: DataOwner4的最优x_4 = 0.0715
new: DataOwner5的最优x_5 = 0.0831
new: DataOwner6的最优x_6 = 0.1060
new: DataOwner7的最优x_7 = 0.0641
new: DataOwner8的最优x_8 = 0.0856
new: DataOwner9的最优x_9 = 0.0692
new: DataOwner10的最优x_10 = 0.0476
eta:0.81
new: DataOwner1的最优x_1 = 0.0717
new: DataOwner2的最优x_2 = 0.0523
new: DataOwner3的最优x_3 = 0.0662
new: DataOwner4的最优x_4 = 0.0724
new: DataOwner5的最优x_5 = 0.0842
new: DataOwner6的最优x_6 = 0.1073
new: DataOwner7的最优x_7 = 0.0649
new: DataOwner8的最优x_8 = 0.0866
new: DataOwner9的最优x_9 = 0.0700
new: DataOwner10的最优x_10 = 0.0482
eta:0.8200000000000001
new: DataOwner1的最优x_1 = 0.0725
new: DataOwner2的最优x_2 = 0.0530
new: DataOwner3的最优x_3 = 0.0670
new: DataOwner4的最优x_4 = 0.0733
new: DataOwner5的最优x_5 = 0.0852
new: DataOwner6的最优x_6 = 0.1086
new: DataOwner7的最优x_7 = 0.0657
new: DataOwner8的最优x_8 = 0.0877
new: DataOwner9的最优x_9 = 0.0709
new: DataOwner10的最优x_10 = 0.0487
eta:0.8300000000000001
new: DataOwner1的最优x_1 = 0.0734
new: DataOwner2的最优x_2 = 0.0536
new: DataOwner3的最优x_3 = 0.0678
new: DataOwner4的最优x_4 = 0.0742
new: DataOwner5的最优x_5 = 0.0862
new: DataOwner6的最优x_6 = 0.1099
new: DataOwner7的最优x_7 = 0.0665
new: DataOwner8的最优x_8 = 0.0888
new: DataOwner9的最优x_9 = 0.0718
new: DataOwner10的最优x_10 = 0.0493
eta:0.8400000000000001
new: DataOwner1的最优x_1 = 0.0743
new: DataOwner2的最优x_2 = 0.0542
new: DataOwner3的最优x_3 = 0.0686
new: DataOwner4的最优x_4 = 0.0750
new: DataOwner5的最优x_5 = 0.0873
new: DataOwner6的最优x_6 = 0.1112
new: DataOwner7的最优x_7 = 0.0673
new: DataOwner8的最优x_8 = 0.0898
new: DataOwner9的最优x_9 = 0.0726
new: DataOwner10的最优x_10 = 0.0499
eta:0.85
new: DataOwner1的最优x_1 = 0.0752
new: DataOwner2的最优x_2 = 0.0549
new: DataOwner3的最优x_3 = 0.0694
new: DataOwner4的最优x_4 = 0.0759
new: DataOwner5的最优x_5 = 0.0883
new: DataOwner6的最优x_6 = 0.1126
new: DataOwner7的最优x_7 = 0.0681
new: DataOwner8的最优x_8 = 0.0909
new: DataOwner9的最优x_9 = 0.0735
new: DataOwner10的最优x_10 = 0.0505
eta:0.86
new: DataOwner1的最优x_1 = 0.0761
new: DataOwner2的最优x_2 = 0.0555
new: DataOwner3的最优x_3 = 0.0702
new: DataOwner4的最优x_4 = 0.0768
new: DataOwner5的最优x_5 = 0.0894
new: DataOwner6的最优x_6 = 0.1139
new: DataOwner7的最优x_7 = 0.0689
new: DataOwner8的最优x_8 = 0.0920
new: DataOwner9的最优x_9 = 0.0744
new: DataOwner10的最优x_10 = 0.0511
eta:0.87
new: DataOwner1的最优x_1 = 0.0770
new: DataOwner2的最优x_2 = 0.0562
new: DataOwner3的最优x_3 = 0.0711
new: DataOwner4的最优x_4 = 0.0777
new: DataOwner5的最优x_5 = 0.0904
new: DataOwner6的最优x_6 = 0.1152
new: DataOwner7的最优x_7 = 0.0697
new: DataOwner8的最优x_8 = 0.0931
new: DataOwner9的最优x_9 = 0.0752
new: DataOwner10的最优x_10 = 0.0517
eta:0.88
new: DataOwner1的最优x_1 = 0.0778
new: DataOwner2的最优x_2 = 0.0568
new: DataOwner3的最优x_3 = 0.0719
new: DataOwner4的最优x_4 = 0.0786
new: DataOwner5的最优x_5 = 0.0914
new: DataOwner6的最优x_6 = 0.1165
new: DataOwner7的最优x_7 = 0.0705
new: DataOwner8的最优x_8 = 0.0941
new: DataOwner9的最优x_9 = 0.0761
new: DataOwner10的最优x_10 = 0.0523
eta:0.89
new: DataOwner1的最优x_1 = 0.0787
new: DataOwner2的最优x_2 = 0.0575
new: DataOwner3的最优x_3 = 0.0727
new: DataOwner4的最优x_4 = 0.0795
new: DataOwner5的最优x_5 = 0.0925
new: DataOwner6的最优x_6 = 0.1179
new: DataOwner7的最优x_7 = 0.0713
new: DataOwner8的最优x_8 = 0.0952
new: DataOwner9的最优x_9 = 0.0769
new: DataOwner10的最优x_10 = 0.0529
eta:0.9
new: DataOwner1的最优x_1 = 0.0796
new: DataOwner2的最优x_2 = 0.0581
new: DataOwner3的最优x_3 = 0.0735
new: DataOwner4的最优x_4 = 0.0804
new: DataOwner5的最优x_5 = 0.0935
new: DataOwner6的最优x_6 = 0.1192
new: DataOwner7的最优x_7 = 0.0721
new: DataOwner8的最优x_8 = 0.0963
new: DataOwner9的最优x_9 = 0.0778
new: DataOwner10的最优x_10 = 0.0535
eta:0.91
new: DataOwner1的最优x_1 = 0.0805
new: DataOwner2的最优x_2 = 0.0588
new: DataOwner3的最优x_3 = 0.0743
new: DataOwner4的最优x_4 = 0.0813
new: DataOwner5的最优x_5 = 0.0946
new: DataOwner6的最优x_6 = 0.1205
new: DataOwner7的最优x_7 = 0.0729
new: DataOwner8的最优x_8 = 0.0973
new: DataOwner9的最优x_9 = 0.0787
new: DataOwner10的最优x_10 = 0.0541
eta:0.92
new: DataOwner1的最优x_1 = 0.0814
new: DataOwner2的最优x_2 = 0.0594
new: DataOwner3的最优x_3 = 0.0751
new: DataOwner4的最优x_4 = 0.0822
new: DataOwner5的最优x_5 = 0.0956
new: DataOwner6的最优x_6 = 0.1218
new: DataOwner7的最优x_7 = 0.0737
new: DataOwner8的最优x_8 = 0.0984
new: DataOwner9的最优x_9 = 0.0795
new: DataOwner10的最优x_10 = 0.0547
eta:0.93
new: DataOwner1的最优x_1 = 0.0823
new: DataOwner2的最优x_2 = 0.0601
new: DataOwner3的最优x_3 = 0.0760
new: DataOwner4的最优x_4 = 0.0831
new: DataOwner5的最优x_5 = 0.0966
new: DataOwner6的最优x_6 = 0.1232
new: DataOwner7的最优x_7 = 0.0745
new: DataOwner8的最优x_8 = 0.0995
new: DataOwner9的最优x_9 = 0.0804
new: DataOwner10的最优x_10 = 0.0553
eta:0.9400000000000001
new: DataOwner1的最优x_1 = 0.0832
new: DataOwner2的最优x_2 = 0.0607
new: DataOwner3的最优x_3 = 0.0768
new: DataOwner4的最优x_4 = 0.0840
new: DataOwner5的最优x_5 = 0.0977
new: DataOwner6的最优x_6 = 0.1245
new: DataOwner7的最优x_7 = 0.0753
new: DataOwner8的最优x_8 = 0.1005
new: DataOwner9的最优x_9 = 0.0813
new: DataOwner10的最优x_10 = 0.0559
eta:0.9500000000000001
new: DataOwner1的最优x_1 = 0.0840
new: DataOwner2的最优x_2 = 0.0613
new: DataOwner3的最优x_3 = 0.0776
new: DataOwner4的最优x_4 = 0.0849
new: DataOwner5的最优x_5 = 0.0987
new: DataOwner6的最优x_6 = 0.1258
new: DataOwner7的最优x_7 = 0.0761
new: DataOwner8的最优x_8 = 0.1016
new: DataOwner9的最优x_9 = 0.0821
new: DataOwner10的最优x_10 = 0.0565
eta:0.9600000000000001
new: DataOwner1的最优x_1 = 0.0849
new: DataOwner2的最优x_2 = 0.0620
new: DataOwner3的最优x_3 = 0.0784
new: DataOwner4的最优x_4 = 0.0858
new: DataOwner5的最优x_5 = 0.0997
new: DataOwner6的最优x_6 = 0.1271
new: DataOwner7的最优x_7 = 0.0769
new: DataOwner8的最优x_8 = 0.1027
new: DataOwner9的最优x_9 = 0.0830
new: DataOwner10的最优x_10 = 0.0571
eta:0.97
new: DataOwner1的最优x_1 = 0.0858
new: DataOwner2的最优x_2 = 0.0626
new: DataOwner3的最优x_3 = 0.0792
new: DataOwner4的最优x_4 = 0.0867
new: DataOwner5的最优x_5 = 0.1008
new: DataOwner6的最优x_6 = 0.1285
new: DataOwner7的最优x_7 = 0.0777
new: DataOwner8的最优x_8 = 0.1038
new: DataOwner9的最优x_9 = 0.0839
new: DataOwner10的最优x_10 = 0.0577
eta:0.98
new: DataOwner1的最优x_1 = 0.0867
new: DataOwner2的最优x_2 = 0.0633
new: DataOwner3的最优x_3 = 0.0800
new: DataOwner4的最优x_4 = 0.0876
new: DataOwner5的最优x_5 = 0.1018
new: DataOwner6的最优x_6 = 0.1298
new: DataOwner7的最优x_7 = 0.0785
new: DataOwner8的最优x_8 = 0.1048
new: DataOwner9的最优x_9 = 0.0847
new: DataOwner10的最优x_10 = 0.0583
eta:0.99
new: DataOwner1的最优x_1 = 0.0876
new: DataOwner2的最优x_2 = 0.0639
new: DataOwner3的最优x_3 = 0.0809
new: DataOwner4的最优x_4 = 0.0884
new: DataOwner5的最优x_5 = 0.1029
new: DataOwner6的最优x_6 = 0.1311
new: DataOwner7的最优x_7 = 0.0793
new: DataOwner8的最优x_8 = 0.1059
new: DataOwner9的最优x_9 = 0.0856
new: DataOwner10的最优x_10 = 0.0589
eta:1.0
new: DataOwner1的最优x_1 = 0.0885
new: DataOwner2的最优x_2 = 0.0646
new: DataOwner3的最优x_3 = 0.0817
new: DataOwner4的最优x_4 = 0.0893
new: DataOwner5的最优x_5 = 0.1039
new: DataOwner6的最优x_6 = 0.1324
new: DataOwner7的最优x_7 = 0.0801
new: DataOwner8的最优x_8 = 0.1070
new: DataOwner9的最优x_9 = 0.0865
new: DataOwner10的最优x_10 = 0.0594
eta:1.01
new: DataOwner1的最优x_1 = 0.0893
new: DataOwner2的最优x_2 = 0.0652
new: DataOwner3的最优x_3 = 0.0825
new: DataOwner4的最优x_4 = 0.0902
new: DataOwner5的最优x_5 = 0.1049
new: DataOwner6的最优x_6 = 0.1338
new: DataOwner7的最优x_7 = 0.0809
new: DataOwner8的最优x_8 = 0.1080
new: DataOwner9的最优x_9 = 0.0873
new: DataOwner10的最优x_10 = 0.0600
eta:1.02
new: DataOwner1的最优x_1 = 0.0902
new: DataOwner2的最优x_2 = 0.0659
new: DataOwner3的最优x_3 = 0.0833
new: DataOwner4的最优x_4 = 0.0911
new: DataOwner5的最优x_5 = 0.1060
new: DataOwner6的最优x_6 = 0.1351
new: DataOwner7的最优x_7 = 0.0817
new: DataOwner8的最优x_8 = 0.1091
new: DataOwner9的最优x_9 = 0.0882
new: DataOwner10的最优x_10 = 0.0606
eta:1.03
new: DataOwner1的最优x_1 = 0.0911
new: DataOwner2的最优x_2 = 0.0665
new: DataOwner3的最优x_3 = 0.0841
new: DataOwner4的最优x_4 = 0.0920
new: DataOwner5的最优x_5 = 0.1070
new: DataOwner6的最优x_6 = 0.1364
new: DataOwner7的最优x_7 = 0.0825
new: DataOwner8的最优x_8 = 0.1102
new: DataOwner9的最优x_9 = 0.0890
new: DataOwner10的最优x_10 = 0.0612
eta:1.04
new: DataOwner1的最优x_1 = 0.0920
new: DataOwner2的最优x_2 = 0.0672
new: DataOwner3的最优x_3 = 0.0849
new: DataOwner4的最优x_4 = 0.0929
new: DataOwner5的最优x_5 = 0.1081
new: DataOwner6的最优x_6 = 0.1377
new: DataOwner7的最优x_7 = 0.0833
new: DataOwner8的最优x_8 = 0.1112
new: DataOwner9的最优x_9 = 0.0899
new: DataOwner10的最优x_10 = 0.0618
eta:1.05
new: DataOwner1的最优x_1 = 0.0929
new: DataOwner2的最优x_2 = 0.0678
new: DataOwner3的最优x_3 = 0.0858
new: DataOwner4的最优x_4 = 0.0938
new: DataOwner5的最优x_5 = 0.1091
new: DataOwner6的最优x_6 = 0.1391
new: DataOwner7的最优x_7 = 0.0841
new: DataOwner8的最优x_8 = 0.1123
new: DataOwner9的最优x_9 = 0.0908
new: DataOwner10的最优x_10 = 0.0624
eta:1.06
new: DataOwner1的最优x_1 = 0.0938
new: DataOwner2的最优x_2 = 0.0685
new: DataOwner3的最优x_3 = 0.0866
new: DataOwner4的最优x_4 = 0.0947
new: DataOwner5的最优x_5 = 0.1101
new: DataOwner6的最优x_6 = 0.1404
new: DataOwner7的最优x_7 = 0.0849
new: DataOwner8的最优x_8 = 0.1134
new: DataOwner9的最优x_9 = 0.0916
new: DataOwner10的最优x_10 = 0.0630
eta:1.07
new: DataOwner1的最优x_1 = 0.0947
new: DataOwner2的最优x_2 = 0.0691
new: DataOwner3的最优x_3 = 0.0874
new: DataOwner4的最优x_4 = 0.0956
new: DataOwner5的最优x_5 = 0.1112
new: DataOwner6的最优x_6 = 0.1417
new: DataOwner7的最优x_7 = 0.0858
new: DataOwner8的最优x_8 = 0.1144
new: DataOwner9的最优x_9 = 0.0925
new: DataOwner10的最优x_10 = 0.0636
eta:1.08
new: DataOwner1的最优x_1 = 0.0955
new: DataOwner2的最优x_2 = 0.0697
new: DataOwner3的最优x_3 = 0.0882
new: DataOwner4的最优x_4 = 0.0965
new: DataOwner5的最优x_5 = 0.1122
new: DataOwner6的最优x_6 = 0.1430
new: DataOwner7的最优x_7 = 0.0866
new: DataOwner8的最优x_8 = 0.1155
new: DataOwner9的最优x_9 = 0.0934
new: DataOwner10的最优x_10 = 0.0642
eta:1.09
new: DataOwner1的最优x_1 = 0.0964
new: DataOwner2的最优x_2 = 0.0704
new: DataOwner3的最优x_3 = 0.0890
new: DataOwner4的最优x_4 = 0.0974
new: DataOwner5的最优x_5 = 0.1133
new: DataOwner6的最优x_6 = 0.1444
new: DataOwner7的最优x_7 = 0.0874
new: DataOwner8的最优x_8 = 0.1166
new: DataOwner9的最优x_9 = 0.0942
new: DataOwner10的最优x_10 = 0.0648
eta:1.1
new: DataOwner1的最优x_1 = 0.0973
new: DataOwner2的最优x_2 = 0.0710
new: DataOwner3的最优x_3 = 0.0898
new: DataOwner4的最优x_4 = 0.0983
new: DataOwner5的最优x_5 = 0.1143
new: DataOwner6的最优x_6 = 0.1457
new: DataOwner7的最优x_7 = 0.0882
new: DataOwner8的最优x_8 = 0.1177
new: DataOwner9的最优x_9 = 0.0951
new: DataOwner10的最优x_10 = 0.0654
eta:1.11
new: DataOwner1的最优x_1 = 0.0982
new: DataOwner2的最优x_2 = 0.0717
new: DataOwner3的最优x_3 = 0.0907
new: DataOwner4的最优x_4 = 0.0992
new: DataOwner5的最优x_5 = 0.1153
new: DataOwner6的最优x_6 = 0.1470
new: DataOwner7的最优x_7 = 0.0890
new: DataOwner8的最优x_8 = 0.1187
new: DataOwner9的最优x_9 = 0.0960
new: DataOwner10的最优x_10 = 0.0660
eta:1.12
new: DataOwner1的最优x_1 = 0.0991
new: DataOwner2的最优x_2 = 0.0723
new: DataOwner3的最优x_3 = 0.0915
new: DataOwner4的最优x_4 = 0.1001
new: DataOwner5的最优x_5 = 0.1164
new: DataOwner6的最优x_6 = 0.1483
new: DataOwner7的最优x_7 = 0.0898
new: DataOwner8的最优x_8 = 0.1198
new: DataOwner9的最优x_9 = 0.0968
new: DataOwner10的最优x_10 = 0.0666
eta:1.1300000000000001
new: DataOwner1的最优x_1 = 0.1000
new: DataOwner2的最优x_2 = 0.0730
new: DataOwner3的最优x_3 = 0.0923
new: DataOwner4的最优x_4 = 0.1010
new: DataOwner5的最优x_5 = 0.1174
new: DataOwner6的最优x_6 = 0.1497
new: DataOwner7的最优x_7 = 0.0906
new: DataOwner8的最优x_8 = 0.1209
new: DataOwner9的最优x_9 = 0.0977
new: DataOwner10的最优x_10 = 0.0672
eta:1.1400000000000001
new: DataOwner1的最优x_1 = 0.1008
new: DataOwner2的最优x_2 = 0.0736
new: DataOwner3的最优x_3 = 0.0931
new: DataOwner4的最优x_4 = 0.1018
new: DataOwner5的最优x_5 = 0.1184
new: DataOwner6的最优x_6 = 0.1510
new: DataOwner7的最优x_7 = 0.0914
new: DataOwner8的最优x_8 = 0.1219
new: DataOwner9的最优x_9 = 0.0986
new: DataOwner10的最优x_10 = 0.0678
eta:1.1500000000000001
new: DataOwner1的最优x_1 = 0.1017
new: DataOwner2的最优x_2 = 0.0743
new: DataOwner3的最优x_3 = 0.0939
new: DataOwner4的最优x_4 = 0.1027
new: DataOwner5的最优x_5 = 0.1195
new: DataOwner6的最优x_6 = 0.1523
new: DataOwner7的最优x_7 = 0.0922
new: DataOwner8的最优x_8 = 0.1230
new: DataOwner9的最优x_9 = 0.0994
new: DataOwner10的最优x_10 = 0.0684
eta:1.1600000000000001
new: DataOwner1的最优x_1 = 0.1026
new: DataOwner2的最优x_2 = 0.0749
new: DataOwner3的最优x_3 = 0.0947
new: DataOwner4的最优x_4 = 0.1036
new: DataOwner5的最优x_5 = 0.1205
new: DataOwner6的最优x_6 = 0.1536
new: DataOwner7的最优x_7 = 0.0930
new: DataOwner8的最优x_8 = 0.1241
new: DataOwner9的最优x_9 = 0.1003
new: DataOwner10的最优x_10 = 0.0690
eta:1.17
new: DataOwner1的最优x_1 = 0.1035
new: DataOwner2的最优x_2 = 0.0756
new: DataOwner3的最优x_3 = 0.0956
new: DataOwner4的最优x_4 = 0.1045
new: DataOwner5的最优x_5 = 0.1216
new: DataOwner6的最优x_6 = 0.1550
new: DataOwner7的最优x_7 = 0.0938
new: DataOwner8的最优x_8 = 0.1251
new: DataOwner9的最优x_9 = 0.1012
new: DataOwner10的最优x_10 = 0.0696
eta:1.18
new: DataOwner1的最优x_1 = 0.1044
new: DataOwner2的最优x_2 = 0.0762
new: DataOwner3的最优x_3 = 0.0964
new: DataOwner4的最优x_4 = 0.1054
new: DataOwner5的最优x_5 = 0.1226
new: DataOwner6的最优x_6 = 0.1563
new: DataOwner7的最优x_7 = 0.0946
new: DataOwner8的最优x_8 = 0.1262
new: DataOwner9的最优x_9 = 0.1020
new: DataOwner10的最优x_10 = 0.0701
eta:1.19
new: DataOwner1的最优x_1 = 0.1053
new: DataOwner2的最优x_2 = 0.0768
new: DataOwner3的最优x_3 = 0.0972
new: DataOwner4的最优x_4 = 0.1063
new: DataOwner5的最优x_5 = 0.1236
new: DataOwner6的最优x_6 = 0.1576
new: DataOwner7的最优x_7 = 0.0954
new: DataOwner8的最优x_8 = 0.1273
new: DataOwner9的最优x_9 = 0.1029
new: DataOwner10的最优x_10 = 0.0707
eta:1.2
new: DataOwner1的最优x_1 = 0.1062
new: DataOwner2的最优x_2 = 0.0775
new: DataOwner3的最优x_3 = 0.0980
new: DataOwner4的最优x_4 = 0.1072
new: DataOwner5的最优x_5 = 0.1247
new: DataOwner6的最优x_6 = 0.1589
new: DataOwner7的最优x_7 = 0.0962
new: DataOwner8的最优x_8 = 0.1284
new: DataOwner9的最优x_9 = 0.1037
new: DataOwner10的最优x_10 = 0.0713
eta:1.21
new: DataOwner1的最优x_1 = 0.1070
new: DataOwner2的最优x_2 = 0.0781
new: DataOwner3的最优x_3 = 0.0988
new: DataOwner4的最优x_4 = 0.1081
new: DataOwner5的最优x_5 = 0.1257
new: DataOwner6的最优x_6 = 0.1603
new: DataOwner7的最优x_7 = 0.0970
new: DataOwner8的最优x_8 = 0.1294
new: DataOwner9的最优x_9 = 0.1046
new: DataOwner10的最优x_10 = 0.0719
eta:1.22
new: DataOwner1的最优x_1 = 0.1079
new: DataOwner2的最优x_2 = 0.0788
new: DataOwner3的最优x_3 = 0.0996
new: DataOwner4的最优x_4 = 0.1090
new: DataOwner5的最优x_5 = 0.1268
new: DataOwner6的最优x_6 = 0.1616
new: DataOwner7的最优x_7 = 0.0978
new: DataOwner8的最优x_8 = 0.1305
new: DataOwner9的最优x_9 = 0.1055
new: DataOwner10的最优x_10 = 0.0725
eta:1.23
new: DataOwner1的最优x_1 = 0.1088
new: DataOwner2的最优x_2 = 0.0794
new: DataOwner3的最优x_3 = 0.1005
new: DataOwner4的最优x_4 = 0.1099
new: DataOwner5的最优x_5 = 0.1278
new: DataOwner6的最优x_6 = 0.1629
new: DataOwner7的最优x_7 = 0.0986
new: DataOwner8的最优x_8 = 0.1316
new: DataOwner9的最优x_9 = 0.1063
new: DataOwner10的最优x_10 = 0.0731
eta:1.24
new: DataOwner1的最优x_1 = 0.1097
new: DataOwner2的最优x_2 = 0.0801
new: DataOwner3的最优x_3 = 0.1013
new: DataOwner4的最优x_4 = 0.1108
new: DataOwner5的最优x_5 = 0.1288
new: DataOwner6的最优x_6 = 0.1642
new: DataOwner7的最优x_7 = 0.0994
new: DataOwner8的最优x_8 = 0.1326
new: DataOwner9的最优x_9 = 0.1072
new: DataOwner10的最优x_10 = 0.0737
eta:1.25
new: DataOwner1的最优x_1 = 0.1106
new: DataOwner2的最优x_2 = 0.0807
new: DataOwner3的最优x_3 = 0.1021
new: DataOwner4的最优x_4 = 0.1117
new: DataOwner5的最优x_5 = 0.1299
new: DataOwner6的最优x_6 = 0.1655
new: DataOwner7的最优x_7 = 0.1002
new: DataOwner8的最优x_8 = 0.1337
new: DataOwner9的最优x_9 = 0.1081
new: DataOwner10的最优x_10 = 0.0743
eta:1.26
new: DataOwner1的最优x_1 = 0.1115
new: DataOwner2的最优x_2 = 0.0814
new: DataOwner3的最优x_3 = 0.1029
new: DataOwner4的最优x_4 = 0.1126
new: DataOwner5的最优x_5 = 0.1309
new: DataOwner6的最优x_6 = 0.1669
new: DataOwner7的最优x_7 = 0.1010
new: DataOwner8的最优x_8 = 0.1348
new: DataOwner9的最优x_9 = 0.1089
new: DataOwner10的最优x_10 = 0.0749
eta:1.27
new: DataOwner1的最优x_1 = 0.1123
new: DataOwner2的最优x_2 = 0.0820
new: DataOwner3的最优x_3 = 0.1037
new: DataOwner4的最优x_4 = 0.1135
new: DataOwner5的最优x_5 = 0.1320
new: DataOwner6的最优x_6 = 0.1682
new: DataOwner7的最优x_7 = 0.1018
new: DataOwner8的最优x_8 = 0.1358
new: DataOwner9的最优x_9 = 0.1098
new: DataOwner10的最优x_10 = 0.0755
eta:1.28
new: DataOwner1的最优x_1 = 0.1132
new: DataOwner2的最优x_2 = 0.0827
new: DataOwner3的最优x_3 = 0.1045
new: DataOwner4的最优x_4 = 0.1144
new: DataOwner5的最优x_5 = 0.1330
new: DataOwner6的最优x_6 = 0.1695
new: DataOwner7的最优x_7 = 0.1026
new: DataOwner8的最优x_8 = 0.1369
new: DataOwner9的最优x_9 = 0.1107
new: DataOwner10的最优x_10 = 0.0761
eta:1.29
new: DataOwner1的最优x_1 = 0.1141
new: DataOwner2的最优x_2 = 0.0833
new: DataOwner3的最优x_3 = 0.1054
new: DataOwner4的最优x_4 = 0.1152
new: DataOwner5的最优x_5 = 0.1340
new: DataOwner6的最优x_6 = 0.1708
new: DataOwner7的最优x_7 = 0.1034
new: DataOwner8的最优x_8 = 0.1380
new: DataOwner9的最优x_9 = 0.1115
new: DataOwner10的最优x_10 = 0.0767
eta:1.3
new: DataOwner1的最优x_1 = 0.1150
new: DataOwner2的最优x_2 = 0.0840
new: DataOwner3的最优x_3 = 0.1062
new: DataOwner4的最优x_4 = 0.1161
new: DataOwner5的最优x_5 = 0.1351
new: DataOwner6的最优x_6 = 0.1722
new: DataOwner7的最优x_7 = 0.1042
new: DataOwner8的最优x_8 = 0.1391
new: DataOwner9的最优x_9 = 0.1124
new: DataOwner10的最优x_10 = 0.0773
eta:1.31
new: DataOwner1的最优x_1 = 0.1159
new: DataOwner2的最优x_2 = 0.0846
new: DataOwner3的最优x_3 = 0.1070
new: DataOwner4的最优x_4 = 0.1170
new: DataOwner5的最优x_5 = 0.1361
new: DataOwner6的最优x_6 = 0.1735
new: DataOwner7的最优x_7 = 0.1050
new: DataOwner8的最优x_8 = 0.1401
new: DataOwner9的最优x_9 = 0.1133
new: DataOwner10的最优x_10 = 0.0779
eta:1.32
new: DataOwner1的最优x_1 = 0.1168
new: DataOwner2的最优x_2 = 0.0852
new: DataOwner3的最优x_3 = 0.1078
new: DataOwner4的最优x_4 = 0.1179
new: DataOwner5的最优x_5 = 0.1372
new: DataOwner6的最优x_6 = 0.1748
new: DataOwner7的最优x_7 = 0.1058
new: DataOwner8的最优x_8 = 0.1412
new: DataOwner9的最优x_9 = 0.1141
new: DataOwner10的最优x_10 = 0.0785
eta:1.33
new: DataOwner1的最优x_1 = 0.1177
new: DataOwner2的最优x_2 = 0.0859
new: DataOwner3的最优x_3 = 0.1086
new: DataOwner4的最优x_4 = 0.1188
new: DataOwner5的最优x_5 = 0.1382
new: DataOwner6的最优x_6 = 0.1761
new: DataOwner7的最优x_7 = 0.1066
new: DataOwner8的最优x_8 = 0.1423
new: DataOwner9的最优x_9 = 0.1150
new: DataOwner10的最优x_10 = 0.0791
eta:1.34
new: DataOwner1的最优x_1 = 0.1185
new: DataOwner2的最优x_2 = 0.0865
new: DataOwner3的最优x_3 = 0.1094
new: DataOwner4的最优x_4 = 0.1197
new: DataOwner5的最优x_5 = 0.1392
new: DataOwner6的最优x_6 = 0.1775
new: DataOwner7的最优x_7 = 0.1074
new: DataOwner8的最优x_8 = 0.1433
new: DataOwner9的最优x_9 = 0.1159
new: DataOwner10的最优x_10 = 0.0797
eta:1.35
new: DataOwner1的最优x_1 = 0.1194
new: DataOwner2的最优x_2 = 0.0872
new: DataOwner3的最优x_3 = 0.1103
new: DataOwner4的最优x_4 = 0.1206
new: DataOwner5的最优x_5 = 0.1403
new: DataOwner6的最优x_6 = 0.1788
new: DataOwner7的最优x_7 = 0.1082
new: DataOwner8的最优x_8 = 0.1444
new: DataOwner9的最优x_9 = 0.1167
new: DataOwner10的最优x_10 = 0.0803
eta:1.36
new: DataOwner1的最优x_1 = 0.1203
new: DataOwner2的最优x_2 = 0.0878
new: DataOwner3的最优x_3 = 0.1111
new: DataOwner4的最优x_4 = 0.1215
new: DataOwner5的最优x_5 = 0.1413
new: DataOwner6的最优x_6 = 0.1801
new: DataOwner7的最优x_7 = 0.1090
new: DataOwner8的最优x_8 = 0.1455
new: DataOwner9的最优x_9 = 0.1176
new: DataOwner10的最优x_10 = 0.0808
eta:1.37
new: DataOwner1的最优x_1 = 0.1212
new: DataOwner2的最优x_2 = 0.0885
new: DataOwner3的最优x_3 = 0.1119
new: DataOwner4的最优x_4 = 0.1224
new: DataOwner5的最优x_5 = 0.1423
new: DataOwner6的最优x_6 = 0.1814
new: DataOwner7的最优x_7 = 0.1098
new: DataOwner8的最优x_8 = 0.1465
new: DataOwner9的最优x_9 = 0.1184
new: DataOwner10的最优x_10 = 0.0814
eta:1.3800000000000001
new: DataOwner1的最优x_1 = 0.1221
new: DataOwner2的最优x_2 = 0.0891
new: DataOwner3的最优x_3 = 0.1127
new: DataOwner4的最优x_4 = 0.1233
new: DataOwner5的最优x_5 = 0.1434
new: DataOwner6的最优x_6 = 0.1828
new: DataOwner7的最优x_7 = 0.1106
new: DataOwner8的最优x_8 = 0.1476
new: DataOwner9的最优x_9 = 0.1193
new: DataOwner10的最优x_10 = 0.0820
eta:1.3900000000000001
new: DataOwner1的最优x_1 = 0.1230
new: DataOwner2的最优x_2 = 0.0898
new: DataOwner3的最优x_3 = 0.1135
new: DataOwner4的最优x_4 = 0.1242
new: DataOwner5的最优x_5 = 0.1444
new: DataOwner6的最优x_6 = 0.1841
new: DataOwner7的最优x_7 = 0.1114
new: DataOwner8的最优x_8 = 0.1487
new: DataOwner9的最优x_9 = 0.1202
new: DataOwner10的最优x_10 = 0.0826
eta:1.4000000000000001
new: DataOwner1的最优x_1 = 0.1238
new: DataOwner2的最优x_2 = 0.0904
new: DataOwner3的最优x_3 = 0.1143
new: DataOwner4的最优x_4 = 0.1251
new: DataOwner5的最优x_5 = 0.1455
new: DataOwner6的最优x_6 = 0.1854
new: DataOwner7的最优x_7 = 0.1122
new: DataOwner8的最优x_8 = 0.1497
new: DataOwner9的最优x_9 = 0.1210
new: DataOwner10的最优x_10 = 0.0832
eta:1.4100000000000001
new: DataOwner1的最优x_1 = 0.1247
new: DataOwner2的最优x_2 = 0.0911
new: DataOwner3的最优x_3 = 0.1152
new: DataOwner4的最优x_4 = 0.1260
new: DataOwner5的最优x_5 = 0.1465
new: DataOwner6的最优x_6 = 0.1867
new: DataOwner7的最优x_7 = 0.1130
new: DataOwner8的最优x_8 = 0.1508
new: DataOwner9的最优x_9 = 0.1219
new: DataOwner10的最优x_10 = 0.0838
eta:1.42
new: DataOwner1的最优x_1 = 0.1256
new: DataOwner2的最优x_2 = 0.0917
new: DataOwner3的最优x_3 = 0.1160
new: DataOwner4的最优x_4 = 0.1269
new: DataOwner5的最优x_5 = 0.1475
new: DataOwner6的最优x_6 = 0.1881
new: DataOwner7的最优x_7 = 0.1138
new: DataOwner8的最优x_8 = 0.1519
new: DataOwner9的最优x_9 = 0.1228
new: DataOwner10的最优x_10 = 0.0844
eta:1.43
new: DataOwner1的最优x_1 = 0.1265
new: DataOwner2的最优x_2 = 0.0923
new: DataOwner3的最优x_3 = 0.1168
new: DataOwner4的最优x_4 = 0.1278
new: DataOwner5的最优x_5 = 0.1486
new: DataOwner6的最优x_6 = 0.1894
new: DataOwner7的最优x_7 = 0.1146
new: DataOwner8的最优x_8 = 0.1530
new: DataOwner9的最优x_9 = 0.1236
new: DataOwner10的最优x_10 = 0.0850
eta:1.44
new: DataOwner1的最优x_1 = 0.1274
new: DataOwner2的最优x_2 = 0.0930
new: DataOwner3的最优x_3 = 0.1176
new: DataOwner4的最优x_4 = 0.1286
new: DataOwner5的最优x_5 = 0.1496
new: DataOwner6的最优x_6 = 0.1907
new: DataOwner7的最优x_7 = 0.1154
new: DataOwner8的最优x_8 = 0.1540
new: DataOwner9的最优x_9 = 0.1245
new: DataOwner10的最优x_10 = 0.0856
eta:1.45
new: DataOwner1的最优x_1 = 0.1283
new: DataOwner2的最优x_2 = 0.0936
new: DataOwner3的最优x_3 = 0.1184
new: DataOwner4的最优x_4 = 0.1295
new: DataOwner5的最优x_5 = 0.1507
new: DataOwner6的最优x_6 = 0.1920
new: DataOwner7的最优x_7 = 0.1162
new: DataOwner8的最优x_8 = 0.1551
new: DataOwner9的最优x_9 = 0.1254
new: DataOwner10的最优x_10 = 0.0862
eta:1.46
new: DataOwner1的最优x_1 = 0.1292
new: DataOwner2的最优x_2 = 0.0943
new: DataOwner3的最优x_3 = 0.1192
new: DataOwner4的最优x_4 = 0.1304
new: DataOwner5的最优x_5 = 0.1517
new: DataOwner6的最优x_6 = 0.1934
new: DataOwner7的最优x_7 = 0.1170
new: DataOwner8的最优x_8 = 0.1562
new: DataOwner9的最优x_9 = 0.1262
new: DataOwner10的最优x_10 = 0.0868
eta:1.47
new: DataOwner1的最优x_1 = 0.1300
new: DataOwner2的最优x_2 = 0.0949
new: DataOwner3的最优x_3 = 0.1201
new: DataOwner4的最优x_4 = 0.1313
new: DataOwner5的最优x_5 = 0.1527
new: DataOwner6的最优x_6 = 0.1947
new: DataOwner7的最优x_7 = 0.1178
new: DataOwner8的最优x_8 = 0.1572
new: DataOwner9的最优x_9 = 0.1271
new: DataOwner10的最优x_10 = 0.0874
eta:1.48
new: DataOwner1的最优x_1 = 0.1309
new: DataOwner2的最优x_2 = 0.0956
new: DataOwner3的最优x_3 = 0.1209
new: DataOwner4的最优x_4 = 0.1322
new: DataOwner5的最优x_5 = 0.1538
new: DataOwner6的最优x_6 = 0.1960
new: DataOwner7的最优x_7 = 0.1186
new: DataOwner8的最优x_8 = 0.1583
new: DataOwner9的最优x_9 = 0.1280
new: DataOwner10的最优x_10 = 0.0880
eta:1.49
new: DataOwner1的最优x_1 = 0.1318
new: DataOwner2的最优x_2 = 0.0962
new: DataOwner3的最优x_3 = 0.1217
new: DataOwner4的最优x_4 = 0.1331
new: DataOwner5的最优x_5 = 0.1548
new: DataOwner6的最优x_6 = 0.1973
new: DataOwner7的最优x_7 = 0.1194
new: DataOwner8的最优x_8 = 0.1594
new: DataOwner9的最优x_9 = 0.1288
new: DataOwner10的最优x_10 = 0.0886
eta:1.5
new: DataOwner1的最优x_1 = 0.1327
new: DataOwner2的最优x_2 = 0.0969
new: DataOwner3的最优x_3 = 0.1225
new: DataOwner4的最优x_4 = 0.1340
new: DataOwner5的最优x_5 = 0.1559
new: DataOwner6的最优x_6 = 0.1987
new: DataOwner7的最优x_7 = 0.1202
new: DataOwner8的最优x_8 = 0.1604
new: DataOwner9的最优x_9 = 0.1297
new: DataOwner10的最优x_10 = 0.0892
eta:1.51
new: DataOwner1的最优x_1 = 0.1336
new: DataOwner2的最优x_2 = 0.0975
new: DataOwner3的最优x_3 = 0.1233
new: DataOwner4的最优x_4 = 0.1349
new: DataOwner5的最优x_5 = 0.1569
new: DataOwner6的最优x_6 = 0.2000
new: DataOwner7的最优x_7 = 0.1210
new: DataOwner8的最优x_8 = 0.1615
new: DataOwner9的最优x_9 = 0.1305
new: DataOwner10的最优x_10 = 0.0898
eta:1.52
new: DataOwner1的最优x_1 = 0.1345
new: DataOwner2的最优x_2 = 0.0982
new: DataOwner3的最优x_3 = 0.1241
new: DataOwner4的最优x_4 = 0.1358
new: DataOwner5的最优x_5 = 0.1579
new: DataOwner6的最优x_6 = 0.2013
new: DataOwner7的最优x_7 = 0.1218
new: DataOwner8的最优x_8 = 0.1626
new: DataOwner9的最优x_9 = 0.1314
new: DataOwner10的最优x_10 = 0.0904
eta:1.53
new: DataOwner1的最优x_1 = 0.1353
new: DataOwner2的最优x_2 = 0.0988
new: DataOwner3的最优x_3 = 0.1250
new: DataOwner4的最优x_4 = 0.1367
new: DataOwner5的最优x_5 = 0.1590
new: DataOwner6的最优x_6 = 0.2026
new: DataOwner7的最优x_7 = 0.1226
new: DataOwner8的最优x_8 = 0.1637
new: DataOwner9的最优x_9 = 0.1323
new: DataOwner10的最优x_10 = 0.0910
eta:1.54
new: DataOwner1的最优x_1 = 0.1362
new: DataOwner2的最优x_2 = 0.0994
new: DataOwner3的最优x_3 = 0.1258
new: DataOwner4的最优x_4 = 0.1376
new: DataOwner5的最优x_5 = 0.1600
new: DataOwner6的最优x_6 = 0.2040
new: DataOwner7的最优x_7 = 0.1234
new: DataOwner8的最优x_8 = 0.1647
new: DataOwner9的最优x_9 = 0.1331
new: DataOwner10的最优x_10 = 0.0915
eta:1.55
new: DataOwner1的最优x_1 = 0.1371
new: DataOwner2的最优x_2 = 0.1001
new: DataOwner3的最优x_3 = 0.1266
new: DataOwner4的最优x_4 = 0.1385
new: DataOwner5的最优x_5 = 0.1610
new: DataOwner6的最优x_6 = 0.2053
new: DataOwner7的最优x_7 = 0.1242
new: DataOwner8的最优x_8 = 0.1658
new: DataOwner9的最优x_9 = 0.1340
new: DataOwner10的最优x_10 = 0.0921
eta:1.56
new: DataOwner1的最优x_1 = 0.1380
new: DataOwner2的最优x_2 = 0.1007
new: DataOwner3的最优x_3 = 0.1274
new: DataOwner4的最优x_4 = 0.1394
new: DataOwner5的最优x_5 = 0.1621
new: DataOwner6的最优x_6 = 0.2066
new: DataOwner7的最优x_7 = 0.1250
new: DataOwner8的最优x_8 = 0.1669
new: DataOwner9的最优x_9 = 0.1349
new: DataOwner10的最优x_10 = 0.0927
eta:1.57
new: DataOwner1的最优x_1 = 0.1389
new: DataOwner2的最优x_2 = 0.1014
new: DataOwner3的最优x_3 = 0.1282
new: DataOwner4的最优x_4 = 0.1403
new: DataOwner5的最优x_5 = 0.1631
new: DataOwner6的最优x_6 = 0.2079
new: DataOwner7的最优x_7 = 0.1258
new: DataOwner8的最优x_8 = 0.1679
new: DataOwner9的最优x_9 = 0.1357
new: DataOwner10的最优x_10 = 0.0933
eta:1.58
new: DataOwner1的最优x_1 = 0.1398
new: DataOwner2的最优x_2 = 0.1020
new: DataOwner3的最优x_3 = 0.1290
new: DataOwner4的最优x_4 = 0.1412
new: DataOwner5的最优x_5 = 0.1642
new: DataOwner6的最优x_6 = 0.2093
new: DataOwner7的最优x_7 = 0.1266
new: DataOwner8的最优x_8 = 0.1690
new: DataOwner9的最优x_9 = 0.1366
new: DataOwner10的最优x_10 = 0.0939
eta:1.59
new: DataOwner1的最优x_1 = 0.1407
new: DataOwner2的最优x_2 = 0.1027
new: DataOwner3的最优x_3 = 0.1299
new: DataOwner4的最优x_4 = 0.1421
new: DataOwner5的最优x_5 = 0.1652
new: DataOwner6的最优x_6 = 0.2106
new: DataOwner7的最优x_7 = 0.1274
new: DataOwner8的最优x_8 = 0.1701
new: DataOwner9的最优x_9 = 0.1375
new: DataOwner10的最优x_10 = 0.0945
eta:1.6
new: DataOwner1的最优x_1 = 0.1415
new: DataOwner2的最优x_2 = 0.1033
new: DataOwner3的最优x_3 = 0.1307
new: DataOwner4的最优x_4 = 0.1429
new: DataOwner5的最优x_5 = 0.1662
new: DataOwner6的最优x_6 = 0.2119
new: DataOwner7的最优x_7 = 0.1282
new: DataOwner8的最优x_8 = 0.1711
new: DataOwner9的最优x_9 = 0.1383
new: DataOwner10的最优x_10 = 0.0951
eta:1.61
new: DataOwner1的最优x_1 = 0.1424
new: DataOwner2的最优x_2 = 0.1040
new: DataOwner3的最优x_3 = 0.1315
new: DataOwner4的最优x_4 = 0.1438
new: DataOwner5的最优x_5 = 0.1673
new: DataOwner6的最优x_6 = 0.2132
new: DataOwner7的最优x_7 = 0.1290
new: DataOwner8的最优x_8 = 0.1722
new: DataOwner9的最优x_9 = 0.1392
new: DataOwner10的最优x_10 = 0.0957
eta:1.62
new: DataOwner1的最优x_1 = 0.1433
new: DataOwner2的最优x_2 = 0.1046
new: DataOwner3的最优x_3 = 0.1323
new: DataOwner4的最优x_4 = 0.1447
new: DataOwner5的最优x_5 = 0.1683
new: DataOwner6的最优x_6 = 0.2146
new: DataOwner7的最优x_7 = 0.1298
new: DataOwner8的最优x_8 = 0.1733
new: DataOwner9的最优x_9 = 0.1401
new: DataOwner10的最优x_10 = 0.0963
eta:1.6300000000000001
new: DataOwner1的最优x_1 = 0.1442
new: DataOwner2的最优x_2 = 0.1053
new: DataOwner3的最优x_3 = 0.1331
new: DataOwner4的最优x_4 = 0.1456
new: DataOwner5的最优x_5 = 0.1694
new: DataOwner6的最优x_6 = 0.2159
new: DataOwner7的最优x_7 = 0.1306
new: DataOwner8的最优x_8 = 0.1743
new: DataOwner9的最优x_9 = 0.1409
new: DataOwner10的最优x_10 = 0.0969
eta:1.6400000000000001
new: DataOwner1的最优x_1 = 0.1451
new: DataOwner2的最优x_2 = 0.1059
new: DataOwner3的最优x_3 = 0.1339
new: DataOwner4的最优x_4 = 0.1465
new: DataOwner5的最优x_5 = 0.1704
new: DataOwner6的最优x_6 = 0.2172
new: DataOwner7的最优x_7 = 0.1314
new: DataOwner8的最优x_8 = 0.1754
new: DataOwner9的最优x_9 = 0.1418
new: DataOwner10的最优x_10 = 0.0975
eta:1.6500000000000001
new: DataOwner1的最优x_1 = 0.1460
new: DataOwner2的最优x_2 = 0.1066
new: DataOwner3的最优x_3 = 0.1348
new: DataOwner4的最优x_4 = 0.1474
new: DataOwner5的最优x_5 = 0.1714
new: DataOwner6的最优x_6 = 0.2185
new: DataOwner7的最优x_7 = 0.1322
new: DataOwner8的最优x_8 = 0.1765
new: DataOwner9的最优x_9 = 0.1427
new: DataOwner10的最优x_10 = 0.0981
eta:1.6600000000000001
new: DataOwner1的最优x_1 = 0.1468
new: DataOwner2的最优x_2 = 0.1072
new: DataOwner3的最优x_3 = 0.1356
new: DataOwner4的最优x_4 = 0.1483
new: DataOwner5的最优x_5 = 0.1725
new: DataOwner6的最优x_6 = 0.2198
new: DataOwner7的最优x_7 = 0.1330
new: DataOwner8的最优x_8 = 0.1776
new: DataOwner9的最优x_9 = 0.1435
new: DataOwner10的最优x_10 = 0.0987
eta:1.6700000000000002
new: DataOwner1的最优x_1 = 0.1477
new: DataOwner2的最优x_2 = 0.1078
new: DataOwner3的最优x_3 = 0.1364
new: DataOwner4的最优x_4 = 0.1492
new: DataOwner5的最优x_5 = 0.1735
new: DataOwner6的最优x_6 = 0.2212
new: DataOwner7的最优x_7 = 0.1338
new: DataOwner8的最优x_8 = 0.1786
new: DataOwner9的最优x_9 = 0.1444
new: DataOwner10的最优x_10 = 0.0993
eta:1.68
new: DataOwner1的最优x_1 = 0.1486
new: DataOwner2的最优x_2 = 0.1085
new: DataOwner3的最优x_3 = 0.1372
new: DataOwner4的最优x_4 = 0.1501
new: DataOwner5的最优x_5 = 0.1746
new: DataOwner6的最优x_6 = 0.2225
new: DataOwner7的最优x_7 = 0.1346
new: DataOwner8的最优x_8 = 0.1797
new: DataOwner9的最优x_9 = 0.1452
new: DataOwner10的最优x_10 = 0.0999
eta:1.69
new: DataOwner1的最优x_1 = 0.1495
new: DataOwner2的最优x_2 = 0.1091
new: DataOwner3的最优x_3 = 0.1380
new: DataOwner4的最优x_4 = 0.1510
new: DataOwner5的最优x_5 = 0.1756
new: DataOwner6的最优x_6 = 0.2238
new: DataOwner7的最优x_7 = 0.1354
new: DataOwner8的最优x_8 = 0.1808
new: DataOwner9的最优x_9 = 0.1461
new: DataOwner10的最优x_10 = 0.1005
eta:1.7
new: DataOwner1的最优x_1 = 0.1504
new: DataOwner2的最优x_2 = 0.1098
new: DataOwner3的最优x_3 = 0.1388
new: DataOwner4的最优x_4 = 0.1519
new: DataOwner5的最优x_5 = 0.1766
new: DataOwner6的最优x_6 = 0.2251
new: DataOwner7的最优x_7 = 0.1362
new: DataOwner8的最优x_8 = 0.1818
new: DataOwner9的最优x_9 = 0.1470
new: DataOwner10的最优x_10 = 0.1011
eta:1.71
new: DataOwner1的最优x_1 = 0.1513
new: DataOwner2的最优x_2 = 0.1104
new: DataOwner3的最优x_3 = 0.1397
new: DataOwner4的最优x_4 = 0.1528
new: DataOwner5的最优x_5 = 0.1777
new: DataOwner6的最优x_6 = 0.2265
new: DataOwner7的最优x_7 = 0.1370
new: DataOwner8的最优x_8 = 0.1829
new: DataOwner9的最优x_9 = 0.1478
new: DataOwner10的最优x_10 = 0.1017
eta:1.72
new: DataOwner1的最优x_1 = 0.1522
new: DataOwner2的最优x_2 = 0.1111
new: DataOwner3的最优x_3 = 0.1405
new: DataOwner4的最优x_4 = 0.1537
new: DataOwner5的最优x_5 = 0.1787
new: DataOwner6的最优x_6 = 0.2278
new: DataOwner7的最优x_7 = 0.1378
new: DataOwner8的最优x_8 = 0.1840
new: DataOwner9的最优x_9 = 0.1487
new: DataOwner10的最优x_10 = 0.1022
eta:1.73
new: DataOwner1的最优x_1 = 0.1530
new: DataOwner2的最优x_2 = 0.1117
new: DataOwner3的最优x_3 = 0.1413
new: DataOwner4的最优x_4 = 0.1546
new: DataOwner5的最优x_5 = 0.1798
new: DataOwner6的最优x_6 = 0.2291
new: DataOwner7的最优x_7 = 0.1386
new: DataOwner8的最优x_8 = 0.1850
new: DataOwner9的最优x_9 = 0.1496
new: DataOwner10的最优x_10 = 0.1028
eta:1.74
new: DataOwner1的最优x_1 = 0.1539
new: DataOwner2的最优x_2 = 0.1124
new: DataOwner3的最优x_3 = 0.1421
new: DataOwner4的最优x_4 = 0.1555
new: DataOwner5的最优x_5 = 0.1808
new: DataOwner6的最优x_6 = 0.2304
new: DataOwner7的最优x_7 = 0.1394
new: DataOwner8的最优x_8 = 0.1861
new: DataOwner9的最优x_9 = 0.1504
new: DataOwner10的最优x_10 = 0.1034
eta:1.75
new: DataOwner1的最优x_1 = 0.1548
new: DataOwner2的最优x_2 = 0.1130
new: DataOwner3的最优x_3 = 0.1429
new: DataOwner4的最优x_4 = 0.1563
new: DataOwner5的最优x_5 = 0.1818
new: DataOwner6的最优x_6 = 0.2318
new: DataOwner7的最优x_7 = 0.1402
new: DataOwner8的最优x_8 = 0.1872
new: DataOwner9的最优x_9 = 0.1513
new: DataOwner10的最优x_10 = 0.1040
eta:1.76
new: DataOwner1的最优x_1 = 0.1557
new: DataOwner2的最优x_2 = 0.1137
new: DataOwner3的最优x_3 = 0.1437
new: DataOwner4的最优x_4 = 0.1572
new: DataOwner5的最优x_5 = 0.1829
new: DataOwner6的最优x_6 = 0.2331
new: DataOwner7的最优x_7 = 0.1410
new: DataOwner8的最优x_8 = 0.1883
new: DataOwner9的最优x_9 = 0.1522
new: DataOwner10的最优x_10 = 0.1046
eta:1.77
new: DataOwner1的最优x_1 = 0.1566
new: DataOwner2的最优x_2 = 0.1143
new: DataOwner3的最优x_3 = 0.1446
new: DataOwner4的最优x_4 = 0.1581
new: DataOwner5的最优x_5 = 0.1839
new: DataOwner6的最优x_6 = 0.2344
new: DataOwner7的最优x_7 = 0.1418
new: DataOwner8的最优x_8 = 0.1893
new: DataOwner9的最优x_9 = 0.1530
new: DataOwner10的最优x_10 = 0.1052
eta:1.78
new: DataOwner1的最优x_1 = 0.1575
new: DataOwner2的最优x_2 = 0.1149
new: DataOwner3的最优x_3 = 0.1454
new: DataOwner4的最优x_4 = 0.1590
new: DataOwner5的最优x_5 = 0.1849
new: DataOwner6的最优x_6 = 0.2357
new: DataOwner7的最优x_7 = 0.1426
new: DataOwner8的最优x_8 = 0.1904
new: DataOwner9的最优x_9 = 0.1539
new: DataOwner10的最优x_10 = 0.1058
eta:1.79
new: DataOwner1的最优x_1 = 0.1583
new: DataOwner2的最优x_2 = 0.1156
new: DataOwner3的最优x_3 = 0.1462
new: DataOwner4的最优x_4 = 0.1599
new: DataOwner5的最优x_5 = 0.1860
new: DataOwner6的最优x_6 = 0.2371
new: DataOwner7的最优x_7 = 0.1435
new: DataOwner8的最优x_8 = 0.1915
new: DataOwner9的最优x_9 = 0.1548
new: DataOwner10的最优x_10 = 0.1064
eta:1.8
new: DataOwner1的最优x_1 = 0.1592
new: DataOwner2的最优x_2 = 0.1162
new: DataOwner3的最优x_3 = 0.1470
new: DataOwner4的最优x_4 = 0.1608
new: DataOwner5的最优x_5 = 0.1870
new: DataOwner6的最优x_6 = 0.2384
new: DataOwner7的最优x_7 = 0.1443
new: DataOwner8的最优x_8 = 0.1925
new: DataOwner9的最优x_9 = 0.1556
new: DataOwner10的最优x_10 = 0.1070
eta:1.81
new: DataOwner1的最优x_1 = 0.1601
new: DataOwner2的最优x_2 = 0.1169
new: DataOwner3的最优x_3 = 0.1478
new: DataOwner4的最优x_4 = 0.1617
new: DataOwner5的最优x_5 = 0.1881
new: DataOwner6的最优x_6 = 0.2397
new: DataOwner7的最优x_7 = 0.1451
new: DataOwner8的最优x_8 = 0.1936
new: DataOwner9的最优x_9 = 0.1565
new: DataOwner10的最优x_10 = 0.1076
eta:1.82
new: DataOwner1的最优x_1 = 0.1610
new: DataOwner2的最优x_2 = 0.1175
new: DataOwner3的最优x_3 = 0.1486
new: DataOwner4的最优x_4 = 0.1626
new: DataOwner5的最优x_5 = 0.1891
new: DataOwner6的最优x_6 = 0.2410
new: DataOwner7的最优x_7 = 0.1459
new: DataOwner8的最优x_8 = 0.1947
new: DataOwner9的最优x_9 = 0.1573
new: DataOwner10的最优x_10 = 0.1082
eta:1.83
new: DataOwner1的最优x_1 = 0.1619
new: DataOwner2的最优x_2 = 0.1182
new: DataOwner3的最优x_3 = 0.1495
new: DataOwner4的最优x_4 = 0.1635
new: DataOwner5的最优x_5 = 0.1901
new: DataOwner6的最优x_6 = 0.2424
new: DataOwner7的最优x_7 = 0.1467
new: DataOwner8的最优x_8 = 0.1957
new: DataOwner9的最优x_9 = 0.1582
new: DataOwner10的最优x_10 = 0.1088
eta:1.84
new: DataOwner1的最优x_1 = 0.1628
new: DataOwner2的最优x_2 = 0.1188
new: DataOwner3的最优x_3 = 0.1503
new: DataOwner4的最优x_4 = 0.1644
new: DataOwner5的最优x_5 = 0.1912
new: DataOwner6的最优x_6 = 0.2437
new: DataOwner7的最优x_7 = 0.1475
new: DataOwner8的最优x_8 = 0.1968
new: DataOwner9的最优x_9 = 0.1591
new: DataOwner10的最优x_10 = 0.1094
eta:1.85
new: DataOwner1的最优x_1 = 0.1637
new: DataOwner2的最优x_2 = 0.1195
new: DataOwner3的最优x_3 = 0.1511
new: DataOwner4的最优x_4 = 0.1653
new: DataOwner5的最优x_5 = 0.1922
new: DataOwner6的最优x_6 = 0.2450
new: DataOwner7的最优x_7 = 0.1483
new: DataOwner8的最优x_8 = 0.1979
new: DataOwner9的最优x_9 = 0.1599
new: DataOwner10的最优x_10 = 0.1100
eta:1.86
new: DataOwner1的最优x_1 = 0.1645
new: DataOwner2的最优x_2 = 0.1201
new: DataOwner3的最优x_3 = 0.1519
new: DataOwner4的最优x_4 = 0.1662
new: DataOwner5的最优x_5 = 0.1933
new: DataOwner6的最优x_6 = 0.2463
new: DataOwner7的最优x_7 = 0.1491
new: DataOwner8的最优x_8 = 0.1989
new: DataOwner9的最优x_9 = 0.1608
new: DataOwner10的最优x_10 = 0.1106
eta:1.87
new: DataOwner1的最优x_1 = 0.1654
new: DataOwner2的最优x_2 = 0.1208
new: DataOwner3的最优x_3 = 0.1527
new: DataOwner4的最优x_4 = 0.1671
new: DataOwner5的最优x_5 = 0.1943
new: DataOwner6的最优x_6 = 0.2477
new: DataOwner7的最优x_7 = 0.1499
new: DataOwner8的最优x_8 = 0.2000
new: DataOwner9的最优x_9 = 0.1617
new: DataOwner10的最优x_10 = 0.1112
eta:1.8800000000000001
new: DataOwner1的最优x_1 = 0.1663
new: DataOwner2的最优x_2 = 0.1214
new: DataOwner3的最优x_3 = 0.1535
new: DataOwner4的最优x_4 = 0.1680
new: DataOwner5的最优x_5 = 0.1953
new: DataOwner6的最优x_6 = 0.2490
new: DataOwner7的最优x_7 = 0.1507
new: DataOwner8的最优x_8 = 0.2011
new: DataOwner9的最优x_9 = 0.1625
new: DataOwner10的最优x_10 = 0.1118
eta:1.8900000000000001
new: DataOwner1的最优x_1 = 0.1672
new: DataOwner2的最优x_2 = 0.1221
new: DataOwner3的最优x_3 = 0.1544
new: DataOwner4的最优x_4 = 0.1689
new: DataOwner5的最优x_5 = 0.1964
new: DataOwner6的最优x_6 = 0.2503
new: DataOwner7的最优x_7 = 0.1515
new: DataOwner8的最优x_8 = 0.2022
new: DataOwner9的最优x_9 = 0.1634
new: DataOwner10的最优x_10 = 0.1124
eta:1.9000000000000001
new: DataOwner1的最优x_1 = 0.1681
new: DataOwner2的最优x_2 = 0.1227
new: DataOwner3的最优x_3 = 0.1552
new: DataOwner4的最优x_4 = 0.1697
new: DataOwner5的最优x_5 = 0.1974
new: DataOwner6的最优x_6 = 0.2516
new: DataOwner7的最优x_7 = 0.1523
new: DataOwner8的最优x_8 = 0.2032
new: DataOwner9的最优x_9 = 0.1643
new: DataOwner10的最优x_10 = 0.1129
eta:1.9100000000000001
new: DataOwner1的最优x_1 = 0.1690
new: DataOwner2的最优x_2 = 0.1233
new: DataOwner3的最优x_3 = 0.1560
new: DataOwner4的最优x_4 = 0.1706
new: DataOwner5的最优x_5 = 0.1985
new: DataOwner6的最优x_6 = 0.2530
new: DataOwner7的最优x_7 = 0.1531
new: DataOwner8的最优x_8 = 0.2043
new: DataOwner9的最优x_9 = 0.1651
new: DataOwner10的最优x_10 = 0.1135
eta:1.9200000000000002
new: DataOwner1的最优x_1 = 0.1698
new: DataOwner2的最优x_2 = 0.1240
new: DataOwner3的最优x_3 = 0.1568
new: DataOwner4的最优x_4 = 0.1715
new: DataOwner5的最优x_5 = 0.1995
new: DataOwner6的最优x_6 = 0.2543
new: DataOwner7的最优x_7 = 0.1539
new: DataOwner8的最优x_8 = 0.2054
new: DataOwner9的最优x_9 = 0.1660
new: DataOwner10的最优x_10 = 0.1141
eta:1.93
new: DataOwner1的最优x_1 = 0.1707
new: DataOwner2的最优x_2 = 0.1246
new: DataOwner3的最优x_3 = 0.1576
new: DataOwner4的最优x_4 = 0.1724
new: DataOwner5的最优x_5 = 0.2005
new: DataOwner6的最优x_6 = 0.2556
new: DataOwner7的最优x_7 = 0.1547
new: DataOwner8的最优x_8 = 0.2064
new: DataOwner9的最优x_9 = 0.1669
new: DataOwner10的最优x_10 = 0.1147
eta:1.94
new: DataOwner1的最优x_1 = 0.1716
new: DataOwner2的最优x_2 = 0.1253
new: DataOwner3的最优x_3 = 0.1584
new: DataOwner4的最优x_4 = 0.1733
new: DataOwner5的最优x_5 = 0.2016
new: DataOwner6的最优x_6 = 0.2569
new: DataOwner7的最优x_7 = 0.1555
new: DataOwner8的最优x_8 = 0.2075
new: DataOwner9的最优x_9 = 0.1677
new: DataOwner10的最优x_10 = 0.1153
eta:1.95
new: DataOwner1的最优x_1 = 0.1725
new: DataOwner2的最优x_2 = 0.1259
new: DataOwner3的最优x_3 = 0.1593
new: DataOwner4的最优x_4 = 0.1742
new: DataOwner5的最优x_5 = 0.2026
new: DataOwner6的最优x_6 = 0.2583
new: DataOwner7的最优x_7 = 0.1563
new: DataOwner8的最优x_8 = 0.2086
new: DataOwner9的最优x_9 = 0.1686
new: DataOwner10的最优x_10 = 0.1159
eta:1.96
new: DataOwner1的最优x_1 = 0.1734
new: DataOwner2的最优x_2 = 0.1266
new: DataOwner3的最优x_3 = 0.1601
new: DataOwner4的最优x_4 = 0.1751
new: DataOwner5的最优x_5 = 0.2037
new: DataOwner6的最优x_6 = 0.2596
new: DataOwner7的最优x_7 = 0.1571
new: DataOwner8的最优x_8 = 0.2096
new: DataOwner9的最优x_9 = 0.1695
new: DataOwner10的最优x_10 = 0.1165
eta:1.97
new: DataOwner1的最优x_1 = 0.1743
new: DataOwner2的最优x_2 = 0.1272
new: DataOwner3的最优x_3 = 0.1609
new: DataOwner4的最优x_4 = 0.1760
new: DataOwner5的最优x_5 = 0.2047
new: DataOwner6的最优x_6 = 0.2609
new: DataOwner7的最优x_7 = 0.1579
new: DataOwner8的最优x_8 = 0.2107
new: DataOwner9的最优x_9 = 0.1703
new: DataOwner10的最优x_10 = 0.1171
eta:1.98
new: DataOwner1的最优x_1 = 0.1752
new: DataOwner2的最优x_2 = 0.1279
new: DataOwner3的最优x_3 = 0.1617
new: DataOwner4的最优x_4 = 0.1769
new: DataOwner5的最优x_5 = 0.2057
new: DataOwner6的最优x_6 = 0.2622
new: DataOwner7的最优x_7 = 0.1587
new: DataOwner8的最优x_8 = 0.2118
new: DataOwner9的最优x_9 = 0.1712
new: DataOwner10的最优x_10 = 0.1177
eta:1.99
new: DataOwner1的最优x_1 = 0.1760
new: DataOwner2的最优x_2 = 0.1285
new: DataOwner3的最优x_3 = 0.1625
new: DataOwner4的最优x_4 = 0.1778
new: DataOwner5的最优x_5 = 0.2068
new: DataOwner6的最优x_6 = 0.2636
new: DataOwner7的最优x_7 = 0.1595
new: DataOwner8的最优x_8 = 0.2129
new: DataOwner9的最优x_9 = 0.1720
new: DataOwner10的最优x_10 = 0.1183
eta:2.0
new: DataOwner1的最优x_1 = 0.1769
new: DataOwner2的最优x_2 = 0.1292
new: DataOwner3的最优x_3 = 0.1633
new: DataOwner4的最优x_4 = 0.1787
new: DataOwner5的最优x_5 = 0.2078
new: DataOwner6的最优x_6 = 0.2649
new: DataOwner7的最优x_7 = 0.1603
new: DataOwner8的最优x_8 = 0.2139
new: DataOwner9的最优x_9 = 0.1729
new: DataOwner10的最优x_10 = 0.1189
eta:2.01
new: DataOwner1的最优x_1 = 0.1778
new: DataOwner2的最优x_2 = 0.1298
new: DataOwner3的最优x_3 = 0.1642
new: DataOwner4的最优x_4 = 0.1796
new: DataOwner5的最优x_5 = 0.2088
new: DataOwner6的最优x_6 = 0.2662
new: DataOwner7的最优x_7 = 0.1611
new: DataOwner8的最优x_8 = 0.2150
new: DataOwner9的最优x_9 = 0.1738
new: DataOwner10的最优x_10 = 0.1195
eta:2.02
new: DataOwner1的最优x_1 = 0.1787
new: DataOwner2的最优x_2 = 0.1304
new: DataOwner3的最优x_3 = 0.1650
new: DataOwner4的最优x_4 = 0.1805
new: DataOwner5的最优x_5 = 0.2099
new: DataOwner6的最优x_6 = 0.2675
new: DataOwner7的最优x_7 = 0.1619
new: DataOwner8的最优x_8 = 0.2161
new: DataOwner9的最优x_9 = 0.1746
new: DataOwner10的最优x_10 = 0.1201
eta:2.03
new: DataOwner1的最优x_1 = 0.1796
new: DataOwner2的最优x_2 = 0.1311
new: DataOwner3的最优x_3 = 0.1658
new: DataOwner4的最优x_4 = 0.1814
new: DataOwner5的最优x_5 = 0.2109
new: DataOwner6的最优x_6 = 0.2689
new: DataOwner7的最优x_7 = 0.1627
new: DataOwner8的最优x_8 = 0.2171
new: DataOwner9的最优x_9 = 0.1755
new: DataOwner10的最优x_10 = 0.1207
eta:2.04
new: DataOwner1的最优x_1 = 0.1805
new: DataOwner2的最优x_2 = 0.1317
new: DataOwner3的最优x_3 = 0.1666
new: DataOwner4的最优x_4 = 0.1823
new: DataOwner5的最优x_5 = 0.2120
new: DataOwner6的最优x_6 = 0.2702
new: DataOwner7的最优x_7 = 0.1635
new: DataOwner8的最优x_8 = 0.2182
new: DataOwner9的最优x_9 = 0.1764
new: DataOwner10的最优x_10 = 0.1213
eta:2.05
new: DataOwner1的最优x_1 = 0.1813
new: DataOwner2的最优x_2 = 0.1324
new: DataOwner3的最优x_3 = 0.1674
new: DataOwner4的最优x_4 = 0.1831
new: DataOwner5的最优x_5 = 0.2130
new: DataOwner6的最优x_6 = 0.2715
new: DataOwner7的最优x_7 = 0.1643
new: DataOwner8的最优x_8 = 0.2193
new: DataOwner9的最优x_9 = 0.1772
new: DataOwner10的最优x_10 = 0.1219
eta:2.0599999999999996
new: DataOwner1的最优x_1 = 0.1822
new: DataOwner2的最优x_2 = 0.1330
new: DataOwner3的最优x_3 = 0.1682
new: DataOwner4的最优x_4 = 0.1840
new: DataOwner5的最优x_5 = 0.2140
new: DataOwner6的最优x_6 = 0.2728
new: DataOwner7的最优x_7 = 0.1651
new: DataOwner8的最优x_8 = 0.2203
new: DataOwner9的最优x_9 = 0.1781
new: DataOwner10的最优x_10 = 0.1225
eta:2.07
new: DataOwner1的最优x_1 = 0.1831
new: DataOwner2的最优x_2 = 0.1337
new: DataOwner3的最优x_3 = 0.1691
new: DataOwner4的最优x_4 = 0.1849
new: DataOwner5的最优x_5 = 0.2151
new: DataOwner6的最优x_6 = 0.2741
new: DataOwner7的最优x_7 = 0.1659
new: DataOwner8的最优x_8 = 0.2214
new: DataOwner9的最优x_9 = 0.1790
new: DataOwner10的最优x_10 = 0.1231
eta:2.0799999999999996
new: DataOwner1的最优x_1 = 0.1840
new: DataOwner2的最优x_2 = 0.1343
new: DataOwner3的最优x_3 = 0.1699
new: DataOwner4的最优x_4 = 0.1858
new: DataOwner5的最优x_5 = 0.2161
new: DataOwner6的最优x_6 = 0.2755
new: DataOwner7的最优x_7 = 0.1667
new: DataOwner8的最优x_8 = 0.2225
new: DataOwner9的最优x_9 = 0.1798
new: DataOwner10的最优x_10 = 0.1236
eta:2.09
new: DataOwner1的最优x_1 = 0.1849
new: DataOwner2的最优x_2 = 0.1350
new: DataOwner3的最优x_3 = 0.1707
new: DataOwner4的最优x_4 = 0.1867
new: DataOwner5的最优x_5 = 0.2172
new: DataOwner6的最优x_6 = 0.2768
new: DataOwner7的最优x_7 = 0.1675
new: DataOwner8的最优x_8 = 0.2235
new: DataOwner9的最优x_9 = 0.1807
new: DataOwner10的最优x_10 = 0.1242
eta:2.0999999999999996
new: DataOwner1的最优x_1 = 0.1858
new: DataOwner2的最优x_2 = 0.1356
new: DataOwner3的最优x_3 = 0.1715
new: DataOwner4的最优x_4 = 0.1876
new: DataOwner5的最优x_5 = 0.2182
new: DataOwner6的最优x_6 = 0.2781
new: DataOwner7的最优x_7 = 0.1683
new: DataOwner8的最优x_8 = 0.2246
new: DataOwner9的最优x_9 = 0.1816
new: DataOwner10的最优x_10 = 0.1248
eta:2.11
new: DataOwner1的最优x_1 = 0.1867
new: DataOwner2的最优x_2 = 0.1363
new: DataOwner3的最优x_3 = 0.1723
new: DataOwner4的最优x_4 = 0.1885
new: DataOwner5的最优x_5 = 0.2192
new: DataOwner6的最优x_6 = 0.2794
new: DataOwner7的最优x_7 = 0.1691
new: DataOwner8的最优x_8 = 0.2257
new: DataOwner9的最优x_9 = 0.1824
new: DataOwner10的最优x_10 = 0.1254
eta:2.1199999999999997
new: DataOwner1的最优x_1 = 0.1875
new: DataOwner2的最优x_2 = 0.1369
new: DataOwner3的最优x_3 = 0.1731
new: DataOwner4的最优x_4 = 0.1894
new: DataOwner5的最优x_5 = 0.2203
new: DataOwner6的最优x_6 = 0.2808
new: DataOwner7的最优x_7 = 0.1699
new: DataOwner8的最优x_8 = 0.2268
new: DataOwner9的最优x_9 = 0.1833
new: DataOwner10的最优x_10 = 0.1260
eta:2.13
new: DataOwner1的最优x_1 = 0.1884
new: DataOwner2的最优x_2 = 0.1375
new: DataOwner3的最优x_3 = 0.1740
new: DataOwner4的最优x_4 = 0.1903
new: DataOwner5的最优x_5 = 0.2213
new: DataOwner6的最优x_6 = 0.2821
new: DataOwner7的最优x_7 = 0.1707
new: DataOwner8的最优x_8 = 0.2278
new: DataOwner9的最优x_9 = 0.1842
new: DataOwner10的最优x_10 = 0.1266
eta:2.1399999999999997
new: DataOwner1的最优x_1 = 0.1893
new: DataOwner2的最优x_2 = 0.1382
new: DataOwner3的最优x_3 = 0.1748
new: DataOwner4的最优x_4 = 0.1912
new: DataOwner5的最优x_5 = 0.2224
new: DataOwner6的最优x_6 = 0.2834
new: DataOwner7的最优x_7 = 0.1715
new: DataOwner8的最优x_8 = 0.2289
new: DataOwner9的最优x_9 = 0.1850
new: DataOwner10的最优x_10 = 0.1272
eta:2.15
new: DataOwner1的最优x_1 = 0.1902
new: DataOwner2的最优x_2 = 0.1388
new: DataOwner3的最优x_3 = 0.1756
new: DataOwner4的最优x_4 = 0.1921
new: DataOwner5的最优x_5 = 0.2234
new: DataOwner6的最优x_6 = 0.2847
new: DataOwner7的最优x_7 = 0.1723
new: DataOwner8的最优x_8 = 0.2300
new: DataOwner9的最优x_9 = 0.1859
new: DataOwner10的最优x_10 = 0.1278
eta:2.1599999999999997
new: DataOwner1的最优x_1 = 0.1911
new: DataOwner2的最优x_2 = 0.1395
new: DataOwner3的最优x_3 = 0.1764
new: DataOwner4的最优x_4 = 0.1930
new: DataOwner5的最优x_5 = 0.2244
new: DataOwner6的最优x_6 = 0.2861
new: DataOwner7的最优x_7 = 0.1731
new: DataOwner8的最优x_8 = 0.2310
new: DataOwner9的最优x_9 = 0.1867
new: DataOwner10的最优x_10 = 0.1284
eta:2.17
new: DataOwner1的最优x_1 = 0.1920
new: DataOwner2的最优x_2 = 0.1401
new: DataOwner3的最优x_3 = 0.1772
new: DataOwner4的最优x_4 = 0.1939
new: DataOwner5的最优x_5 = 0.2255
new: DataOwner6的最优x_6 = 0.2874
new: DataOwner7的最优x_7 = 0.1739
new: DataOwner8的最优x_8 = 0.2321
new: DataOwner9的最优x_9 = 0.1876
new: DataOwner10的最优x_10 = 0.1290
eta:2.1799999999999997
new: DataOwner1的最优x_1 = 0.1928
new: DataOwner2的最优x_2 = 0.1408
new: DataOwner3的最优x_3 = 0.1780
new: DataOwner4的最优x_4 = 0.1948
new: DataOwner5的最优x_5 = 0.2265
new: DataOwner6的最优x_6 = 0.2887
new: DataOwner7的最优x_7 = 0.1747
new: DataOwner8的最优x_8 = 0.2332
new: DataOwner9的最优x_9 = 0.1885
new: DataOwner10的最优x_10 = 0.1296
eta:2.19
new: DataOwner1的最优x_1 = 0.1937
new: DataOwner2的最优x_2 = 0.1414
new: DataOwner3的最优x_3 = 0.1789
new: DataOwner4的最优x_4 = 0.1957
new: DataOwner5的最优x_5 = 0.2275
new: DataOwner6的最优x_6 = 0.2900
new: DataOwner7的最优x_7 = 0.1755
new: DataOwner8的最优x_8 = 0.2342
new: DataOwner9的最优x_9 = 0.1893
new: DataOwner10的最优x_10 = 0.1302
eta:2.1999999999999997
new: DataOwner1的最优x_1 = 0.1946
new: DataOwner2的最优x_2 = 0.1421
new: DataOwner3的最优x_3 = 0.1797
new: DataOwner4的最优x_4 = 0.1965
new: DataOwner5的最优x_5 = 0.2286
new: DataOwner6的最优x_6 = 0.2914
new: DataOwner7的最优x_7 = 0.1763
new: DataOwner8的最优x_8 = 0.2353
new: DataOwner9的最优x_9 = 0.1902
new: DataOwner10的最优x_10 = 0.1308
eta:2.21
new: DataOwner1的最优x_1 = 0.1955
new: DataOwner2的最优x_2 = 0.1427
new: DataOwner3的最优x_3 = 0.1805
new: DataOwner4的最优x_4 = 0.1974
new: DataOwner5的最优x_5 = 0.2296
new: DataOwner6的最优x_6 = 0.2927
new: DataOwner7的最优x_7 = 0.1771
new: DataOwner8的最优x_8 = 0.2364
new: DataOwner9的最优x_9 = 0.1911
new: DataOwner10的最优x_10 = 0.1314
eta:2.2199999999999998
new: DataOwner1的最优x_1 = 0.1964
new: DataOwner2的最优x_2 = 0.1434
new: DataOwner3的最优x_3 = 0.1813
new: DataOwner4的最优x_4 = 0.1983
new: DataOwner5的最优x_5 = 0.2307
new: DataOwner6的最优x_6 = 0.2940
new: DataOwner7的最优x_7 = 0.1779
new: DataOwner8的最优x_8 = 0.2375
new: DataOwner9的最优x_9 = 0.1919
new: DataOwner10的最优x_10 = 0.1320
eta:2.23
new: DataOwner1的最优x_1 = 0.1973
new: DataOwner2的最优x_2 = 0.1440
new: DataOwner3的最优x_3 = 0.1821
new: DataOwner4的最优x_4 = 0.1992
new: DataOwner5的最优x_5 = 0.2317
new: DataOwner6的最优x_6 = 0.2953
new: DataOwner7的最优x_7 = 0.1787
new: DataOwner8的最优x_8 = 0.2385
new: DataOwner9的最优x_9 = 0.1928
new: DataOwner10的最优x_10 = 0.1326
eta:2.2399999999999998
new: DataOwner1的最优x_1 = 0.1982
new: DataOwner2的最优x_2 = 0.1447
new: DataOwner3的最优x_3 = 0.1829
new: DataOwner4的最优x_4 = 0.2001
new: DataOwner5的最优x_5 = 0.2327
new: DataOwner6的最优x_6 = 0.2967
new: DataOwner7的最优x_7 = 0.1795
new: DataOwner8的最优x_8 = 0.2396
new: DataOwner9的最优x_9 = 0.1937
new: DataOwner10的最优x_10 = 0.1332
eta:2.25
new: DataOwner1的最优x_1 = 0.1990
new: DataOwner2的最优x_2 = 0.1453
new: DataOwner3的最优x_3 = 0.1838
new: DataOwner4的最优x_4 = 0.2010
new: DataOwner5的最优x_5 = 0.2338
new: DataOwner6的最优x_6 = 0.2980
new: DataOwner7的最优x_7 = 0.1803
new: DataOwner8的最优x_8 = 0.2407
new: DataOwner9的最优x_9 = 0.1945
new: DataOwner10的最优x_10 = 0.1338
eta:2.26
new: DataOwner1的最优x_1 = 0.1999
new: DataOwner2的最优x_2 = 0.1459
new: DataOwner3的最优x_3 = 0.1846
new: DataOwner4的最优x_4 = 0.2019
new: DataOwner5的最优x_5 = 0.2348
new: DataOwner6的最优x_6 = 0.2993
new: DataOwner7的最优x_7 = 0.1811
new: DataOwner8的最优x_8 = 0.2417
new: DataOwner9的最优x_9 = 0.1954
new: DataOwner10的最优x_10 = 0.1343
eta:2.27
new: DataOwner1的最优x_1 = 0.2008
new: DataOwner2的最优x_2 = 0.1466
new: DataOwner3的最优x_3 = 0.1854
new: DataOwner4的最优x_4 = 0.2028
new: DataOwner5的最优x_5 = 0.2359
new: DataOwner6的最优x_6 = 0.3006
new: DataOwner7的最优x_7 = 0.1819
new: DataOwner8的最优x_8 = 0.2428
new: DataOwner9的最优x_9 = 0.1963
new: DataOwner10的最优x_10 = 0.1349
eta:2.28
new: DataOwner1的最优x_1 = 0.2017
new: DataOwner2的最优x_2 = 0.1472
new: DataOwner3的最优x_3 = 0.1862
new: DataOwner4的最优x_4 = 0.2037
new: DataOwner5的最优x_5 = 0.2369
new: DataOwner6的最优x_6 = 0.3020
new: DataOwner7的最优x_7 = 0.1827
new: DataOwner8的最优x_8 = 0.2439
new: DataOwner9的最优x_9 = 0.1971
new: DataOwner10的最优x_10 = 0.1355
eta:2.29
new: DataOwner1的最优x_1 = 0.2026
new: DataOwner2的最优x_2 = 0.1479
new: DataOwner3的最优x_3 = 0.1870
new: DataOwner4的最优x_4 = 0.2046
new: DataOwner5的最优x_5 = 0.2379
new: DataOwner6的最优x_6 = 0.3033
new: DataOwner7的最优x_7 = 0.1835
new: DataOwner8的最优x_8 = 0.2449
new: DataOwner9的最优x_9 = 0.1980
new: DataOwner10的最优x_10 = 0.1361
eta:2.3
new: DataOwner1的最优x_1 = 0.2035
new: DataOwner2的最优x_2 = 0.1485
new: DataOwner3的最优x_3 = 0.1878
new: DataOwner4的最优x_4 = 0.2055
new: DataOwner5的最优x_5 = 0.2390
new: DataOwner6的最优x_6 = 0.3046
new: DataOwner7的最优x_7 = 0.1843
new: DataOwner8的最优x_8 = 0.2460
new: DataOwner9的最优x_9 = 0.1988
new: DataOwner10的最优x_10 = 0.1367
eta:2.31
new: DataOwner1的最优x_1 = 0.2043
new: DataOwner2的最优x_2 = 0.1492
new: DataOwner3的最优x_3 = 0.1887
new: DataOwner4的最优x_4 = 0.2064
new: DataOwner5的最优x_5 = 0.2400
new: DataOwner6的最优x_6 = 0.3059
new: DataOwner7的最优x_7 = 0.1851
new: DataOwner8的最优x_8 = 0.2471
new: DataOwner9的最优x_9 = 0.1997
new: DataOwner10的最优x_10 = 0.1373
eta:2.32
new: DataOwner1的最优x_1 = 0.2052
new: DataOwner2的最优x_2 = 0.1498
new: DataOwner3的最优x_3 = 0.1895
new: DataOwner4的最优x_4 = 0.2073
new: DataOwner5的最优x_5 = 0.2411
new: DataOwner6的最优x_6 = 0.3073
new: DataOwner7的最优x_7 = 0.1859
new: DataOwner8的最优x_8 = 0.2482
new: DataOwner9的最优x_9 = 0.2006
new: DataOwner10的最优x_10 = 0.1379
eta:2.3299999999999996
new: DataOwner1的最优x_1 = 0.2061
new: DataOwner2的最优x_2 = 0.1505
new: DataOwner3的最优x_3 = 0.1903
new: DataOwner4的最优x_4 = 0.2082
new: DataOwner5的最优x_5 = 0.2421
new: DataOwner6的最优x_6 = 0.3086
new: DataOwner7的最优x_7 = 0.1867
new: DataOwner8的最优x_8 = 0.2492
new: DataOwner9的最优x_9 = 0.2014
new: DataOwner10的最优x_10 = 0.1385
eta:2.34
new: DataOwner1的最优x_1 = 0.2070
new: DataOwner2的最优x_2 = 0.1511
new: DataOwner3的最优x_3 = 0.1911
new: DataOwner4的最优x_4 = 0.2091
new: DataOwner5的最优x_5 = 0.2431
new: DataOwner6的最优x_6 = 0.3099
new: DataOwner7的最优x_7 = 0.1875
new: DataOwner8的最优x_8 = 0.2503
new: DataOwner9的最优x_9 = 0.2023
new: DataOwner10的最优x_10 = 0.1391
eta:2.3499999999999996
new: DataOwner1的最优x_1 = 0.2079
new: DataOwner2的最优x_2 = 0.1518
new: DataOwner3的最优x_3 = 0.1919
new: DataOwner4的最优x_4 = 0.2099
new: DataOwner5的最优x_5 = 0.2442
new: DataOwner6的最优x_6 = 0.3112
new: DataOwner7的最优x_7 = 0.1883
new: DataOwner8的最优x_8 = 0.2514
new: DataOwner9的最优x_9 = 0.2032
new: DataOwner10的最优x_10 = 0.1397
eta:2.36
new: DataOwner1的最优x_1 = 0.2088
new: DataOwner2的最优x_2 = 0.1524
new: DataOwner3的最优x_3 = 0.1927
new: DataOwner4的最优x_4 = 0.2108
new: DataOwner5的最优x_5 = 0.2452
new: DataOwner6的最优x_6 = 0.3126
new: DataOwner7的最优x_7 = 0.1891
new: DataOwner8的最优x_8 = 0.2524
new: DataOwner9的最优x_9 = 0.2040
new: DataOwner10的最优x_10 = 0.1403
eta:2.3699999999999997
new: DataOwner1的最优x_1 = 0.2097
new: DataOwner2的最优x_2 = 0.1530
new: DataOwner3的最优x_3 = 0.1936
new: DataOwner4的最优x_4 = 0.2117
new: DataOwner5的最优x_5 = 0.2463
new: DataOwner6的最优x_6 = 0.3139
new: DataOwner7的最优x_7 = 0.1899
new: DataOwner8的最优x_8 = 0.2535
new: DataOwner9的最优x_9 = 0.2049
new: DataOwner10的最优x_10 = 0.1409
eta:2.38
new: DataOwner1的最优x_1 = 0.2105
new: DataOwner2的最优x_2 = 0.1537
new: DataOwner3的最优x_3 = 0.1944
new: DataOwner4的最优x_4 = 0.2126
new: DataOwner5的最优x_5 = 0.2473
new: DataOwner6的最优x_6 = 0.3152
new: DataOwner7的最优x_7 = 0.1907
new: DataOwner8的最优x_8 = 0.2546
new: DataOwner9的最优x_9 = 0.2058
new: DataOwner10的最优x_10 = 0.1415
eta:2.3899999999999997
new: DataOwner1的最优x_1 = 0.2114
new: DataOwner2的最优x_2 = 0.1543
new: DataOwner3的最优x_3 = 0.1952
new: DataOwner4的最优x_4 = 0.2135
new: DataOwner5的最优x_5 = 0.2483
new: DataOwner6的最优x_6 = 0.3165
new: DataOwner7的最优x_7 = 0.1915
new: DataOwner8的最优x_8 = 0.2556
new: DataOwner9的最优x_9 = 0.2066
new: DataOwner10的最优x_10 = 0.1421
eta:2.4
new: DataOwner1的最优x_1 = 0.2123
new: DataOwner2的最优x_2 = 0.1550
new: DataOwner3的最优x_3 = 0.1960
new: DataOwner4的最优x_4 = 0.2144
new: DataOwner5的最优x_5 = 0.2494
new: DataOwner6的最优x_6 = 0.3179
new: DataOwner7的最优x_7 = 0.1923
new: DataOwner8的最优x_8 = 0.2567
new: DataOwner9的最优x_9 = 0.2075
new: DataOwner10的最优x_10 = 0.1427
eta:2.4099999999999997
new: DataOwner1的最优x_1 = 0.2132
new: DataOwner2的最优x_2 = 0.1556
new: DataOwner3的最优x_3 = 0.1968
new: DataOwner4的最优x_4 = 0.2153
new: DataOwner5的最优x_5 = 0.2504
new: DataOwner6的最优x_6 = 0.3192
new: DataOwner7的最优x_7 = 0.1931
new: DataOwner8的最优x_8 = 0.2578
new: DataOwner9的最优x_9 = 0.2084
new: DataOwner10的最优x_10 = 0.1433
eta:2.42
new: DataOwner1的最优x_1 = 0.2141
new: DataOwner2的最优x_2 = 0.1563
new: DataOwner3的最优x_3 = 0.1976
new: DataOwner4的最优x_4 = 0.2162
new: DataOwner5的最优x_5 = 0.2514
new: DataOwner6的最优x_6 = 0.3205
new: DataOwner7的最优x_7 = 0.1939
new: DataOwner8的最优x_8 = 0.2588
new: DataOwner9的最优x_9 = 0.2092
new: DataOwner10的最优x_10 = 0.1439
eta:2.4299999999999997
new: DataOwner1的最优x_1 = 0.2150
new: DataOwner2的最优x_2 = 0.1569
new: DataOwner3的最优x_3 = 0.1985
new: DataOwner4的最优x_4 = 0.2171
new: DataOwner5的最优x_5 = 0.2525
new: DataOwner6的最优x_6 = 0.3218
new: DataOwner7的最优x_7 = 0.1947
new: DataOwner8的最优x_8 = 0.2599
new: DataOwner9的最优x_9 = 0.2101
new: DataOwner10的最优x_10 = 0.1445
eta:2.44
new: DataOwner1的最优x_1 = 0.2158
new: DataOwner2的最优x_2 = 0.1576
new: DataOwner3的最优x_3 = 0.1993
new: DataOwner4的最优x_4 = 0.2180
new: DataOwner5的最优x_5 = 0.2535
new: DataOwner6的最优x_6 = 0.3232
new: DataOwner7的最优x_7 = 0.1955
new: DataOwner8的最优x_8 = 0.2610
new: DataOwner9的最优x_9 = 0.2110
new: DataOwner10的最优x_10 = 0.1450
eta:2.4499999999999997
new: DataOwner1的最优x_1 = 0.2167
new: DataOwner2的最优x_2 = 0.1582
new: DataOwner3的最优x_3 = 0.2001
new: DataOwner4的最优x_4 = 0.2189
new: DataOwner5的最优x_5 = 0.2546
new: DataOwner6的最优x_6 = 0.3245
new: DataOwner7的最优x_7 = 0.1963
new: DataOwner8的最优x_8 = 0.2621
new: DataOwner9的最优x_9 = 0.2118
new: DataOwner10的最优x_10 = 0.1456
eta:2.46
new: DataOwner1的最优x_1 = 0.2176
new: DataOwner2的最优x_2 = 0.1589
new: DataOwner3的最优x_3 = 0.2009
new: DataOwner4的最优x_4 = 0.2198
new: DataOwner5的最优x_5 = 0.2556
new: DataOwner6的最优x_6 = 0.3258
new: DataOwner7的最优x_7 = 0.1971
new: DataOwner8的最优x_8 = 0.2631
new: DataOwner9的最优x_9 = 0.2127
new: DataOwner10的最优x_10 = 0.1462
eta:2.4699999999999998
new: DataOwner1的最优x_1 = 0.2185
new: DataOwner2的最优x_2 = 0.1595
new: DataOwner3的最优x_3 = 0.2017
new: DataOwner4的最优x_4 = 0.2207
new: DataOwner5的最优x_5 = 0.2566
new: DataOwner6的最优x_6 = 0.3271
new: DataOwner7的最优x_7 = 0.1979
new: DataOwner8的最优x_8 = 0.2642
new: DataOwner9的最优x_9 = 0.2135
new: DataOwner10的最优x_10 = 0.1468
eta:2.48
new: DataOwner1的最优x_1 = 0.2194
new: DataOwner2的最优x_2 = 0.1602
new: DataOwner3的最优x_3 = 0.2025
new: DataOwner4的最优x_4 = 0.2216
new: DataOwner5的最优x_5 = 0.2577
new: DataOwner6的最优x_6 = 0.3284
new: DataOwner7的最优x_7 = 0.1987
new: DataOwner8的最优x_8 = 0.2653
new: DataOwner9的最优x_9 = 0.2144
new: DataOwner10的最优x_10 = 0.1474
eta:2.4899999999999998
new: DataOwner1的最优x_1 = 0.2203
new: DataOwner2的最优x_2 = 0.1608
new: DataOwner3的最优x_3 = 0.2034
new: DataOwner4的最优x_4 = 0.2225
new: DataOwner5的最优x_5 = 0.2587
new: DataOwner6的最优x_6 = 0.3298
new: DataOwner7的最优x_7 = 0.1995
new: DataOwner8的最优x_8 = 0.2663
new: DataOwner9的最优x_9 = 0.2153
new: DataOwner10的最优x_10 = 0.1480
eta:2.5
new: DataOwner1的最优x_1 = 0.2212
new: DataOwner2的最优x_2 = 0.1614
new: DataOwner3的最优x_3 = 0.2042
new: DataOwner4的最优x_4 = 0.2234
new: DataOwner5的最优x_5 = 0.2598
new: DataOwner6的最优x_6 = 0.3311
new: DataOwner7的最优x_7 = 0.2004
new: DataOwner8的最优x_8 = 0.2674
new: DataOwner9的最优x_9 = 0.2161
new: DataOwner10的最优x_10 = 0.1486
eta:2.51
new: DataOwner1的最优x_1 = 0.2220
new: DataOwner2的最优x_2 = 0.1621
new: DataOwner3的最优x_3 = 0.2050
new: DataOwner4的最优x_4 = 0.2242
new: DataOwner5的最优x_5 = 0.2608
new: DataOwner6的最优x_6 = 0.3324
new: DataOwner7的最优x_7 = 0.2012
new: DataOwner8的最优x_8 = 0.2685
new: DataOwner9的最优x_9 = 0.2170
new: DataOwner10的最优x_10 = 0.1492
eta:2.52
new: DataOwner1的最优x_1 = 0.2229
new: DataOwner2的最优x_2 = 0.1627
new: DataOwner3的最优x_3 = 0.2058
new: DataOwner4的最优x_4 = 0.2251
new: DataOwner5的最优x_5 = 0.2618
new: DataOwner6的最优x_6 = 0.3337
new: DataOwner7的最优x_7 = 0.2020
new: DataOwner8的最优x_8 = 0.2695
new: DataOwner9的最优x_9 = 0.2179
new: DataOwner10的最优x_10 = 0.1498
eta:2.53
new: DataOwner1的最优x_1 = 0.2238
new: DataOwner2的最优x_2 = 0.1634
new: DataOwner3的最优x_3 = 0.2066
new: DataOwner4的最优x_4 = 0.2260
new: DataOwner5的最优x_5 = 0.2629
new: DataOwner6的最优x_6 = 0.3351
new: DataOwner7的最优x_7 = 0.2028
new: DataOwner8的最优x_8 = 0.2706
new: DataOwner9的最优x_9 = 0.2187
new: DataOwner10的最优x_10 = 0.1504
eta:2.54
new: DataOwner1的最优x_1 = 0.2247
new: DataOwner2的最优x_2 = 0.1640
new: DataOwner3的最优x_3 = 0.2074
new: DataOwner4的最优x_4 = 0.2269
new: DataOwner5的最优x_5 = 0.2639
new: DataOwner6的最优x_6 = 0.3364
new: DataOwner7的最优x_7 = 0.2036
new: DataOwner8的最优x_8 = 0.2717
new: DataOwner9的最优x_9 = 0.2196
new: DataOwner10的最优x_10 = 0.1510
eta:2.55
new: DataOwner1的最优x_1 = 0.2256
new: DataOwner2的最优x_2 = 0.1647
new: DataOwner3的最优x_3 = 0.2083
new: DataOwner4的最优x_4 = 0.2278
new: DataOwner5的最优x_5 = 0.2650
new: DataOwner6的最优x_6 = 0.3377
new: DataOwner7的最优x_7 = 0.2044
new: DataOwner8的最优x_8 = 0.2728
new: DataOwner9的最优x_9 = 0.2205
new: DataOwner10的最优x_10 = 0.1516
eta:2.56
new: DataOwner1的最优x_1 = 0.2265
new: DataOwner2的最优x_2 = 0.1653
new: DataOwner3的最优x_3 = 0.2091
new: DataOwner4的最优x_4 = 0.2287
new: DataOwner5的最优x_5 = 0.2660
new: DataOwner6的最优x_6 = 0.3390
new: DataOwner7的最优x_7 = 0.2052
new: DataOwner8的最优x_8 = 0.2738
new: DataOwner9的最优x_9 = 0.2213
new: DataOwner10的最优x_10 = 0.1522
eta:2.57
new: DataOwner1的最优x_1 = 0.2273
new: DataOwner2的最优x_2 = 0.1660
new: DataOwner3的最优x_3 = 0.2099
new: DataOwner4的最优x_4 = 0.2296
new: DataOwner5的最优x_5 = 0.2670
new: DataOwner6的最优x_6 = 0.3404
new: DataOwner7的最优x_7 = 0.2060
new: DataOwner8的最优x_8 = 0.2749
new: DataOwner9的最优x_9 = 0.2222
new: DataOwner10的最优x_10 = 0.1528
eta:2.5799999999999996
new: DataOwner1的最优x_1 = 0.2282
new: DataOwner2的最优x_2 = 0.1666
new: DataOwner3的最优x_3 = 0.2107
new: DataOwner4的最优x_4 = 0.2305
new: DataOwner5的最优x_5 = 0.2681
new: DataOwner6的最优x_6 = 0.3417
new: DataOwner7的最优x_7 = 0.2068
new: DataOwner8的最优x_8 = 0.2760
new: DataOwner9的最优x_9 = 0.2231
new: DataOwner10的最优x_10 = 0.1534
eta:2.59
new: DataOwner1的最优x_1 = 0.2291
new: DataOwner2的最优x_2 = 0.1673
new: DataOwner3的最优x_3 = 0.2115
new: DataOwner4的最优x_4 = 0.2314
new: DataOwner5的最优x_5 = 0.2691
new: DataOwner6的最优x_6 = 0.3430
new: DataOwner7的最优x_7 = 0.2076
new: DataOwner8的最优x_8 = 0.2770
new: DataOwner9的最优x_9 = 0.2239
new: DataOwner10的最优x_10 = 0.1540
eta:2.5999999999999996
new: DataOwner1的最优x_1 = 0.2300
new: DataOwner2的最优x_2 = 0.1679
new: DataOwner3的最优x_3 = 0.2123
new: DataOwner4的最优x_4 = 0.2323
new: DataOwner5的最优x_5 = 0.2701
new: DataOwner6的最优x_6 = 0.3443
new: DataOwner7的最优x_7 = 0.2084
new: DataOwner8的最优x_8 = 0.2781
new: DataOwner9的最优x_9 = 0.2248
new: DataOwner10的最优x_10 = 0.1546
eta:2.61
new: DataOwner1的最优x_1 = 0.2309
new: DataOwner2的最优x_2 = 0.1685
new: DataOwner3的最优x_3 = 0.2132
new: DataOwner4的最优x_4 = 0.2332
new: DataOwner5的最优x_5 = 0.2712
new: DataOwner6的最优x_6 = 0.3457
new: DataOwner7的最优x_7 = 0.2092
new: DataOwner8的最优x_8 = 0.2792
new: DataOwner9的最优x_9 = 0.2256
new: DataOwner10的最优x_10 = 0.1552
eta:2.6199999999999997
new: DataOwner1的最优x_1 = 0.2318
new: DataOwner2的最优x_2 = 0.1692
new: DataOwner3的最优x_3 = 0.2140
new: DataOwner4的最优x_4 = 0.2341
new: DataOwner5的最优x_5 = 0.2722
new: DataOwner6的最优x_6 = 0.3470
new: DataOwner7的最优x_7 = 0.2100
new: DataOwner8的最优x_8 = 0.2802
new: DataOwner9的最优x_9 = 0.2265
new: DataOwner10的最优x_10 = 0.1557
eta:2.63
new: DataOwner1的最优x_1 = 0.2327
new: DataOwner2的最优x_2 = 0.1698
new: DataOwner3的最优x_3 = 0.2148
new: DataOwner4的最优x_4 = 0.2350
new: DataOwner5的最优x_5 = 0.2733
new: DataOwner6的最优x_6 = 0.3483
new: DataOwner7的最优x_7 = 0.2108
new: DataOwner8的最优x_8 = 0.2813
new: DataOwner9的最优x_9 = 0.2274
new: DataOwner10的最优x_10 = 0.1563
eta:2.6399999999999997
new: DataOwner1的最优x_1 = 0.2335
new: DataOwner2的最优x_2 = 0.1705
new: DataOwner3的最优x_3 = 0.2156
new: DataOwner4的最优x_4 = 0.2359
new: DataOwner5的最优x_5 = 0.2743
new: DataOwner6的最优x_6 = 0.3496
new: DataOwner7的最优x_7 = 0.2116
new: DataOwner8的最优x_8 = 0.2824
new: DataOwner9的最优x_9 = 0.2282
new: DataOwner10的最优x_10 = 0.1569
eta:2.65
new: DataOwner1的最优x_1 = 0.2344
new: DataOwner2的最优x_2 = 0.1711
new: DataOwner3的最优x_3 = 0.2164
new: DataOwner4的最优x_4 = 0.2368
new: DataOwner5的最优x_5 = 0.2753
new: DataOwner6的最优x_6 = 0.3510
new: DataOwner7的最优x_7 = 0.2124
new: DataOwner8的最优x_8 = 0.2834
new: DataOwner9的最优x_9 = 0.2291
new: DataOwner10的最优x_10 = 0.1575
eta:2.6599999999999997
new: DataOwner1的最优x_1 = 0.2353
new: DataOwner2的最优x_2 = 0.1718
new: DataOwner3的最优x_3 = 0.2172
new: DataOwner4的最优x_4 = 0.2376
new: DataOwner5的最优x_5 = 0.2764
new: DataOwner6的最优x_6 = 0.3523
new: DataOwner7的最优x_7 = 0.2132
new: DataOwner8的最优x_8 = 0.2845
new: DataOwner9的最优x_9 = 0.2300
new: DataOwner10的最优x_10 = 0.1581
eta:2.67
new: DataOwner1的最优x_1 = 0.2362
new: DataOwner2的最优x_2 = 0.1724
new: DataOwner3的最优x_3 = 0.2181
new: DataOwner4的最优x_4 = 0.2385
new: DataOwner5的最优x_5 = 0.2774
new: DataOwner6的最优x_6 = 0.3536
new: DataOwner7的最优x_7 = 0.2140
new: DataOwner8的最优x_8 = 0.2856
new: DataOwner9的最优x_9 = 0.2308
new: DataOwner10的最优x_10 = 0.1587
eta:2.6799999999999997
new: DataOwner1的最优x_1 = 0.2371
new: DataOwner2的最优x_2 = 0.1731
new: DataOwner3的最优x_3 = 0.2189
new: DataOwner4的最优x_4 = 0.2394
new: DataOwner5的最优x_5 = 0.2785
new: DataOwner6的最优x_6 = 0.3549
new: DataOwner7的最优x_7 = 0.2148
new: DataOwner8的最优x_8 = 0.2867
new: DataOwner9的最优x_9 = 0.2317
new: DataOwner10的最优x_10 = 0.1593
eta:2.69
new: DataOwner1的最优x_1 = 0.2380
new: DataOwner2的最优x_2 = 0.1737
new: DataOwner3的最优x_3 = 0.2197
new: DataOwner4的最优x_4 = 0.2403
new: DataOwner5的最优x_5 = 0.2795
new: DataOwner6的最优x_6 = 0.3563
new: DataOwner7的最优x_7 = 0.2156
new: DataOwner8的最优x_8 = 0.2877
new: DataOwner9的最优x_9 = 0.2326
new: DataOwner10的最优x_10 = 0.1599
eta:2.6999999999999997
new: DataOwner1的最优x_1 = 0.2388
new: DataOwner2的最优x_2 = 0.1744
new: DataOwner3的最优x_3 = 0.2205
new: DataOwner4的最优x_4 = 0.2412
new: DataOwner5的最优x_5 = 0.2805
new: DataOwner6的最优x_6 = 0.3576
new: DataOwner7的最优x_7 = 0.2164
new: DataOwner8的最优x_8 = 0.2888
new: DataOwner9的最优x_9 = 0.2334
new: DataOwner10的最优x_10 = 0.1605
eta:2.71
new: DataOwner1的最优x_1 = 0.2397
new: DataOwner2的最优x_2 = 0.1750
new: DataOwner3的最优x_3 = 0.2213
new: DataOwner4的最优x_4 = 0.2421
new: DataOwner5的最优x_5 = 0.2816
new: DataOwner6的最优x_6 = 0.3589
new: DataOwner7的最优x_7 = 0.2172
new: DataOwner8的最优x_8 = 0.2899
new: DataOwner9的最优x_9 = 0.2343
new: DataOwner10的最优x_10 = 0.1611
eta:2.7199999999999998
new: DataOwner1的最优x_1 = 0.2406
new: DataOwner2的最优x_2 = 0.1756
new: DataOwner3的最优x_3 = 0.2221
new: DataOwner4的最优x_4 = 0.2430
new: DataOwner5的最优x_5 = 0.2826
new: DataOwner6的最优x_6 = 0.3602
new: DataOwner7的最优x_7 = 0.2180
new: DataOwner8的最优x_8 = 0.2909
new: DataOwner9的最优x_9 = 0.2352
new: DataOwner10的最优x_10 = 0.1617
eta:2.73
new: DataOwner1的最优x_1 = 0.2415
new: DataOwner2的最优x_2 = 0.1763
new: DataOwner3的最优x_3 = 0.2230
new: DataOwner4的最优x_4 = 0.2439
new: DataOwner5的最优x_5 = 0.2837
new: DataOwner6的最优x_6 = 0.3616
new: DataOwner7的最优x_7 = 0.2188
new: DataOwner8的最优x_8 = 0.2920
new: DataOwner9的最优x_9 = 0.2360
new: DataOwner10的最优x_10 = 0.1623
eta:2.7399999999999998
new: DataOwner1的最优x_1 = 0.2424
new: DataOwner2的最优x_2 = 0.1769
new: DataOwner3的最优x_3 = 0.2238
new: DataOwner4的最优x_4 = 0.2448
new: DataOwner5的最优x_5 = 0.2847
new: DataOwner6的最优x_6 = 0.3629
new: DataOwner7的最优x_7 = 0.2196
new: DataOwner8的最优x_8 = 0.2931
new: DataOwner9的最优x_9 = 0.2369
new: DataOwner10的最优x_10 = 0.1629
eta:2.75
new: DataOwner1的最优x_1 = 0.2433
new: DataOwner2的最优x_2 = 0.1776
new: DataOwner3的最优x_3 = 0.2246
new: DataOwner4的最优x_4 = 0.2457
new: DataOwner5的最优x_5 = 0.2857
new: DataOwner6的最优x_6 = 0.3642
new: DataOwner7的最优x_7 = 0.2204
new: DataOwner8的最优x_8 = 0.2941
new: DataOwner9的最优x_9 = 0.2378
new: DataOwner10的最优x_10 = 0.1635
eta:2.76
new: DataOwner1的最优x_1 = 0.2442
new: DataOwner2的最优x_2 = 0.1782
new: DataOwner3的最优x_3 = 0.2254
new: DataOwner4的最优x_4 = 0.2466
new: DataOwner5的最优x_5 = 0.2868
new: DataOwner6的最优x_6 = 0.3655
new: DataOwner7的最优x_7 = 0.2212
new: DataOwner8的最优x_8 = 0.2952
new: DataOwner9的最优x_9 = 0.2386
new: DataOwner10的最优x_10 = 0.1641
eta:2.77
new: DataOwner1的最优x_1 = 0.2450
new: DataOwner2的最优x_2 = 0.1789
new: DataOwner3的最优x_3 = 0.2262
new: DataOwner4的最优x_4 = 0.2475
new: DataOwner5的最优x_5 = 0.2878
new: DataOwner6的最优x_6 = 0.3669
new: DataOwner7的最优x_7 = 0.2220
new: DataOwner8的最优x_8 = 0.2963
new: DataOwner9的最优x_9 = 0.2395
new: DataOwner10的最优x_10 = 0.1647
eta:2.78
new: DataOwner1的最优x_1 = 0.2459
new: DataOwner2的最优x_2 = 0.1795
new: DataOwner3的最优x_3 = 0.2270
new: DataOwner4的最优x_4 = 0.2484
new: DataOwner5的最优x_5 = 0.2889
new: DataOwner6的最优x_6 = 0.3682
new: DataOwner7的最优x_7 = 0.2228
new: DataOwner8的最优x_8 = 0.2974
new: DataOwner9的最优x_9 = 0.2403
new: DataOwner10的最优x_10 = 0.1653
eta:2.79
new: DataOwner1的最优x_1 = 0.2468
new: DataOwner2的最优x_2 = 0.1802
new: DataOwner3的最优x_3 = 0.2279
new: DataOwner4的最优x_4 = 0.2493
new: DataOwner5的最优x_5 = 0.2899
new: DataOwner6的最优x_6 = 0.3695
new: DataOwner7的最优x_7 = 0.2236
new: DataOwner8的最优x_8 = 0.2984
new: DataOwner9的最优x_9 = 0.2412
new: DataOwner10的最优x_10 = 0.1659
eta:2.8
new: DataOwner1的最优x_1 = 0.2477
new: DataOwner2的最优x_2 = 0.1808
new: DataOwner3的最优x_3 = 0.2287
new: DataOwner4的最优x_4 = 0.2502
new: DataOwner5的最优x_5 = 0.2909
new: DataOwner6的最优x_6 = 0.3708
new: DataOwner7的最优x_7 = 0.2244
new: DataOwner8的最优x_8 = 0.2995
new: DataOwner9的最优x_9 = 0.2421
new: DataOwner10的最优x_10 = 0.1664
eta:2.81
new: DataOwner1的最优x_1 = 0.2486
new: DataOwner2的最优x_2 = 0.1815
new: DataOwner3的最优x_3 = 0.2295
new: DataOwner4的最优x_4 = 0.2510
new: DataOwner5的最优x_5 = 0.2920
new: DataOwner6的最优x_6 = 0.3722
new: DataOwner7的最优x_7 = 0.2252
new: DataOwner8的最优x_8 = 0.3006
new: DataOwner9的最优x_9 = 0.2429
new: DataOwner10的最优x_10 = 0.1670
eta:2.82
new: DataOwner1的最优x_1 = 0.2495
new: DataOwner2的最优x_2 = 0.1821
new: DataOwner3的最优x_3 = 0.2303
new: DataOwner4的最优x_4 = 0.2519
new: DataOwner5的最优x_5 = 0.2930
new: DataOwner6的最优x_6 = 0.3735
new: DataOwner7的最优x_7 = 0.2260
new: DataOwner8的最优x_8 = 0.3016
new: DataOwner9的最优x_9 = 0.2438
new: DataOwner10的最优x_10 = 0.1676
eta:2.8299999999999996
new: DataOwner1的最优x_1 = 0.2503
new: DataOwner2的最优x_2 = 0.1828
new: DataOwner3的最优x_3 = 0.2311
new: DataOwner4的最优x_4 = 0.2528
new: DataOwner5的最优x_5 = 0.2940
new: DataOwner6的最优x_6 = 0.3748
new: DataOwner7的最优x_7 = 0.2268
new: DataOwner8的最优x_8 = 0.3027
new: DataOwner9的最优x_9 = 0.2447
new: DataOwner10的最优x_10 = 0.1682
eta:2.84
new: DataOwner1的最优x_1 = 0.2512
new: DataOwner2的最优x_2 = 0.1834
new: DataOwner3的最优x_3 = 0.2319
new: DataOwner4的最优x_4 = 0.2537
new: DataOwner5的最优x_5 = 0.2951
new: DataOwner6的最优x_6 = 0.3761
new: DataOwner7的最优x_7 = 0.2276
new: DataOwner8的最优x_8 = 0.3038
new: DataOwner9的最优x_9 = 0.2455
new: DataOwner10的最优x_10 = 0.1688
eta:2.8499999999999996
new: DataOwner1的最优x_1 = 0.2521
new: DataOwner2的最优x_2 = 0.1840
new: DataOwner3的最优x_3 = 0.2328
new: DataOwner4的最优x_4 = 0.2546
new: DataOwner5的最优x_5 = 0.2961
new: DataOwner6的最优x_6 = 0.3775
new: DataOwner7的最优x_7 = 0.2284
new: DataOwner8的最优x_8 = 0.3048
new: DataOwner9的最优x_9 = 0.2464
new: DataOwner10的最优x_10 = 0.1694
eta:2.86
new: DataOwner1的最优x_1 = 0.2530
new: DataOwner2的最优x_2 = 0.1847
new: DataOwner3的最优x_3 = 0.2336
new: DataOwner4的最优x_4 = 0.2555
new: DataOwner5的最优x_5 = 0.2972
new: DataOwner6的最优x_6 = 0.3788
new: DataOwner7的最优x_7 = 0.2292
new: DataOwner8的最优x_8 = 0.3059
new: DataOwner9的最优x_9 = 0.2473
new: DataOwner10的最优x_10 = 0.1700
eta:2.8699999999999997
new: DataOwner1的最优x_1 = 0.2539
new: DataOwner2的最优x_2 = 0.1853
new: DataOwner3的最优x_3 = 0.2344
new: DataOwner4的最优x_4 = 0.2564
new: DataOwner5的最优x_5 = 0.2982
new: DataOwner6的最优x_6 = 0.3801
new: DataOwner7的最优x_7 = 0.2300
new: DataOwner8的最优x_8 = 0.3070
new: DataOwner9的最优x_9 = 0.2481
new: DataOwner10的最优x_10 = 0.1706
eta:2.88
new: DataOwner1的最优x_1 = 0.2548
new: DataOwner2的最优x_2 = 0.1860
new: DataOwner3的最优x_3 = 0.2352
new: DataOwner4的最优x_4 = 0.2573
new: DataOwner5的最优x_5 = 0.2992
new: DataOwner6的最优x_6 = 0.3814
new: DataOwner7的最优x_7 = 0.2308
new: DataOwner8的最优x_8 = 0.3080
new: DataOwner9的最优x_9 = 0.2490
new: DataOwner10的最优x_10 = 0.1712
eta:2.8899999999999997
new: DataOwner1的最优x_1 = 0.2557
new: DataOwner2的最优x_2 = 0.1866
new: DataOwner3的最优x_3 = 0.2360
new: DataOwner4的最优x_4 = 0.2582
new: DataOwner5的最优x_5 = 0.3003
new: DataOwner6的最优x_6 = 0.3827
new: DataOwner7的最优x_7 = 0.2316
new: DataOwner8的最优x_8 = 0.3091
new: DataOwner9的最优x_9 = 0.2499
new: DataOwner10的最优x_10 = 0.1718
eta:2.9
new: DataOwner1的最优x_1 = 0.2565
new: DataOwner2的最优x_2 = 0.1873
new: DataOwner3的最优x_3 = 0.2369
new: DataOwner4的最优x_4 = 0.2591
new: DataOwner5的最优x_5 = 0.3013
new: DataOwner6的最优x_6 = 0.3841
new: DataOwner7的最优x_7 = 0.2324
new: DataOwner8的最优x_8 = 0.3102
new: DataOwner9的最优x_9 = 0.2507
new: DataOwner10的最优x_10 = 0.1724
eta:2.9099999999999997
new: DataOwner1的最优x_1 = 0.2574
new: DataOwner2的最优x_2 = 0.1879
new: DataOwner3的最优x_3 = 0.2377
new: DataOwner4的最优x_4 = 0.2600
new: DataOwner5的最优x_5 = 0.3024
new: DataOwner6的最优x_6 = 0.3854
new: DataOwner7的最优x_7 = 0.2332
new: DataOwner8的最优x_8 = 0.3113
new: DataOwner9的最优x_9 = 0.2516
new: DataOwner10的最优x_10 = 0.1730
eta:2.92
new: DataOwner1的最优x_1 = 0.2583
new: DataOwner2的最优x_2 = 0.1886
new: DataOwner3的最优x_3 = 0.2385
new: DataOwner4的最优x_4 = 0.2609
new: DataOwner5的最优x_5 = 0.3034
new: DataOwner6的最优x_6 = 0.3867
new: DataOwner7的最优x_7 = 0.2340
new: DataOwner8的最优x_8 = 0.3123
new: DataOwner9的最优x_9 = 0.2525
new: DataOwner10的最优x_10 = 0.1736
eta:2.9299999999999997
new: DataOwner1的最优x_1 = 0.2592
new: DataOwner2的最优x_2 = 0.1892
new: DataOwner3的最优x_3 = 0.2393
new: DataOwner4的最优x_4 = 0.2618
new: DataOwner5的最优x_5 = 0.3044
new: DataOwner6的最优x_6 = 0.3880
new: DataOwner7的最优x_7 = 0.2348
new: DataOwner8的最优x_8 = 0.3134
new: DataOwner9的最优x_9 = 0.2533
new: DataOwner10的最优x_10 = 0.1742
eta:2.94
new: DataOwner1的最优x_1 = 0.2601
new: DataOwner2的最优x_2 = 0.1899
new: DataOwner3的最优x_3 = 0.2401
new: DataOwner4的最优x_4 = 0.2627
new: DataOwner5的最优x_5 = 0.3055
new: DataOwner6的最优x_6 = 0.3894
new: DataOwner7的最优x_7 = 0.2356
new: DataOwner8的最优x_8 = 0.3145
new: DataOwner9的最优x_9 = 0.2542
new: DataOwner10的最优x_10 = 0.1748
eta:2.9499999999999997
new: DataOwner1的最优x_1 = 0.2610
new: DataOwner2的最优x_2 = 0.1905
new: DataOwner3的最优x_3 = 0.2409
new: DataOwner4的最优x_4 = 0.2636
new: DataOwner5的最优x_5 = 0.3065
new: DataOwner6的最优x_6 = 0.3907
new: DataOwner7的最优x_7 = 0.2364
new: DataOwner8的最优x_8 = 0.3155
new: DataOwner9的最优x_9 = 0.2550
new: DataOwner10的最优x_10 = 0.1754
eta:2.96
new: DataOwner1的最优x_1 = 0.2618
new: DataOwner2的最优x_2 = 0.1911
new: DataOwner3的最优x_3 = 0.2418
new: DataOwner4的最优x_4 = 0.2644
new: DataOwner5的最优x_5 = 0.3076
new: DataOwner6的最优x_6 = 0.3920
new: DataOwner7的最优x_7 = 0.2372
new: DataOwner8的最优x_8 = 0.3166
new: DataOwner9的最优x_9 = 0.2559
new: DataOwner10的最优x_10 = 0.1760
eta:2.9699999999999998
new: DataOwner1的最优x_1 = 0.2627
new: DataOwner2的最优x_2 = 0.1918
new: DataOwner3的最优x_3 = 0.2426
new: DataOwner4的最优x_4 = 0.2653
new: DataOwner5的最优x_5 = 0.3086
new: DataOwner6的最优x_6 = 0.3933
new: DataOwner7的最优x_7 = 0.2380
new: DataOwner8的最优x_8 = 0.3177
new: DataOwner9的最优x_9 = 0.2568
new: DataOwner10的最优x_10 = 0.1766
eta:2.98
new: DataOwner1的最优x_1 = 0.2636
new: DataOwner2的最优x_2 = 0.1924
new: DataOwner3的最优x_3 = 0.2434
new: DataOwner4的最优x_4 = 0.2662
new: DataOwner5的最优x_5 = 0.3096
new: DataOwner6的最优x_6 = 0.3947
new: DataOwner7的最优x_7 = 0.2388
new: DataOwner8的最优x_8 = 0.3187
new: DataOwner9的最优x_9 = 0.2576
new: DataOwner10的最优x_10 = 0.1771
eta:2.9899999999999998
new: DataOwner1的最优x_1 = 0.2645
new: DataOwner2的最优x_2 = 0.1931
new: DataOwner3的最优x_3 = 0.2442
new: DataOwner4的最优x_4 = 0.2671
new: DataOwner5的最优x_5 = 0.3107
new: DataOwner6的最优x_6 = 0.3960
new: DataOwner7的最优x_7 = 0.2396
new: DataOwner8的最优x_8 = 0.3198
new: DataOwner9的最优x_9 = 0.2585
new: DataOwner10的最优x_10 = 0.1777
eta:3.0
new: DataOwner1的最优x_1 = 0.2654
new: DataOwner2的最优x_2 = 0.1937
new: DataOwner3的最优x_3 = 0.2450
new: DataOwner4的最优x_4 = 0.2680
new: DataOwner5的最优x_5 = 0.3117
new: DataOwner6的最优x_6 = 0.3973
new: DataOwner7的最优x_7 = 0.2404
new: DataOwner8的最优x_8 = 0.3209
new: DataOwner9的最优x_9 = 0.2594
new: DataOwner10的最优x_10 = 0.1783
DONE
----- literation 3: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.313579138606405
DataOwner1的分配到的支付 ： 0.1160
DataOwner2的分配到的支付 ： 0.1441
DataOwner3的分配到的支付 ： 0.1495
DataOwner4的分配到的支付 ： 0.1116
DataOwner5的分配到的支付 ： 0.1459
DataOwner6的分配到的支付 ： 0.1768
DataOwner7的分配到的支付 ： 0.1361
DataOwner8的分配到的支付 ： 0.1545
DataOwner9的分配到的支付 ： 0.1076
DataOwner10的分配到的支付 ： 0.0714
DONE
----- literation 3: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC2
DataOwner3 把数据交给 CPC3
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC10
DataOwner9 把数据交给 CPC8
DataOwner10 把数据交给 CPC9
DONE
----- literation 3: 模型训练 -----
CPC1调整模型中, 本轮训练的数据量为：575.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.85%
Epoch 1/5, Loss: 0.6261
Epoch 2/5, Loss: 0.6121
Epoch 3/5, Loss: 0.5994
Epoch 4/5, Loss: 0.5898
Epoch 5/5, Loss: 0.5807
新模型评估：
Accuracy: 76.83%
CPC2调整模型中, 本轮训练的数据量为：1473.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.85%
Epoch 1/5, Loss: 0.6399
Epoch 2/5, Loss: 0.6235
Epoch 3/5, Loss: 0.6431
Epoch 4/5, Loss: 0.5781
Epoch 5/5, Loss: 0.5977
新模型评估：
Accuracy: 77.25%
CPC3调整模型中, 本轮训练的数据量为：1494.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.85%
Epoch 1/5, Loss: 0.6442
Epoch 2/5, Loss: 0.6151
Epoch 3/5, Loss: 0.5973
Epoch 4/5, Loss: 0.5851
Epoch 5/5, Loss: 0.5668
新模型评估：
Accuracy: 77.31%
CPC4调整模型中, 本轮训练的数据量为：1242.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.85%
Epoch 1/5, Loss: 0.6595
Epoch 2/5, Loss: 0.6326
Epoch 3/5, Loss: 0.6083
Epoch 4/5, Loss: 0.5952
Epoch 5/5, Loss: 0.5816
新模型评估：
Accuracy: 75.72%
CPC5调整模型中, 本轮训练的数据量为：176.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.85%
Epoch 1/5, Loss: 0.5433
Epoch 2/5, Loss: 0.5395
Epoch 3/5, Loss: 0.5246
Epoch 4/5, Loss: 0.5196
Epoch 5/5, Loss: 0.5159
新模型评估：
Accuracy: 75.37%
CPC6调整模型中, 本轮训练的数据量为：409.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.85%
Epoch 1/5, Loss: 0.6606
Epoch 2/5, Loss: 0.6423
Epoch 3/5, Loss: 0.6436
Epoch 4/5, Loss: 0.6336
Epoch 5/5, Loss: 0.6463
新模型评估：
Accuracy: 77.26%
CPC7调整模型中, 本轮训练的数据量为：1875.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.85%
Epoch 1/5, Loss: 0.6506
Epoch 2/5, Loss: 0.6177
Epoch 3/5, Loss: 0.5954
Epoch 4/5, Loss: 0.5762
Epoch 5/5, Loss: 0.5484
新模型评估：
Accuracy: 76.67%
CPC10调整模型中, 本轮训练的数据量为：372.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.85%
Epoch 1/5, Loss: 0.6821
Epoch 2/5, Loss: 0.6701
Epoch 3/5, Loss: 0.6606
Epoch 4/5, Loss: 0.6554
Epoch 5/5, Loss: 0.6479
新模型评估：
Accuracy: 77.33%
CPC8调整模型中, 本轮训练的数据量为：534.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.85%
Epoch 1/5, Loss: 0.6523
Epoch 2/5, Loss: 0.6303
Epoch 3/5, Loss: 0.6170
Epoch 4/5, Loss: 0.6141
Epoch 5/5, Loss: 0.6048
新模型评估：
Accuracy: 76.94%
CPC9调整模型中, 本轮训练的数据量为：183.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.85%
Epoch 1/5, Loss: 0.6356
Epoch 2/5, Loss: 0.6311
Epoch 3/5, Loss: 0.6212
Epoch 4/5, Loss: 0.6151
Epoch 5/5, Loss: 0.6081
新模型评估：
Accuracy: 76.27%
DONE
最终的列表：
[0.27678035080685637, 0.3014760054596696, 0.24480106144959393, 0.2611868060124596, 0.26054945922303613, 0.22164014196785037, 0.28599081178770003, 0.22864294143720176, 0.2252331402568338, 0.12166571028734295, 0.11166571028734294, 0.19575228966864383, 0.15450607653969717, 0.165106685872902, 0.20587755438844263, 0.15627224113236207, 0.19590916172139253, 0.241354203186488, 0.2528041353132157, 0.22526790468196162, 0.19742877396670427, 0.20526790468198178, 0.21298064292354804, 0.22056876711162623, 0.27117067960420294, 0.23537810167590933, 0.2426026866284292, 0.2497094086432512, 0.2566998697693824, 0.26357563961264513, 0.27033825620503343, 0.32953909519652475, 0.28353002892082363, 0.46780408915867117, 0.2962868919906408, 0.30250576510864985, 0.30862009535054, 0.37455513907518545, 0.32054045799758646, 0.3263490925372391, 0.39545393356021685, 0.3376695902024146, 0.34318391149291333, 0.3486025486022383, 0.3539266745561858, 0.3591574409522002, 0.5887136447301864, 0.36934339731004434, 0.44632563745549614, 0.45221320020083156, 0.45800043434717475, 0.46368859907161875, 0.46927893000503396, 0.3977741407229052, 0.4022132002008315, 0.4854749351166523, 0.4108435182750616, 0.49580474785770434, 0.419149667224323, 0.42318345618266917, 0.5993409260078756, 0.43101688584328135, 0.4348182516567981, 0.43854384450826644, 0.4421944918774138, 0.4457710077733653, 0.5379115893994405, 0.5421683140487429, 0.45606371052482053, 0.45935158090505157, 0.462569197569877, 0.46571729945388185, 0.46879661391945837, 0.47180785697005834, 0.47475173349996136, 0.4776289375062013, 0.4804401523166095, 0.48318605080646604, 0.48586729561252096, 0.48848453934395364, 0.49103842476620985, 0.49352958501908173, 0.49595864379758026, 0.49832621553953316, 0.5006329056091493, 0.5028793104738635, 0.5050660177451368, 0.5071936070145847, 0.5092626486846114, 0.5112737054656747, 0.5132273318621733, 0.5151240744641375, 0.5169644720951482, 0.5187490559587448, 0.5204783497817383, 0.5221528699538122, 0.5237731256638269, 0.5253396190332618, 0.5268528452424033, 0.5283132926768794, 0.5297214430134689, 0.5310777713800217, 0.5323827464604893, 0.5336368305874557, 0.5348404799102524, 0.5359941444535805, 0.5370982682419345, 0.5381532894225929, 0.5391596403486558, 0.5401177476843793, 0.5410280325164125, 0.5418909104366523, 0.5427067916444059, 0.5434760810379138, 0.5441991783051745, 0.5448764780132338, 0.5455083696949399, 0.5460952379345982, 0.5466374624510206, 0.5471354181796169, 0.5475894753522299, 0.5479999995754998, 0.5483673519072461, 0.5486918889332428, 0.5489739628379433, 0.5492139214788625, 0.54941210845646, 0.5495688631831563, 0.549684520951816, 0.5497594130010119, 0.5497938665810556, 0.5497882050174141, 0.5497427477732655, 0.5496578105107914, 0.5495337051512148, 0.5493707399336103, 0.549169219472871, 0.548929444816024, 0.5486517134978806, 0.5483363195954327, 0.5479835537814957, 0.5475937033756832, 0.5471670523985097, 0.5467038816182628, 0.5462044685976031, 0.5456690877655244, 0.545098010451569, 0.5444915048407273, 0.5438498362098463, 0.5431732668730564, 0.5424620561383506, 0.5417164604709668, 0.5409367335105622, 0.5401231260925567, 0.5392758862983891, 0.5383952594916788, 0.5374814883635068, 0.5365348129678686, 0.5355554707333952, 0.5345436966322432, 0.5334997229553384, 0.5324237796102413, 0.531316094023605, 0.5301768912064602, 0.5290063936845566, 0.5278048220307148, 0.5265723939343181, 0.5253093250846266, 0.5240158289749672, 0.5226921167909333, 0.5213383975168444, 0.5199548780154595, 0.5185417629939861, 0.5170992550636859, 0.5156275547625206, 0.5141268605816933, 0.5125973690279797, 0.5110392744950729, 0.5094527695885214, 0.5078380448679527, 0.5061952890073984, 0.5045246887879651, 0.5028264291376701, 0.5011006931339568, 0.4993476620887858, 0.497567515458438, 0.49576043096284694, 0.4939265846296468, 0.49206615067969484, 0.4901793016950624, 0.4882662085736298, 0.4863270404973259, 0.48436196522144637, 0.48237114860252017, 0.48035475509500114, 0.47831294753600373, 0.476245887211159, 0.47415373388315096, 0.4720366457800116, 0.46989477967075377, 0.4677282908294442, 0.46553733308795797, 0.4633220587871505, 0.4610826189470192, 0.4588191631277727, 0.45653183956917154, 0.454220794963232, 0.4518861748410221, 0.4495281233245434, 0.4471467832267373, 0.4447422960590073, 0.442314802047036, 0.43986444014522874, 0.43739134805398905, 0.43489566223314347, 0.43237751791716095, 0.42983704913016263, 0.42727438870006784, 0.42468966827288934, 0.4220830183265991, 0.4194545681922226, 0.4168044460314606, 0.4141327789224927, 0.4114396927979844, 0.4087253125003385, 0.4059897617825525, 0.403233163319904, 0.40045563872788836, 0.39765730856813786, 0.3948382923632665, 0.39199870861159036, 0.3891386747940948, 0.3862583073882444, 0.38335772187996664, 0.3804370327729658, 0.37749635360177347, 0.3745357970761112, 0.3715554744210814, 0.3685554967271738, 0.365535973622948, 0.3624970139536505, 0.35943872565784085, 0.3563612157774423, 0.3532645904841365, 0.3501489550066026, 0.34701441380574494, 0.34386107041637803, 0.34068902754251296, 0.3374983870560917, 0.33428924998489595, 0.33106171654649375, 0.32781588608247736, 0.32455185735651915, 0.32126972802655906, 0.31796959514194834, 0.3146515549507618, 0.3113157029126925, 0.3079621337600744, 0.30459094142379195, 0.3012022191231374, 0.2977960593282467, 0.2943725537818813, 0.29093179350418374, 0.2874738688007894, 0.2839988692696953, 0.28050688380991984, 0.2769980006275632, 0.2734723072435412, 0.26992989050059, 0.266370836570216, 0.2627952309595569, 0.2592031585184911, 0.25559470344577173, 0.25196994929484573, 0.24832897898591622, 0.24467187480241215, 0.24099871840640263, 0.23730959084072456, 0.23360457253686207, 0.22988374331777983, 0.22614718240884768, 0.2223949684409341, 0.21862717945655463, 0.2148438929161216, 0.2110451857035689, 0.20723113413229877, 0.20340181395029866, 0.19955730034638242, 0.19569766795520271, 0.19182299086268406, 0.18793334261164052, 0.1840287962070617, 0.18010942412104658, 0.17617529829819167, 0.1722264901608912, 0.1682630706178081, 0.16428511005030888, 0.16029267835330474, 0.15628584490759545, 0.1522646785945585]
**** log-parameter_analysis 运行时间： 2025-01-16 23:10:02 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../data/model/mnist_cnn_model
初始化模型的准确率：
Accuracy: 26.82%
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.05333042959028392
DataOwner2: noise random: 0.002465562356408857
DataOwner3: noise random: 0.06340355900931897
DataOwner4: noise random: 0.00916433623490257
DataOwner5: noise random: 0.05201252528923949
DataOwner6: noise random: 0.0862004153168488
DataOwner7: noise random: 0.07312518015964657
DataOwner8: noise random: 0.09617793152653337
DataOwner9: noise random: 0.06266964457055588
DataOwner10: noise random: 0.00945153699197724
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9988420936485858, 0.9999975407367466, 0.9983729131363964, 0.9999661269385938, 0.9989071757293772, 0.9969912957476931, 0.9978354893214628, 0.996256705335168, 0.9984134690677692, 0.9999637468649726]
归一化后的数据质量列表avg_f_list: [0.9691125921318742, 1.0, 0.9565704601794401, 0.9991602464481704, 0.9708523661075991, 0.9196370685600095, 0.9422040484761397, 0.9, 0.9576546012072875, 0.999096622327735]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3430
DataOwner1的最优x_1 = 0.1295
DataOwner2的最优x_2 = 0.1575
DataOwner3的最优x_3 = 0.1171
DataOwner4的最优x_4 = 0.1568
DataOwner5的最优x_5 = 0.1312
DataOwner6的最优x_6 = 0.0760
DataOwner7的最优x_7 = 0.1019
DataOwner8的最优x_8 = 0.0512
DataOwner9的最优x_9 = 0.1182
DataOwner10的最优x_10 = 0.1568
每个DataOwner应该贡献数据比例 xn_list = [0.12954339642177673, 0.15751670008586452, 0.1170526248600555, 0.1568052098457323, 0.13122202901239088, 0.07595342855446488, 0.1018651780240644, 0.05116388819165711, 0.11815990280091734, 0.156751197068638]
ModelOwner的最大效用 U(Eta) = 0.5831
DONE
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3429880115694797
DataOwner1的分配到的支付 ： 0.1453
DataOwner2的分配到的支付 ： 0.1822
DataOwner3的分配到的支付 ： 0.1295
DataOwner4的分配到的支付 ： 0.1813
DataOwner5的分配到的支付 ： 0.1474
DataOwner6的分配到的支付 ： 0.0808
DataOwner7的分配到的支付 ： 0.1110
DataOwner8的分配到的支付 ： 0.0533
DataOwner9的分配到的支付 ： 0.1309
DataOwner10的分配到的支付 ： 0.1812
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC1', 'DataOwner2': 'CPC2', 'DataOwner3': 'CPC3', 'DataOwner4': 'CPC4', 'DataOwner5': 'CPC5', 'DataOwner6': 'CPC6', 'DataOwner7': 'CPC7', 'DataOwner8': 'CPC8', 'DataOwner9': 'CPC9', 'DataOwner10': 'CPC10'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC2
DataOwner3 把数据交给 CPC3
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC8
DataOwner9 把数据交给 CPC9
DataOwner10 把数据交给 CPC10
DONE
----- literation 1: 模型训练 -----
CPC1调整模型中, 本轮训练的数据量为：190.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 26.82%
Epoch 1/5, Loss: 2.3145
Epoch 2/5, Loss: 2.3138
Epoch 3/5, Loss: 2.3127
Epoch 4/5, Loss: 2.3116
Epoch 5/5, Loss: 2.3108
新模型评估：
Accuracy: 28.34%
Model saved to ../../data/model/mnist_cnn_model
CPC2调整模型中, 本轮训练的数据量为：1613.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 28.34%
Epoch 1/5, Loss: 2.2904
Epoch 2/5, Loss: 2.2829
Epoch 3/5, Loss: 2.2690
Epoch 4/5, Loss: 2.2496
Epoch 5/5, Loss: 2.2240
新模型评估：
Accuracy: 46.55%
Model saved to ../../data/model/mnist_cnn_model
CPC3调整模型中, 本轮训练的数据量为：171.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 46.55%
Epoch 1/5, Loss: 2.2083
Epoch 2/5, Loss: 2.2073
Epoch 3/5, Loss: 2.1993
Epoch 4/5, Loss: 2.1961
Epoch 5/5, Loss: 2.1913
新模型评估：
Accuracy: 46.96%
Model saved to ../../data/model/mnist_cnn_model
CPC4调整模型中, 本轮训练的数据量为：229.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 46.96%
Epoch 1/5, Loss: 2.2022
Epoch 2/5, Loss: 2.1928
Epoch 3/5, Loss: 2.1969
Epoch 4/5, Loss: 2.1857
Epoch 5/5, Loss: 2.1835
新模型评估：
Accuracy: 47.96%
Model saved to ../../data/model/mnist_cnn_model
CPC5调整模型中, 本轮训练的数据量为：1536.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 47.96%
Epoch 1/5, Loss: 2.1588
Epoch 2/5, Loss: 2.1281
Epoch 3/5, Loss: 2.0930
Epoch 4/5, Loss: 2.0526
Epoch 5/5, Loss: 2.0055
新模型评估：
Accuracy: 48.51%
Model saved to ../../data/model/mnist_cnn_model
CPC6调整模型中, 本轮训练的数据量为：555.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 48.51%
Epoch 1/5, Loss: 1.9669
Epoch 2/5, Loss: 1.9446
Epoch 3/5, Loss: 1.9296
Epoch 4/5, Loss: 1.9101
Epoch 5/5, Loss: 1.8874
新模型评估：
Accuracy: 51.15%
Model saved to ../../data/model/mnist_cnn_model
CPC7调整模型中, 本轮训练的数据量为：596.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 51.15%
Epoch 1/5, Loss: 1.8651
Epoch 2/5, Loss: 1.8496
Epoch 3/5, Loss: 1.8246
Epoch 4/5, Loss: 1.7939
Epoch 5/5, Loss: 1.7698
新模型评估：
Accuracy: 51.78%
Model saved to ../../data/model/mnist_cnn_model
CPC8调整模型中, 本轮训练的数据量为：673.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 51.78%
Epoch 1/5, Loss: 1.8330
Epoch 2/5, Loss: 1.8025
Epoch 3/5, Loss: 1.7815
Epoch 4/5, Loss: 1.7545
Epoch 5/5, Loss: 1.7275
新模型评估：
Accuracy: 57.81%
Model saved to ../../data/model/mnist_cnn_model
CPC9调整模型中, 本轮训练的数据量为：691.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 57.81%
Epoch 1/5, Loss: 1.6731
Epoch 2/5, Loss: 1.6446
Epoch 3/5, Loss: 1.6168
Epoch 4/5, Loss: 1.5869
Epoch 5/5, Loss: 1.5603
新模型评估：
Accuracy: 61.46%
Model saved to ../../data/model/mnist_cnn_model
CPC10调整模型中, 本轮训练的数据量为：229.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 61.46%
Epoch 1/5, Loss: 1.5799
Epoch 2/5, Loss: 1.5650
Epoch 3/5, Loss: 1.5500
Epoch 4/5, Loss: 1.5480
Epoch 5/5, Loss: 1.5137
新模型评估：
Accuracy: 64.29%
Model saved to ../../data/model/mnist_cnn_model
DONE
========================= literation: 2 =========================
----- literation 2: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3430
DataOwner1的最优x_1 = 0.1295
DataOwner2的最优x_2 = 0.1575
DataOwner3的最优x_3 = 0.1171
DataOwner4的最优x_4 = 0.1568
DataOwner5的最优x_5 = 0.1312
DataOwner6的最优x_6 = 0.0760
DataOwner7的最优x_7 = 0.1019
DataOwner8的最优x_8 = 0.0512
DataOwner9的最优x_9 = 0.1182
DataOwner10的最优x_10 = 0.1568
每个DataOwner应该贡献数据比例 xn_list = [0.12954339642177673, 0.15751670008586452, 0.1170526248600555, 0.1568052098457323, 0.13122202901239088, 0.07595342855446488, 0.1018651780240644, 0.05116388819165711, 0.11815990280091734, 0.156751197068638]
ModelOwner的最大效用 U(Eta) = 0.5831
DONE
----- literation 2: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3429880115694797
DataOwner1的分配到的支付 ： 0.1453
DataOwner2的分配到的支付 ： 0.1822
DataOwner3的分配到的支付 ： 0.1295
DataOwner4的分配到的支付 ： 0.1813
DataOwner5的分配到的支付 ： 0.1474
DataOwner6的分配到的支付 ： 0.0808
DataOwner7的分配到的支付 ： 0.1110
DataOwner8的分配到的支付 ： 0.0533
DataOwner9的分配到的支付 ： 0.1309
DataOwner10的分配到的支付 ： 0.1812
DONE
----- literation 2: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC2
DataOwner3 把数据交给 CPC3
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC8
DataOwner9 把数据交给 CPC9
DataOwner10 把数据交给 CPC10
DONE
----- literation 2: 模型训练 -----
重新调整fn，进而调整xn、Eta
正在评估DataOwner1的数据质量, 本轮评估的样本数据量为：190.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 64.29%
Epoch 1/5, Loss: 1.5494
Epoch 2/5, Loss: 1.5360
Epoch 3/5, Loss: 1.5264
Epoch 4/5, Loss: 1.5163
Epoch 5/5, Loss: 1.5055
新模型评估：
Accuracy: 64.47%
loss差为：
0.04397960503896092
单位数据loss差为：
0.00023147160546821538
正在评估DataOwner2的数据质量, 本轮评估的样本数据量为：1613.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 64.29%
Epoch 1/5, Loss: 1.4776
Epoch 2/5, Loss: 1.4088
Epoch 3/5, Loss: 1.3549
Epoch 4/5, Loss: 1.2975
Epoch 5/5, Loss: 1.2385
新模型评估：
Accuracy: 70.20%
loss差为：
0.2391393138812139
单位数据loss差为：
0.00014825747915760316
正在评估DataOwner3的数据质量, 本轮评估的样本数据量为：171.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 64.29%
Epoch 1/5, Loss: 1.4877
Epoch 2/5, Loss: 1.4864
Epoch 3/5, Loss: 1.4687
Epoch 4/5, Loss: 1.4386
Epoch 5/5, Loss: 1.4537
新模型评估：
Accuracy: 65.52%
loss差为：
0.03403902053833008
单位数据loss差为：
0.0001990585996393572
正在评估DataOwner4的数据质量, 本轮评估的样本数据量为：229.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 64.29%
Epoch 1/5, Loss: 1.5171
Epoch 2/5, Loss: 1.5096
Epoch 3/5, Loss: 1.4935
Epoch 4/5, Loss: 1.4785
Epoch 5/5, Loss: 1.4705
新模型评估：
Accuracy: 66.33%
loss差为：
0.04659178853034973
单位数据loss差为：
0.00020345759183558834
正在评估DataOwner5的数据质量, 本轮评估的样本数据量为：1536.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 64.29%
Epoch 1/5, Loss: 1.4907
Epoch 2/5, Loss: 1.4303
Epoch 3/5, Loss: 1.3721
Epoch 4/5, Loss: 1.3143
Epoch 5/5, Loss: 1.2565
新模型评估：
Accuracy: 69.10%
loss差为：
0.2341781655947368
单位数据loss差为：
0.00015245974322574012
正在评估DataOwner6的数据质量, 本轮评估的样本数据量为：555.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 64.29%
Epoch 1/5, Loss: 1.5513
Epoch 2/5, Loss: 1.5278
Epoch 3/5, Loss: 1.5020
Epoch 4/5, Loss: 1.4780
Epoch 5/5, Loss: 1.4599
新模型评估：
Accuracy: 67.50%
loss差为：
0.09141357739766454
单位数据loss差为：
0.00016470914846426041
正在评估DataOwner7的数据质量, 本轮评估的样本数据量为：596.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 64.29%
Epoch 1/5, Loss: 1.4908
Epoch 2/5, Loss: 1.4577
Epoch 3/5, Loss: 1.4445
Epoch 4/5, Loss: 1.4186
Epoch 5/5, Loss: 1.3830
新模型评估：
Accuracy: 65.97%
loss差为：
0.10783271789550786
单位数据loss差为：
0.00018092737901930848
正在评估DataOwner8的数据质量, 本轮评估的样本数据量为：673.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 64.29%
Epoch 1/5, Loss: 1.5348
Epoch 2/5, Loss: 1.5126
Epoch 3/5, Loss: 1.4844
Epoch 4/5, Loss: 1.4583
Epoch 5/5, Loss: 1.4321
新模型评估：
Accuracy: 65.14%
loss差为：
0.10274474187330762
单位数据loss差为：
0.00015266677841501875
正在评估DataOwner9的数据质量, 本轮评估的样本数据量为：691.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 64.29%
Epoch 1/5, Loss: 1.5062
Epoch 2/5, Loss: 1.4764
Epoch 3/5, Loss: 1.4494
Epoch 4/5, Loss: 1.4223
Epoch 5/5, Loss: 1.3934
新模型评估：
Accuracy: 66.75%
loss差为：
0.11275283856825391
单位数据loss差为：
0.0001631734277398754
正在评估DataOwner10的数据质量, 本轮评估的样本数据量为：229.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 64.29%
Epoch 1/5, Loss: 1.5270
Epoch 2/5, Loss: 1.5201
Epoch 3/5, Loss: 1.5035
Epoch 4/5, Loss: 1.5025
Epoch 5/5, Loss: 1.4686
新模型评估：
Accuracy: 67.21%
loss差为：
0.05846148729324341
单位数据loss差为：
0.0002552903375250804
经过服务器调节后的真实数据质量：
数据质量列表avg_f_list: [0.00023147160546821538, 0.00014825747915760316, 0.0001990585996393572, 0.00020345759183558834, 0.00015245974322574012, 0.00016470914846426041, 0.00018092737901930848, 0.00015266677841501875, 0.0001631734277398754, 0.0002552903375250804]
归一化后的数据质量列表avg_f_list:[0.9777463365734961, 0.9, 0.9474631073640376, 0.9515730529109724, 0.9039261439264841, 0.9153706717335097, 0.9305232433852597, 0.9041195753572021, 0.9139358593330855, 1.0]
CPC1调整模型中, 本轮训练的数据量为：190.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 64.29%
Epoch 1/5, Loss: 1.5483
Epoch 2/5, Loss: 1.5379
Epoch 3/5, Loss: 1.5262
Epoch 4/5, Loss: 1.5166
Epoch 5/5, Loss: 1.5096
新模型评估：
Accuracy: 64.37%
Model saved to ../../data/model/mnist_cnn_model
CPC2调整模型中, 本轮训练的数据量为：1613.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 64.37%
Epoch 1/5, Loss: 1.4465
Epoch 2/5, Loss: 1.3882
Epoch 3/5, Loss: 1.3259
Epoch 4/5, Loss: 1.2715
Epoch 5/5, Loss: 1.2049
新模型评估：
Accuracy: 70.18%
Model saved to ../../data/model/mnist_cnn_model
CPC3调整模型中, 本轮训练的数据量为：171.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 70.18%
Epoch 1/5, Loss: 1.1754
Epoch 2/5, Loss: 1.1752
Epoch 3/5, Loss: 1.1615
Epoch 4/5, Loss: 1.1584
Epoch 5/5, Loss: 1.1448
新模型评估：
Accuracy: 71.87%
Model saved to ../../data/model/mnist_cnn_model
CPC4调整模型中, 本轮训练的数据量为：229.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 71.87%
Epoch 1/5, Loss: 1.1566
Epoch 2/5, Loss: 1.1423
Epoch 3/5, Loss: 1.1585
Epoch 4/5, Loss: 1.1342
Epoch 5/5, Loss: 1.1093
新模型评估：
Accuracy: 72.81%
Model saved to ../../data/model/mnist_cnn_model
CPC5调整模型中, 本轮训练的数据量为：1536.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 72.81%
Epoch 1/5, Loss: 1.1005
Epoch 2/5, Loss: 1.0475
Epoch 3/5, Loss: 0.9983
Epoch 4/5, Loss: 0.9510
Epoch 5/5, Loss: 0.9065
新模型评估：
Accuracy: 74.02%
Model saved to ../../data/model/mnist_cnn_model
CPC6调整模型中, 本轮训练的数据量为：555.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 74.02%
Epoch 1/5, Loss: 0.9513
Epoch 2/5, Loss: 0.9300
Epoch 3/5, Loss: 0.9131
Epoch 4/5, Loss: 0.8948
Epoch 5/5, Loss: 0.8798
新模型评估：
Accuracy: 75.50%
Model saved to ../../data/model/mnist_cnn_model
CPC7调整模型中, 本轮训练的数据量为：596.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 75.50%
Epoch 1/5, Loss: 0.7819
Epoch 2/5, Loss: 0.7818
Epoch 3/5, Loss: 0.7670
Epoch 4/5, Loss: 0.7503
Epoch 5/5, Loss: 0.7342
新模型评估：
Accuracy: 74.75%
CPC8调整模型中, 本轮训练的数据量为：673.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 75.50%
Epoch 1/5, Loss: 0.8708
Epoch 2/5, Loss: 0.8595
Epoch 3/5, Loss: 0.8408
Epoch 4/5, Loss: 0.8270
Epoch 5/5, Loss: 0.7979
新模型评估：
Accuracy: 74.32%
CPC9调整模型中, 本轮训练的数据量为：691.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 75.50%
Epoch 1/5, Loss: 0.8265
Epoch 2/5, Loss: 0.8062
Epoch 3/5, Loss: 0.7884
Epoch 4/5, Loss: 0.7699
Epoch 5/5, Loss: 0.7548
新模型评估：
Accuracy: 74.92%
CPC10调整模型中, 本轮训练的数据量为：229.00 :
Model loaded from ../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 75.50%
Epoch 1/5, Loss: 0.8832
Epoch 2/5, Loss: 0.8877
Epoch 3/5, Loss: 0.8696
Epoch 4/5, Loss: 0.8633
Epoch 5/5, Loss: 0.8453
新模型评估：
Accuracy: 78.18%
Model saved to ../../data/model/mnist_cnn_model
DONE
========================= literation: 3 =========================
----- literation 3: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3096
DataOwner1的最优x_1 = 0.1585
DataOwner2的最优x_2 = 0.0814
DataOwner3的最优x_3 = 0.1316
DataOwner4的最优x_4 = 0.1355
DataOwner5的最优x_5 = 0.0860
DataOwner6的最优x_6 = 0.0989
DataOwner7的最优x_7 = 0.1150
DataOwner8的最优x_8 = 0.0862
DataOwner9的最优x_9 = 0.0973
DataOwner10的最优x_10 = 0.1760
每个DataOwner应该贡献数据比例 xn_list = [0.15846850607603902, 0.08143974091942317, 0.13164928852206384, 0.13550773114846237, 0.08601982641779658, 0.09890790460593121, 0.11496417456677381, 0.08624334044626925, 0.09732901295227193, 0.17597495422881376]
ModelOwner的最大效用 U(Eta) = 0.5454
Eta开始变化：
eta:0.01
**** log-parameter_analysis 运行时间： 2025-01-16 23:16:20 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
**** log-parameter_analysis 运行时间： 2025-01-16 23:17:03 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
初始化模型的准确率：
Accuracy: 26.82%
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.05061304292405353
DataOwner2: noise random: 0.07209495379438274
DataOwner3: noise random: 0.019392753636367478
DataOwner4: noise random: 0.03150579655057539
DataOwner5: noise random: 0.029215295941000398
DataOwner6: noise random: 0.09730857922041566
DataOwner7: noise random: 0.03401022819651341
DataOwner8: noise random: 0.04524425263152835
DataOwner9: noise random: 0.05335695663353808
DataOwner10: noise random: 0.01587623647847064
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9989652208362877, 0.9978953534555668, 0.9998479345454895, 0.9995987401835525, 0.9996558357396389, 0.9961663541405955, 0.9995326053834194, 0.9991712219260226, 0.9988512893050251, 0.9998980397804336]
归一化后的数据质量列表avg_f_list: [0.9750027458318714, 0.9463329305264394, 0.9986573028979402, 0.9919795066956925, 0.9935095272171646, 0.9, 0.990207256658681, 0.980523068539007, 0.9719496609190826, 1.0]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3591
DataOwner1的最优x_1 = 0.1234
DataOwner2的最优x_2 = 0.0929
DataOwner3的最优x_3 = 0.1459
DataOwner4的最优x_4 = 0.1398
DataOwner5的最优x_5 = 0.1412
DataOwner6的最优x_6 = 0.0346
DataOwner7的最优x_7 = 0.1381
DataOwner8的最优x_8 = 0.1289
DataOwner9的最优x_9 = 0.1204
DataOwner10的最优x_10 = 0.1471
每个DataOwner应该贡献数据比例 xn_list = [0.12342877930136804, 0.09288388904264357, 0.14590554092513597, 0.13979238029042346, 0.14120869892531246, 0.03455184982718184, 0.1381400383174937, 0.12888290666747879, 0.12035543424392725, 0.14711349811976918]
ModelOwner的最大效用 U(Eta) = 0.6021
DONE
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3591057383755207
DataOwner1的分配到的支付 ： 0.1373
DataOwner2的分配到的支付 ： 0.1003
DataOwner3的分配到的支付 ： 0.1662
DataOwner4的分配到的支付 ： 0.1582
DataOwner5的分配到的支付 ： 0.1601
DataOwner6的分配到的支付 ： 0.0355
DataOwner7的分配到的支付 ： 0.1561
DataOwner8的分配到的支付 ： 0.1442
DataOwner9的分配到的支付 ： 0.1335
DataOwner10的分配到的支付 ： 0.1678
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC1', 'DataOwner2': 'CPC2', 'DataOwner3': 'CPC3', 'DataOwner4': 'CPC4', 'DataOwner5': 'CPC5', 'DataOwner6': 'CPC6', 'DataOwner7': 'CPC7', 'DataOwner8': 'CPC8', 'DataOwner9': 'CPC9', 'DataOwner10': 'CPC10'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC2
DataOwner3 把数据交给 CPC3
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC8
DataOwner9 把数据交给 CPC9
DataOwner10 把数据交给 CPC10
DONE
----- literation 1: 模型训练 -----
CPC1调整模型中, 本轮训练的数据量为：978.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 26.82%
Epoch 1/5, Loss: 2.2987
Epoch 2/5, Loss: 2.2939
Epoch 3/5, Loss: 2.2885
Epoch 4/5, Loss: 2.2846
Epoch 5/5, Loss: 2.2765
新模型评估：
Accuracy: 36.64%
Model saved to ../../../data/model/mnist_cnn_model
CPC2调整模型中, 本轮训练的数据量为：1366.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 36.64%
Epoch 1/5, Loss: 2.2655
Epoch 2/5, Loss: 2.2522
Epoch 3/5, Loss: 2.2340
Epoch 4/5, Loss: 2.2144
Epoch 5/5, Loss: 2.1843
新模型评估：
Accuracy: 45.30%
Model saved to ../../../data/model/mnist_cnn_model
CPC3调整模型中, 本轮训练的数据量为：660.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 45.30%
Epoch 1/5, Loss: 2.1653
Epoch 2/5, Loss: 2.1494
Epoch 3/5, Loss: 2.1301
Epoch 4/5, Loss: 2.1140
Epoch 5/5, Loss: 2.0990
新模型评估：
Accuracy: 49.87%
Model saved to ../../../data/model/mnist_cnn_model
CPC4调整模型中, 本轮训练的数据量为：316.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 49.87%
Epoch 1/5, Loss: 2.0970
Epoch 2/5, Loss: 2.0875
Epoch 3/5, Loss: 2.0784
Epoch 4/5, Loss: 2.0698
Epoch 5/5, Loss: 2.0610
新模型评估：
Accuracy: 50.24%
Model saved to ../../../data/model/mnist_cnn_model
CPC5调整模型中, 本轮训练的数据量为：319.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 50.24%
Epoch 1/5, Loss: 2.0477
Epoch 2/5, Loss: 2.0383
Epoch 3/5, Loss: 2.0292
Epoch 4/5, Loss: 2.0199
Epoch 5/5, Loss: 2.0099
新模型评估：
Accuracy: 50.42%
Model saved to ../../../data/model/mnist_cnn_model
CPC6调整模型中, 本轮训练的数据量为：273.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 50.42%
Epoch 1/5, Loss: 2.0450
Epoch 2/5, Loss: 2.0356
Epoch 3/5, Loss: 2.0135
Epoch 4/5, Loss: 2.0091
Epoch 5/5, Loss: 2.0233
新模型评估：
Accuracy: 51.16%
Model saved to ../../../data/model/mnist_cnn_model
CPC7调整模型中, 本轮训练的数据量为：312.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 51.16%
Epoch 1/5, Loss: 1.9815
Epoch 2/5, Loss: 1.9713
Epoch 3/5, Loss: 1.9574
Epoch 4/5, Loss: 1.9449
Epoch 5/5, Loss: 1.9339
新模型评估：
Accuracy: 52.89%
Model saved to ../../../data/model/mnist_cnn_model
CPC8调整模型中, 本轮训练的数据量为：875.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 52.89%
Epoch 1/5, Loss: 1.9059
Epoch 2/5, Loss: 1.8731
Epoch 3/5, Loss: 1.8419
Epoch 4/5, Loss: 1.8121
Epoch 5/5, Loss: 1.7771
新模型评估：
Accuracy: 58.15%
Model saved to ../../../data/model/mnist_cnn_model
CPC9调整模型中, 本轮训练的数据量为：408.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 58.15%
Epoch 1/5, Loss: 1.7874
Epoch 2/5, Loss: 1.7737
Epoch 3/5, Loss: 1.7501
Epoch 4/5, Loss: 1.7424
Epoch 5/5, Loss: 1.7193
新模型评估：
Accuracy: 59.21%
Model saved to ../../../data/model/mnist_cnn_model
CPC10调整模型中, 本轮训练的数据量为：1165.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 59.21%
Epoch 1/5, Loss: 1.6959
Epoch 2/5, Loss: 1.6506
Epoch 3/5, Loss: 1.5954
Epoch 4/5, Loss: 1.5524
Epoch 5/5, Loss: 1.5081
新模型评估：
Accuracy: 67.93%
Model saved to ../../../data/model/mnist_cnn_model
DONE
========================= literation: 2 =========================
----- literation 2: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3591
DataOwner1的最优x_1 = 0.1234
DataOwner2的最优x_2 = 0.0929
DataOwner3的最优x_3 = 0.1459
DataOwner4的最优x_4 = 0.1398
DataOwner5的最优x_5 = 0.1412
DataOwner6的最优x_6 = 0.0346
DataOwner7的最优x_7 = 0.1381
DataOwner8的最优x_8 = 0.1289
DataOwner9的最优x_9 = 0.1204
DataOwner10的最优x_10 = 0.1471
每个DataOwner应该贡献数据比例 xn_list = [0.12342877930136804, 0.09288388904264357, 0.14590554092513597, 0.13979238029042346, 0.14120869892531246, 0.03455184982718184, 0.1381400383174937, 0.12888290666747879, 0.12035543424392725, 0.14711349811976918]
ModelOwner的最大效用 U(Eta) = 0.6021
DONE
----- literation 2: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3591057383755207
DataOwner1的分配到的支付 ： 0.1373
DataOwner2的分配到的支付 ： 0.1003
DataOwner3的分配到的支付 ： 0.1662
DataOwner4的分配到的支付 ： 0.1582
DataOwner5的分配到的支付 ： 0.1601
DataOwner6的分配到的支付 ： 0.0355
DataOwner7的分配到的支付 ： 0.1561
DataOwner8的分配到的支付 ： 0.1442
DataOwner9的分配到的支付 ： 0.1335
DataOwner10的分配到的支付 ： 0.1678
DONE
----- literation 2: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC2
DataOwner3 把数据交给 CPC3
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC8
DataOwner9 把数据交给 CPC9
DataOwner10 把数据交给 CPC10
DONE
----- literation 2: 模型训练 -----
重新调整fn，进而调整xn、Eta
正在评估DataOwner1的数据质量, 本轮评估的样本数据量为：978.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 67.93%
Epoch 1/5, Loss: 1.4837
Epoch 2/5, Loss: 1.4330
Epoch 3/5, Loss: 1.3941
Epoch 4/5, Loss: 1.3462
Epoch 5/5, Loss: 1.3222
新模型评估：
Accuracy: 71.69%
loss差为：
0.1614663526415825
单位数据loss差为：
0.00016509852008341768
正在评估DataOwner2的数据质量, 本轮评估的样本数据量为：1366.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 67.93%
Epoch 1/5, Loss: 1.4399
Epoch 2/5, Loss: 1.3841
Epoch 3/5, Loss: 1.3272
Epoch 4/5, Loss: 1.2779
Epoch 5/5, Loss: 1.2348
新模型评估：
Accuracy: 71.47%
loss差为：
0.2050615820017727
单位数据loss差为：
0.00015011828843467987
正在评估DataOwner3的数据质量, 本轮评估的样本数据量为：660.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 67.93%
Epoch 1/5, Loss: 1.4879
Epoch 2/5, Loss: 1.4557
Epoch 3/5, Loss: 1.4271
Epoch 4/5, Loss: 1.3910
Epoch 5/5, Loss: 1.3614
新模型评估：
Accuracy: 68.34%
loss差为：
0.126499436118386
单位数据loss差为：
0.00019166581230058483
正在评估DataOwner4的数据质量, 本轮评估的样本数据量为：316.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 67.93%
Epoch 1/5, Loss: 1.4778
Epoch 2/5, Loss: 1.4645
Epoch 3/5, Loss: 1.4483
Epoch 4/5, Loss: 1.4336
Epoch 5/5, Loss: 1.4194
新模型评估：
Accuracy: 68.08%
loss差为：
0.05844395160675053
单位数据loss差为：
0.00018494921394541307
正在评估DataOwner5的数据质量, 本轮评估的样本数据量为：319.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 67.93%
Epoch 1/5, Loss: 1.5024
Epoch 2/5, Loss: 1.4843
Epoch 3/5, Loss: 1.4677
Epoch 4/5, Loss: 1.4516
Epoch 5/5, Loss: 1.4362
新模型评估：
Accuracy: 65.47%
loss差为：
0.0661638498306274
单位数据loss差为：
0.0002074101875568257
正在评估DataOwner6的数据质量, 本轮评估的样本数据量为：273.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 67.93%
Epoch 1/5, Loss: 1.4951
Epoch 2/5, Loss: 1.4399
Epoch 3/5, Loss: 1.4396
Epoch 4/5, Loss: 1.4224
Epoch 5/5, Loss: 1.4084
新模型评估：
Accuracy: 68.15%
loss差为：
0.0866431951522828
单位数据loss差为：
0.00031737434121715315
正在评估DataOwner7的数据质量, 本轮评估的样本数据量为：312.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 67.93%
Epoch 1/5, Loss: 1.4629
Epoch 2/5, Loss: 1.4473
Epoch 3/5, Loss: 1.4318
Epoch 4/5, Loss: 1.4144
Epoch 5/5, Loss: 1.4059
新模型评估：
Accuracy: 67.56%
loss差为：
0.056969642639160156
单位数据loss差为：
0.00018259500845884666
正在评估DataOwner8的数据质量, 本轮评估的样本数据量为：875.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 67.93%
Epoch 1/5, Loss: 1.4451
Epoch 2/5, Loss: 1.4072
Epoch 3/5, Loss: 1.3682
Epoch 4/5, Loss: 1.3292
Epoch 5/5, Loss: 1.2947
新模型评估：
Accuracy: 69.23%
loss差为：
0.15036771127155846
单位数据loss差为：
0.0001718488128817811
正在评估DataOwner9的数据质量, 本轮评估的样本数据量为：408.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 67.93%
Epoch 1/5, Loss: 1.4493
Epoch 2/5, Loss: 1.4253
Epoch 3/5, Loss: 1.4159
Epoch 4/5, Loss: 1.4016
Epoch 5/5, Loss: 1.3587
新模型评估：
Accuracy: 65.89%
loss差为：
0.09057218687874924
单位数据loss差为：
0.00022199065411458146
正在评估DataOwner10的数据质量, 本轮评估的样本数据量为：1165.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 67.93%
Epoch 1/5, Loss: 1.4368
Epoch 2/5, Loss: 1.3944
Epoch 3/5, Loss: 1.3461
Epoch 4/5, Loss: 1.2989
Epoch 5/5, Loss: 1.2595
新模型评估：
Accuracy: 69.78%
loss差为：
0.17734144863329426
单位数据loss差为：
0.0001522244194277204
经过服务器调节后的真实数据质量：
数据质量列表avg_f_list: [0.00016509852008341768, 0.00015011828843467987, 0.00019166581230058483, 0.00018494921394541307, 0.0002074101875568257, 0.00031737434121715315, 0.00018259500845884666, 0.0001718488128817811, 0.00022199065411458146, 0.0001522244194277204]
归一化后的数据质量列表avg_f_list:[0.9089564660886865, 0.9, 0.9248406698440624, 0.9208249118230913, 0.9342540064583836, 1.0, 0.9194173660587368, 0.9129923695349688, 0.942971458721064, 0.9012592255753995]
CPC1调整模型中, 本轮训练的数据量为：978.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 67.93%
Epoch 1/5, Loss: 1.4910
Epoch 2/5, Loss: 1.4359
Epoch 3/5, Loss: 1.4020
Epoch 4/5, Loss: 1.3575
Epoch 5/5, Loss: 1.3264
新模型评估：
Accuracy: 71.41%
Model saved to ../../../data/model/mnist_cnn_model
CPC2调整模型中, 本轮训练的数据量为：1366.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 71.41%
Epoch 1/5, Loss: 1.2505
Epoch 2/5, Loss: 1.1988
Epoch 3/5, Loss: 1.1505
Epoch 4/5, Loss: 1.1056
Epoch 5/5, Loss: 1.0624
新模型评估：
Accuracy: 74.43%
Model saved to ../../../data/model/mnist_cnn_model
CPC3调整模型中, 本轮训练的数据量为：660.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 74.43%
Epoch 1/5, Loss: 1.0832
Epoch 2/5, Loss: 1.0564
Epoch 3/5, Loss: 1.0293
Epoch 4/5, Loss: 1.0100
Epoch 5/5, Loss: 0.9703
新模型评估：
Accuracy: 74.20%
CPC4调整模型中, 本轮训练的数据量为：316.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 74.43%
Epoch 1/5, Loss: 1.0591
Epoch 2/5, Loss: 1.0413
Epoch 3/5, Loss: 1.0292
Epoch 4/5, Loss: 1.0155
Epoch 5/5, Loss: 1.0013
新模型评估：
Accuracy: 74.22%
CPC5调整模型中, 本轮训练的数据量为：319.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 74.43%
Epoch 1/5, Loss: 1.0742
Epoch 2/5, Loss: 1.0561
Epoch 3/5, Loss: 1.0426
Epoch 4/5, Loss: 1.0289
Epoch 5/5, Loss: 1.0165
新模型评估：
Accuracy: 71.38%
CPC6调整模型中, 本轮训练的数据量为：273.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 74.43%
Epoch 1/5, Loss: 1.0710
Epoch 2/5, Loss: 1.0600
Epoch 3/5, Loss: 1.0073
Epoch 4/5, Loss: 1.0426
Epoch 5/5, Loss: 0.9997
新模型评估：
Accuracy: 74.84%
Model saved to ../../../data/model/mnist_cnn_model
CPC7调整模型中, 本轮训练的数据量为：312.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 74.84%
Epoch 1/5, Loss: 1.0129
Epoch 2/5, Loss: 1.0023
Epoch 3/5, Loss: 0.9872
Epoch 4/5, Loss: 0.9768
Epoch 5/5, Loss: 0.9630
新模型评估：
Accuracy: 74.68%
CPC8调整模型中, 本轮训练的数据量为：875.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 74.84%
Epoch 1/5, Loss: 0.9860
Epoch 2/5, Loss: 0.9572
Epoch 3/5, Loss: 0.9218
Epoch 4/5, Loss: 0.9010
Epoch 5/5, Loss: 0.8784
新模型评估：
Accuracy: 75.47%
Model saved to ../../../data/model/mnist_cnn_model
CPC9调整模型中, 本轮训练的数据量为：408.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 75.47%
Epoch 1/5, Loss: 0.8914
Epoch 2/5, Loss: 0.8496
Epoch 3/5, Loss: 0.8779
Epoch 4/5, Loss: 0.8420
Epoch 5/5, Loss: 0.8206
新模型评估：
Accuracy: 73.89%
CPC10调整模型中, 本轮训练的数据量为：1165.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 75.47%
Epoch 1/5, Loss: 0.8597
Epoch 2/5, Loss: 0.8260
Epoch 3/5, Loss: 0.7921
Epoch 4/5, Loss: 0.7683
Epoch 5/5, Loss: 0.7393
新模型评估：
Accuracy: 75.82%
Model saved to ../../../data/model/mnist_cnn_model
DONE
========================= literation: 3 =========================
----- literation 3: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.2998
DataOwner1的最优x_1 = 0.0993
DataOwner2的最优x_2 = 0.0893
DataOwner3的最优x_3 = 0.1160
DataOwner4的最优x_4 = 0.1119
DataOwner5的最优x_5 = 0.1254
DataOwner6的最优x_6 = 0.1807
DataOwner7的最优x_7 = 0.1105
DataOwner8的最优x_8 = 0.1037
DataOwner9的最优x_9 = 0.1337
DataOwner10的最优x_10 = 0.0908
每个DataOwner应该贡献数据比例 xn_list = [0.09931525674025675, 0.08932699431354675, 0.11604472564701845, 0.11193011868650679, 0.12539768152589625, 0.18065186664736363, 0.1104698509467405, 0.10368266646184639, 0.1337070536244652, 0.09075643941680754]
ModelOwner的最大效用 U(Eta) = 0.5347
Eta开始变化：
eta:0.01
eta:0.02
eta:0.03
eta:0.04
eta:0.05
eta:0.060000000000000005
eta:0.06999999999999999
eta:0.08
eta:0.09
eta:0.09999999999999999
eta:0.11
eta:0.12
eta:0.13
eta:0.14
eta:0.15000000000000002
eta:0.16
new: DataOwner1的最优x_1 = 0.0122
new: DataOwner2的最优x_2 = 0.0110
new: DataOwner3的最优x_3 = 0.0143
new: DataOwner4的最优x_4 = 0.0138
new: DataOwner5的最优x_5 = 0.0154
new: DataOwner6的最优x_6 = 0.0222
new: DataOwner7的最优x_7 = 0.0136
new: DataOwner8的最优x_8 = 0.0128
new: DataOwner9的最优x_9 = 0.0165
new: DataOwner10的最优x_10 = 0.0112
eta:0.17
new: DataOwner1的最优x_1 = 0.0130
new: DataOwner2的最优x_2 = 0.0117
new: DataOwner3的最优x_3 = 0.0152
new: DataOwner4的最优x_4 = 0.0146
new: DataOwner5的最优x_5 = 0.0164
new: DataOwner6的最优x_6 = 0.0236
new: DataOwner7的最优x_7 = 0.0144
new: DataOwner8的最优x_8 = 0.0136
new: DataOwner9的最优x_9 = 0.0175
new: DataOwner10的最优x_10 = 0.0119
eta:0.18000000000000002
eta:0.19
eta:0.2
eta:0.21000000000000002
eta:0.22
eta:0.23
eta:0.24000000000000002
new: DataOwner1的最优x_1 = 0.0183
new: DataOwner2的最优x_2 = 0.0165
new: DataOwner3的最优x_3 = 0.0214
new: DataOwner4的最优x_4 = 0.0207
new: DataOwner5的最优x_5 = 0.0232
new: DataOwner6的最优x_6 = 0.0334
new: DataOwner7的最优x_7 = 0.0204
new: DataOwner8的最优x_8 = 0.0191
new: DataOwner9的最优x_9 = 0.0247
new: DataOwner10的最优x_10 = 0.0168
eta:0.25
eta:0.26
new: DataOwner1的最优x_1 = 0.0199
new: DataOwner2的最优x_2 = 0.0179
new: DataOwner3的最优x_3 = 0.0232
new: DataOwner4的最优x_4 = 0.0224
new: DataOwner5的最优x_5 = 0.0251
new: DataOwner6的最优x_6 = 0.0361
new: DataOwner7的最优x_7 = 0.0221
new: DataOwner8的最优x_8 = 0.0207
new: DataOwner9的最优x_9 = 0.0267
new: DataOwner10的最优x_10 = 0.0182
eta:0.27
new: DataOwner1的最优x_1 = 0.0206
new: DataOwner2的最优x_2 = 0.0186
new: DataOwner3的最优x_3 = 0.0241
new: DataOwner4的最优x_4 = 0.0233
new: DataOwner5的最优x_5 = 0.0260
new: DataOwner6的最优x_6 = 0.0375
new: DataOwner7的最优x_7 = 0.0229
new: DataOwner8的最优x_8 = 0.0215
new: DataOwner9的最优x_9 = 0.0278
new: DataOwner10的最优x_10 = 0.0189
eta:0.28
new: DataOwner1的最优x_1 = 0.0214
new: DataOwner2的最优x_2 = 0.0192
new: DataOwner3的最优x_3 = 0.0250
new: DataOwner4的最优x_4 = 0.0241
new: DataOwner5的最优x_5 = 0.0270
new: DataOwner6的最优x_6 = 0.0389
new: DataOwner7的最优x_7 = 0.0238
new: DataOwner8的最优x_8 = 0.0223
new: DataOwner9的最优x_9 = 0.0288
new: DataOwner10的最优x_10 = 0.0196
eta:0.29000000000000004
new: DataOwner1的最优x_1 = 0.0222
new: DataOwner2的最优x_2 = 0.0199
new: DataOwner3的最优x_3 = 0.0259
new: DataOwner4的最优x_4 = 0.0250
new: DataOwner5的最优x_5 = 0.0280
new: DataOwner6的最优x_6 = 0.0403
new: DataOwner7的最优x_7 = 0.0246
new: DataOwner8的最优x_8 = 0.0231
new: DataOwner9的最优x_9 = 0.0298
new: DataOwner10的最优x_10 = 0.0202
eta:0.3
new: DataOwner1的最优x_1 = 0.0229
new: DataOwner2的最优x_2 = 0.0206
new: DataOwner3的最优x_3 = 0.0268
new: DataOwner4的最优x_4 = 0.0258
new: DataOwner5的最优x_5 = 0.0289
new: DataOwner6的最优x_6 = 0.0417
new: DataOwner7的最优x_7 = 0.0255
new: DataOwner8的最优x_8 = 0.0239
new: DataOwner9的最优x_9 = 0.0309
new: DataOwner10的最优x_10 = 0.0209
eta:0.31
new: DataOwner1的最优x_1 = 0.0237
new: DataOwner2的最优x_2 = 0.0213
new: DataOwner3的最优x_3 = 0.0277
new: DataOwner4的最优x_4 = 0.0267
new: DataOwner5的最优x_5 = 0.0299
new: DataOwner6的最优x_6 = 0.0431
new: DataOwner7的最优x_7 = 0.0263
new: DataOwner8的最优x_8 = 0.0247
new: DataOwner9的最优x_9 = 0.0319
new: DataOwner10的最优x_10 = 0.0216
eta:0.32
new: DataOwner1的最优x_1 = 0.0245
new: DataOwner2的最优x_2 = 0.0220
new: DataOwner3的最优x_3 = 0.0286
new: DataOwner4的最优x_4 = 0.0276
new: DataOwner5的最优x_5 = 0.0309
new: DataOwner6的最优x_6 = 0.0445
new: DataOwner7的最优x_7 = 0.0272
new: DataOwner8的最优x_8 = 0.0255
new: DataOwner9的最优x_9 = 0.0329
new: DataOwner10的最优x_10 = 0.0223
eta:0.33
new: DataOwner1的最优x_1 = 0.0252
new: DataOwner2的最优x_2 = 0.0227
new: DataOwner3的最优x_3 = 0.0295
new: DataOwner4的最优x_4 = 0.0284
new: DataOwner5的最优x_5 = 0.0318
new: DataOwner6的最优x_6 = 0.0459
new: DataOwner7的最优x_7 = 0.0280
new: DataOwner8的最优x_8 = 0.0263
new: DataOwner9的最优x_9 = 0.0339
new: DataOwner10的最优x_10 = 0.0230
eta:0.34
new: DataOwner1的最优x_1 = 0.0260
new: DataOwner2的最优x_2 = 0.0234
new: DataOwner3的最优x_3 = 0.0304
new: DataOwner4的最优x_4 = 0.0293
new: DataOwner5的最优x_5 = 0.0328
new: DataOwner6的最优x_6 = 0.0473
new: DataOwner7的最优x_7 = 0.0289
new: DataOwner8的最优x_8 = 0.0271
new: DataOwner9的最优x_9 = 0.0350
new: DataOwner10的最优x_10 = 0.0237
eta:0.35000000000000003
new: DataOwner1的最优x_1 = 0.0267
new: DataOwner2的最优x_2 = 0.0241
new: DataOwner3的最优x_3 = 0.0312
new: DataOwner4的最优x_4 = 0.0301
new: DataOwner5的最优x_5 = 0.0338
new: DataOwner6的最优x_6 = 0.0486
new: DataOwner7的最优x_7 = 0.0297
new: DataOwner8的最优x_8 = 0.0279
new: DataOwner9的最优x_9 = 0.0360
new: DataOwner10的最优x_10 = 0.0244
eta:0.36000000000000004
new: DataOwner1的最优x_1 = 0.0275
new: DataOwner2的最优x_2 = 0.0247
new: DataOwner3的最优x_3 = 0.0321
new: DataOwner4的最优x_4 = 0.0310
new: DataOwner5的最优x_5 = 0.0347
new: DataOwner6的最优x_6 = 0.0500
new: DataOwner7的最优x_7 = 0.0306
new: DataOwner8的最优x_8 = 0.0287
new: DataOwner9的最优x_9 = 0.0370
new: DataOwner10的最优x_10 = 0.0251
eta:0.37
eta:0.38
new: DataOwner1的最优x_1 = 0.0290
new: DataOwner2的最优x_2 = 0.0261
new: DataOwner3的最优x_3 = 0.0339
new: DataOwner4的最优x_4 = 0.0327
new: DataOwner5的最优x_5 = 0.0367
new: DataOwner6的最优x_6 = 0.0528
new: DataOwner7的最优x_7 = 0.0323
new: DataOwner8的最优x_8 = 0.0303
new: DataOwner9的最优x_9 = 0.0391
new: DataOwner10的最优x_10 = 0.0265
eta:0.39
new: DataOwner1的最优x_1 = 0.0298
new: DataOwner2的最优x_2 = 0.0268
new: DataOwner3的最优x_3 = 0.0348
new: DataOwner4的最优x_4 = 0.0336
new: DataOwner5的最优x_5 = 0.0376
new: DataOwner6的最优x_6 = 0.0542
new: DataOwner7的最优x_7 = 0.0331
new: DataOwner8的最优x_8 = 0.0311
new: DataOwner9的最优x_9 = 0.0401
new: DataOwner10的最优x_10 = 0.0272
eta:0.4
eta:0.41000000000000003
eta:0.42000000000000004
new: DataOwner1的最优x_1 = 0.0321
new: DataOwner2的最优x_2 = 0.0289
new: DataOwner3的最优x_3 = 0.0375
new: DataOwner4的最优x_4 = 0.0362
new: DataOwner5的最优x_5 = 0.0405
new: DataOwner6的最优x_6 = 0.0584
new: DataOwner7的最优x_7 = 0.0357
new: DataOwner8的最优x_8 = 0.0335
new: DataOwner9的最优x_9 = 0.0432
new: DataOwner10的最优x_10 = 0.0293
eta:0.43
eta:0.44
new: DataOwner1的最优x_1 = 0.0336
new: DataOwner2的最优x_2 = 0.0302
new: DataOwner3的最优x_3 = 0.0393
new: DataOwner4的最优x_4 = 0.0379
new: DataOwner5的最优x_5 = 0.0424
new: DataOwner6的最优x_6 = 0.0612
new: DataOwner7的最优x_7 = 0.0374
new: DataOwner8的最优x_8 = 0.0351
new: DataOwner9的最优x_9 = 0.0453
new: DataOwner10的最优x_10 = 0.0307
eta:0.45
new: DataOwner1的最优x_1 = 0.0344
new: DataOwner2的最优x_2 = 0.0309
new: DataOwner3的最优x_3 = 0.0402
new: DataOwner4的最优x_4 = 0.0388
new: DataOwner5的最优x_5 = 0.0434
new: DataOwner6的最优x_6 = 0.0625
new: DataOwner7的最优x_7 = 0.0382
new: DataOwner8的最优x_8 = 0.0359
new: DataOwner9的最优x_9 = 0.0463
new: DataOwner10的最优x_10 = 0.0314
eta:0.46
new: DataOwner1的最优x_1 = 0.0351
new: DataOwner2的最优x_2 = 0.0316
new: DataOwner3的最优x_3 = 0.0411
new: DataOwner4的最优x_4 = 0.0396
new: DataOwner5的最优x_5 = 0.0444
new: DataOwner6的最优x_6 = 0.0639
new: DataOwner7的最优x_7 = 0.0391
new: DataOwner8的最优x_8 = 0.0367
new: DataOwner9的最优x_9 = 0.0473
new: DataOwner10的最优x_10 = 0.0321
eta:0.47000000000000003
new: DataOwner1的最优x_1 = 0.0359
new: DataOwner2的最优x_2 = 0.0323
new: DataOwner3的最优x_3 = 0.0420
new: DataOwner4的最优x_4 = 0.0405
new: DataOwner5的最优x_5 = 0.0453
new: DataOwner6的最优x_6 = 0.0653
new: DataOwner7的最优x_7 = 0.0399
new: DataOwner8的最优x_8 = 0.0375
new: DataOwner9的最优x_9 = 0.0483
new: DataOwner10的最优x_10 = 0.0328
eta:0.48000000000000004
new: DataOwner1的最优x_1 = 0.0367
new: DataOwner2的最优x_2 = 0.0330
new: DataOwner3的最优x_3 = 0.0429
new: DataOwner4的最优x_4 = 0.0413
new: DataOwner5的最优x_5 = 0.0463
new: DataOwner6的最优x_6 = 0.0667
new: DataOwner7的最优x_7 = 0.0408
new: DataOwner8的最优x_8 = 0.0383
new: DataOwner9的最优x_9 = 0.0494
new: DataOwner10的最优x_10 = 0.0335
eta:0.49
eta:0.5
eta:0.51
eta:0.52
eta:0.53
new: DataOwner1的最优x_1 = 0.0405
new: DataOwner2的最优x_2 = 0.0364
new: DataOwner3的最优x_3 = 0.0473
new: DataOwner4的最优x_4 = 0.0456
new: DataOwner5的最优x_5 = 0.0511
new: DataOwner6的最优x_6 = 0.0737
new: DataOwner7的最优x_7 = 0.0450
new: DataOwner8的最优x_8 = 0.0423
new: DataOwner9的最优x_9 = 0.0545
new: DataOwner10的最优x_10 = 0.0370
eta:0.54
new: DataOwner1的最优x_1 = 0.0413
new: DataOwner2的最优x_2 = 0.0371
new: DataOwner3的最优x_3 = 0.0482
new: DataOwner4的最优x_4 = 0.0465
new: DataOwner5的最优x_5 = 0.0521
new: DataOwner6的最优x_6 = 0.0751
new: DataOwner7的最优x_7 = 0.0459
new: DataOwner8的最优x_8 = 0.0431
new: DataOwner9的最优x_9 = 0.0555
new: DataOwner10的最优x_10 = 0.0377
eta:0.55
new: DataOwner1的最优x_1 = 0.0420
new: DataOwner2的最优x_2 = 0.0378
new: DataOwner3的最优x_3 = 0.0491
new: DataOwner4的最优x_4 = 0.0474
new: DataOwner5的最优x_5 = 0.0531
new: DataOwner6的最优x_6 = 0.0764
new: DataOwner7的最优x_7 = 0.0467
new: DataOwner8的最优x_8 = 0.0439
new: DataOwner9的最优x_9 = 0.0566
new: DataOwner10的最优x_10 = 0.0384
eta:0.56
new: DataOwner1的最优x_1 = 0.0428
new: DataOwner2的最优x_2 = 0.0385
new: DataOwner3的最优x_3 = 0.0500
new: DataOwner4的最优x_4 = 0.0482
new: DataOwner5的最优x_5 = 0.0540
new: DataOwner6的最优x_6 = 0.0778
new: DataOwner7的最优x_7 = 0.0476
new: DataOwner8的最优x_8 = 0.0447
new: DataOwner9的最优x_9 = 0.0576
new: DataOwner10的最优x_10 = 0.0391
eta:0.5700000000000001
eta:0.5800000000000001
new: DataOwner1的最优x_1 = 0.0443
new: DataOwner2的最优x_2 = 0.0399
new: DataOwner3的最优x_3 = 0.0518
new: DataOwner4的最优x_4 = 0.0499
new: DataOwner5的最优x_5 = 0.0560
new: DataOwner6的最优x_6 = 0.0806
new: DataOwner7的最优x_7 = 0.0493
new: DataOwner8的最优x_8 = 0.0463
new: DataOwner9的最优x_9 = 0.0597
new: DataOwner10的最优x_10 = 0.0405
eta:0.59
new: DataOwner1的最优x_1 = 0.0451
new: DataOwner2的最优x_2 = 0.0405
new: DataOwner3的最优x_3 = 0.0527
new: DataOwner4的最优x_4 = 0.0508
new: DataOwner5的最优x_5 = 0.0569
new: DataOwner6的最优x_6 = 0.0820
new: DataOwner7的最优x_7 = 0.0501
new: DataOwner8的最优x_8 = 0.0471
new: DataOwner9的最优x_9 = 0.0607
new: DataOwner10的最优x_10 = 0.0412
eta:0.6
new: DataOwner1的最优x_1 = 0.0458
new: DataOwner2的最优x_2 = 0.0412
new: DataOwner3的最优x_3 = 0.0536
new: DataOwner4的最优x_4 = 0.0517
new: DataOwner5的最优x_5 = 0.0579
new: DataOwner6的最优x_6 = 0.0834
new: DataOwner7的最优x_7 = 0.0510
new: DataOwner8的最优x_8 = 0.0479
new: DataOwner9的最优x_9 = 0.0617
new: DataOwner10的最优x_10 = 0.0419
eta:0.61
new: DataOwner1的最优x_1 = 0.0466
new: DataOwner2的最优x_2 = 0.0419
new: DataOwner3的最优x_3 = 0.0545
new: DataOwner4的最优x_4 = 0.0525
new: DataOwner5的最优x_5 = 0.0588
new: DataOwner6的最优x_6 = 0.0848
new: DataOwner7的最优x_7 = 0.0518
new: DataOwner8的最优x_8 = 0.0487
new: DataOwner9的最优x_9 = 0.0627
new: DataOwner10的最优x_10 = 0.0426
eta:0.62
new: DataOwner1的最优x_1 = 0.0474
new: DataOwner2的最优x_2 = 0.0426
new: DataOwner3的最优x_3 = 0.0554
new: DataOwner4的最优x_4 = 0.0534
new: DataOwner5的最优x_5 = 0.0598
new: DataOwner6的最优x_6 = 0.0862
new: DataOwner7的最优x_7 = 0.0527
new: DataOwner8的最优x_8 = 0.0495
new: DataOwner9的最优x_9 = 0.0638
new: DataOwner10的最优x_10 = 0.0433
eta:0.63
new: DataOwner1的最优x_1 = 0.0481
new: DataOwner2的最优x_2 = 0.0433
new: DataOwner3的最优x_3 = 0.0562
new: DataOwner4的最优x_4 = 0.0543
new: DataOwner5的最优x_5 = 0.0608
new: DataOwner6的最优x_6 = 0.0876
new: DataOwner7的最优x_7 = 0.0535
new: DataOwner8的最优x_8 = 0.0503
new: DataOwner9的最优x_9 = 0.0648
new: DataOwner10的最优x_10 = 0.0440
eta:0.64
new: DataOwner1的最优x_1 = 0.0489
new: DataOwner2的最优x_2 = 0.0440
new: DataOwner3的最优x_3 = 0.0571
new: DataOwner4的最优x_4 = 0.0551
new: DataOwner5的最优x_5 = 0.0617
new: DataOwner6的最优x_6 = 0.0890
new: DataOwner7的最优x_7 = 0.0544
new: DataOwner8的最优x_8 = 0.0511
new: DataOwner9的最优x_9 = 0.0658
new: DataOwner10的最优x_10 = 0.0447
eta:0.65
new: DataOwner1的最优x_1 = 0.0497
new: DataOwner2的最优x_2 = 0.0447
new: DataOwner3的最优x_3 = 0.0580
new: DataOwner4的最优x_4 = 0.0560
new: DataOwner5的最优x_5 = 0.0627
new: DataOwner6的最优x_6 = 0.0903
new: DataOwner7的最优x_7 = 0.0552
new: DataOwner8的最优x_8 = 0.0518
new: DataOwner9的最优x_9 = 0.0669
new: DataOwner10的最优x_10 = 0.0454
eta:0.66
new: DataOwner1的最优x_1 = 0.0504
new: DataOwner2的最优x_2 = 0.0454
new: DataOwner3的最优x_3 = 0.0589
new: DataOwner4的最优x_4 = 0.0568
new: DataOwner5的最优x_5 = 0.0637
new: DataOwner6的最优x_6 = 0.0917
new: DataOwner7的最优x_7 = 0.0561
new: DataOwner8的最优x_8 = 0.0526
new: DataOwner9的最优x_9 = 0.0679
new: DataOwner10的最优x_10 = 0.0461
eta:0.67
eta:0.68
new: DataOwner1的最优x_1 = 0.0520
new: DataOwner2的最优x_2 = 0.0467
new: DataOwner3的最优x_3 = 0.0607
new: DataOwner4的最优x_4 = 0.0586
new: DataOwner5的最优x_5 = 0.0656
new: DataOwner6的最优x_6 = 0.0945
new: DataOwner7的最优x_7 = 0.0578
new: DataOwner8的最优x_8 = 0.0542
new: DataOwner9的最优x_9 = 0.0700
new: DataOwner10的最优x_10 = 0.0475
eta:0.6900000000000001
new: DataOwner1的最优x_1 = 0.0527
new: DataOwner2的最优x_2 = 0.0474
new: DataOwner3的最优x_3 = 0.0616
new: DataOwner4的最优x_4 = 0.0594
new: DataOwner5的最优x_5 = 0.0666
new: DataOwner6的最优x_6 = 0.0959
new: DataOwner7的最优x_7 = 0.0586
new: DataOwner8的最优x_8 = 0.0550
new: DataOwner9的最优x_9 = 0.0710
new: DataOwner10的最优x_10 = 0.0482
eta:0.7000000000000001
new: DataOwner1的最优x_1 = 0.0535
new: DataOwner2的最优x_2 = 0.0481
new: DataOwner3的最优x_3 = 0.0625
new: DataOwner4的最优x_4 = 0.0603
new: DataOwner5的最优x_5 = 0.0675
new: DataOwner6的最优x_6 = 0.0973
new: DataOwner7的最优x_7 = 0.0595
new: DataOwner8的最优x_8 = 0.0558
new: DataOwner9的最优x_9 = 0.0720
new: DataOwner10的最优x_10 = 0.0489
eta:0.7100000000000001
new: DataOwner1的最优x_1 = 0.0543
new: DataOwner2的最优x_2 = 0.0488
new: DataOwner3的最优x_3 = 0.0634
new: DataOwner4的最优x_4 = 0.0611
new: DataOwner5的最优x_5 = 0.0685
new: DataOwner6的最优x_6 = 0.0987
new: DataOwner7的最优x_7 = 0.0603
new: DataOwner8的最优x_8 = 0.0566
new: DataOwner9的最优x_9 = 0.0730
new: DataOwner10的最优x_10 = 0.0496
eta:0.72
new: DataOwner1的最优x_1 = 0.0550
new: DataOwner2的最优x_2 = 0.0495
new: DataOwner3的最优x_3 = 0.0643
new: DataOwner4的最优x_4 = 0.0620
new: DataOwner5的最优x_5 = 0.0695
new: DataOwner6的最优x_6 = 0.1001
new: DataOwner7的最优x_7 = 0.0612
new: DataOwner8的最优x_8 = 0.0574
new: DataOwner9的最优x_9 = 0.0741
new: DataOwner10的最优x_10 = 0.0503
eta:0.73
new: DataOwner1的最优x_1 = 0.0558
new: DataOwner2的最优x_2 = 0.0502
new: DataOwner3的最优x_3 = 0.0652
new: DataOwner4的最优x_4 = 0.0629
new: DataOwner5的最优x_5 = 0.0704
new: DataOwner6的最优x_6 = 0.1015
new: DataOwner7的最优x_7 = 0.0620
new: DataOwner8的最优x_8 = 0.0582
new: DataOwner9的最优x_9 = 0.0751
new: DataOwner10的最优x_10 = 0.0510
eta:0.74
new: DataOwner1的最优x_1 = 0.0565
new: DataOwner2的最优x_2 = 0.0509
new: DataOwner3的最优x_3 = 0.0661
new: DataOwner4的最优x_4 = 0.0637
new: DataOwner5的最优x_5 = 0.0714
new: DataOwner6的最优x_6 = 0.1028
new: DataOwner7的最优x_7 = 0.0629
new: DataOwner8的最优x_8 = 0.0590
new: DataOwner9的最优x_9 = 0.0761
new: DataOwner10的最优x_10 = 0.0517
eta:0.75
new: DataOwner1的最优x_1 = 0.0573
new: DataOwner2的最优x_2 = 0.0515
new: DataOwner3的最优x_3 = 0.0670
new: DataOwner4的最优x_4 = 0.0646
new: DataOwner5的最优x_5 = 0.0724
new: DataOwner6的最优x_6 = 0.1042
new: DataOwner7的最优x_7 = 0.0637
new: DataOwner8的最优x_8 = 0.0598
new: DataOwner9的最优x_9 = 0.0772
new: DataOwner10的最优x_10 = 0.0524
eta:0.76
new: DataOwner1的最优x_1 = 0.0581
new: DataOwner2的最优x_2 = 0.0522
new: DataOwner3的最优x_3 = 0.0679
new: DataOwner4的最优x_4 = 0.0654
new: DataOwner5的最优x_5 = 0.0733
new: DataOwner6的最优x_6 = 0.1056
new: DataOwner7的最优x_7 = 0.0646
new: DataOwner8的最优x_8 = 0.0606
new: DataOwner9的最优x_9 = 0.0782
new: DataOwner10的最优x_10 = 0.0531
eta:0.77
new: DataOwner1的最优x_1 = 0.0588
new: DataOwner2的最优x_2 = 0.0529
new: DataOwner3的最优x_3 = 0.0687
new: DataOwner4的最优x_4 = 0.0663
new: DataOwner5的最优x_5 = 0.0743
new: DataOwner6的最优x_6 = 0.1070
new: DataOwner7的最优x_7 = 0.0654
new: DataOwner8的最优x_8 = 0.0614
new: DataOwner9的最优x_9 = 0.0792
new: DataOwner10的最优x_10 = 0.0538
eta:0.78
new: DataOwner1的最优x_1 = 0.0596
new: DataOwner2的最优x_2 = 0.0536
new: DataOwner3的最优x_3 = 0.0696
new: DataOwner4的最优x_4 = 0.0672
new: DataOwner5的最优x_5 = 0.0753
new: DataOwner6的最优x_6 = 0.1084
new: DataOwner7的最优x_7 = 0.0663
new: DataOwner8的最优x_8 = 0.0622
new: DataOwner9的最优x_9 = 0.0802
new: DataOwner10的最优x_10 = 0.0545
eta:0.79
new: DataOwner1的最优x_1 = 0.0604
new: DataOwner2的最优x_2 = 0.0543
new: DataOwner3的最优x_3 = 0.0705
new: DataOwner4的最优x_4 = 0.0680
new: DataOwner5的最优x_5 = 0.0762
new: DataOwner6的最优x_6 = 0.1098
new: DataOwner7的最优x_7 = 0.0671
new: DataOwner8的最优x_8 = 0.0630
new: DataOwner9的最优x_9 = 0.0813
new: DataOwner10的最优x_10 = 0.0552
eta:0.8
new: DataOwner1的最优x_1 = 0.0611
new: DataOwner2的最优x_2 = 0.0550
new: DataOwner3的最优x_3 = 0.0714
new: DataOwner4的最优x_4 = 0.0689
new: DataOwner5的最优x_5 = 0.0772
new: DataOwner6的最优x_6 = 0.1112
new: DataOwner7的最优x_7 = 0.0680
new: DataOwner8的最优x_8 = 0.0638
new: DataOwner9的最优x_9 = 0.0823
new: DataOwner10的最优x_10 = 0.0559
eta:0.81
new: DataOwner1的最优x_1 = 0.0619
new: DataOwner2的最优x_2 = 0.0557
new: DataOwner3的最优x_3 = 0.0723
new: DataOwner4的最优x_4 = 0.0698
new: DataOwner5的最优x_5 = 0.0781
new: DataOwner6的最优x_6 = 0.1126
new: DataOwner7的最优x_7 = 0.0688
new: DataOwner8的最优x_8 = 0.0646
new: DataOwner9的最优x_9 = 0.0833
new: DataOwner10的最优x_10 = 0.0566
eta:0.8200000000000001
new: DataOwner1的最优x_1 = 0.0627
new: DataOwner2的最优x_2 = 0.0564
new: DataOwner3的最优x_3 = 0.0732
new: DataOwner4的最优x_4 = 0.0706
new: DataOwner5的最优x_5 = 0.0791
new: DataOwner6的最优x_6 = 0.1140
new: DataOwner7的最优x_7 = 0.0697
new: DataOwner8的最优x_8 = 0.0654
new: DataOwner9的最优x_9 = 0.0844
new: DataOwner10的最优x_10 = 0.0573
eta:0.8300000000000001
new: DataOwner1的最优x_1 = 0.0634
new: DataOwner2的最优x_2 = 0.0570
new: DataOwner3的最优x_3 = 0.0741
new: DataOwner4的最优x_4 = 0.0715
new: DataOwner5的最优x_5 = 0.0801
new: DataOwner6的最优x_6 = 0.1154
new: DataOwner7的最优x_7 = 0.0705
new: DataOwner8的最优x_8 = 0.0662
new: DataOwner9的最优x_9 = 0.0854
new: DataOwner10的最优x_10 = 0.0580
eta:0.8400000000000001
new: DataOwner1的最优x_1 = 0.0642
new: DataOwner2的最优x_2 = 0.0577
new: DataOwner3的最优x_3 = 0.0750
new: DataOwner4的最优x_4 = 0.0723
new: DataOwner5的最优x_5 = 0.0810
new: DataOwner6的最优x_6 = 0.1167
new: DataOwner7的最优x_7 = 0.0714
new: DataOwner8的最优x_8 = 0.0670
new: DataOwner9的最优x_9 = 0.0864
new: DataOwner10的最优x_10 = 0.0587
eta:0.85
new: DataOwner1的最优x_1 = 0.0649
new: DataOwner2的最优x_2 = 0.0584
new: DataOwner3的最优x_3 = 0.0759
new: DataOwner4的最优x_4 = 0.0732
new: DataOwner5的最优x_5 = 0.0820
new: DataOwner6的最优x_6 = 0.1181
new: DataOwner7的最优x_7 = 0.0722
new: DataOwner8的最优x_8 = 0.0678
new: DataOwner9的最优x_9 = 0.0874
new: DataOwner10的最优x_10 = 0.0594
eta:0.86
new: DataOwner1的最优x_1 = 0.0657
new: DataOwner2的最优x_2 = 0.0591
new: DataOwner3的最优x_3 = 0.0768
new: DataOwner4的最优x_4 = 0.0741
new: DataOwner5的最优x_5 = 0.0830
new: DataOwner6的最优x_6 = 0.1195
new: DataOwner7的最优x_7 = 0.0731
new: DataOwner8的最优x_8 = 0.0686
new: DataOwner9的最优x_9 = 0.0885
new: DataOwner10的最优x_10 = 0.0600
eta:0.87
new: DataOwner1的最优x_1 = 0.0665
new: DataOwner2的最优x_2 = 0.0598
new: DataOwner3的最优x_3 = 0.0777
new: DataOwner4的最优x_4 = 0.0749
new: DataOwner5的最优x_5 = 0.0839
new: DataOwner6的最优x_6 = 0.1209
new: DataOwner7的最优x_7 = 0.0739
new: DataOwner8的最优x_8 = 0.0694
new: DataOwner9的最优x_9 = 0.0895
new: DataOwner10的最优x_10 = 0.0607
eta:0.88
new: DataOwner1的最优x_1 = 0.0672
new: DataOwner2的最优x_2 = 0.0605
new: DataOwner3的最优x_3 = 0.0786
new: DataOwner4的最优x_4 = 0.0758
new: DataOwner5的最优x_5 = 0.0849
new: DataOwner6的最优x_6 = 0.1223
new: DataOwner7的最优x_7 = 0.0748
new: DataOwner8的最优x_8 = 0.0702
new: DataOwner9的最优x_9 = 0.0905
new: DataOwner10的最优x_10 = 0.0614
eta:0.89
new: DataOwner1的最优x_1 = 0.0680
new: DataOwner2的最优x_2 = 0.0612
new: DataOwner3的最优x_3 = 0.0795
new: DataOwner4的最优x_4 = 0.0766
new: DataOwner5的最优x_5 = 0.0859
new: DataOwner6的最优x_6 = 0.1237
new: DataOwner7的最优x_7 = 0.0756
new: DataOwner8的最优x_8 = 0.0710
new: DataOwner9的最优x_9 = 0.0916
new: DataOwner10的最优x_10 = 0.0621
eta:0.9
new: DataOwner1的最优x_1 = 0.0688
new: DataOwner2的最优x_2 = 0.0619
new: DataOwner3的最优x_3 = 0.0804
new: DataOwner4的最优x_4 = 0.0775
new: DataOwner5的最优x_5 = 0.0868
new: DataOwner6的最优x_6 = 0.1251
new: DataOwner7的最优x_7 = 0.0765
new: DataOwner8的最优x_8 = 0.0718
new: DataOwner9的最优x_9 = 0.0926
new: DataOwner10的最优x_10 = 0.0628
eta:0.91
new: DataOwner1的最优x_1 = 0.0695
new: DataOwner2的最优x_2 = 0.0625
new: DataOwner3的最优x_3 = 0.0812
new: DataOwner4的最优x_4 = 0.0784
new: DataOwner5的最优x_5 = 0.0878
new: DataOwner6的最优x_6 = 0.1265
new: DataOwner7的最优x_7 = 0.0773
new: DataOwner8的最优x_8 = 0.0726
new: DataOwner9的最优x_9 = 0.0936
new: DataOwner10的最优x_10 = 0.0635
eta:0.92
new: DataOwner1的最优x_1 = 0.0703
new: DataOwner2的最优x_2 = 0.0632
new: DataOwner3的最优x_3 = 0.0821
new: DataOwner4的最优x_4 = 0.0792
new: DataOwner5的最优x_5 = 0.0888
new: DataOwner6的最优x_6 = 0.1279
new: DataOwner7的最优x_7 = 0.0782
new: DataOwner8的最优x_8 = 0.0734
new: DataOwner9的最优x_9 = 0.0946
new: DataOwner10的最优x_10 = 0.0642
eta:0.93
new: DataOwner1的最优x_1 = 0.0711
new: DataOwner2的最优x_2 = 0.0639
new: DataOwner3的最优x_3 = 0.0830
new: DataOwner4的最优x_4 = 0.0801
new: DataOwner5的最优x_5 = 0.0897
new: DataOwner6的最优x_6 = 0.1293
new: DataOwner7的最优x_7 = 0.0790
new: DataOwner8的最优x_8 = 0.0742
new: DataOwner9的最优x_9 = 0.0957
new: DataOwner10的最优x_10 = 0.0649
eta:0.9400000000000001
new: DataOwner1的最优x_1 = 0.0718
new: DataOwner2的最优x_2 = 0.0646
new: DataOwner3的最优x_3 = 0.0839
new: DataOwner4的最优x_4 = 0.0809
new: DataOwner5的最优x_5 = 0.0907
new: DataOwner6的最优x_6 = 0.1306
new: DataOwner7的最优x_7 = 0.0799
new: DataOwner8的最优x_8 = 0.0750
new: DataOwner9的最优x_9 = 0.0967
new: DataOwner10的最优x_10 = 0.0656
eta:0.9500000000000001
new: DataOwner1的最优x_1 = 0.0726
new: DataOwner2的最优x_2 = 0.0653
new: DataOwner3的最优x_3 = 0.0848
new: DataOwner4的最优x_4 = 0.0818
new: DataOwner5的最优x_5 = 0.0917
new: DataOwner6的最优x_6 = 0.1320
new: DataOwner7的最优x_7 = 0.0807
new: DataOwner8的最优x_8 = 0.0758
new: DataOwner9的最优x_9 = 0.0977
new: DataOwner10的最优x_10 = 0.0663
eta:0.9600000000000001
new: DataOwner1的最优x_1 = 0.0734
new: DataOwner2的最优x_2 = 0.0660
new: DataOwner3的最优x_3 = 0.0857
new: DataOwner4的最优x_4 = 0.0827
new: DataOwner5的最优x_5 = 0.0926
new: DataOwner6的最优x_6 = 0.1334
new: DataOwner7的最优x_7 = 0.0816
new: DataOwner8的最优x_8 = 0.0766
new: DataOwner9的最优x_9 = 0.0988
new: DataOwner10的最优x_10 = 0.0670
eta:0.97
new: DataOwner1的最优x_1 = 0.0741
new: DataOwner2的最优x_2 = 0.0667
new: DataOwner3的最优x_3 = 0.0866
new: DataOwner4的最优x_4 = 0.0835
new: DataOwner5的最优x_5 = 0.0936
new: DataOwner6的最优x_6 = 0.1348
new: DataOwner7的最优x_7 = 0.0824
new: DataOwner8的最优x_8 = 0.0774
new: DataOwner9的最优x_9 = 0.0998
new: DataOwner10的最优x_10 = 0.0677
eta:0.98
new: DataOwner1的最优x_1 = 0.0749
new: DataOwner2的最优x_2 = 0.0673
new: DataOwner3的最优x_3 = 0.0875
new: DataOwner4的最优x_4 = 0.0844
new: DataOwner5的最优x_5 = 0.0945
new: DataOwner6的最优x_6 = 0.1362
new: DataOwner7的最优x_7 = 0.0833
new: DataOwner8的最优x_8 = 0.0782
new: DataOwner9的最优x_9 = 0.1008
new: DataOwner10的最优x_10 = 0.0684
eta:0.99
new: DataOwner1的最优x_1 = 0.0756
new: DataOwner2的最优x_2 = 0.0680
new: DataOwner3的最优x_3 = 0.0884
new: DataOwner4的最优x_4 = 0.0853
new: DataOwner5的最优x_5 = 0.0955
new: DataOwner6的最优x_6 = 0.1376
new: DataOwner7的最优x_7 = 0.0841
new: DataOwner8的最优x_8 = 0.0790
new: DataOwner9的最优x_9 = 0.1018
new: DataOwner10的最优x_10 = 0.0691
eta:1.0
new: DataOwner1的最优x_1 = 0.0764
new: DataOwner2的最优x_2 = 0.0687
new: DataOwner3的最优x_3 = 0.0893
new: DataOwner4的最优x_4 = 0.0861
new: DataOwner5的最优x_5 = 0.0965
new: DataOwner6的最优x_6 = 0.1390
new: DataOwner7的最优x_7 = 0.0850
new: DataOwner8的最优x_8 = 0.0798
new: DataOwner9的最优x_9 = 0.1029
new: DataOwner10的最优x_10 = 0.0698
eta:1.01
new: DataOwner1的最优x_1 = 0.0772
new: DataOwner2的最优x_2 = 0.0694
new: DataOwner3的最优x_3 = 0.0902
new: DataOwner4的最优x_4 = 0.0870
new: DataOwner5的最优x_5 = 0.0974
new: DataOwner6的最优x_6 = 0.1404
new: DataOwner7的最优x_7 = 0.0858
new: DataOwner8的最优x_8 = 0.0806
new: DataOwner9的最优x_9 = 0.1039
new: DataOwner10的最优x_10 = 0.0705
eta:1.02
new: DataOwner1的最优x_1 = 0.0779
new: DataOwner2的最优x_2 = 0.0701
new: DataOwner3的最优x_3 = 0.0911
new: DataOwner4的最优x_4 = 0.0878
new: DataOwner5的最优x_5 = 0.0984
new: DataOwner6的最优x_6 = 0.1418
new: DataOwner7的最优x_7 = 0.0867
new: DataOwner8的最优x_8 = 0.0814
new: DataOwner9的最优x_9 = 0.1049
new: DataOwner10的最优x_10 = 0.0712
eta:1.03
new: DataOwner1的最优x_1 = 0.0787
new: DataOwner2的最优x_2 = 0.0708
new: DataOwner3的最优x_3 = 0.0920
new: DataOwner4的最优x_4 = 0.0887
new: DataOwner5的最优x_5 = 0.0994
new: DataOwner6的最优x_6 = 0.1432
new: DataOwner7的最优x_7 = 0.0875
new: DataOwner8的最优x_8 = 0.0822
new: DataOwner9的最优x_9 = 0.1060
new: DataOwner10的最优x_10 = 0.0719
eta:1.04
new: DataOwner1的最优x_1 = 0.0795
new: DataOwner2的最优x_2 = 0.0715
new: DataOwner3的最优x_3 = 0.0929
new: DataOwner4的最优x_4 = 0.0896
new: DataOwner5的最优x_5 = 0.1003
new: DataOwner6的最优x_6 = 0.1445
new: DataOwner7的最优x_7 = 0.0884
new: DataOwner8的最优x_8 = 0.0830
new: DataOwner9的最优x_9 = 0.1070
new: DataOwner10的最优x_10 = 0.0726
eta:1.05
new: DataOwner1的最优x_1 = 0.0802
new: DataOwner2的最优x_2 = 0.0722
new: DataOwner3的最优x_3 = 0.0937
new: DataOwner4的最优x_4 = 0.0904
new: DataOwner5的最优x_5 = 0.1013
new: DataOwner6的最优x_6 = 0.1459
new: DataOwner7的最优x_7 = 0.0892
new: DataOwner8的最优x_8 = 0.0838
new: DataOwner9的最优x_9 = 0.1080
new: DataOwner10的最优x_10 = 0.0733
eta:1.06
new: DataOwner1的最优x_1 = 0.0810
new: DataOwner2的最优x_2 = 0.0728
new: DataOwner3的最优x_3 = 0.0946
new: DataOwner4的最优x_4 = 0.0913
new: DataOwner5的最优x_5 = 0.1023
new: DataOwner6的最优x_6 = 0.1473
new: DataOwner7的最优x_7 = 0.0901
new: DataOwner8的最优x_8 = 0.0846
new: DataOwner9的最优x_9 = 0.1090
new: DataOwner10的最优x_10 = 0.0740
eta:1.07
new: DataOwner1的最优x_1 = 0.0818
new: DataOwner2的最优x_2 = 0.0735
new: DataOwner3的最优x_3 = 0.0955
new: DataOwner4的最优x_4 = 0.0921
new: DataOwner5的最优x_5 = 0.1032
new: DataOwner6的最优x_6 = 0.1487
new: DataOwner7的最优x_7 = 0.0909
new: DataOwner8的最优x_8 = 0.0854
new: DataOwner9的最优x_9 = 0.1101
new: DataOwner10的最优x_10 = 0.0747
eta:1.08
new: DataOwner1的最优x_1 = 0.0825
new: DataOwner2的最优x_2 = 0.0742
new: DataOwner3的最优x_3 = 0.0964
new: DataOwner4的最优x_4 = 0.0930
new: DataOwner5的最优x_5 = 0.1042
new: DataOwner6的最优x_6 = 0.1501
new: DataOwner7的最优x_7 = 0.0918
new: DataOwner8的最优x_8 = 0.0862
new: DataOwner9的最优x_9 = 0.1111
new: DataOwner10的最优x_10 = 0.0754
eta:1.09
new: DataOwner1的最优x_1 = 0.0833
new: DataOwner2的最优x_2 = 0.0749
new: DataOwner3的最优x_3 = 0.0973
new: DataOwner4的最优x_4 = 0.0939
new: DataOwner5的最优x_5 = 0.1052
new: DataOwner6的最优x_6 = 0.1515
new: DataOwner7的最优x_7 = 0.0926
new: DataOwner8的最优x_8 = 0.0869
new: DataOwner9的最优x_9 = 0.1121
new: DataOwner10的最优x_10 = 0.0761
eta:1.1
new: DataOwner1的最优x_1 = 0.0840
new: DataOwner2的最优x_2 = 0.0756
new: DataOwner3的最优x_3 = 0.0982
new: DataOwner4的最优x_4 = 0.0947
new: DataOwner5的最优x_5 = 0.1061
new: DataOwner6的最优x_6 = 0.1529
new: DataOwner7的最优x_7 = 0.0935
new: DataOwner8的最优x_8 = 0.0877
new: DataOwner9的最优x_9 = 0.1132
new: DataOwner10的最优x_10 = 0.0768
eta:1.11
new: DataOwner1的最优x_1 = 0.0848
new: DataOwner2的最优x_2 = 0.0763
new: DataOwner3的最优x_3 = 0.0991
new: DataOwner4的最优x_4 = 0.0956
new: DataOwner5的最优x_5 = 0.1071
new: DataOwner6的最优x_6 = 0.1543
new: DataOwner7的最优x_7 = 0.0943
new: DataOwner8的最优x_8 = 0.0885
new: DataOwner9的最优x_9 = 0.1142
new: DataOwner10的最优x_10 = 0.0775
eta:1.12
new: DataOwner1的最优x_1 = 0.0856
new: DataOwner2的最优x_2 = 0.0770
new: DataOwner3的最优x_3 = 0.1000
new: DataOwner4的最优x_4 = 0.0964
new: DataOwner5的最优x_5 = 0.1081
new: DataOwner6的最优x_6 = 0.1557
new: DataOwner7的最优x_7 = 0.0952
new: DataOwner8的最优x_8 = 0.0893
new: DataOwner9的最优x_9 = 0.1152
new: DataOwner10的最优x_10 = 0.0782
eta:1.1300000000000001
new: DataOwner1的最优x_1 = 0.0863
new: DataOwner2的最优x_2 = 0.0777
new: DataOwner3的最优x_3 = 0.1009
new: DataOwner4的最优x_4 = 0.0973
new: DataOwner5的最优x_5 = 0.1090
new: DataOwner6的最优x_6 = 0.1571
new: DataOwner7的最优x_7 = 0.0960
new: DataOwner8的最优x_8 = 0.0901
new: DataOwner9的最优x_9 = 0.1162
new: DataOwner10的最优x_10 = 0.0789
eta:1.1400000000000001
new: DataOwner1的最优x_1 = 0.0871
new: DataOwner2的最优x_2 = 0.0783
new: DataOwner3的最优x_3 = 0.1018
new: DataOwner4的最优x_4 = 0.0982
new: DataOwner5的最优x_5 = 0.1100
new: DataOwner6的最优x_6 = 0.1584
new: DataOwner7的最优x_7 = 0.0969
new: DataOwner8的最优x_8 = 0.0909
new: DataOwner9的最优x_9 = 0.1173
new: DataOwner10的最优x_10 = 0.0796
eta:1.1500000000000001
new: DataOwner1的最优x_1 = 0.0879
new: DataOwner2的最优x_2 = 0.0790
new: DataOwner3的最优x_3 = 0.1027
new: DataOwner4的最优x_4 = 0.0990
new: DataOwner5的最优x_5 = 0.1109
new: DataOwner6的最优x_6 = 0.1598
new: DataOwner7的最优x_7 = 0.0977
new: DataOwner8的最优x_8 = 0.0917
new: DataOwner9的最优x_9 = 0.1183
new: DataOwner10的最优x_10 = 0.0803
eta:1.1600000000000001
new: DataOwner1的最优x_1 = 0.0886
new: DataOwner2的最优x_2 = 0.0797
new: DataOwner3的最优x_3 = 0.1036
new: DataOwner4的最优x_4 = 0.0999
new: DataOwner5的最优x_5 = 0.1119
new: DataOwner6的最优x_6 = 0.1612
new: DataOwner7的最优x_7 = 0.0986
new: DataOwner8的最优x_8 = 0.0925
new: DataOwner9的最优x_9 = 0.1193
new: DataOwner10的最优x_10 = 0.0810
eta:1.17
new: DataOwner1的最优x_1 = 0.0894
new: DataOwner2的最优x_2 = 0.0804
new: DataOwner3的最优x_3 = 0.1045
new: DataOwner4的最优x_4 = 0.1008
new: DataOwner5的最优x_5 = 0.1129
new: DataOwner6的最优x_6 = 0.1626
new: DataOwner7的最优x_7 = 0.0994
new: DataOwner8的最优x_8 = 0.0933
new: DataOwner9的最优x_9 = 0.1204
new: DataOwner10的最优x_10 = 0.0817
eta:1.18
new: DataOwner1的最优x_1 = 0.0902
new: DataOwner2的最优x_2 = 0.0811
new: DataOwner3的最优x_3 = 0.1053
new: DataOwner4的最优x_4 = 0.1016
new: DataOwner5的最优x_5 = 0.1138
new: DataOwner6的最优x_6 = 0.1640
new: DataOwner7的最优x_7 = 0.1003
new: DataOwner8的最优x_8 = 0.0941
new: DataOwner9的最优x_9 = 0.1214
new: DataOwner10的最优x_10 = 0.0824
eta:1.19
new: DataOwner1的最优x_1 = 0.0909
new: DataOwner2的最优x_2 = 0.0818
new: DataOwner3的最优x_3 = 0.1062
new: DataOwner4的最优x_4 = 0.1025
new: DataOwner5的最优x_5 = 0.1148
new: DataOwner6的最优x_6 = 0.1654
new: DataOwner7的最优x_7 = 0.1011
new: DataOwner8的最优x_8 = 0.0949
new: DataOwner9的最优x_9 = 0.1224
new: DataOwner10的最优x_10 = 0.0831
eta:1.2
new: DataOwner1的最优x_1 = 0.0917
new: DataOwner2的最优x_2 = 0.0825
new: DataOwner3的最优x_3 = 0.1071
new: DataOwner4的最优x_4 = 0.1033
new: DataOwner5的最优x_5 = 0.1158
new: DataOwner6的最优x_6 = 0.1668
new: DataOwner7的最优x_7 = 0.1020
new: DataOwner8的最优x_8 = 0.0957
new: DataOwner9的最优x_9 = 0.1234
new: DataOwner10的最优x_10 = 0.0838
eta:1.21
new: DataOwner1的最优x_1 = 0.0925
new: DataOwner2的最优x_2 = 0.0832
new: DataOwner3的最优x_3 = 0.1080
new: DataOwner4的最优x_4 = 0.1042
new: DataOwner5的最优x_5 = 0.1167
new: DataOwner6的最优x_6 = 0.1682
new: DataOwner7的最优x_7 = 0.1028
new: DataOwner8的最优x_8 = 0.0965
new: DataOwner9的最优x_9 = 0.1245
new: DataOwner10的最优x_10 = 0.0845
eta:1.22
new: DataOwner1的最优x_1 = 0.0932
new: DataOwner2的最优x_2 = 0.0838
new: DataOwner3的最优x_3 = 0.1089
new: DataOwner4的最优x_4 = 0.1051
new: DataOwner5的最优x_5 = 0.1177
new: DataOwner6的最优x_6 = 0.1696
new: DataOwner7的最优x_7 = 0.1037
new: DataOwner8的最优x_8 = 0.0973
new: DataOwner9的最优x_9 = 0.1255
new: DataOwner10的最优x_10 = 0.0852
eta:1.23
new: DataOwner1的最优x_1 = 0.0940
new: DataOwner2的最优x_2 = 0.0845
new: DataOwner3的最优x_3 = 0.1098
new: DataOwner4的最优x_4 = 0.1059
new: DataOwner5的最优x_5 = 0.1187
new: DataOwner6的最优x_6 = 0.1710
new: DataOwner7的最优x_7 = 0.1045
new: DataOwner8的最优x_8 = 0.0981
new: DataOwner9的最优x_9 = 0.1265
new: DataOwner10的最优x_10 = 0.0859
eta:1.24
new: DataOwner1的最优x_1 = 0.0947
new: DataOwner2的最优x_2 = 0.0852
new: DataOwner3的最优x_3 = 0.1107
new: DataOwner4的最优x_4 = 0.1068
new: DataOwner5的最优x_5 = 0.1196
new: DataOwner6的最优x_6 = 0.1723
new: DataOwner7的最优x_7 = 0.1054
new: DataOwner8的最优x_8 = 0.0989
new: DataOwner9的最优x_9 = 0.1276
new: DataOwner10的最优x_10 = 0.0866
eta:1.25
new: DataOwner1的最优x_1 = 0.0955
new: DataOwner2的最优x_2 = 0.0859
new: DataOwner3的最优x_3 = 0.1116
new: DataOwner4的最优x_4 = 0.1076
new: DataOwner5的最优x_5 = 0.1206
new: DataOwner6的最优x_6 = 0.1737
new: DataOwner7的最优x_7 = 0.1062
new: DataOwner8的最优x_8 = 0.0997
new: DataOwner9的最优x_9 = 0.1286
new: DataOwner10的最优x_10 = 0.0873
eta:1.26
new: DataOwner1的最优x_1 = 0.0963
new: DataOwner2的最优x_2 = 0.0866
new: DataOwner3的最优x_3 = 0.1125
new: DataOwner4的最优x_4 = 0.1085
new: DataOwner5的最优x_5 = 0.1216
new: DataOwner6的最优x_6 = 0.1751
new: DataOwner7的最优x_7 = 0.1071
new: DataOwner8的最优x_8 = 0.1005
new: DataOwner9的最优x_9 = 0.1296
new: DataOwner10的最优x_10 = 0.0880
eta:1.27
new: DataOwner1的最优x_1 = 0.0970
new: DataOwner2的最优x_2 = 0.0873
new: DataOwner3的最优x_3 = 0.1134
new: DataOwner4的最优x_4 = 0.1094
new: DataOwner5的最优x_5 = 0.1225
new: DataOwner6的最优x_6 = 0.1765
new: DataOwner7的最优x_7 = 0.1079
new: DataOwner8的最优x_8 = 0.1013
new: DataOwner9的最优x_9 = 0.1306
new: DataOwner10的最优x_10 = 0.0887
eta:1.28
new: DataOwner1的最优x_1 = 0.0978
new: DataOwner2的最优x_2 = 0.0880
new: DataOwner3的最优x_3 = 0.1143
new: DataOwner4的最优x_4 = 0.1102
new: DataOwner5的最优x_5 = 0.1235
new: DataOwner6的最优x_6 = 0.1779
new: DataOwner7的最优x_7 = 0.1088
new: DataOwner8的最优x_8 = 0.1021
new: DataOwner9的最优x_9 = 0.1317
new: DataOwner10的最优x_10 = 0.0894
eta:1.29
new: DataOwner1的最优x_1 = 0.0986
new: DataOwner2的最优x_2 = 0.0887
new: DataOwner3的最优x_3 = 0.1152
new: DataOwner4的最优x_4 = 0.1111
new: DataOwner5的最优x_5 = 0.1245
new: DataOwner6的最优x_6 = 0.1793
new: DataOwner7的最优x_7 = 0.1096
new: DataOwner8的最优x_8 = 0.1029
new: DataOwner9的最优x_9 = 0.1327
new: DataOwner10的最优x_10 = 0.0901
eta:1.3
new: DataOwner1的最优x_1 = 0.0993
new: DataOwner2的最优x_2 = 0.0893
new: DataOwner3的最优x_3 = 0.1161
new: DataOwner4的最优x_4 = 0.1119
new: DataOwner5的最优x_5 = 0.1254
new: DataOwner6的最优x_6 = 0.1807
new: DataOwner7的最优x_7 = 0.1105
new: DataOwner8的最优x_8 = 0.1037
new: DataOwner9的最优x_9 = 0.1337
new: DataOwner10的最优x_10 = 0.0908
eta:1.31
new: DataOwner1的最优x_1 = 0.1001
new: DataOwner2的最优x_2 = 0.0900
new: DataOwner3的最优x_3 = 0.1170
new: DataOwner4的最优x_4 = 0.1128
new: DataOwner5的最优x_5 = 0.1264
new: DataOwner6的最优x_6 = 0.1821
new: DataOwner7的最优x_7 = 0.1113
new: DataOwner8的最优x_8 = 0.1045
new: DataOwner9的最优x_9 = 0.1348
new: DataOwner10的最优x_10 = 0.0915
eta:1.32
new: DataOwner1的最优x_1 = 0.1009
new: DataOwner2的最优x_2 = 0.0907
new: DataOwner3的最优x_3 = 0.1178
new: DataOwner4的最优x_4 = 0.1137
new: DataOwner5的最优x_5 = 0.1273
new: DataOwner6的最优x_6 = 0.1835
new: DataOwner7的最优x_7 = 0.1122
new: DataOwner8的最优x_8 = 0.1053
new: DataOwner9的最优x_9 = 0.1358
new: DataOwner10的最优x_10 = 0.0922
eta:1.33
new: DataOwner1的最优x_1 = 0.1016
new: DataOwner2的最优x_2 = 0.0914
new: DataOwner3的最优x_3 = 0.1187
new: DataOwner4的最优x_4 = 0.1145
new: DataOwner5的最优x_5 = 0.1283
new: DataOwner6的最优x_6 = 0.1849
new: DataOwner7的最优x_7 = 0.1130
new: DataOwner8的最优x_8 = 0.1061
new: DataOwner9的最优x_9 = 0.1368
new: DataOwner10的最优x_10 = 0.0929
eta:1.34
new: DataOwner1的最优x_1 = 0.1024
new: DataOwner2的最优x_2 = 0.0921
new: DataOwner3的最优x_3 = 0.1196
new: DataOwner4的最优x_4 = 0.1154
new: DataOwner5的最优x_5 = 0.1293
new: DataOwner6的最优x_6 = 0.1862
new: DataOwner7的最优x_7 = 0.1139
new: DataOwner8的最优x_8 = 0.1069
new: DataOwner9的最优x_9 = 0.1378
new: DataOwner10的最优x_10 = 0.0936
eta:1.35
new: DataOwner1的最优x_1 = 0.1032
new: DataOwner2的最优x_2 = 0.0928
new: DataOwner3的最优x_3 = 0.1205
new: DataOwner4的最优x_4 = 0.1163
new: DataOwner5的最优x_5 = 0.1302
new: DataOwner6的最优x_6 = 0.1876
new: DataOwner7的最优x_7 = 0.1147
new: DataOwner8的最优x_8 = 0.1077
new: DataOwner9的最优x_9 = 0.1389
new: DataOwner10的最优x_10 = 0.0943
eta:1.36
new: DataOwner1的最优x_1 = 0.1039
new: DataOwner2的最优x_2 = 0.0935
new: DataOwner3的最优x_3 = 0.1214
new: DataOwner4的最优x_4 = 0.1171
new: DataOwner5的最优x_5 = 0.1312
new: DataOwner6的最优x_6 = 0.1890
new: DataOwner7的最优x_7 = 0.1156
new: DataOwner8的最优x_8 = 0.1085
new: DataOwner9的最优x_9 = 0.1399
new: DataOwner10的最优x_10 = 0.0950
eta:1.37
new: DataOwner1的最优x_1 = 0.1047
new: DataOwner2的最优x_2 = 0.0942
new: DataOwner3的最优x_3 = 0.1223
new: DataOwner4的最优x_4 = 0.1180
new: DataOwner5的最优x_5 = 0.1322
new: DataOwner6的最优x_6 = 0.1904
new: DataOwner7的最优x_7 = 0.1164
new: DataOwner8的最优x_8 = 0.1093
new: DataOwner9的最优x_9 = 0.1409
new: DataOwner10的最优x_10 = 0.0957
eta:1.3800000000000001
new: DataOwner1的最优x_1 = 0.1054
new: DataOwner2的最优x_2 = 0.0948
new: DataOwner3的最优x_3 = 0.1232
new: DataOwner4的最优x_4 = 0.1188
new: DataOwner5的最优x_5 = 0.1331
new: DataOwner6的最优x_6 = 0.1918
new: DataOwner7的最优x_7 = 0.1173
new: DataOwner8的最优x_8 = 0.1101
new: DataOwner9的最优x_9 = 0.1420
new: DataOwner10的最优x_10 = 0.0964
eta:1.3900000000000001
new: DataOwner1的最优x_1 = 0.1062
new: DataOwner2的最优x_2 = 0.0955
new: DataOwner3的最优x_3 = 0.1241
new: DataOwner4的最优x_4 = 0.1197
new: DataOwner5的最优x_5 = 0.1341
new: DataOwner6的最优x_6 = 0.1932
new: DataOwner7的最优x_7 = 0.1181
new: DataOwner8的最优x_8 = 0.1109
new: DataOwner9的最优x_9 = 0.1430
new: DataOwner10的最优x_10 = 0.0971
eta:1.4000000000000001
new: DataOwner1的最优x_1 = 0.1070
new: DataOwner2的最优x_2 = 0.0962
new: DataOwner3的最优x_3 = 0.1250
new: DataOwner4的最优x_4 = 0.1206
new: DataOwner5的最优x_5 = 0.1351
new: DataOwner6的最优x_6 = 0.1946
new: DataOwner7的最优x_7 = 0.1190
new: DataOwner8的最优x_8 = 0.1117
new: DataOwner9的最优x_9 = 0.1440
new: DataOwner10的最优x_10 = 0.0978
eta:1.4100000000000001
new: DataOwner1的最优x_1 = 0.1077
new: DataOwner2的最优x_2 = 0.0969
new: DataOwner3的最优x_3 = 0.1259
new: DataOwner4的最优x_4 = 0.1214
new: DataOwner5的最优x_5 = 0.1360
new: DataOwner6的最优x_6 = 0.1960
new: DataOwner7的最优x_7 = 0.1198
new: DataOwner8的最优x_8 = 0.1125
new: DataOwner9的最优x_9 = 0.1450
new: DataOwner10的最优x_10 = 0.0985
eta:1.42
new: DataOwner1的最优x_1 = 0.1085
new: DataOwner2的最优x_2 = 0.0976
new: DataOwner3的最优x_3 = 0.1268
new: DataOwner4的最优x_4 = 0.1223
new: DataOwner5的最优x_5 = 0.1370
new: DataOwner6的最优x_6 = 0.1974
new: DataOwner7的最优x_7 = 0.1207
new: DataOwner8的最优x_8 = 0.1133
new: DataOwner9的最优x_9 = 0.1461
new: DataOwner10的最优x_10 = 0.0991
eta:1.43
new: DataOwner1的最优x_1 = 0.1093
new: DataOwner2的最优x_2 = 0.0983
new: DataOwner3的最优x_3 = 0.1277
new: DataOwner4的最优x_4 = 0.1231
new: DataOwner5的最优x_5 = 0.1380
new: DataOwner6的最优x_6 = 0.1987
new: DataOwner7的最优x_7 = 0.1215
new: DataOwner8的最优x_8 = 0.1141
new: DataOwner9的最优x_9 = 0.1471
new: DataOwner10的最优x_10 = 0.0998
eta:1.44
new: DataOwner1的最优x_1 = 0.1100
new: DataOwner2的最优x_2 = 0.0990
new: DataOwner3的最优x_3 = 0.1286
new: DataOwner4的最优x_4 = 0.1240
new: DataOwner5的最优x_5 = 0.1389
new: DataOwner6的最优x_6 = 0.2001
new: DataOwner7的最优x_7 = 0.1224
new: DataOwner8的最优x_8 = 0.1149
new: DataOwner9的最优x_9 = 0.1481
new: DataOwner10的最优x_10 = 0.1005
eta:1.45
new: DataOwner1的最优x_1 = 0.1108
new: DataOwner2的最优x_2 = 0.0997
new: DataOwner3的最优x_3 = 0.1295
new: DataOwner4的最优x_4 = 0.1249
new: DataOwner5的最优x_5 = 0.1399
new: DataOwner6的最优x_6 = 0.2015
new: DataOwner7的最优x_7 = 0.1232
new: DataOwner8的最优x_8 = 0.1157
new: DataOwner9的最优x_9 = 0.1492
new: DataOwner10的最优x_10 = 0.1012
eta:1.46
new: DataOwner1的最优x_1 = 0.1116
new: DataOwner2的最优x_2 = 0.1003
new: DataOwner3的最优x_3 = 0.1303
new: DataOwner4的最优x_4 = 0.1257
new: DataOwner5的最优x_5 = 0.1409
new: DataOwner6的最优x_6 = 0.2029
new: DataOwner7的最优x_7 = 0.1241
new: DataOwner8的最优x_8 = 0.1165
new: DataOwner9的最优x_9 = 0.1502
new: DataOwner10的最优x_10 = 0.1019
eta:1.47
new: DataOwner1的最优x_1 = 0.1123
new: DataOwner2的最优x_2 = 0.1010
new: DataOwner3的最优x_3 = 0.1312
new: DataOwner4的最优x_4 = 0.1266
new: DataOwner5的最优x_5 = 0.1418
new: DataOwner6的最优x_6 = 0.2043
new: DataOwner7的最优x_7 = 0.1249
new: DataOwner8的最优x_8 = 0.1173
new: DataOwner9的最优x_9 = 0.1512
new: DataOwner10的最优x_10 = 0.1026
eta:1.48
new: DataOwner1的最优x_1 = 0.1131
new: DataOwner2的最优x_2 = 0.1017
new: DataOwner3的最优x_3 = 0.1321
new: DataOwner4的最优x_4 = 0.1274
new: DataOwner5的最优x_5 = 0.1428
new: DataOwner6的最优x_6 = 0.2057
new: DataOwner7的最优x_7 = 0.1258
new: DataOwner8的最优x_8 = 0.1181
new: DataOwner9的最优x_9 = 0.1522
new: DataOwner10的最优x_10 = 0.1033
eta:1.49
new: DataOwner1的最优x_1 = 0.1138
new: DataOwner2的最优x_2 = 0.1024
new: DataOwner3的最优x_3 = 0.1330
new: DataOwner4的最优x_4 = 0.1283
new: DataOwner5的最优x_5 = 0.1437
new: DataOwner6的最优x_6 = 0.2071
new: DataOwner7的最优x_7 = 0.1266
new: DataOwner8的最优x_8 = 0.1189
new: DataOwner9的最优x_9 = 0.1533
new: DataOwner10的最优x_10 = 0.1040
eta:1.5
new: DataOwner1的最优x_1 = 0.1146
new: DataOwner2的最优x_2 = 0.1031
new: DataOwner3的最优x_3 = 0.1339
new: DataOwner4的最优x_4 = 0.1292
new: DataOwner5的最优x_5 = 0.1447
new: DataOwner6的最优x_6 = 0.2085
new: DataOwner7的最优x_7 = 0.1275
new: DataOwner8的最优x_8 = 0.1197
new: DataOwner9的最优x_9 = 0.1543
new: DataOwner10的最优x_10 = 0.1047
eta:1.51
new: DataOwner1的最优x_1 = 0.1154
new: DataOwner2的最优x_2 = 0.1038
new: DataOwner3的最优x_3 = 0.1348
new: DataOwner4的最优x_4 = 0.1300
new: DataOwner5的最优x_5 = 0.1457
new: DataOwner6的最优x_6 = 0.2099
new: DataOwner7的最优x_7 = 0.1283
new: DataOwner8的最优x_8 = 0.1205
new: DataOwner9的最优x_9 = 0.1553
new: DataOwner10的最优x_10 = 0.1054
eta:1.52
new: DataOwner1的最优x_1 = 0.1161
new: DataOwner2的最优x_2 = 0.1045
new: DataOwner3的最优x_3 = 0.1357
new: DataOwner4的最优x_4 = 0.1309
new: DataOwner5的最优x_5 = 0.1466
new: DataOwner6的最优x_6 = 0.2113
new: DataOwner7的最优x_7 = 0.1292
new: DataOwner8的最优x_8 = 0.1212
new: DataOwner9的最优x_9 = 0.1564
new: DataOwner10的最优x_10 = 0.1061
eta:1.53
new: DataOwner1的最优x_1 = 0.1169
new: DataOwner2的最优x_2 = 0.1051
new: DataOwner3的最优x_3 = 0.1366
new: DataOwner4的最优x_4 = 0.1318
new: DataOwner5的最优x_5 = 0.1476
new: DataOwner6的最优x_6 = 0.2126
new: DataOwner7的最优x_7 = 0.1300
new: DataOwner8的最优x_8 = 0.1220
new: DataOwner9的最优x_9 = 0.1574
new: DataOwner10的最优x_10 = 0.1068
eta:1.54
new: DataOwner1的最优x_1 = 0.1177
new: DataOwner2的最优x_2 = 0.1058
new: DataOwner3的最优x_3 = 0.1375
new: DataOwner4的最优x_4 = 0.1326
new: DataOwner5的最优x_5 = 0.1486
new: DataOwner6的最优x_6 = 0.2140
new: DataOwner7的最优x_7 = 0.1309
new: DataOwner8的最优x_8 = 0.1228
new: DataOwner9的最优x_9 = 0.1584
new: DataOwner10的最优x_10 = 0.1075
eta:1.55
new: DataOwner1的最优x_1 = 0.1184
new: DataOwner2的最优x_2 = 0.1065
new: DataOwner3的最优x_3 = 0.1384
new: DataOwner4的最优x_4 = 0.1335
new: DataOwner5的最优x_5 = 0.1495
new: DataOwner6的最优x_6 = 0.2154
new: DataOwner7的最优x_7 = 0.1317
new: DataOwner8的最优x_8 = 0.1236
new: DataOwner9的最优x_9 = 0.1594
new: DataOwner10的最优x_10 = 0.1082
eta:1.56
new: DataOwner1的最优x_1 = 0.1192
new: DataOwner2的最优x_2 = 0.1072
new: DataOwner3的最优x_3 = 0.1393
new: DataOwner4的最优x_4 = 0.1343
new: DataOwner5的最优x_5 = 0.1505
new: DataOwner6的最优x_6 = 0.2168
new: DataOwner7的最优x_7 = 0.1326
new: DataOwner8的最优x_8 = 0.1244
new: DataOwner9的最优x_9 = 0.1605
new: DataOwner10的最优x_10 = 0.1089
eta:1.57
new: DataOwner1的最优x_1 = 0.1200
new: DataOwner2的最优x_2 = 0.1079
new: DataOwner3的最优x_3 = 0.1402
new: DataOwner4的最优x_4 = 0.1352
new: DataOwner5的最优x_5 = 0.1515
new: DataOwner6的最优x_6 = 0.2182
new: DataOwner7的最优x_7 = 0.1334
new: DataOwner8的最优x_8 = 0.1252
new: DataOwner9的最优x_9 = 0.1615
new: DataOwner10的最优x_10 = 0.1096
eta:1.58
new: DataOwner1的最优x_1 = 0.1207
new: DataOwner2的最优x_2 = 0.1086
new: DataOwner3的最优x_3 = 0.1411
new: DataOwner4的最优x_4 = 0.1361
new: DataOwner5的最优x_5 = 0.1524
new: DataOwner6的最优x_6 = 0.2196
new: DataOwner7的最优x_7 = 0.1343
new: DataOwner8的最优x_8 = 0.1260
new: DataOwner9的最优x_9 = 0.1625
new: DataOwner10的最优x_10 = 0.1103
eta:1.59
new: DataOwner1的最优x_1 = 0.1215
new: DataOwner2的最优x_2 = 0.1093
new: DataOwner3的最优x_3 = 0.1420
new: DataOwner4的最优x_4 = 0.1369
new: DataOwner5的最优x_5 = 0.1534
new: DataOwner6的最优x_6 = 0.2210
new: DataOwner7的最优x_7 = 0.1351
new: DataOwner8的最优x_8 = 0.1268
new: DataOwner9的最优x_9 = 0.1636
new: DataOwner10的最优x_10 = 0.1110
eta:1.6
new: DataOwner1的最优x_1 = 0.1223
new: DataOwner2的最优x_2 = 0.1100
new: DataOwner3的最优x_3 = 0.1428
new: DataOwner4的最优x_4 = 0.1378
new: DataOwner5的最优x_5 = 0.1544
new: DataOwner6的最优x_6 = 0.2224
new: DataOwner7的最优x_7 = 0.1360
new: DataOwner8的最优x_8 = 0.1276
new: DataOwner9的最优x_9 = 0.1646
new: DataOwner10的最优x_10 = 0.1117
eta:1.61
new: DataOwner1的最优x_1 = 0.1230
new: DataOwner2的最优x_2 = 0.1106
new: DataOwner3的最优x_3 = 0.1437
new: DataOwner4的最优x_4 = 0.1386
new: DataOwner5的最优x_5 = 0.1553
new: DataOwner6的最优x_6 = 0.2238
new: DataOwner7的最优x_7 = 0.1368
new: DataOwner8的最优x_8 = 0.1284
new: DataOwner9的最优x_9 = 0.1656
new: DataOwner10的最优x_10 = 0.1124
eta:1.62
new: DataOwner1的最优x_1 = 0.1238
new: DataOwner2的最优x_2 = 0.1113
new: DataOwner3的最优x_3 = 0.1446
new: DataOwner4的最优x_4 = 0.1395
new: DataOwner5的最优x_5 = 0.1563
new: DataOwner6的最优x_6 = 0.2252
new: DataOwner7的最优x_7 = 0.1377
new: DataOwner8的最优x_8 = 0.1292
new: DataOwner9的最优x_9 = 0.1666
new: DataOwner10的最优x_10 = 0.1131
eta:1.6300000000000001
new: DataOwner1的最优x_1 = 0.1245
new: DataOwner2的最优x_2 = 0.1120
new: DataOwner3的最优x_3 = 0.1455
new: DataOwner4的最优x_4 = 0.1404
new: DataOwner5的最优x_5 = 0.1573
new: DataOwner6的最优x_6 = 0.2265
new: DataOwner7的最优x_7 = 0.1385
new: DataOwner8的最优x_8 = 0.1300
new: DataOwner9的最优x_9 = 0.1677
new: DataOwner10的最优x_10 = 0.1138
eta:1.6400000000000001
new: DataOwner1的最优x_1 = 0.1253
new: DataOwner2的最优x_2 = 0.1127
new: DataOwner3的最优x_3 = 0.1464
new: DataOwner4的最优x_4 = 0.1412
new: DataOwner5的最优x_5 = 0.1582
new: DataOwner6的最优x_6 = 0.2279
new: DataOwner7的最优x_7 = 0.1394
new: DataOwner8的最优x_8 = 0.1308
new: DataOwner9的最优x_9 = 0.1687
new: DataOwner10的最优x_10 = 0.1145
eta:1.6500000000000001
new: DataOwner1的最优x_1 = 0.1261
new: DataOwner2的最优x_2 = 0.1134
new: DataOwner3的最优x_3 = 0.1473
new: DataOwner4的最优x_4 = 0.1421
new: DataOwner5的最优x_5 = 0.1592
new: DataOwner6的最优x_6 = 0.2293
new: DataOwner7的最优x_7 = 0.1402
new: DataOwner8的最优x_8 = 0.1316
new: DataOwner9的最优x_9 = 0.1697
new: DataOwner10的最优x_10 = 0.1152
eta:1.6600000000000001
new: DataOwner1的最优x_1 = 0.1268
new: DataOwner2的最优x_2 = 0.1141
new: DataOwner3的最优x_3 = 0.1482
new: DataOwner4的最优x_4 = 0.1429
new: DataOwner5的最优x_5 = 0.1601
new: DataOwner6的最优x_6 = 0.2307
new: DataOwner7的最优x_7 = 0.1411
new: DataOwner8的最优x_8 = 0.1324
new: DataOwner9的最优x_9 = 0.1708
new: DataOwner10的最优x_10 = 0.1159
eta:1.6700000000000002
new: DataOwner1的最优x_1 = 0.1276
new: DataOwner2的最优x_2 = 0.1148
new: DataOwner3的最优x_3 = 0.1491
new: DataOwner4的最优x_4 = 0.1438
new: DataOwner5的最优x_5 = 0.1611
new: DataOwner6的最优x_6 = 0.2321
new: DataOwner7的最优x_7 = 0.1419
new: DataOwner8的最优x_8 = 0.1332
new: DataOwner9的最优x_9 = 0.1718
new: DataOwner10的最优x_10 = 0.1166
eta:1.68
new: DataOwner1的最优x_1 = 0.1284
new: DataOwner2的最优x_2 = 0.1155
new: DataOwner3的最优x_3 = 0.1500
new: DataOwner4的最优x_4 = 0.1447
new: DataOwner5的最优x_5 = 0.1621
new: DataOwner6的最优x_6 = 0.2335
new: DataOwner7的最优x_7 = 0.1428
new: DataOwner8的最优x_8 = 0.1340
new: DataOwner9的最优x_9 = 0.1728
new: DataOwner10的最优x_10 = 0.1173
eta:1.69
new: DataOwner1的最优x_1 = 0.1291
new: DataOwner2的最优x_2 = 0.1161
new: DataOwner3的最优x_3 = 0.1509
new: DataOwner4的最优x_4 = 0.1455
new: DataOwner5的最优x_5 = 0.1630
new: DataOwner6的最优x_6 = 0.2349
new: DataOwner7的最优x_7 = 0.1436
new: DataOwner8的最优x_8 = 0.1348
new: DataOwner9的最优x_9 = 0.1738
new: DataOwner10的最优x_10 = 0.1180
eta:1.7
new: DataOwner1的最优x_1 = 0.1299
new: DataOwner2的最优x_2 = 0.1168
new: DataOwner3的最优x_3 = 0.1518
new: DataOwner4的最优x_4 = 0.1464
new: DataOwner5的最优x_5 = 0.1640
new: DataOwner6的最优x_6 = 0.2363
new: DataOwner7的最优x_7 = 0.1445
new: DataOwner8的最优x_8 = 0.1356
new: DataOwner9的最优x_9 = 0.1749
new: DataOwner10的最优x_10 = 0.1187
eta:1.71
new: DataOwner1的最优x_1 = 0.1307
new: DataOwner2的最优x_2 = 0.1175
new: DataOwner3的最优x_3 = 0.1527
new: DataOwner4的最优x_4 = 0.1473
new: DataOwner5的最优x_5 = 0.1650
new: DataOwner6的最优x_6 = 0.2377
new: DataOwner7的最优x_7 = 0.1453
new: DataOwner8的最优x_8 = 0.1364
new: DataOwner9的最优x_9 = 0.1759
new: DataOwner10的最优x_10 = 0.1194
eta:1.72
new: DataOwner1的最优x_1 = 0.1314
new: DataOwner2的最优x_2 = 0.1182
new: DataOwner3的最优x_3 = 0.1536
new: DataOwner4的最优x_4 = 0.1481
new: DataOwner5的最优x_5 = 0.1659
new: DataOwner6的最优x_6 = 0.2391
new: DataOwner7的最优x_7 = 0.1462
new: DataOwner8的最优x_8 = 0.1372
new: DataOwner9的最优x_9 = 0.1769
new: DataOwner10的最优x_10 = 0.1201
eta:1.73
new: DataOwner1的最优x_1 = 0.1322
new: DataOwner2的最优x_2 = 0.1189
new: DataOwner3的最优x_3 = 0.1545
new: DataOwner4的最优x_4 = 0.1490
new: DataOwner5的最优x_5 = 0.1669
new: DataOwner6的最优x_6 = 0.2404
new: DataOwner7的最优x_7 = 0.1470
new: DataOwner8的最优x_8 = 0.1380
new: DataOwner9的最优x_9 = 0.1780
new: DataOwner10的最优x_10 = 0.1208
eta:1.74
new: DataOwner1的最优x_1 = 0.1330
new: DataOwner2的最优x_2 = 0.1196
new: DataOwner3的最优x_3 = 0.1553
new: DataOwner4的最优x_4 = 0.1498
new: DataOwner5的最优x_5 = 0.1679
new: DataOwner6的最优x_6 = 0.2418
new: DataOwner7的最优x_7 = 0.1479
new: DataOwner8的最优x_8 = 0.1388
new: DataOwner9的最优x_9 = 0.1790
new: DataOwner10的最优x_10 = 0.1215
eta:1.75
new: DataOwner1的最优x_1 = 0.1337
new: DataOwner2的最优x_2 = 0.1203
new: DataOwner3的最优x_3 = 0.1562
new: DataOwner4的最优x_4 = 0.1507
new: DataOwner5的最优x_5 = 0.1688
new: DataOwner6的最优x_6 = 0.2432
new: DataOwner7的最优x_7 = 0.1487
new: DataOwner8的最优x_8 = 0.1396
new: DataOwner9的最优x_9 = 0.1800
new: DataOwner10的最优x_10 = 0.1222
eta:1.76
new: DataOwner1的最优x_1 = 0.1345
new: DataOwner2的最优x_2 = 0.1210
new: DataOwner3的最优x_3 = 0.1571
new: DataOwner4的最优x_4 = 0.1516
new: DataOwner5的最优x_5 = 0.1698
new: DataOwner6的最优x_6 = 0.2446
new: DataOwner7的最优x_7 = 0.1496
new: DataOwner8的最优x_8 = 0.1404
new: DataOwner9的最优x_9 = 0.1810
new: DataOwner10的最优x_10 = 0.1229
eta:1.77
new: DataOwner1的最优x_1 = 0.1352
new: DataOwner2的最优x_2 = 0.1216
new: DataOwner3的最优x_3 = 0.1580
new: DataOwner4的最优x_4 = 0.1524
new: DataOwner5的最优x_5 = 0.1708
new: DataOwner6的最优x_6 = 0.2460
new: DataOwner7的最优x_7 = 0.1504
new: DataOwner8的最优x_8 = 0.1412
new: DataOwner9的最优x_9 = 0.1821
new: DataOwner10的最优x_10 = 0.1236
eta:1.78
new: DataOwner1的最优x_1 = 0.1360
new: DataOwner2的最优x_2 = 0.1223
new: DataOwner3的最优x_3 = 0.1589
new: DataOwner4的最优x_4 = 0.1533
new: DataOwner5的最优x_5 = 0.1717
new: DataOwner6的最优x_6 = 0.2474
new: DataOwner7的最优x_7 = 0.1513
new: DataOwner8的最优x_8 = 0.1420
new: DataOwner9的最优x_9 = 0.1831
new: DataOwner10的最优x_10 = 0.1243
eta:1.79
new: DataOwner1的最优x_1 = 0.1368
new: DataOwner2的最优x_2 = 0.1230
new: DataOwner3的最优x_3 = 0.1598
new: DataOwner4的最优x_4 = 0.1541
new: DataOwner5的最优x_5 = 0.1727
new: DataOwner6的最优x_6 = 0.2488
new: DataOwner7的最优x_7 = 0.1521
new: DataOwner8的最优x_8 = 0.1428
new: DataOwner9的最优x_9 = 0.1841
new: DataOwner10的最优x_10 = 0.1250
eta:1.8
new: DataOwner1的最优x_1 = 0.1375
new: DataOwner2的最优x_2 = 0.1237
new: DataOwner3的最优x_3 = 0.1607
new: DataOwner4的最优x_4 = 0.1550
new: DataOwner5的最优x_5 = 0.1737
new: DataOwner6的最优x_6 = 0.2502
new: DataOwner7的最优x_7 = 0.1530
new: DataOwner8的最优x_8 = 0.1436
new: DataOwner9的最优x_9 = 0.1852
new: DataOwner10的最优x_10 = 0.1257
eta:1.81
new: DataOwner1的最优x_1 = 0.1383
new: DataOwner2的最优x_2 = 0.1244
new: DataOwner3的最优x_3 = 0.1616
new: DataOwner4的最优x_4 = 0.1559
new: DataOwner5的最优x_5 = 0.1746
new: DataOwner6的最优x_6 = 0.2516
new: DataOwner7的最优x_7 = 0.1538
new: DataOwner8的最优x_8 = 0.1444
new: DataOwner9的最优x_9 = 0.1862
new: DataOwner10的最优x_10 = 0.1264
eta:1.82
new: DataOwner1的最优x_1 = 0.1391
new: DataOwner2的最优x_2 = 0.1251
new: DataOwner3的最优x_3 = 0.1625
new: DataOwner4的最优x_4 = 0.1567
new: DataOwner5的最优x_5 = 0.1756
new: DataOwner6的最优x_6 = 0.2530
new: DataOwner7的最优x_7 = 0.1547
new: DataOwner8的最优x_8 = 0.1452
new: DataOwner9的最优x_9 = 0.1872
new: DataOwner10的最优x_10 = 0.1271
eta:1.83
new: DataOwner1的最优x_1 = 0.1398
new: DataOwner2的最优x_2 = 0.1258
new: DataOwner3的最优x_3 = 0.1634
new: DataOwner4的最优x_4 = 0.1576
new: DataOwner5的最优x_5 = 0.1765
new: DataOwner6的最优x_6 = 0.2543
new: DataOwner7的最优x_7 = 0.1555
new: DataOwner8的最优x_8 = 0.1460
new: DataOwner9的最优x_9 = 0.1882
new: DataOwner10的最优x_10 = 0.1278
eta:1.84
new: DataOwner1的最优x_1 = 0.1406
new: DataOwner2的最优x_2 = 0.1265
new: DataOwner3的最优x_3 = 0.1643
new: DataOwner4的最优x_4 = 0.1584
new: DataOwner5的最优x_5 = 0.1775
new: DataOwner6的最优x_6 = 0.2557
new: DataOwner7的最优x_7 = 0.1564
new: DataOwner8的最优x_8 = 0.1468
new: DataOwner9的最优x_9 = 0.1893
new: DataOwner10的最优x_10 = 0.1285
eta:1.85
new: DataOwner1的最优x_1 = 0.1414
new: DataOwner2的最优x_2 = 0.1271
new: DataOwner3的最优x_3 = 0.1652
new: DataOwner4的最优x_4 = 0.1593
new: DataOwner5的最优x_5 = 0.1785
new: DataOwner6的最优x_6 = 0.2571
new: DataOwner7的最优x_7 = 0.1572
new: DataOwner8的最优x_8 = 0.1476
new: DataOwner9的最优x_9 = 0.1903
new: DataOwner10的最优x_10 = 0.1292
eta:1.86
new: DataOwner1的最优x_1 = 0.1421
new: DataOwner2的最优x_2 = 0.1278
new: DataOwner3的最优x_3 = 0.1661
new: DataOwner4的最优x_4 = 0.1602
new: DataOwner5的最优x_5 = 0.1794
new: DataOwner6的最优x_6 = 0.2585
new: DataOwner7的最优x_7 = 0.1581
new: DataOwner8的最优x_8 = 0.1484
new: DataOwner9的最优x_9 = 0.1913
new: DataOwner10的最优x_10 = 0.1299
eta:1.87
new: DataOwner1的最优x_1 = 0.1429
new: DataOwner2的最优x_2 = 0.1285
new: DataOwner3的最优x_3 = 0.1670
new: DataOwner4的最优x_4 = 0.1610
new: DataOwner5的最优x_5 = 0.1804
new: DataOwner6的最优x_6 = 0.2599
new: DataOwner7的最优x_7 = 0.1589
new: DataOwner8的最优x_8 = 0.1492
new: DataOwner9的最优x_9 = 0.1924
new: DataOwner10的最优x_10 = 0.1306
eta:1.8800000000000001
new: DataOwner1的最优x_1 = 0.1436
new: DataOwner2的最优x_2 = 0.1292
new: DataOwner3的最优x_3 = 0.1678
new: DataOwner4的最优x_4 = 0.1619
new: DataOwner5的最优x_5 = 0.1814
new: DataOwner6的最优x_6 = 0.2613
new: DataOwner7的最优x_7 = 0.1598
new: DataOwner8的最优x_8 = 0.1500
new: DataOwner9的最优x_9 = 0.1934
new: DataOwner10的最优x_10 = 0.1313
eta:1.8900000000000001
new: DataOwner1的最优x_1 = 0.1444
new: DataOwner2的最优x_2 = 0.1299
new: DataOwner3的最优x_3 = 0.1687
new: DataOwner4的最优x_4 = 0.1628
new: DataOwner5的最优x_5 = 0.1823
new: DataOwner6的最优x_6 = 0.2627
new: DataOwner7的最优x_7 = 0.1606
new: DataOwner8的最优x_8 = 0.1508
new: DataOwner9的最优x_9 = 0.1944
new: DataOwner10的最优x_10 = 0.1320
eta:1.9000000000000001
new: DataOwner1的最优x_1 = 0.1452
new: DataOwner2的最优x_2 = 0.1306
new: DataOwner3的最优x_3 = 0.1696
new: DataOwner4的最优x_4 = 0.1636
new: DataOwner5的最优x_5 = 0.1833
new: DataOwner6的最优x_6 = 0.2641
new: DataOwner7的最优x_7 = 0.1615
new: DataOwner8的最优x_8 = 0.1516
new: DataOwner9的最优x_9 = 0.1954
new: DataOwner10的最优x_10 = 0.1327
eta:1.9100000000000001
new: DataOwner1的最优x_1 = 0.1459
new: DataOwner2的最优x_2 = 0.1313
new: DataOwner3的最优x_3 = 0.1705
new: DataOwner4的最优x_4 = 0.1645
new: DataOwner5的最优x_5 = 0.1843
new: DataOwner6的最优x_6 = 0.2655
new: DataOwner7的最优x_7 = 0.1623
new: DataOwner8的最优x_8 = 0.1524
new: DataOwner9的最优x_9 = 0.1965
new: DataOwner10的最优x_10 = 0.1334
eta:1.9200000000000002
new: DataOwner1的最优x_1 = 0.1467
new: DataOwner2的最优x_2 = 0.1320
new: DataOwner3的最优x_3 = 0.1714
new: DataOwner4的最优x_4 = 0.1653
new: DataOwner5的最优x_5 = 0.1852
new: DataOwner6的最优x_6 = 0.2669
new: DataOwner7的最优x_7 = 0.1632
new: DataOwner8的最优x_8 = 0.1532
new: DataOwner9的最优x_9 = 0.1975
new: DataOwner10的最优x_10 = 0.1341
eta:1.93
new: DataOwner1的最优x_1 = 0.1475
new: DataOwner2的最优x_2 = 0.1326
new: DataOwner3的最优x_3 = 0.1723
new: DataOwner4的最优x_4 = 0.1662
new: DataOwner5的最优x_5 = 0.1862
new: DataOwner6的最优x_6 = 0.2682
new: DataOwner7的最优x_7 = 0.1640
new: DataOwner8的最优x_8 = 0.1540
new: DataOwner9的最优x_9 = 0.1985
new: DataOwner10的最优x_10 = 0.1348
eta:1.94
new: DataOwner1的最优x_1 = 0.1482
new: DataOwner2的最优x_2 = 0.1333
new: DataOwner3的最优x_3 = 0.1732
new: DataOwner4的最优x_4 = 0.1671
new: DataOwner5的最优x_5 = 0.1872
new: DataOwner6的最优x_6 = 0.2696
new: DataOwner7的最优x_7 = 0.1649
new: DataOwner8的最优x_8 = 0.1548
new: DataOwner9的最优x_9 = 0.1996
new: DataOwner10的最优x_10 = 0.1355
eta:1.95
new: DataOwner1的最优x_1 = 0.1490
new: DataOwner2的最优x_2 = 0.1340
new: DataOwner3的最优x_3 = 0.1741
new: DataOwner4的最优x_4 = 0.1679
new: DataOwner5的最优x_5 = 0.1881
new: DataOwner6的最优x_6 = 0.2710
new: DataOwner7的最优x_7 = 0.1657
new: DataOwner8的最优x_8 = 0.1555
new: DataOwner9的最优x_9 = 0.2006
new: DataOwner10的最优x_10 = 0.1362
eta:1.96
new: DataOwner1的最优x_1 = 0.1498
new: DataOwner2的最优x_2 = 0.1347
new: DataOwner3的最优x_3 = 0.1750
new: DataOwner4的最优x_4 = 0.1688
new: DataOwner5的最优x_5 = 0.1891
new: DataOwner6的最优x_6 = 0.2724
new: DataOwner7的最优x_7 = 0.1666
new: DataOwner8的最优x_8 = 0.1563
new: DataOwner9的最优x_9 = 0.2016
new: DataOwner10的最优x_10 = 0.1369
eta:1.97
new: DataOwner1的最优x_1 = 0.1505
new: DataOwner2的最优x_2 = 0.1354
new: DataOwner3的最优x_3 = 0.1759
new: DataOwner4的最优x_4 = 0.1696
new: DataOwner5的最优x_5 = 0.1901
new: DataOwner6的最优x_6 = 0.2738
new: DataOwner7的最优x_7 = 0.1674
new: DataOwner8的最优x_8 = 0.1571
new: DataOwner9的最优x_9 = 0.2027
new: DataOwner10的最优x_10 = 0.1376
eta:1.98
new: DataOwner1的最优x_1 = 0.1513
new: DataOwner2的最优x_2 = 0.1361
new: DataOwner3的最优x_3 = 0.1768
new: DataOwner4的最优x_4 = 0.1705
new: DataOwner5的最优x_5 = 0.1910
new: DataOwner6的最优x_6 = 0.2752
new: DataOwner7的最优x_7 = 0.1683
new: DataOwner8的最优x_8 = 0.1579
new: DataOwner9的最优x_9 = 0.2037
new: DataOwner10的最优x_10 = 0.1383
eta:1.99
new: DataOwner1的最优x_1 = 0.1521
new: DataOwner2的最优x_2 = 0.1368
new: DataOwner3的最优x_3 = 0.1777
new: DataOwner4的最优x_4 = 0.1714
new: DataOwner5的最优x_5 = 0.1920
new: DataOwner6的最优x_6 = 0.2766
new: DataOwner7的最优x_7 = 0.1691
new: DataOwner8的最优x_8 = 0.1587
new: DataOwner9的最优x_9 = 0.2047
new: DataOwner10的最优x_10 = 0.1389
eta:2.0
new: DataOwner1的最优x_1 = 0.1528
new: DataOwner2的最优x_2 = 0.1374
new: DataOwner3的最优x_3 = 0.1786
new: DataOwner4的最优x_4 = 0.1722
new: DataOwner5的最优x_5 = 0.1930
new: DataOwner6的最优x_6 = 0.2780
new: DataOwner7的最优x_7 = 0.1700
new: DataOwner8的最优x_8 = 0.1595
new: DataOwner9的最优x_9 = 0.2057
new: DataOwner10的最优x_10 = 0.1396
eta:2.01
new: DataOwner1的最优x_1 = 0.1536
new: DataOwner2的最优x_2 = 0.1381
new: DataOwner3的最优x_3 = 0.1795
new: DataOwner4的最优x_4 = 0.1731
new: DataOwner5的最优x_5 = 0.1939
new: DataOwner6的最优x_6 = 0.2794
new: DataOwner7的最优x_7 = 0.1708
new: DataOwner8的最优x_8 = 0.1603
new: DataOwner9的最优x_9 = 0.2068
new: DataOwner10的最优x_10 = 0.1403
eta:2.02
new: DataOwner1的最优x_1 = 0.1543
new: DataOwner2的最优x_2 = 0.1388
new: DataOwner3的最优x_3 = 0.1803
new: DataOwner4的最优x_4 = 0.1740
new: DataOwner5的最优x_5 = 0.1949
new: DataOwner6的最优x_6 = 0.2808
new: DataOwner7的最优x_7 = 0.1717
new: DataOwner8的最优x_8 = 0.1611
new: DataOwner9的最优x_9 = 0.2078
new: DataOwner10的最优x_10 = 0.1410
eta:2.03
new: DataOwner1的最优x_1 = 0.1551
new: DataOwner2的最优x_2 = 0.1395
new: DataOwner3的最优x_3 = 0.1812
new: DataOwner4的最优x_4 = 0.1748
new: DataOwner5的最优x_5 = 0.1958
new: DataOwner6的最优x_6 = 0.2821
new: DataOwner7的最优x_7 = 0.1725
new: DataOwner8的最优x_8 = 0.1619
new: DataOwner9的最优x_9 = 0.2088
new: DataOwner10的最优x_10 = 0.1417
eta:2.04
new: DataOwner1的最优x_1 = 0.1559
new: DataOwner2的最优x_2 = 0.1402
new: DataOwner3的最优x_3 = 0.1821
new: DataOwner4的最优x_4 = 0.1757
new: DataOwner5的最优x_5 = 0.1968
new: DataOwner6的最优x_6 = 0.2835
new: DataOwner7的最优x_7 = 0.1734
new: DataOwner8的最优x_8 = 0.1627
new: DataOwner9的最优x_9 = 0.2099
new: DataOwner10的最优x_10 = 0.1424
eta:2.05
new: DataOwner1的最优x_1 = 0.1566
new: DataOwner2的最优x_2 = 0.1409
new: DataOwner3的最优x_3 = 0.1830
new: DataOwner4的最优x_4 = 0.1765
new: DataOwner5的最优x_5 = 0.1978
new: DataOwner6的最优x_6 = 0.2849
new: DataOwner7的最优x_7 = 0.1742
new: DataOwner8的最优x_8 = 0.1635
new: DataOwner9的最优x_9 = 0.2109
new: DataOwner10的最优x_10 = 0.1431
eta:2.0599999999999996
new: DataOwner1的最优x_1 = 0.1574
new: DataOwner2的最优x_2 = 0.1416
new: DataOwner3的最优x_3 = 0.1839
new: DataOwner4的最优x_4 = 0.1774
new: DataOwner5的最优x_5 = 0.1987
new: DataOwner6的最优x_6 = 0.2863
new: DataOwner7的最优x_7 = 0.1751
new: DataOwner8的最优x_8 = 0.1643
new: DataOwner9的最优x_9 = 0.2119
new: DataOwner10的最优x_10 = 0.1438
eta:2.07
new: DataOwner1的最优x_1 = 0.1582
new: DataOwner2的最优x_2 = 0.1423
new: DataOwner3的最优x_3 = 0.1848
new: DataOwner4的最优x_4 = 0.1783
new: DataOwner5的最优x_5 = 0.1997
new: DataOwner6的最优x_6 = 0.2877
new: DataOwner7的最优x_7 = 0.1759
new: DataOwner8的最优x_8 = 0.1651
new: DataOwner9的最优x_9 = 0.2129
new: DataOwner10的最优x_10 = 0.1445
eta:2.0799999999999996
new: DataOwner1的最优x_1 = 0.1589
new: DataOwner2的最优x_2 = 0.1429
new: DataOwner3的最优x_3 = 0.1857
new: DataOwner4的最优x_4 = 0.1791
new: DataOwner5的最优x_5 = 0.2007
new: DataOwner6的最优x_6 = 0.2891
new: DataOwner7的最优x_7 = 0.1768
new: DataOwner8的最优x_8 = 0.1659
new: DataOwner9的最优x_9 = 0.2140
new: DataOwner10的最优x_10 = 0.1452
eta:2.09
new: DataOwner1的最优x_1 = 0.1597
new: DataOwner2的最优x_2 = 0.1436
new: DataOwner3的最优x_3 = 0.1866
new: DataOwner4的最优x_4 = 0.1800
new: DataOwner5的最优x_5 = 0.2016
new: DataOwner6的最优x_6 = 0.2905
new: DataOwner7的最优x_7 = 0.1776
new: DataOwner8的最优x_8 = 0.1667
new: DataOwner9的最优x_9 = 0.2150
new: DataOwner10的最优x_10 = 0.1459
eta:2.0999999999999996
new: DataOwner1的最优x_1 = 0.1605
new: DataOwner2的最优x_2 = 0.1443
new: DataOwner3的最优x_3 = 0.1875
new: DataOwner4的最优x_4 = 0.1808
new: DataOwner5的最优x_5 = 0.2026
new: DataOwner6的最优x_6 = 0.2919
new: DataOwner7的最优x_7 = 0.1785
new: DataOwner8的最优x_8 = 0.1675
new: DataOwner9的最优x_9 = 0.2160
new: DataOwner10的最优x_10 = 0.1466
eta:2.11
new: DataOwner1的最优x_1 = 0.1612
new: DataOwner2的最优x_2 = 0.1450
new: DataOwner3的最优x_3 = 0.1884
new: DataOwner4的最优x_4 = 0.1817
new: DataOwner5的最优x_5 = 0.2036
new: DataOwner6的最优x_6 = 0.2933
new: DataOwner7的最优x_7 = 0.1793
new: DataOwner8的最优x_8 = 0.1683
new: DataOwner9的最优x_9 = 0.2171
new: DataOwner10的最优x_10 = 0.1473
eta:2.1199999999999997
new: DataOwner1的最优x_1 = 0.1620
new: DataOwner2的最优x_2 = 0.1457
new: DataOwner3的最优x_3 = 0.1893
new: DataOwner4的最优x_4 = 0.1826
new: DataOwner5的最优x_5 = 0.2045
new: DataOwner6的最优x_6 = 0.2946
new: DataOwner7的最优x_7 = 0.1802
new: DataOwner8的最优x_8 = 0.1691
new: DataOwner9的最优x_9 = 0.2181
new: DataOwner10的最优x_10 = 0.1480
eta:2.13
new: DataOwner1的最优x_1 = 0.1628
new: DataOwner2的最优x_2 = 0.1464
new: DataOwner3的最优x_3 = 0.1902
new: DataOwner4的最优x_4 = 0.1834
new: DataOwner5的最优x_5 = 0.2055
new: DataOwner6的最优x_6 = 0.2960
new: DataOwner7的最优x_7 = 0.1810
new: DataOwner8的最优x_8 = 0.1699
new: DataOwner9的最优x_9 = 0.2191
new: DataOwner10的最优x_10 = 0.1487
eta:2.1399999999999997
new: DataOwner1的最优x_1 = 0.1635
new: DataOwner2的最优x_2 = 0.1471
new: DataOwner3的最优x_3 = 0.1911
new: DataOwner4的最优x_4 = 0.1843
new: DataOwner5的最优x_5 = 0.2065
new: DataOwner6的最优x_6 = 0.2974
new: DataOwner7的最优x_7 = 0.1819
new: DataOwner8的最优x_8 = 0.1707
new: DataOwner9的最优x_9 = 0.2201
new: DataOwner10的最优x_10 = 0.1494
eta:2.15
new: DataOwner1的最优x_1 = 0.1643
new: DataOwner2的最优x_2 = 0.1478
new: DataOwner3的最优x_3 = 0.1920
new: DataOwner4的最优x_4 = 0.1851
new: DataOwner5的最优x_5 = 0.2074
new: DataOwner6的最优x_6 = 0.2988
new: DataOwner7的最优x_7 = 0.1827
new: DataOwner8的最优x_8 = 0.1715
new: DataOwner9的最优x_9 = 0.2212
new: DataOwner10的最优x_10 = 0.1501
eta:2.1599999999999997
new: DataOwner1的最优x_1 = 0.1650
new: DataOwner2的最优x_2 = 0.1484
new: DataOwner3的最优x_3 = 0.1928
new: DataOwner4的最优x_4 = 0.1860
new: DataOwner5的最优x_5 = 0.2084
new: DataOwner6的最优x_6 = 0.3002
new: DataOwner7的最优x_7 = 0.1836
new: DataOwner8的最优x_8 = 0.1723
new: DataOwner9的最优x_9 = 0.2222
new: DataOwner10的最优x_10 = 0.1508
eta:2.17
new: DataOwner1的最优x_1 = 0.1658
new: DataOwner2的最优x_2 = 0.1491
new: DataOwner3的最优x_3 = 0.1937
new: DataOwner4的最优x_4 = 0.1869
new: DataOwner5的最优x_5 = 0.2094
new: DataOwner6的最优x_6 = 0.3016
new: DataOwner7的最优x_7 = 0.1844
new: DataOwner8的最优x_8 = 0.1731
new: DataOwner9的最优x_9 = 0.2232
new: DataOwner10的最优x_10 = 0.1515
eta:2.1799999999999997
new: DataOwner1的最优x_1 = 0.1666
new: DataOwner2的最优x_2 = 0.1498
new: DataOwner3的最优x_3 = 0.1946
new: DataOwner4的最优x_4 = 0.1877
new: DataOwner5的最优x_5 = 0.2103
new: DataOwner6的最优x_6 = 0.3030
new: DataOwner7的最优x_7 = 0.1853
new: DataOwner8的最优x_8 = 0.1739
new: DataOwner9的最优x_9 = 0.2243
new: DataOwner10的最优x_10 = 0.1522
eta:2.19
new: DataOwner1的最优x_1 = 0.1673
new: DataOwner2的最优x_2 = 0.1505
new: DataOwner3的最优x_3 = 0.1955
new: DataOwner4的最优x_4 = 0.1886
new: DataOwner5的最优x_5 = 0.2113
new: DataOwner6的最优x_6 = 0.3044
new: DataOwner7的最优x_7 = 0.1861
new: DataOwner8的最优x_8 = 0.1747
new: DataOwner9的最优x_9 = 0.2253
new: DataOwner10的最优x_10 = 0.1529
eta:2.1999999999999997
new: DataOwner1的最优x_1 = 0.1681
new: DataOwner2的最优x_2 = 0.1512
new: DataOwner3的最优x_3 = 0.1964
new: DataOwner4的最优x_4 = 0.1895
new: DataOwner5的最优x_5 = 0.2122
new: DataOwner6的最优x_6 = 0.3058
new: DataOwner7的最优x_7 = 0.1870
new: DataOwner8的最优x_8 = 0.1755
new: DataOwner9的最优x_9 = 0.2263
new: DataOwner10的最优x_10 = 0.1536
eta:2.21
new: DataOwner1的最优x_1 = 0.1689
new: DataOwner2的最优x_2 = 0.1519
new: DataOwner3的最优x_3 = 0.1973
new: DataOwner4的最优x_4 = 0.1903
new: DataOwner5的最优x_5 = 0.2132
new: DataOwner6的最优x_6 = 0.3072
new: DataOwner7的最优x_7 = 0.1878
new: DataOwner8的最优x_8 = 0.1763
new: DataOwner9的最优x_9 = 0.2273
new: DataOwner10的最优x_10 = 0.1543
eta:2.2199999999999998
new: DataOwner1的最优x_1 = 0.1696
new: DataOwner2的最优x_2 = 0.1526
new: DataOwner3的最优x_3 = 0.1982
new: DataOwner4的最优x_4 = 0.1912
new: DataOwner5的最优x_5 = 0.2142
new: DataOwner6的最优x_6 = 0.3085
new: DataOwner7的最优x_7 = 0.1887
new: DataOwner8的最优x_8 = 0.1771
new: DataOwner9的最优x_9 = 0.2284
new: DataOwner10的最优x_10 = 0.1550
eta:2.23
new: DataOwner1的最优x_1 = 0.1704
new: DataOwner2的最优x_2 = 0.1533
new: DataOwner3的最优x_3 = 0.1991
new: DataOwner4的最优x_4 = 0.1920
new: DataOwner5的最优x_5 = 0.2151
new: DataOwner6的最优x_6 = 0.3099
new: DataOwner7的最优x_7 = 0.1895
new: DataOwner8的最优x_8 = 0.1779
new: DataOwner9的最优x_9 = 0.2294
new: DataOwner10的最优x_10 = 0.1557
eta:2.2399999999999998
new: DataOwner1的最优x_1 = 0.1712
new: DataOwner2的最优x_2 = 0.1539
new: DataOwner3的最优x_3 = 0.2000
new: DataOwner4的最优x_4 = 0.1929
new: DataOwner5的最优x_5 = 0.2161
new: DataOwner6的最优x_6 = 0.3113
new: DataOwner7的最优x_7 = 0.1904
new: DataOwner8的最优x_8 = 0.1787
new: DataOwner9的最优x_9 = 0.2304
new: DataOwner10的最优x_10 = 0.1564
eta:2.25
new: DataOwner1的最优x_1 = 0.1719
new: DataOwner2的最优x_2 = 0.1546
new: DataOwner3的最优x_3 = 0.2009
new: DataOwner4的最优x_4 = 0.1938
new: DataOwner5的最优x_5 = 0.2171
new: DataOwner6的最优x_6 = 0.3127
new: DataOwner7的最优x_7 = 0.1912
new: DataOwner8的最优x_8 = 0.1795
new: DataOwner9的最优x_9 = 0.2315
new: DataOwner10的最优x_10 = 0.1571
eta:2.26
new: DataOwner1的最优x_1 = 0.1727
new: DataOwner2的最优x_2 = 0.1553
new: DataOwner3的最优x_3 = 0.2018
new: DataOwner4的最优x_4 = 0.1946
new: DataOwner5的最优x_5 = 0.2180
new: DataOwner6的最优x_6 = 0.3141
new: DataOwner7的最优x_7 = 0.1921
new: DataOwner8的最优x_8 = 0.1803
new: DataOwner9的最优x_9 = 0.2325
new: DataOwner10的最优x_10 = 0.1578
eta:2.27
new: DataOwner1的最优x_1 = 0.1734
new: DataOwner2的最优x_2 = 0.1560
new: DataOwner3的最优x_3 = 0.2027
new: DataOwner4的最优x_4 = 0.1955
new: DataOwner5的最优x_5 = 0.2190
new: DataOwner6的最优x_6 = 0.3155
new: DataOwner7的最优x_7 = 0.1929
new: DataOwner8的最优x_8 = 0.1811
new: DataOwner9的最优x_9 = 0.2335
new: DataOwner10的最优x_10 = 0.1585
eta:2.28
new: DataOwner1的最优x_1 = 0.1742
new: DataOwner2的最优x_2 = 0.1567
new: DataOwner3的最优x_3 = 0.2036
new: DataOwner4的最优x_4 = 0.1963
new: DataOwner5的最优x_5 = 0.2200
new: DataOwner6的最优x_6 = 0.3169
new: DataOwner7的最优x_7 = 0.1938
new: DataOwner8的最优x_8 = 0.1819
new: DataOwner9的最优x_9 = 0.2345
new: DataOwner10的最优x_10 = 0.1592
eta:2.29
new: DataOwner1的最优x_1 = 0.1750
new: DataOwner2的最优x_2 = 0.1574
new: DataOwner3的最优x_3 = 0.2045
new: DataOwner4的最优x_4 = 0.1972
new: DataOwner5的最优x_5 = 0.2209
new: DataOwner6的最优x_6 = 0.3183
new: DataOwner7的最优x_7 = 0.1946
new: DataOwner8的最优x_8 = 0.1827
new: DataOwner9的最优x_9 = 0.2356
new: DataOwner10的最优x_10 = 0.1599
eta:2.3
new: DataOwner1的最优x_1 = 0.1757
new: DataOwner2的最优x_2 = 0.1581
new: DataOwner3的最优x_3 = 0.2053
new: DataOwner4的最优x_4 = 0.1981
new: DataOwner5的最优x_5 = 0.2219
new: DataOwner6的最优x_6 = 0.3197
new: DataOwner7的最优x_7 = 0.1955
new: DataOwner8的最优x_8 = 0.1835
new: DataOwner9的最优x_9 = 0.2366
new: DataOwner10的最优x_10 = 0.1606
eta:2.31
new: DataOwner1的最优x_1 = 0.1765
new: DataOwner2的最优x_2 = 0.1588
new: DataOwner3的最优x_3 = 0.2062
new: DataOwner4的最优x_4 = 0.1989
new: DataOwner5的最优x_5 = 0.2229
new: DataOwner6的最优x_6 = 0.3211
new: DataOwner7的最优x_7 = 0.1963
new: DataOwner8的最优x_8 = 0.1843
new: DataOwner9的最优x_9 = 0.2376
new: DataOwner10的最优x_10 = 0.1613
eta:2.32
new: DataOwner1的最优x_1 = 0.1773
new: DataOwner2的最优x_2 = 0.1594
new: DataOwner3的最优x_3 = 0.2071
new: DataOwner4的最优x_4 = 0.1998
new: DataOwner5的最优x_5 = 0.2238
new: DataOwner6的最优x_6 = 0.3224
new: DataOwner7的最优x_7 = 0.1972
new: DataOwner8的最优x_8 = 0.1851
new: DataOwner9的最优x_9 = 0.2387
new: DataOwner10的最优x_10 = 0.1620
eta:2.3299999999999996
new: DataOwner1的最优x_1 = 0.1780
new: DataOwner2的最优x_2 = 0.1601
new: DataOwner3的最优x_3 = 0.2080
new: DataOwner4的最优x_4 = 0.2006
new: DataOwner5的最优x_5 = 0.2248
new: DataOwner6的最优x_6 = 0.3238
new: DataOwner7的最优x_7 = 0.1980
new: DataOwner8的最优x_8 = 0.1859
new: DataOwner9的最优x_9 = 0.2397
new: DataOwner10的最优x_10 = 0.1627
eta:2.34
new: DataOwner1的最优x_1 = 0.1788
new: DataOwner2的最优x_2 = 0.1608
new: DataOwner3的最优x_3 = 0.2089
new: DataOwner4的最优x_4 = 0.2015
new: DataOwner5的最优x_5 = 0.2258
new: DataOwner6的最优x_6 = 0.3252
new: DataOwner7的最优x_7 = 0.1989
new: DataOwner8的最优x_8 = 0.1867
new: DataOwner9的最优x_9 = 0.2407
new: DataOwner10的最优x_10 = 0.1634
eta:2.3499999999999996
new: DataOwner1的最优x_1 = 0.1796
new: DataOwner2的最优x_2 = 0.1615
new: DataOwner3的最优x_3 = 0.2098
new: DataOwner4的最优x_4 = 0.2024
new: DataOwner5的最优x_5 = 0.2267
new: DataOwner6的最优x_6 = 0.3266
new: DataOwner7的最优x_7 = 0.1997
new: DataOwner8的最优x_8 = 0.1875
new: DataOwner9的最优x_9 = 0.2417
new: DataOwner10的最优x_10 = 0.1641
eta:2.36
new: DataOwner1的最优x_1 = 0.1803
new: DataOwner2的最优x_2 = 0.1622
new: DataOwner3的最优x_3 = 0.2107
new: DataOwner4的最优x_4 = 0.2032
new: DataOwner5的最优x_5 = 0.2277
new: DataOwner6的最优x_6 = 0.3280
new: DataOwner7的最优x_7 = 0.2006
new: DataOwner8的最优x_8 = 0.1883
new: DataOwner9的最优x_9 = 0.2428
new: DataOwner10的最优x_10 = 0.1648
eta:2.3699999999999997
new: DataOwner1的最优x_1 = 0.1811
new: DataOwner2的最优x_2 = 0.1629
new: DataOwner3的最优x_3 = 0.2116
new: DataOwner4的最优x_4 = 0.2041
new: DataOwner5的最优x_5 = 0.2286
new: DataOwner6的最优x_6 = 0.3294
new: DataOwner7的最优x_7 = 0.2014
new: DataOwner8的最优x_8 = 0.1891
new: DataOwner9的最优x_9 = 0.2438
new: DataOwner10的最优x_10 = 0.1655
eta:2.38
new: DataOwner1的最优x_1 = 0.1819
new: DataOwner2的最优x_2 = 0.1636
new: DataOwner3的最优x_3 = 0.2125
new: DataOwner4的最优x_4 = 0.2050
new: DataOwner5的最优x_5 = 0.2296
new: DataOwner6的最优x_6 = 0.3308
new: DataOwner7的最优x_7 = 0.2023
new: DataOwner8的最优x_8 = 0.1898
new: DataOwner9的最优x_9 = 0.2448
new: DataOwner10的最优x_10 = 0.1662
eta:2.3899999999999997
new: DataOwner1的最优x_1 = 0.1826
new: DataOwner2的最优x_2 = 0.1643
new: DataOwner3的最优x_3 = 0.2134
new: DataOwner4的最优x_4 = 0.2058
new: DataOwner5的最优x_5 = 0.2306
new: DataOwner6的最优x_6 = 0.3322
new: DataOwner7的最优x_7 = 0.2031
new: DataOwner8的最优x_8 = 0.1906
new: DataOwner9的最优x_9 = 0.2459
new: DataOwner10的最优x_10 = 0.1669
eta:2.4
new: DataOwner1的最优x_1 = 0.1834
new: DataOwner2的最优x_2 = 0.1649
new: DataOwner3的最优x_3 = 0.2143
new: DataOwner4的最优x_4 = 0.2067
new: DataOwner5的最优x_5 = 0.2315
new: DataOwner6的最优x_6 = 0.3336
new: DataOwner7的最优x_7 = 0.2040
new: DataOwner8的最优x_8 = 0.1914
new: DataOwner9的最优x_9 = 0.2469
new: DataOwner10的最优x_10 = 0.1676
eta:2.4099999999999997
new: DataOwner1的最优x_1 = 0.1841
new: DataOwner2的最优x_2 = 0.1656
new: DataOwner3的最优x_3 = 0.2152
new: DataOwner4的最优x_4 = 0.2075
new: DataOwner5的最优x_5 = 0.2325
new: DataOwner6的最优x_6 = 0.3350
new: DataOwner7的最优x_7 = 0.2048
new: DataOwner8的最优x_8 = 0.1922
new: DataOwner9的最优x_9 = 0.2479
new: DataOwner10的最优x_10 = 0.1683
eta:2.42
new: DataOwner1的最优x_1 = 0.1849
new: DataOwner2的最优x_2 = 0.1663
new: DataOwner3的最优x_3 = 0.2161
new: DataOwner4的最优x_4 = 0.2084
new: DataOwner5的最优x_5 = 0.2335
new: DataOwner6的最优x_6 = 0.3363
new: DataOwner7的最优x_7 = 0.2057
new: DataOwner8的最优x_8 = 0.1930
new: DataOwner9的最优x_9 = 0.2489
new: DataOwner10的最优x_10 = 0.1690
eta:2.4299999999999997
new: DataOwner1的最优x_1 = 0.1857
new: DataOwner2的最优x_2 = 0.1670
new: DataOwner3的最优x_3 = 0.2169
new: DataOwner4的最优x_4 = 0.2093
new: DataOwner5的最优x_5 = 0.2344
new: DataOwner6的最优x_6 = 0.3377
new: DataOwner7的最优x_7 = 0.2065
new: DataOwner8的最优x_8 = 0.1938
new: DataOwner9的最优x_9 = 0.2500
new: DataOwner10的最优x_10 = 0.1697
eta:2.44
new: DataOwner1的最优x_1 = 0.1864
new: DataOwner2的最优x_2 = 0.1677
new: DataOwner3的最优x_3 = 0.2178
new: DataOwner4的最优x_4 = 0.2101
new: DataOwner5的最优x_5 = 0.2354
new: DataOwner6的最优x_6 = 0.3391
new: DataOwner7的最优x_7 = 0.2074
new: DataOwner8的最优x_8 = 0.1946
new: DataOwner9的最优x_9 = 0.2510
new: DataOwner10的最优x_10 = 0.1704
eta:2.4499999999999997
new: DataOwner1的最优x_1 = 0.1872
new: DataOwner2的最优x_2 = 0.1684
new: DataOwner3的最优x_3 = 0.2187
new: DataOwner4的最优x_4 = 0.2110
new: DataOwner5的最优x_5 = 0.2364
new: DataOwner6的最优x_6 = 0.3405
new: DataOwner7的最优x_7 = 0.2082
new: DataOwner8的最优x_8 = 0.1954
new: DataOwner9的最优x_9 = 0.2520
new: DataOwner10的最优x_10 = 0.1711
eta:2.46
new: DataOwner1的最优x_1 = 0.1880
new: DataOwner2的最优x_2 = 0.1691
new: DataOwner3的最优x_3 = 0.2196
new: DataOwner4的最优x_4 = 0.2118
new: DataOwner5的最优x_5 = 0.2373
new: DataOwner6的最优x_6 = 0.3419
new: DataOwner7的最优x_7 = 0.2091
new: DataOwner8的最优x_8 = 0.1962
new: DataOwner9的最优x_9 = 0.2531
new: DataOwner10的最优x_10 = 0.1718
eta:2.4699999999999998
new: DataOwner1的最优x_1 = 0.1887
new: DataOwner2的最优x_2 = 0.1697
new: DataOwner3的最优x_3 = 0.2205
new: DataOwner4的最优x_4 = 0.2127
new: DataOwner5的最优x_5 = 0.2383
new: DataOwner6的最优x_6 = 0.3433
new: DataOwner7的最优x_7 = 0.2099
new: DataOwner8的最优x_8 = 0.1970
new: DataOwner9的最优x_9 = 0.2541
new: DataOwner10的最优x_10 = 0.1725
eta:2.48
new: DataOwner1的最优x_1 = 0.1895
new: DataOwner2的最优x_2 = 0.1704
new: DataOwner3的最优x_3 = 0.2214
new: DataOwner4的最优x_4 = 0.2136
new: DataOwner5的最优x_5 = 0.2393
new: DataOwner6的最优x_6 = 0.3447
new: DataOwner7的最优x_7 = 0.2108
new: DataOwner8的最优x_8 = 0.1978
new: DataOwner9的最优x_9 = 0.2551
new: DataOwner10的最优x_10 = 0.1732
eta:2.4899999999999998
new: DataOwner1的最优x_1 = 0.1903
new: DataOwner2的最优x_2 = 0.1711
new: DataOwner3的最优x_3 = 0.2223
new: DataOwner4的最优x_4 = 0.2144
new: DataOwner5的最优x_5 = 0.2402
new: DataOwner6的最优x_6 = 0.3461
new: DataOwner7的最优x_7 = 0.2116
new: DataOwner8的最优x_8 = 0.1986
new: DataOwner9的最优x_9 = 0.2561
new: DataOwner10的最优x_10 = 0.1739
eta:2.5
new: DataOwner1的最优x_1 = 0.1910
new: DataOwner2的最优x_2 = 0.1718
new: DataOwner3的最优x_3 = 0.2232
new: DataOwner4的最优x_4 = 0.2153
new: DataOwner5的最优x_5 = 0.2412
new: DataOwner6的最优x_6 = 0.3475
new: DataOwner7的最优x_7 = 0.2125
new: DataOwner8的最优x_8 = 0.1994
new: DataOwner9的最优x_9 = 0.2572
new: DataOwner10的最优x_10 = 0.1746
eta:2.51
new: DataOwner1的最优x_1 = 0.1918
new: DataOwner2的最优x_2 = 0.1725
new: DataOwner3的最优x_3 = 0.2241
new: DataOwner4的最优x_4 = 0.2161
new: DataOwner5的最优x_5 = 0.2422
new: DataOwner6的最优x_6 = 0.3489
new: DataOwner7的最优x_7 = 0.2133
new: DataOwner8的最优x_8 = 0.2002
new: DataOwner9的最优x_9 = 0.2582
new: DataOwner10的最优x_10 = 0.1753
eta:2.52
new: DataOwner1的最优x_1 = 0.1925
new: DataOwner2的最优x_2 = 0.1732
new: DataOwner3的最优x_3 = 0.2250
new: DataOwner4的最优x_4 = 0.2170
new: DataOwner5的最优x_5 = 0.2431
new: DataOwner6的最优x_6 = 0.3502
new: DataOwner7的最优x_7 = 0.2142
new: DataOwner8的最优x_8 = 0.2010
new: DataOwner9的最优x_9 = 0.2592
new: DataOwner10的最优x_10 = 0.1760
eta:2.53
new: DataOwner1的最优x_1 = 0.1933
new: DataOwner2的最优x_2 = 0.1739
new: DataOwner3的最优x_3 = 0.2259
new: DataOwner4的最优x_4 = 0.2179
new: DataOwner5的最优x_5 = 0.2441
new: DataOwner6的最优x_6 = 0.3516
new: DataOwner7的最优x_7 = 0.2150
new: DataOwner8的最优x_8 = 0.2018
new: DataOwner9的最优x_9 = 0.2603
new: DataOwner10的最优x_10 = 0.1767
eta:2.54
new: DataOwner1的最优x_1 = 0.1941
new: DataOwner2的最优x_2 = 0.1746
new: DataOwner3的最优x_3 = 0.2268
new: DataOwner4的最优x_4 = 0.2187
new: DataOwner5的最优x_5 = 0.2450
new: DataOwner6的最优x_6 = 0.3530
new: DataOwner7的最优x_7 = 0.2159
new: DataOwner8的最优x_8 = 0.2026
new: DataOwner9的最优x_9 = 0.2613
new: DataOwner10的最优x_10 = 0.1774
eta:2.55
new: DataOwner1的最优x_1 = 0.1948
new: DataOwner2的最优x_2 = 0.1752
new: DataOwner3的最优x_3 = 0.2277
new: DataOwner4的最优x_4 = 0.2196
new: DataOwner5的最优x_5 = 0.2460
new: DataOwner6的最优x_6 = 0.3544
new: DataOwner7的最优x_7 = 0.2167
new: DataOwner8的最优x_8 = 0.2034
new: DataOwner9的最优x_9 = 0.2623
new: DataOwner10的最优x_10 = 0.1781
eta:2.56
new: DataOwner1的最优x_1 = 0.1956
new: DataOwner2的最优x_2 = 0.1759
new: DataOwner3的最优x_3 = 0.2286
new: DataOwner4的最优x_4 = 0.2205
new: DataOwner5的最优x_5 = 0.2470
new: DataOwner6的最优x_6 = 0.3558
new: DataOwner7的最优x_7 = 0.2176
new: DataOwner8的最优x_8 = 0.2042
new: DataOwner9的最优x_9 = 0.2633
new: DataOwner10的最优x_10 = 0.1787
eta:2.57
new: DataOwner1的最优x_1 = 0.1964
new: DataOwner2的最优x_2 = 0.1766
new: DataOwner3的最优x_3 = 0.2294
new: DataOwner4的最优x_4 = 0.2213
new: DataOwner5的最优x_5 = 0.2479
new: DataOwner6的最优x_6 = 0.3572
new: DataOwner7的最优x_7 = 0.2184
new: DataOwner8的最优x_8 = 0.2050
new: DataOwner9的最优x_9 = 0.2644
new: DataOwner10的最优x_10 = 0.1794
eta:2.5799999999999996
new: DataOwner1的最优x_1 = 0.1971
new: DataOwner2的最优x_2 = 0.1773
new: DataOwner3的最优x_3 = 0.2303
new: DataOwner4的最优x_4 = 0.2222
new: DataOwner5的最优x_5 = 0.2489
new: DataOwner6的最优x_6 = 0.3586
new: DataOwner7的最优x_7 = 0.2193
new: DataOwner8的最优x_8 = 0.2058
new: DataOwner9的最优x_9 = 0.2654
new: DataOwner10的最优x_10 = 0.1801
eta:2.59
new: DataOwner1的最优x_1 = 0.1979
new: DataOwner2的最优x_2 = 0.1780
new: DataOwner3的最优x_3 = 0.2312
new: DataOwner4的最优x_4 = 0.2230
new: DataOwner5的最优x_5 = 0.2499
new: DataOwner6的最优x_6 = 0.3600
new: DataOwner7的最优x_7 = 0.2201
new: DataOwner8的最优x_8 = 0.2066
new: DataOwner9的最优x_9 = 0.2664
new: DataOwner10的最优x_10 = 0.1808
eta:2.5999999999999996
new: DataOwner1的最优x_1 = 0.1987
new: DataOwner2的最优x_2 = 0.1787
new: DataOwner3的最优x_3 = 0.2321
new: DataOwner4的最优x_4 = 0.2239
new: DataOwner5的最优x_5 = 0.2508
new: DataOwner6的最优x_6 = 0.3614
new: DataOwner7的最优x_7 = 0.2210
new: DataOwner8的最优x_8 = 0.2074
new: DataOwner9的最优x_9 = 0.2675
new: DataOwner10的最优x_10 = 0.1815
eta:2.61
new: DataOwner1的最优x_1 = 0.1994
new: DataOwner2的最优x_2 = 0.1794
new: DataOwner3的最优x_3 = 0.2330
new: DataOwner4的最优x_4 = 0.2248
new: DataOwner5的最优x_5 = 0.2518
new: DataOwner6的最优x_6 = 0.3628
new: DataOwner7的最优x_7 = 0.2218
new: DataOwner8的最优x_8 = 0.2082
new: DataOwner9的最优x_9 = 0.2685
new: DataOwner10的最优x_10 = 0.1822
eta:2.6199999999999997
new: DataOwner1的最优x_1 = 0.2002
new: DataOwner2的最优x_2 = 0.1801
new: DataOwner3的最优x_3 = 0.2339
new: DataOwner4的最优x_4 = 0.2256
new: DataOwner5的最优x_5 = 0.2528
new: DataOwner6的最优x_6 = 0.3641
new: DataOwner7的最优x_7 = 0.2227
new: DataOwner8的最优x_8 = 0.2090
new: DataOwner9的最优x_9 = 0.2695
new: DataOwner10的最优x_10 = 0.1829
eta:2.63
new: DataOwner1的最优x_1 = 0.2010
new: DataOwner2的最优x_2 = 0.1807
new: DataOwner3的最优x_3 = 0.2348
new: DataOwner4的最优x_4 = 0.2265
new: DataOwner5的最优x_5 = 0.2537
new: DataOwner6的最优x_6 = 0.3655
new: DataOwner7的最优x_7 = 0.2235
new: DataOwner8的最优x_8 = 0.2098
new: DataOwner9的最优x_9 = 0.2705
new: DataOwner10的最优x_10 = 0.1836
eta:2.6399999999999997
new: DataOwner1的最优x_1 = 0.2017
new: DataOwner2的最优x_2 = 0.1814
new: DataOwner3的最优x_3 = 0.2357
new: DataOwner4的最优x_4 = 0.2273
new: DataOwner5的最优x_5 = 0.2547
new: DataOwner6的最优x_6 = 0.3669
new: DataOwner7的最优x_7 = 0.2244
new: DataOwner8的最优x_8 = 0.2106
new: DataOwner9的最优x_9 = 0.2716
new: DataOwner10的最优x_10 = 0.1843
eta:2.65
new: DataOwner1的最优x_1 = 0.2025
new: DataOwner2的最优x_2 = 0.1821
new: DataOwner3的最优x_3 = 0.2366
new: DataOwner4的最优x_4 = 0.2282
new: DataOwner5的最优x_5 = 0.2557
new: DataOwner6的最优x_6 = 0.3683
new: DataOwner7的最优x_7 = 0.2252
new: DataOwner8的最优x_8 = 0.2114
new: DataOwner9的最优x_9 = 0.2726
new: DataOwner10的最优x_10 = 0.1850
eta:2.6599999999999997
new: DataOwner1的最优x_1 = 0.2032
new: DataOwner2的最优x_2 = 0.1828
new: DataOwner3的最优x_3 = 0.2375
new: DataOwner4的最优x_4 = 0.2291
new: DataOwner5的最优x_5 = 0.2566
new: DataOwner6的最优x_6 = 0.3697
new: DataOwner7的最优x_7 = 0.2261
new: DataOwner8的最优x_8 = 0.2122
new: DataOwner9的最优x_9 = 0.2736
new: DataOwner10的最优x_10 = 0.1857
eta:2.67
new: DataOwner1的最优x_1 = 0.2040
new: DataOwner2的最优x_2 = 0.1835
new: DataOwner3的最优x_3 = 0.2384
new: DataOwner4的最优x_4 = 0.2299
new: DataOwner5的最优x_5 = 0.2576
new: DataOwner6的最优x_6 = 0.3711
new: DataOwner7的最优x_7 = 0.2269
new: DataOwner8的最优x_8 = 0.2130
new: DataOwner9的最优x_9 = 0.2747
new: DataOwner10的最优x_10 = 0.1864
eta:2.6799999999999997
new: DataOwner1的最优x_1 = 0.2048
new: DataOwner2的最优x_2 = 0.1842
new: DataOwner3的最优x_3 = 0.2393
new: DataOwner4的最优x_4 = 0.2308
new: DataOwner5的最优x_5 = 0.2586
new: DataOwner6的最优x_6 = 0.3725
new: DataOwner7的最优x_7 = 0.2278
new: DataOwner8的最优x_8 = 0.2138
new: DataOwner9的最优x_9 = 0.2757
new: DataOwner10的最优x_10 = 0.1871
eta:2.69
new: DataOwner1的最优x_1 = 0.2055
new: DataOwner2的最优x_2 = 0.1849
new: DataOwner3的最优x_3 = 0.2402
new: DataOwner4的最优x_4 = 0.2316
new: DataOwner5的最优x_5 = 0.2595
new: DataOwner6的最优x_6 = 0.3739
new: DataOwner7的最优x_7 = 0.2286
new: DataOwner8的最优x_8 = 0.2146
new: DataOwner9的最优x_9 = 0.2767
new: DataOwner10的最优x_10 = 0.1878
eta:2.6999999999999997
new: DataOwner1的最优x_1 = 0.2063
new: DataOwner2的最优x_2 = 0.1856
new: DataOwner3的最优x_3 = 0.2411
new: DataOwner4的最优x_4 = 0.2325
new: DataOwner5的最优x_5 = 0.2605
new: DataOwner6的最优x_6 = 0.3753
new: DataOwner7的最优x_7 = 0.2295
new: DataOwner8的最优x_8 = 0.2154
new: DataOwner9的最优x_9 = 0.2777
new: DataOwner10的最优x_10 = 0.1885
eta:2.71
new: DataOwner1的最优x_1 = 0.2071
new: DataOwner2的最优x_2 = 0.1862
new: DataOwner3的最优x_3 = 0.2419
new: DataOwner4的最优x_4 = 0.2334
new: DataOwner5的最优x_5 = 0.2614
new: DataOwner6的最优x_6 = 0.3767
new: DataOwner7的最优x_7 = 0.2303
new: DataOwner8的最优x_8 = 0.2162
new: DataOwner9的最优x_9 = 0.2788
new: DataOwner10的最优x_10 = 0.1892
eta:2.7199999999999998
new: DataOwner1的最优x_1 = 0.2078
new: DataOwner2的最优x_2 = 0.1869
new: DataOwner3的最优x_3 = 0.2428
new: DataOwner4的最优x_4 = 0.2342
new: DataOwner5的最优x_5 = 0.2624
new: DataOwner6的最优x_6 = 0.3780
new: DataOwner7的最优x_7 = 0.2312
new: DataOwner8的最优x_8 = 0.2170
new: DataOwner9的最优x_9 = 0.2798
new: DataOwner10的最优x_10 = 0.1899
eta:2.73
new: DataOwner1的最优x_1 = 0.2086
new: DataOwner2的最优x_2 = 0.1876
new: DataOwner3的最优x_3 = 0.2437
new: DataOwner4的最优x_4 = 0.2351
new: DataOwner5的最优x_5 = 0.2634
new: DataOwner6的最优x_6 = 0.3794
new: DataOwner7的最优x_7 = 0.2320
new: DataOwner8的最优x_8 = 0.2178
new: DataOwner9的最优x_9 = 0.2808
new: DataOwner10的最优x_10 = 0.1906
eta:2.7399999999999998
new: DataOwner1的最优x_1 = 0.2094
new: DataOwner2的最优x_2 = 0.1883
new: DataOwner3的最优x_3 = 0.2446
new: DataOwner4的最优x_4 = 0.2360
new: DataOwner5的最优x_5 = 0.2643
new: DataOwner6的最优x_6 = 0.3808
new: DataOwner7的最优x_7 = 0.2329
new: DataOwner8的最优x_8 = 0.2186
new: DataOwner9的最优x_9 = 0.2819
new: DataOwner10的最优x_10 = 0.1913
eta:2.75
new: DataOwner1的最优x_1 = 0.2101
new: DataOwner2的最优x_2 = 0.1890
new: DataOwner3的最优x_3 = 0.2455
new: DataOwner4的最优x_4 = 0.2368
new: DataOwner5的最优x_5 = 0.2653
new: DataOwner6的最优x_6 = 0.3822
new: DataOwner7的最优x_7 = 0.2337
new: DataOwner8的最优x_8 = 0.2194
new: DataOwner9的最优x_9 = 0.2829
new: DataOwner10的最优x_10 = 0.1920
eta:2.76
new: DataOwner1的最优x_1 = 0.2109
new: DataOwner2的最优x_2 = 0.1897
new: DataOwner3的最优x_3 = 0.2464
new: DataOwner4的最优x_4 = 0.2377
new: DataOwner5的最优x_5 = 0.2663
new: DataOwner6的最优x_6 = 0.3836
new: DataOwner7的最优x_7 = 0.2346
new: DataOwner8的最优x_8 = 0.2202
new: DataOwner9的最优x_9 = 0.2839
new: DataOwner10的最优x_10 = 0.1927
eta:2.77
new: DataOwner1的最优x_1 = 0.2117
new: DataOwner2的最优x_2 = 0.1904
new: DataOwner3的最优x_3 = 0.2473
new: DataOwner4的最优x_4 = 0.2385
new: DataOwner5的最优x_5 = 0.2672
new: DataOwner6的最优x_6 = 0.3850
new: DataOwner7的最优x_7 = 0.2354
new: DataOwner8的最优x_8 = 0.2210
new: DataOwner9的最优x_9 = 0.2849
new: DataOwner10的最优x_10 = 0.1934
eta:2.78
new: DataOwner1的最优x_1 = 0.2124
new: DataOwner2的最优x_2 = 0.1911
new: DataOwner3的最优x_3 = 0.2482
new: DataOwner4的最优x_4 = 0.2394
new: DataOwner5的最优x_5 = 0.2682
new: DataOwner6的最优x_6 = 0.3864
new: DataOwner7的最优x_7 = 0.2363
new: DataOwner8的最优x_8 = 0.2218
new: DataOwner9的最优x_9 = 0.2860
new: DataOwner10的最优x_10 = 0.1941
eta:2.79
new: DataOwner1的最优x_1 = 0.2132
new: DataOwner2的最优x_2 = 0.1917
new: DataOwner3的最优x_3 = 0.2491
new: DataOwner4的最优x_4 = 0.2403
new: DataOwner5的最优x_5 = 0.2692
new: DataOwner6的最优x_6 = 0.3878
new: DataOwner7的最优x_7 = 0.2371
new: DataOwner8的最优x_8 = 0.2226
new: DataOwner9的最优x_9 = 0.2870
new: DataOwner10的最优x_10 = 0.1948
eta:2.8
new: DataOwner1的最优x_1 = 0.2139
new: DataOwner2的最优x_2 = 0.1924
new: DataOwner3的最优x_3 = 0.2500
new: DataOwner4的最优x_4 = 0.2411
new: DataOwner5的最优x_5 = 0.2701
new: DataOwner6的最优x_6 = 0.3892
new: DataOwner7的最优x_7 = 0.2380
new: DataOwner8的最优x_8 = 0.2234
new: DataOwner9的最优x_9 = 0.2880
new: DataOwner10的最优x_10 = 0.1955
eta:2.81
new: DataOwner1的最优x_1 = 0.2147
new: DataOwner2的最优x_2 = 0.1931
new: DataOwner3的最优x_3 = 0.2509
new: DataOwner4的最优x_4 = 0.2420
new: DataOwner5的最优x_5 = 0.2711
new: DataOwner6的最优x_6 = 0.3905
new: DataOwner7的最优x_7 = 0.2388
new: DataOwner8的最优x_8 = 0.2242
new: DataOwner9的最优x_9 = 0.2891
new: DataOwner10的最优x_10 = 0.1962
eta:2.82
new: DataOwner1的最优x_1 = 0.2155
new: DataOwner2的最优x_2 = 0.1938
new: DataOwner3的最优x_3 = 0.2518
new: DataOwner4的最优x_4 = 0.2428
new: DataOwner5的最优x_5 = 0.2721
new: DataOwner6的最优x_6 = 0.3919
new: DataOwner7的最优x_7 = 0.2397
new: DataOwner8的最优x_8 = 0.2249
new: DataOwner9的最优x_9 = 0.2901
new: DataOwner10的最优x_10 = 0.1969
eta:2.8299999999999996
new: DataOwner1的最优x_1 = 0.2162
new: DataOwner2的最优x_2 = 0.1945
new: DataOwner3的最优x_3 = 0.2527
new: DataOwner4的最优x_4 = 0.2437
new: DataOwner5的最优x_5 = 0.2730
new: DataOwner6的最优x_6 = 0.3933
new: DataOwner7的最优x_7 = 0.2405
new: DataOwner8的最优x_8 = 0.2257
new: DataOwner9的最优x_9 = 0.2911
new: DataOwner10的最优x_10 = 0.1976
eta:2.84
new: DataOwner1的最优x_1 = 0.2170
new: DataOwner2的最优x_2 = 0.1952
new: DataOwner3的最优x_3 = 0.2536
new: DataOwner4的最优x_4 = 0.2446
new: DataOwner5的最优x_5 = 0.2740
new: DataOwner6的最优x_6 = 0.3947
new: DataOwner7的最优x_7 = 0.2414
new: DataOwner8的最优x_8 = 0.2265
new: DataOwner9的最优x_9 = 0.2921
new: DataOwner10的最优x_10 = 0.1983
eta:2.8499999999999996
new: DataOwner1的最优x_1 = 0.2178
new: DataOwner2的最优x_2 = 0.1959
new: DataOwner3的最优x_3 = 0.2544
new: DataOwner4的最优x_4 = 0.2454
new: DataOwner5的最优x_5 = 0.2750
new: DataOwner6的最优x_6 = 0.3961
new: DataOwner7的最优x_7 = 0.2422
new: DataOwner8的最优x_8 = 0.2273
new: DataOwner9的最优x_9 = 0.2932
new: DataOwner10的最优x_10 = 0.1990
eta:2.86
new: DataOwner1的最优x_1 = 0.2185
new: DataOwner2的最优x_2 = 0.1966
new: DataOwner3的最优x_3 = 0.2553
new: DataOwner4的最优x_4 = 0.2463
new: DataOwner5的最优x_5 = 0.2759
new: DataOwner6的最优x_6 = 0.3975
new: DataOwner7的最优x_7 = 0.2431
new: DataOwner8的最优x_8 = 0.2281
new: DataOwner9的最优x_9 = 0.2942
new: DataOwner10的最优x_10 = 0.1997
eta:2.8699999999999997
new: DataOwner1的最优x_1 = 0.2193
new: DataOwner2的最优x_2 = 0.1972
new: DataOwner3的最优x_3 = 0.2562
new: DataOwner4的最优x_4 = 0.2471
new: DataOwner5的最优x_5 = 0.2769
new: DataOwner6的最优x_6 = 0.3989
new: DataOwner7的最优x_7 = 0.2439
new: DataOwner8的最优x_8 = 0.2289
new: DataOwner9的最优x_9 = 0.2952
new: DataOwner10的最优x_10 = 0.2004
eta:2.88
new: DataOwner1的最优x_1 = 0.2201
new: DataOwner2的最优x_2 = 0.1979
new: DataOwner3的最优x_3 = 0.2571
new: DataOwner4的最优x_4 = 0.2480
new: DataOwner5的最优x_5 = 0.2778
new: DataOwner6的最优x_6 = 0.4003
new: DataOwner7的最优x_7 = 0.2448
new: DataOwner8的最优x_8 = 0.2297
new: DataOwner9的最优x_9 = 0.2963
new: DataOwner10的最优x_10 = 0.2011
eta:2.8899999999999997
new: DataOwner1的最优x_1 = 0.2208
new: DataOwner2的最优x_2 = 0.1986
new: DataOwner3的最优x_3 = 0.2580
new: DataOwner4的最优x_4 = 0.2489
new: DataOwner5的最优x_5 = 0.2788
new: DataOwner6的最优x_6 = 0.4017
new: DataOwner7的最优x_7 = 0.2456
new: DataOwner8的最优x_8 = 0.2305
new: DataOwner9的最优x_9 = 0.2973
new: DataOwner10的最优x_10 = 0.2018
eta:2.9
new: DataOwner1的最优x_1 = 0.2216
new: DataOwner2的最优x_2 = 0.1993
new: DataOwner3的最优x_3 = 0.2589
new: DataOwner4的最优x_4 = 0.2497
new: DataOwner5的最优x_5 = 0.2798
new: DataOwner6的最优x_6 = 0.4031
new: DataOwner7的最优x_7 = 0.2465
new: DataOwner8的最优x_8 = 0.2313
new: DataOwner9的最优x_9 = 0.2983
new: DataOwner10的最优x_10 = 0.2025
eta:2.9099999999999997
new: DataOwner1的最优x_1 = 0.2223
new: DataOwner2的最优x_2 = 0.2000
new: DataOwner3的最优x_3 = 0.2598
new: DataOwner4的最优x_4 = 0.2506
new: DataOwner5的最优x_5 = 0.2807
new: DataOwner6的最优x_6 = 0.4044
new: DataOwner7的最优x_7 = 0.2473
new: DataOwner8的最优x_8 = 0.2321
new: DataOwner9的最优x_9 = 0.2993
new: DataOwner10的最优x_10 = 0.2032
eta:2.92
new: DataOwner1的最优x_1 = 0.2231
new: DataOwner2的最优x_2 = 0.2007
new: DataOwner3的最优x_3 = 0.2607
new: DataOwner4的最优x_4 = 0.2515
new: DataOwner5的最优x_5 = 0.2817
new: DataOwner6的最优x_6 = 0.4058
new: DataOwner7的最优x_7 = 0.2482
new: DataOwner8的最优x_8 = 0.2329
new: DataOwner9的最优x_9 = 0.3004
new: DataOwner10的最优x_10 = 0.2039
eta:2.9299999999999997
new: DataOwner1的最优x_1 = 0.2239
new: DataOwner2的最优x_2 = 0.2014
new: DataOwner3的最优x_3 = 0.2616
new: DataOwner4的最优x_4 = 0.2523
new: DataOwner5的最优x_5 = 0.2827
new: DataOwner6的最优x_6 = 0.4072
new: DataOwner7的最优x_7 = 0.2490
new: DataOwner8的最优x_8 = 0.2337
new: DataOwner9的最优x_9 = 0.3014
new: DataOwner10的最优x_10 = 0.2046
eta:2.94
new: DataOwner1的最优x_1 = 0.2246
new: DataOwner2的最优x_2 = 0.2020
new: DataOwner3的最优x_3 = 0.2625
new: DataOwner4的最优x_4 = 0.2532
new: DataOwner5的最优x_5 = 0.2836
new: DataOwner6的最优x_6 = 0.4086
new: DataOwner7的最优x_7 = 0.2499
new: DataOwner8的最优x_8 = 0.2345
new: DataOwner9的最优x_9 = 0.3024
new: DataOwner10的最优x_10 = 0.2053
eta:2.9499999999999997
new: DataOwner1的最优x_1 = 0.2254
new: DataOwner2的最优x_2 = 0.2027
new: DataOwner3的最优x_3 = 0.2634
new: DataOwner4的最优x_4 = 0.2540
new: DataOwner5的最优x_5 = 0.2846
new: DataOwner6的最优x_6 = 0.4100
new: DataOwner7的最优x_7 = 0.2507
new: DataOwner8的最优x_8 = 0.2353
new: DataOwner9的最优x_9 = 0.3035
new: DataOwner10的最优x_10 = 0.2060
eta:2.96
new: DataOwner1的最优x_1 = 0.2262
new: DataOwner2的最优x_2 = 0.2034
new: DataOwner3的最优x_3 = 0.2643
new: DataOwner4的最优x_4 = 0.2549
new: DataOwner5的最优x_5 = 0.2856
new: DataOwner6的最优x_6 = 0.4114
new: DataOwner7的最优x_7 = 0.2516
new: DataOwner8的最优x_8 = 0.2361
new: DataOwner9的最优x_9 = 0.3045
new: DataOwner10的最优x_10 = 0.2067
eta:2.9699999999999998
new: DataOwner1的最优x_1 = 0.2269
new: DataOwner2的最优x_2 = 0.2041
new: DataOwner3的最优x_3 = 0.2652
new: DataOwner4的最优x_4 = 0.2558
new: DataOwner5的最优x_5 = 0.2865
new: DataOwner6的最优x_6 = 0.4128
new: DataOwner7的最优x_7 = 0.2524
new: DataOwner8的最优x_8 = 0.2369
new: DataOwner9的最优x_9 = 0.3055
new: DataOwner10的最优x_10 = 0.2074
eta:2.98
new: DataOwner1的最优x_1 = 0.2277
new: DataOwner2的最优x_2 = 0.2048
new: DataOwner3的最优x_3 = 0.2661
new: DataOwner4的最优x_4 = 0.2566
new: DataOwner5的最优x_5 = 0.2875
new: DataOwner6的最优x_6 = 0.4142
new: DataOwner7的最优x_7 = 0.2533
new: DataOwner8的最优x_8 = 0.2377
new: DataOwner9的最优x_9 = 0.3065
new: DataOwner10的最优x_10 = 0.2081
eta:2.9899999999999998
new: DataOwner1的最优x_1 = 0.2285
new: DataOwner2的最优x_2 = 0.2055
new: DataOwner3的最优x_3 = 0.2669
new: DataOwner4的最优x_4 = 0.2575
new: DataOwner5的最优x_5 = 0.2885
new: DataOwner6的最优x_6 = 0.4156
new: DataOwner7的最优x_7 = 0.2541
new: DataOwner8的最优x_8 = 0.2385
new: DataOwner9的最优x_9 = 0.3076
new: DataOwner10的最优x_10 = 0.2088
eta:3.0
new: DataOwner1的最优x_1 = 0.2292
new: DataOwner2的最优x_2 = 0.2062
new: DataOwner3的最优x_3 = 0.2678
new: DataOwner4的最优x_4 = 0.2583
new: DataOwner5的最优x_5 = 0.2894
new: DataOwner6的最优x_6 = 0.4170
new: DataOwner7的最优x_7 = 0.2550
new: DataOwner8的最优x_8 = 0.2393
new: DataOwner9的最优x_9 = 0.3086
new: DataOwner10的最优x_10 = 0.2095
DONE
----- literation 3: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.2997899392939436
DataOwner1的分配到的支付 ： 0.1143
DataOwner2的分配到的支付 ： 0.0852
DataOwner3的分配到的支付 ： 0.1375
DataOwner4的分配到的支付 ： 0.1312
DataOwner5的分配到的支付 ： 0.1345
DataOwner6的分配到的支付 ： 0.1841
DataOwner7的分配到的支付 ： 0.1294
DataOwner8的分配到的支付 ： 0.1199
DataOwner9的分配到的支付 ： 0.1285
DataOwner10的分配到的支付 ： 0.1351
DONE
----- literation 3: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC2
DataOwner3 把数据交给 CPC3
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC8
DataOwner9 把数据交给 CPC9
DataOwner10 把数据交给 CPC10
DONE
----- literation 3: 模型训练 -----
CPC1调整模型中, 本轮训练的数据量为：978.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 75.82%
Epoch 1/5, Loss: 0.7502
Epoch 2/5, Loss: 0.7413
Epoch 3/5, Loss: 0.7049
Epoch 4/5, Loss: 0.7021
Epoch 5/5, Loss: 0.6747
新模型评估：
Accuracy: 76.38%
Model saved to ../../../data/model/mnist_cnn_model
CPC2调整模型中, 本轮训练的数据量为：1366.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 76.38%
Epoch 1/5, Loss: 0.6711
Epoch 2/5, Loss: 0.6490
Epoch 3/5, Loss: 0.6287
Epoch 4/5, Loss: 0.5999
Epoch 5/5, Loss: 0.5815
新模型评估：
Accuracy: 77.17%
Model saved to ../../../data/model/mnist_cnn_model
CPC3调整模型中, 本轮训练的数据量为：660.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.17%
Epoch 1/5, Loss: 0.5289
Epoch 2/5, Loss: 0.5071
Epoch 3/5, Loss: 0.4943
Epoch 4/5, Loss: 0.4968
Epoch 5/5, Loss: 0.4973
新模型评估：
Accuracy: 78.80%
Model saved to ../../../data/model/mnist_cnn_model
CPC4调整模型中, 本轮训练的数据量为：316.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.80%
Epoch 1/5, Loss: 0.5694
Epoch 2/5, Loss: 0.5597
Epoch 3/5, Loss: 0.5491
Epoch 4/5, Loss: 0.5424
Epoch 5/5, Loss: 0.5372
新模型评估：
Accuracy: 76.74%
CPC5调整模型中, 本轮训练的数据量为：319.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.80%
Epoch 1/5, Loss: 0.5419
Epoch 2/5, Loss: 0.5280
Epoch 3/5, Loss: 0.5189
Epoch 4/5, Loss: 0.5120
Epoch 5/5, Loss: 0.5065
新模型评估：
Accuracy: 77.59%
CPC6调整模型中, 本轮训练的数据量为：1431.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.80%
Epoch 1/5, Loss: 0.5739
Epoch 2/5, Loss: 0.5557
Epoch 3/5, Loss: 0.5404
Epoch 4/5, Loss: 0.5217
Epoch 5/5, Loss: 0.5122
新模型评估：
Accuracy: 76.51%
CPC7调整模型中, 本轮训练的数据量为：312.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.80%
Epoch 1/5, Loss: 0.5659
Epoch 2/5, Loss: 0.5575
Epoch 3/5, Loss: 0.5527
Epoch 4/5, Loss: 0.5362
Epoch 5/5, Loss: 0.5365
新模型评估：
Accuracy: 76.96%
CPC8调整模型中, 本轮训练的数据量为：875.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.80%
Epoch 1/5, Loss: 0.5198
Epoch 2/5, Loss: 0.5028
Epoch 3/5, Loss: 0.4968
Epoch 4/5, Loss: 0.4824
Epoch 5/5, Loss: 0.4749
新模型评估：
Accuracy: 77.72%
CPC9调整模型中, 本轮训练的数据量为：454.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.80%
Epoch 1/5, Loss: 0.5322
Epoch 2/5, Loss: 0.5415
Epoch 3/5, Loss: 0.5637
Epoch 4/5, Loss: 0.5571
Epoch 5/5, Loss: 0.5641
新模型评估：
Accuracy: 76.28%
CPC10调整模型中, 本轮训练的数据量为：1165.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.80%
Epoch 1/5, Loss: 0.5409
Epoch 2/5, Loss: 0.5378
Epoch 3/5, Loss: 0.5167
Epoch 4/5, Loss: 0.5083
Epoch 5/5, Loss: 0.5011
新模型评估：
Accuracy: 77.22%
DONE
最终的列表：
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.15285673453012114, 0.1611690360336873, 0.1611690360336873, 0.1611690360336873, 0.1611690360336873, 0.1611690360336873, 0.1611690360336873, 0.1611690360336873, 0.21573146763191528, 0.21573146763191528, 0.2302098123104489, 0.2372722952806745, 0.24421911586688416, 0.2510518316129473, 0.25777196880940006, 0.26438102330883356, 0.27088046141856353, 0.27727172049219656, 0.28355620987446256, 0.2897353115269479, 0.295810380736725, 0.295810380736725, 0.3076537138499581, 0.3134245610842403, 0.3134245610842403, 0.3134245610842403, 0.3301488201150091, 0.3301488201150091, 0.3408201260223009, 0.3460158141262499, 0.3511196963862255, 0.35613287545654276, 0.3610564342345392, 0.3610564342345392, 0.3610564342345392, 0.3610564342345392, 0.3610564342345392, 0.3843665008270294, 0.38877403025715107, 0.3930990044634607, 0.3973423641456786, 0.3973423641456786, 0.4055879231287358, 0.40959192533381683, 0.41351791946630423, 0.4173667698614162, 0.4211393263042197, 0.4248364249955151, 0.42845888820827593, 0.43200752492426864, 0.4354831310221291, 0.4354831310221291, 0.442218371200111, 0.44547953413748587, 0.44867072469973246, 0.45179267743247176, 0.4548461153665311, 0.4578317502816396, 0.46075028290452824, 0.4636024031654795, 0.46638879040945125, 0.4691101136050986, 0.4717670315655649, 0.4743601931450787, 0.47689023744286785, 0.4793577940068625, 0.48176348297026816, 0.4841079153438992, 0.486391693091085, 0.48861540934906167, 0.49077964860734535, 0.4928849868574189, 0.49493199176667, 0.4969212228341201, 0.49885323155173233, 0.5007285615399467, 0.502547748726821, 0.5043113214633171, 0.506019800681, 0.5076737000408355, 0.5092735259846805, 0.5108197780460677, 0.5123129487952933, 0.5137535240576601, 0.5151419830226942, 0.516478798351405, 0.5177644363214511, 0.5189993569034468, 0.520184013908995, 0.5213188550800976, 0.5224043222036907, 0.5234408512155813, 0.5244288723046828, 0.5253688100081948, 0.5262610833205366, 0.5271061057787574, 0.5279042855633724, 0.528656025586596, 0.5293617235869295, 0.5300217722131502, 0.5306365591135662, 0.5312064670186711, 0.5317318738255825, 0.5322131526780516, 0.5326506720463844, 0.5330447958045457, 0.533395883307209, 0.5337042894635102, 0.533970364810362, 0.5341944555839624, 0.5343769037898578, 0.5345180472726745, 0.5346182197779281, 0.53467775103165, 0.5346969667802199, 0.5346761888860612, 0.5346157353629202, 0.5345159204482197, 0.5343770546614148, 0.5341994448608098, 0.5339833943035714, 0.5337292026991927, 0.5334371662659396, 0.5331075777836256, 0.5327407268661579, 0.5323368989300961, 0.5318963774019971, 0.5314194416211506, 0.5309063680157937, 0.5303574296303541, 0.5297728968008206, 0.5291530365343804, 0.5284981130507957, 0.5278083873998571, 0.527084117871282, 0.5263255598396348, 0.5255329659130079, 0.5247065858371143, 0.5238466666694293, 0.5229534527671136, 0.5220271858058365, 0.521068104856343, 0.520076446395755, 0.5190524443575446, 0.5179963301651496, 0.5169083327654505, 0.5157886786659502, 0.5146375919576969, 0.5134552944063095, 0.5122420053702561, 0.5109979419490549, 0.5097233189576322, 0.5084183489697016, 0.5070832423479312, 0.5057182072769069, 0.5043234497909963, 0.5028991737981212, 0.5014455811331806, 0.4999628715315112, 0.4984512427421799, 0.4969108904723216, 0.49534200846010545, 0.49374478847522085, 0.49211942041665147, 0.49046609217560655, 0.48878498990506136, 0.48707629777236616, 0.48534019819790863, 0.48357687177885667, 0.48178649733261647, 0.4799692519220462, 0.47812531087241084, 0.4762548478135902, 0.4743580346657099, 0.4724350416959784, 0.4704860375248412, 0.46851118914356094, 0.4665106619504231, 0.4644846197498307, 0.46243322479245097, 0.4603566377816817, 0.45825501789899525, 0.4561285228214409, 0.4539773087428378, 0.45180153038825344, 0.4496013410361739, 0.4473768925260746, 0.4451283353050499, 0.44285581840510524, 0.44055948948042456, 0.4382394948538595, 0.43589597948335745, 0.4335290869771318, 0.4311389596614008, 0.42872573855919915, 0.4262895634127264, 0.42383057270369706, 0.4213489036473015, 0.41884469225091436, 0.4163180732837799, 0.4137691803350023, 0.41119814572431324, 0.4086051007119136, 0.40599017531683623, 0.4033534984310134, 0.4006951978068227, 0.3980154000775822, 0.3953142307663784, 0.39259181430013346, 0.389848274022337, 0.387083732205487, 0.3842983100635493, 0.38149212776394803, 0.3786653044434227, 0.37581795820223407, 0.3729502061513217, 0.37006216438637063, 0.36715394802023305, 0.3642256711884224, 0.36127744706062925, 0.358309387852104, 0.35532160483199116, 0.35231420834151006, 0.34928730778438677, 0.34624101166913634, 0.34317542759218655, 0.34009066225748086, 0.3369868214871272, 0.3338640102293384, 0.3307223325688087, 0.3275618917359724, 0.32438279011757976, 0.3211851292654191, 0.31796900988694476, 0.31473453190909817, 0.3114817944174706, 0.3082108957133416, 0.304921933289525, 0.30161500387293927, 0.29829020342519863, 0.29494762711205924, 0.2915873693077713, 0.28820952381896525, 0.28481418345795717, 0.2814014404437062, 0.27797138625089746, 0.27452411162793355, 0.27105970661141265, 0.2675782605342074, 0.2640798620312692, 0.2605645990483567, 0.2570325588486164, 0.2534838280193332, 0.24991849248126385, 0.2463366374898177, 0.242738347647395, 0.23912370690841156, 0.23549279858516714, 0.2318457053548899, 0.22818250926629124, 0.2245032917454628, 0.2208081336025609, 0.21709711503791507, 0.2133703156480964, 0.2096278144328938, 0.20586968979819797, 0.2020960195642294, 0.19830688097471194, 0.194502350695394, 0.19068250482551807, 0.18684741890015788, 0.18299716789729636, 0.1791318262434567, 0.17525146781607326, 0.17135616595929548, 0.16744599346972633, 0.16352102262107948, 0.15958132515952705, 0.155626972309999, 0.15165803478318374, 0.1476745827789938, 0.14367668599085004, 0.13966441361215365, 0.13563783434013432, 0.13159701638070453]
**** log-parameter_analysis 运行时间： 2025-01-17 22:34:51 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.09393294726302918
DataOwner2: noise random: 0.006922930992007293
DataOwner3: noise random: 0.04180116398420369
DataOwner4: noise random: 0.03709630802766041
DataOwner5: noise random: 0.07037700683146399
DataOwner6: noise random: 0.04740013205791659
DataOwner7: noise random: 0.07178372572622836
DataOwner8: noise random: 0.03690370479838099
DataOwner9: noise random: 0.03455150614895173
DataOwner10: noise random: 0.02731898898927353
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9964072362859457, 0.9999806907703578, 0.9992933878935594, 0.999442432771095, 0.9979974633721433, 0.9990899287030421, 0.9979185594159252, 0.9994492673544436, 0.9995160019213243, 0.9996970559903172]
归一化后的数据质量列表avg_f_list: [0.9, 1.0, 0.9807664298007284, 0.9849373204105234, 0.9445011149053226, 0.9750728022085833, 0.9422930566646953, 0.9851285802510619, 0.9869960887689897, 0.9920627286207835]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3525
DataOwner1的最优x_1 = 0.0416
DataOwner2的最优x_2 = 0.1515
DataOwner3的最优x_3 = 0.1340
DataOwner4的最优x_4 = 0.1379
DataOwner5的最优x_5 = 0.0965
DataOwner6的最优x_6 = 0.1285
DataOwner7的最优x_7 = 0.0941
DataOwner8的最优x_8 = 0.1381
DataOwner9的最优x_9 = 0.1398
DataOwner10的最优x_10 = 0.1445
每个DataOwner应该贡献数据比例 xn_list = [0.04157022869840666, 0.15152926076661963, 0.13396476348371167, 0.13789977485519053, 0.09653787092599003, 0.12847598157743456, 0.09405997030914102, 0.1380785052493272, 0.13981585512874026, 0.14445871613330444]
ModelOwner的最大效用 U(Eta) = 0.5942
DONE
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3524595127368417
DataOwner1的分配到的支付 ： 0.0429
DataOwner2的分配到的支付 ： 0.1739
DataOwner3的分配到的支付 ： 0.1508
DataOwner4的分配到的支付 ： 0.1559
DataOwner5的分配到的支付 ： 0.1046
DataOwner6的分配到的支付 ： 0.1438
DataOwner7的分配到的支付 ： 0.1017
DataOwner8的分配到的支付 ： 0.1561
DataOwner9的分配到的支付 ： 0.1584
DataOwner10的分配到的支付 ： 0.1645
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC2', 'DataOwner5': 'CPC1', 'DataOwner2': 'CPC3', 'DataOwner3': 'CPC4', 'DataOwner4': 'CPC5', 'DataOwner6': 'CPC6', 'DataOwner7': 'CPC7', 'DataOwner8': 'CPC8', 'DataOwner9': 'CPC9', 'DataOwner10': 'CPC10'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC2
DataOwner5 把数据交给 CPC1
DataOwner2 把数据交给 CPC3
DataOwner3 把数据交给 CPC4
DataOwner4 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC8
DataOwner9 把数据交给 CPC9
DataOwner10 把数据交给 CPC10
DONE
----- literation 1: 模型训练 -----
CPC2调整模型中, 本轮训练的数据量为：67.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 26.82%
Epoch 1/5, Loss: 2.2614
Epoch 2/5, Loss: 2.2852
Epoch 3/5, Loss: 2.2642
Epoch 4/5, Loss: 2.2725
Epoch 5/5, Loss: 2.2734
新模型评估：
Accuracy: 27.53%
Model saved to ../../../data/model/mnist_cnn_model
CPC1调整模型中, 本轮训练的数据量为：782.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 27.53%
Epoch 1/5, Loss: 2.2932
Epoch 2/5, Loss: 2.2963
Epoch 3/5, Loss: 2.2892
Epoch 4/5, Loss: 2.2860
Epoch 5/5, Loss: 2.2785
新模型评估：
Accuracy: 40.84%
Model saved to ../../../data/model/mnist_cnn_model
CPC3调整模型中, 本轮训练的数据量为：245.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 40.84%
Epoch 1/5, Loss: 2.2811
Epoch 2/5, Loss: 2.2789
Epoch 3/5, Loss: 2.2762
Epoch 4/5, Loss: 2.2750
Epoch 5/5, Loss: 2.2729
新模型评估：
Accuracy: 44.09%
Model saved to ../../../data/model/mnist_cnn_model
CPC4调整模型中, 本轮训练的数据量为：1086.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 44.09%
Epoch 1/5, Loss: 2.2630
Epoch 2/5, Loss: 2.2522
Epoch 3/5, Loss: 2.2396
Epoch 4/5, Loss: 2.2241
Epoch 5/5, Loss: 2.2053
新模型评估：
Accuracy: 47.09%
Model saved to ../../../data/model/mnist_cnn_model
CPC5调整模型中, 本轮训练的数据量为：894.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 47.09%
Epoch 1/5, Loss: 2.1888
Epoch 2/5, Loss: 2.1725
Epoch 3/5, Loss: 2.1548
Epoch 4/5, Loss: 2.1361
Epoch 5/5, Loss: 2.1150
新模型评估：
Accuracy: 48.67%
Model saved to ../../../data/model/mnist_cnn_model
CPC6调整模型中, 本轮训练的数据量为：2291.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 48.67%
Epoch 1/5, Loss: 2.0806
Epoch 2/5, Loss: 2.0212
Epoch 3/5, Loss: 1.9485
Epoch 4/5, Loss: 1.8650
Epoch 5/5, Loss: 1.7702
新模型评估：
Accuracy: 56.54%
Model saved to ../../../data/model/mnist_cnn_model
CPC7调整模型中, 本轮训练的数据量为：152.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 56.54%
Epoch 1/5, Loss: 1.7158
Epoch 2/5, Loss: 1.6818
Epoch 3/5, Loss: 1.6921
Epoch 4/5, Loss: 1.6965
Epoch 5/5, Loss: 1.7030
新模型评估：
Accuracy: 57.26%
Model saved to ../../../data/model/mnist_cnn_model
CPC8调整模型中, 本轮训练的数据量为：1343.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 57.26%
Epoch 1/5, Loss: 1.6713
Epoch 2/5, Loss: 1.6190
Epoch 3/5, Loss: 1.5673
Epoch 4/5, Loss: 1.5149
Epoch 5/5, Loss: 1.4621
新模型评估：
Accuracy: 65.45%
Model saved to ../../../data/model/mnist_cnn_model
CPC9调整模型中, 本轮训练的数据量为：226.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 65.45%
Epoch 1/5, Loss: 1.5026
Epoch 2/5, Loss: 1.4877
Epoch 3/5, Loss: 1.4771
Epoch 4/5, Loss: 1.4633
Epoch 5/5, Loss: 1.4474
新模型评估：
Accuracy: 65.53%
Model saved to ../../../data/model/mnist_cnn_model
CPC10调整模型中, 本轮训练的数据量为：468.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 65.53%
Epoch 1/5, Loss: 1.3545
Epoch 2/5, Loss: 1.3446
Epoch 3/5, Loss: 1.3109
Epoch 4/5, Loss: 1.3016
Epoch 5/5, Loss: 1.2647
新模型评估：
Accuracy: 70.02%
Model saved to ../../../data/model/mnist_cnn_model
DONE
========================= literation: 2 =========================
----- literation 2: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3525
DataOwner1的最优x_1 = 0.0416
DataOwner2的最优x_2 = 0.1515
DataOwner3的最优x_3 = 0.1340
DataOwner4的最优x_4 = 0.1379
DataOwner5的最优x_5 = 0.0965
DataOwner6的最优x_6 = 0.1285
DataOwner7的最优x_7 = 0.0941
DataOwner8的最优x_8 = 0.1381
DataOwner9的最优x_9 = 0.1398
DataOwner10的最优x_10 = 0.1445
每个DataOwner应该贡献数据比例 xn_list = [0.04157022869840666, 0.15152926076661963, 0.13396476348371167, 0.13789977485519053, 0.09653787092599003, 0.12847598157743456, 0.09405997030914102, 0.1380785052493272, 0.13981585512874026, 0.14445871613330444]
ModelOwner的最大效用 U(Eta) = 0.5942
DONE
----- literation 2: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3524595127368417
DataOwner1的分配到的支付 ： 0.0429
DataOwner2的分配到的支付 ： 0.1739
DataOwner3的分配到的支付 ： 0.1508
DataOwner4的分配到的支付 ： 0.1559
DataOwner5的分配到的支付 ： 0.1046
DataOwner6的分配到的支付 ： 0.1438
DataOwner7的分配到的支付 ： 0.1017
DataOwner8的分配到的支付 ： 0.1561
DataOwner9的分配到的支付 ： 0.1584
DataOwner10的分配到的支付 ： 0.1645
DONE
----- literation 2: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC2
DataOwner5 把数据交给 CPC1
DataOwner2 把数据交给 CPC3
DataOwner3 把数据交给 CPC4
DataOwner4 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC8
DataOwner9 把数据交给 CPC9
DataOwner10 把数据交给 CPC10
DONE
----- literation 2: 模型训练 -----
重新调整fn，进而调整xn、Eta
正在评估DataOwner1的数据质量, 本轮评估的样本数据量为：67.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 70.02%
Epoch 1/5, Loss: 1.2291
Epoch 2/5, Loss: 1.3420
Epoch 3/5, Loss: 1.2084
Epoch 4/5, Loss: 1.2915
Epoch 5/5, Loss: 1.0498
新模型评估：
Accuracy: 70.22%
loss差为：
0.17935866117477417
单位数据loss差为：
0.0026769949429070773
正在评估DataOwner5的数据质量, 本轮评估的样本数据量为：782.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 70.02%
Epoch 1/5, Loss: 1.3440
Epoch 2/5, Loss: 1.2938
Epoch 3/5, Loss: 1.2627
Epoch 4/5, Loss: 1.2380
Epoch 5/5, Loss: 1.2130
新模型评估：
Accuracy: 69.72%
loss差为：
0.13103307210482096
单位数据loss差为：
0.0001675614732798222
正在评估DataOwner2的数据质量, 本轮评估的样本数据量为：245.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 72.01%
Epoch 1/5, Loss: 0.9629
Epoch 2/5, Loss: 0.9524
Epoch 3/5, Loss: 0.9439
Epoch 4/5, Loss: 0.9286
Epoch 5/5, Loss: 0.9230
新模型评估：
Accuracy: 75.05%
loss差为：
0.0398348867893219
单位数据loss差为：
0.00016259137465029347
正在评估DataOwner3的数据质量, 本轮评估的样本数据量为：1086.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 72.01%
Epoch 1/5, Loss: 0.9453
Epoch 2/5, Loss: 0.9103
Epoch 3/5, Loss: 0.8807
Epoch 4/5, Loss: 0.8536
Epoch 5/5, Loss: 0.8259
新模型评估：
Accuracy: 75.34%
loss差为：
0.11943992797066183
单位数据loss差为：
0.00010998151746838105
正在评估DataOwner4的数据质量, 本轮评估的样本数据量为：894.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 72.01%
Epoch 1/5, Loss: 0.9093
Epoch 2/5, Loss: 0.8799
Epoch 3/5, Loss: 0.8547
Epoch 4/5, Loss: 0.8310
Epoch 5/5, Loss: 0.8073
新模型评估：
Accuracy: 73.53%
loss差为：
0.10206811342920574
单位数据loss差为：
0.00011417014924967085
正在评估DataOwner6的数据质量, 本轮评估的样本数据量为：2291.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 26.82%
Epoch 1/5, Loss: 2.2916
Epoch 2/5, Loss: 2.2799
Epoch 3/5, Loss: 2.2585
Epoch 4/5, Loss: 2.2217
Epoch 5/5, Loss: 2.1622
新模型评估：
Accuracy: 53.52%
loss差为：
0.12938055064943077
单位数据loss差为：
5.647339618045865e-05
正在评估DataOwner7的数据质量, 本轮评估的样本数据量为：152.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
**** log-parameter_analysis 运行时间： 2025-01-17 22:42:03 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.03909528644185309
DataOwner2: noise random: 0.059804858563670384
DataOwner3: noise random: 0.08621001502060306
DataOwner4: noise random: 0.019022035563856024
DataOwner5: noise random: 0.08014997925842934
DataOwner6: noise random: 0.06159052367419313
DataOwner7: noise random: 0.004156411603494492
DataOwner8: noise random: 0.0777477156427723
DataOwner9: noise random: 0.04785129147860178
DataOwner10: noise random: 0.007695907074791209
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9993813432112229, 0.9985554273657783, 0.9969846961074036, 0.9998535226000185, 0.9974016162118857, 0.9984672014688297, 0.9999930167392177, 0.9975493377086392, 0.9990723208339377, 0.9999759790462467]
归一化后的数据质量列表avg_f_list: [0.9796672761032792, 0.952212893857245, 0.9, 0.9953630561275941, 0.9138588985520043, 0.9492801646788607, 1.0, 0.9187693291487715, 0.9693950207453526, 0.9994336477039438]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3383
DataOwner1的最优x_1 = 0.1426
DataOwner2的最优x_2 = 0.1161
DataOwner3的最优x_3 = 0.0557
DataOwner4的最优x_4 = 0.1564
DataOwner5的最优x_5 = 0.0731
DataOwner6的最优x_6 = 0.1131
DataOwner7的最优x_7 = 0.1603
DataOwner8的最优x_8 = 0.0791
DataOwner9的最优x_9 = 0.1331
DataOwner10的最优x_10 = 0.1599
每个DataOwner应该贡献数据比例 xn_list = [0.14264580520771064, 0.11610569012407498, 0.055703950198206856, 0.15643435480398474, 0.07314585511653897, 0.11307473263459368, 0.16032913747310895, 0.079067897825616, 0.13309131806658234, 0.15985766924909664]
ModelOwner的最大效用 U(Eta) = 0.5778
DONE
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3383375142427196
DataOwner1的分配到的支付 ： 0.1623
DataOwner2的分配到的支付 ： 0.1284
DataOwner3的分配到的支付 ： 0.0582
DataOwner4的分配到的支付 ： 0.1809
DataOwner5的分配到的支付 ： 0.0777
DataOwner6的分配到的支付 ： 0.1247
DataOwner7的分配到的支付 ： 0.1862
DataOwner8的分配到的支付 ： 0.0844
DataOwner9的分配到的支付 ： 0.1499
DataOwner10的分配到的支付 ： 0.1856
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC1', 'DataOwner2': 'CPC2', 'DataOwner3': 'CPC3', 'DataOwner4': 'CPC4', 'DataOwner5': 'CPC5', 'DataOwner6': 'CPC9', 'DataOwner8': 'CPC6', 'DataOwner7': 'CPC10', 'DataOwner9': 'CPC7', 'DataOwner10': 'CPC8'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC2
DataOwner3 把数据交给 CPC3
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC9
DataOwner8 把数据交给 CPC6
DataOwner7 把数据交给 CPC10
DataOwner9 把数据交给 CPC7
DataOwner10 把数据交给 CPC8
DONE
----- literation 1: 模型训练 -----
CPC1调整模型中, 本轮训练的数据量为：1352.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 26.82%
Epoch 1/5, Loss: 2.2906
Epoch 2/5, Loss: 2.2877
Epoch 3/5, Loss: 2.2785
Epoch 4/5, Loss: 2.2643
Epoch 5/5, Loss: 2.2528
新模型评估：
Accuracy: 39.15%
Model saved to ../../../data/model/mnist_cnn_model
CPC2调整模型中, 本轮训练的数据量为：366.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 39.15%
Epoch 1/5, Loss: 2.2325
Epoch 2/5, Loss: 2.2268
Epoch 3/5, Loss: 2.2209
Epoch 4/5, Loss: 2.2144
Epoch 5/5, Loss: 2.2071
新模型评估：
Accuracy: 43.75%
Model saved to ../../../data/model/mnist_cnn_model
CPC3调整模型中, 本轮训练的数据量为：263.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
**** log-parameter_analysis 运行时间： 2025-01-17 22:43:05 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.01141658216311292
DataOwner2: noise random: 0.05565190315100332
DataOwner3: noise random: 0.010916139971950733
DataOwner4: noise random: 0.0769201904117881
DataOwner5: noise random: 0.08815028355753296
DataOwner6: noise random: 0.0624994012975338
DataOwner7: noise random: 0.05529858492801413
DataOwner8: noise random: 0.005483372368984474
DataOwner9: noise random: 0.03809842906985659
DataOwner10: noise random: 0.06667113618827175
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9999471873173084, 0.9987432504538906, 0.9999517448753736, 0.9976119634542615, 0.9968531824745663, 0.9984188375826696, 0.9987643065341315, 0.9999878154433819, 0.9994128779125186, 0.9982000609764101]
归一化后的数据质量列表avg_f_list: [0.9987038952732996, 0.9602963089499568, 0.9988492889481131, 0.92420637399159, 0.9, 0.9499469993354547, 0.9609680328950061, 1.0, 0.9816585374880273, 0.9429676620913202]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3434
DataOwner1的最优x_1 = 0.1562
DataOwner2的最优x_2 = 0.1205
DataOwner3的最优x_3 = 0.1563
DataOwner4的最优x_4 = 0.0810
DataOwner5的最优x_5 = 0.0508
DataOwner6的最优x_6 = 0.1098
DataOwner7的最优x_7 = 0.1212
DataOwner8的最优x_8 = 0.1573
DataOwner9的最优x_9 = 0.1411
DataOwner10的最优x_10 = 0.1024
每个DataOwner应该贡献数据比例 xn_list = [0.1561627231351038, 0.12052825671408214, 0.15628652932141768, 0.08104903908944151, 0.05075644114984305, 0.10984783103649408, 0.12120469649055904, 0.15726361617988255, 0.1410878961132977, 0.10236310190127434]
ModelOwner的最大效用 U(Eta) = 0.5836
DONE
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3433998984317428
DataOwner1的分配到的支付 ： 0.1804
DataOwner2的分配到的支付 ： 0.1339
DataOwner3的分配到的支付 ： 0.1806
DataOwner4的分配到的支付 ： 0.0866
DataOwner5的分配到的支付 ： 0.0528
DataOwner6的分配到的支付 ： 0.1207
DataOwner7的分配到的支付 ： 0.1347
DataOwner8的分配到的支付 ： 0.1819
DataOwner9的分配到的支付 ： 0.1602
DataOwner10的分配到的支付 ： 0.1116
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC2', 'DataOwner3': 'CPC1', 'DataOwner2': 'CPC8', 'DataOwner5': 'CPC3', 'DataOwner4': 'CPC9', 'DataOwner6': 'CPC4', 'DataOwner7': 'CPC5', 'DataOwner9': 'CPC6', 'DataOwner10': 'CPC7', 'DataOwner8': 'CPC10'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC2
DataOwner3 把数据交给 CPC1
DataOwner2 把数据交给 CPC8
DataOwner5 把数据交给 CPC3
DataOwner4 把数据交给 CPC9
DataOwner6 把数据交给 CPC4
DataOwner7 把数据交给 CPC5
DataOwner9 把数据交给 CPC6
DataOwner10 把数据交给 CPC7
DataOwner8 把数据交给 CPC10
DONE
----- literation 1: 模型训练 -----
CPC2调整模型中, 本轮训练的数据量为：670.00 :
**** log-parameter_analysis 运行时间： 2025-01-17 22:43:19 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.08065760343487995
DataOwner2: noise random: 0.09687856469168825
DataOwner3: noise random: 0.061060884961292465
DataOwner4: noise random: 0.08522541577276416
DataOwner5: noise random: 0.05161926395959814
DataOwner6: noise random: 0.041944524868440115
DataOwner7: noise random: 0.024401955417553214
DataOwner8: noise random: 0.0266746620789178
DataOwner9: noise random: 0.0235813870487416
DataOwner10: noise random: 0.0428312153758623
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9973624715747037, 0.996209191841275, 0.9984899384588928, 0.9970648000066891, 0.9989237292684428, 0.999289443626023, 0.9997593075702995, 0.9997119774705103, 0.9997754948607911, 0.9992575432502593]
归一化后的数据质量列表avg_f_list: [0.932338242911989, 0.9, 0.9639526872825085, 0.9239914600843473, 0.9761162865946283, 0.9863710057135293, 0.9995461044559872, 0.9982189570001996, 1.0, 0.9854765114546524]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3490
DataOwner1的最优x_1 = 0.0856
DataOwner2的最优x_2 = 0.0451
DataOwner3的最优x_3 = 0.1200
DataOwner4的最优x_4 = 0.0757
DataOwner5的最优x_5 = 0.1320
DataOwner6的最优x_6 = 0.1416
DataOwner7的最优x_7 = 0.1533
DataOwner8的最优x_8 = 0.1522
DataOwner9的最优x_9 = 0.1537
DataOwner10的最优x_10 = 0.1408
每个DataOwner应该贡献数据比例 xn_list = [0.08561678691061485, 0.04508518847886938, 0.1199716184755712, 0.07571152539799639, 0.13196348483884654, 0.14158790352619505, 0.15333655356129425, 0.15218344053767285, 0.15372940034487728, 0.14076548149004456]
ModelOwner的最大效用 U(Eta) = 0.5902
DONE
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3490396993980278
DataOwner1的分配到的支付 ： 0.0919
DataOwner2的分配到的支付 ： 0.0467
DataOwner3的分配到的支付 ： 0.1331
DataOwner4的分配到的支付 ： 0.0805
DataOwner5的分配到的支付 ： 0.1483
DataOwner6的分配到的支付 ： 0.1607
DataOwner7的分配到的支付 ： 0.1764
DataOwner8的分配到的支付 ： 0.1748
DataOwner9的分配到的支付 ： 0.1769
DataOwner10的分配到的支付 ： 0.1597
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC1', 'DataOwner2': 'CPC2', 'DataOwner3': 'CPC3', 'DataOwner4': 'CPC4', 'DataOwner5': 'CPC5', 'DataOwner6': 'CPC6', 'DataOwner7': 'CPC7', 'DataOwner8': 'CPC8', 'DataOwner9': 'CPC9', 'DataOwner10': 'CPC10'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC2
DataOwner3 把数据交给 CPC3
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC8
DataOwner9 把数据交给 CPC9
DataOwner10 把数据交给 CPC10
DONE
----- literation 1: 模型训练 -----
CPC1调整模型中, 本轮训练的数据量为：571.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 26.82%
Epoch 1/5, Loss: 2.2988
Epoch 2/5, Loss: 2.2967
Epoch 3/5, Loss: 2.2945
Epoch 4/5, Loss: 2.2921
Epoch 5/5, Loss: 2.2893
新模型评估：
Accuracy: 35.42%
Model saved to ../../../data/model/mnist_cnn_model
CPC2调整模型中, 本轮训练的数据量为：100.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 35.42%
Epoch 1/5, Loss: 2.2936
Epoch 2/5, Loss: 2.2983
Epoch 3/5, Loss: 2.2968
Epoch 4/5, Loss: 2.2921
Epoch 5/5, Loss: 2.2938
新模型评估：
Accuracy: 36.47%
Model saved to ../../../data/model/mnist_cnn_model
CPC3调整模型中, 本轮训练的数据量为：533.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 36.47%
Epoch 1/5, Loss: 2.2868
Epoch 2/5, Loss: 2.2824
Epoch 3/5, Loss: 2.2788
Epoch 4/5, Loss: 2.2720
Epoch 5/5, Loss: 2.2677
新模型评估：
Accuracy: 43.41%
Model saved to ../../../data/model/mnist_cnn_model
CPC4调整模型中, 本轮训练的数据量为：504.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 43.41%
Epoch 1/5, Loss: 2.2720
Epoch 2/5, Loss: 2.2672
Epoch 3/5, Loss: 2.2624
Epoch 4/5, Loss: 2.2569
Epoch 5/5, Loss: 2.2512
新模型评估：
Accuracy: 40.88%
CPC5调整模型中, 本轮训练的数据量为：293.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 71.31%
Epoch 1/5, Loss: 1.1043
Epoch 2/5, Loss: 1.0923
Epoch 3/5, Loss: 1.0586
Epoch 4/5, Loss: 1.0741
Epoch 5/5, Loss: 1.0501
新模型评估：
Accuracy: 73.88%
Model saved to ../../../data/model/mnist_cnn_model
CPC6调整模型中, 本轮训练的数据量为：943.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 73.88%
Epoch 1/5, Loss: 1.0132
Epoch 2/5, Loss: 0.9765
Epoch 3/5, Loss: 0.9464
Epoch 4/5, Loss: 0.9219
Epoch 5/5, Loss: 0.8979
新模型评估：
Accuracy: 75.44%
Model saved to ../../../data/model/mnist_cnn_model
CPC7调整模型中, 本轮训练的数据量为：3066.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 75.44%
Epoch 1/5, Loss: 0.8453
Epoch 2/5, Loss: 0.7749
Epoch 3/5, Loss: 0.7161
Epoch 4/5, Loss: 0.6662
Epoch 5/5, Loss: 0.6223
新模型评估：
Accuracy: 77.65%
Model saved to ../../../data/model/mnist_cnn_model
CPC8调整模型中, 本轮训练的数据量为：1014.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.65%
Epoch 1/5, Loss: 0.5992
Epoch 2/5, Loss: 0.5852
Epoch 3/5, Loss: 0.5708
Epoch 4/5, Loss: 0.5583
Epoch 5/5, Loss: 0.5451
新模型评估：
Accuracy: 75.79%
CPC9调整模型中, 本轮训练的数据量为：341.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.65%
Epoch 1/5, Loss: 0.6515
Epoch 2/5, Loss: 0.6490
Epoch 3/5, Loss: 0.6213
Epoch 4/5, Loss: 0.6672
Epoch 5/5, Loss: 0.6370
新模型评估：
Accuracy: 75.86%
CPC10调整模型中, 本轮训练的数据量为：312.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.65%
Epoch 1/5, Loss: 0.6340
Epoch 2/5, Loss: 0.6217
Epoch 3/5, Loss: 0.6196
Epoch 4/5, Loss: 0.6092
Epoch 5/5, Loss: 0.6048
新模型评估：
Accuracy: 77.10%
DONE
========================= literation: 2 =========================
----- literation 2: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3490
DataOwner1的最优x_1 = 0.0856
DataOwner2的最优x_2 = 0.0451
DataOwner3的最优x_3 = 0.1200
DataOwner4的最优x_4 = 0.0757
DataOwner5的最优x_5 = 0.1320
DataOwner6的最优x_6 = 0.1416
DataOwner7的最优x_7 = 0.1533
DataOwner8的最优x_8 = 0.1522
DataOwner9的最优x_9 = 0.1537
DataOwner10的最优x_10 = 0.1408
每个DataOwner应该贡献数据比例 xn_list = [0.08561678691061485, 0.04508518847886938, 0.1199716184755712, 0.07571152539799639, 0.13196348483884654, 0.14158790352619505, 0.15333655356129425, 0.15218344053767285, 0.15372940034487728, 0.14076548149004456]
ModelOwner的最大效用 U(Eta) = 0.5902
DONE
----- literation 2: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3490396993980278
DataOwner1的分配到的支付 ： 0.0919
DataOwner2的分配到的支付 ： 0.0467
DataOwner3的分配到的支付 ： 0.1331
DataOwner4的分配到的支付 ： 0.0805
DataOwner5的分配到的支付 ： 0.1483
DataOwner6的分配到的支付 ： 0.1607
DataOwner7的分配到的支付 ： 0.1764
DataOwner8的分配到的支付 ： 0.1748
DataOwner9的分配到的支付 ： 0.1769
DataOwner10的分配到的支付 ： 0.1597
DONE
----- literation 2: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC2
DataOwner3 把数据交给 CPC3
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC8
DataOwner9 把数据交给 CPC9
DataOwner10 把数据交给 CPC10
DONE
----- literation 2: 模型训练 -----
重新调整fn，进而调整xn、Eta
正在评估DataOwner1的数据质量, 本轮评估的样本数据量为：571.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.65%
Epoch 1/5, Loss: 0.6091
Epoch 2/5, Loss: 0.5949
Epoch 3/5, Loss: 0.5861
Epoch 4/5, Loss: 0.5762
Epoch 5/5, Loss: 0.5664
新模型评估：
Accuracy: 77.04%
loss差为：
0.042641447650061726
单位数据loss差为：
7.467854229432877e-05
正在评估DataOwner2的数据质量, 本轮评估的样本数据量为：100.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.65%
Epoch 1/5, Loss: 0.6414
Epoch 2/5, Loss: 0.5945
Epoch 3/5, Loss: 0.6208
Epoch 4/5, Loss: 0.6126
Epoch 5/5, Loss: 0.5808
新模型评估：
Accuracy: 76.48%
loss差为：
0.06051735579967499
单位数据loss差为：
0.0006051735579967499
正在评估DataOwner3的数据质量, 本轮评估的样本数据量为：533.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.65%
Epoch 1/5, Loss: 0.6102
Epoch 2/5, Loss: 0.5829
Epoch 3/5, Loss: 0.5893
Epoch 4/5, Loss: 0.5679
Epoch 5/5, Loss: 0.5571
新模型评估：
Accuracy: 75.96%
loss差为：
0.05316678351826143
单位数据loss差为：
9.975006288604396e-05
正在评估DataOwner4的数据质量, 本轮评估的样本数据量为：504.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.65%
Epoch 1/5, Loss: 0.6516
Epoch 2/5, Loss: 0.6384
Epoch 3/5, Loss: 0.6307
Epoch 4/5, Loss: 0.6231
Epoch 5/5, Loss: 0.6141
新模型评估：
Accuracy: 75.68%
loss差为：
0.037495166063308716
单位数据loss差为：
7.439517076053316e-05
正在评估DataOwner5的数据质量, 本轮评估的样本数据量为：293.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.65%
Epoch 1/5, Loss: 0.6927
Epoch 2/5, Loss: 0.6792
Epoch 3/5, Loss: 0.6532
Epoch 4/5, Loss: 0.6647
Epoch 5/5, Loss: 0.6449
新模型评估：
Accuracy: 77.98%
loss差为：
0.04776606559753416
单位数据loss差为：
0.00016302411466735206
正在评估DataOwner6的数据质量, 本轮评估的样本数据量为：943.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.65%
Epoch 1/5, Loss: 0.6389
Epoch 2/5, Loss: 0.6189
Epoch 3/5, Loss: 0.6026
Epoch 4/5, Loss: 0.5897
Epoch 5/5, Loss: 0.5782
新模型评估：
Accuracy: 75.94%
loss差为：
0.06075703303019209
单位数据loss差为：
6.442951540847517e-05
正在评估DataOwner7的数据质量, 本轮评估的样本数据量为：3066.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.65%
Epoch 1/5, Loss: 0.6144
Epoch 2/5, Loss: 0.5796
Epoch 3/5, Loss: 0.5508
Epoch 4/5, Loss: 0.5267
Epoch 5/5, Loss: 0.5048
新模型评估：
Accuracy: 76.49%
loss差为：
0.10962621929744887
单位数据loss差为：
3.575545313028339e-05
正在评估DataOwner8的数据质量, 本轮评估的样本数据量为：1014.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.65%
Epoch 1/5, Loss: 0.5918
Epoch 2/5, Loss: 0.5750
Epoch 3/5, Loss: 0.5607
Epoch 4/5, Loss: 0.5464
Epoch 5/5, Loss: 0.5359
新模型评估：
Accuracy: 76.26%
loss差为：
0.0559641532599926
单位数据loss差为：
5.519147264299073e-05
正在评估DataOwner9的数据质量, 本轮评估的样本数据量为：341.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.65%
Epoch 1/5, Loss: 0.7079
Epoch 2/5, Loss: 0.6735
Epoch 3/5, Loss: 0.6808
Epoch 4/5, Loss: 0.6865
Epoch 5/5, Loss: 0.7045
新模型评估：
Accuracy: 77.41%
loss差为：
0.003337611754735348
单位数据loss差为：
9.787717755822135e-06
正在评估DataOwner10的数据质量, 本轮评估的样本数据量为：312.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.65%
Epoch 1/5, Loss: 0.6767
Epoch 2/5, Loss: 0.6686
Epoch 3/5, Loss: 0.6573
Epoch 4/5, Loss: 0.6494
Epoch 5/5, Loss: 0.6459
新模型评估：
Accuracy: 76.90%
loss差为：
0.03088845014572139
单位数据loss差为：
9.900144277474804e-05
经过服务器调节后的真实数据质量：
数据质量列表avg_f_list: [7.467854229432877e-05, 0.0006051735579967499, 9.975006288604396e-05, 7.439517076053316e-05, 0.00016302411466735206, 6.442951540847517e-05, 3.575545313028339e-05, 5.519147264299073e-05, 9.787717755822135e-06, 9.900144277474804e-05]
归一化后的数据质量列表avg_f_list:[0.9108989532757864, 1.0, 0.9151099235235117, 0.9108513586716418, 0.9257373263780546, 0.9091775440327137, 0.9043614969687478, 0.9076259379747419, 0.9, 0.914984186554189]
CPC1调整模型中, 本轮训练的数据量为：571.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.72%
Epoch 1/5, Loss: 0.4205
Epoch 2/5, Loss: 0.4088
Epoch 3/5, Loss: 0.4020
Epoch 4/5, Loss: 0.3969
Epoch 5/5, Loss: 0.3929
新模型评估：
Accuracy: 77.34%
CPC2调整模型中, 本轮训练的数据量为：100.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.72%
Epoch 1/5, Loss: 0.4171
Epoch 2/5, Loss: 0.3845
Epoch 3/5, Loss: 0.3751
Epoch 4/5, Loss: 0.3817
Epoch 5/5, Loss: 0.3778
新模型评估：
Accuracy: 77.24%
CPC3调整模型中, 本轮训练的数据量为：533.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.72%
Epoch 1/5, Loss: 0.3992
Epoch 2/5, Loss: 0.4002
Epoch 3/5, Loss: 0.3981
Epoch 4/5, Loss: 0.4235
Epoch 5/5, Loss: 0.3808
新模型评估：
Accuracy: 76.96%
CPC4调整模型中, 本轮训练的数据量为：504.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.72%
Epoch 1/5, Loss: 0.4824
Epoch 2/5, Loss: 0.4699
Epoch 3/5, Loss: 0.4676
Epoch 4/5, Loss: 0.4617
Epoch 5/5, Loss: 0.4572
新模型评估：
Accuracy: 76.19%
CPC5调整模型中, 本轮训练的数据量为：293.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.72%
Epoch 1/5, Loss: 0.5114
Epoch 2/5, Loss: 0.4997
Epoch 3/5, Loss: 0.4743
Epoch 4/5, Loss: 0.4824
Epoch 5/5, Loss: 0.4710
新模型评估：
Accuracy: 78.36%
Model saved to ../../../data/model/mnist_cnn_model
CPC6调整模型中, 本轮训练的数据量为：943.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.36%
Epoch 1/5, Loss: 0.4444
Epoch 2/5, Loss: 0.4287
Epoch 3/5, Loss: 0.4219
Epoch 4/5, Loss: 0.4196
Epoch 5/5, Loss: 0.4108
新模型评估：
Accuracy: 76.60%
CPC7调整模型中, 本轮训练的数据量为：3066.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.36%
Epoch 1/5, Loss: 0.4449
Epoch 2/5, Loss: 0.4320
Epoch 3/5, Loss: 0.4215
Epoch 4/5, Loss: 0.4115
Epoch 5/5, Loss: 0.4035
新模型评估：
Accuracy: 76.53%
CPC8调整模型中, 本轮训练的数据量为：1014.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.77%
Epoch 1/5, Loss: 0.4106
Epoch 2/5, Loss: 0.3992
Epoch 3/5, Loss: 0.3939
Epoch 4/5, Loss: 0.3869
Epoch 5/5, Loss: 0.3829
新模型评估：
Accuracy: 76.83%
CPC9调整模型中, 本轮训练的数据量为：341.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.77%
Epoch 1/5, Loss: 0.5724
Epoch 2/5, Loss: 0.4827
Epoch 3/5, Loss: 0.5135
Epoch 4/5, Loss: 0.4921
Epoch 5/5, Loss: 0.4852
新模型评估：
Accuracy: 78.08%
CPC10调整模型中, 本轮训练的数据量为：312.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.77%
Epoch 1/5, Loss: 0.4977
Epoch 2/5, Loss: 0.4865
Epoch 3/5, Loss: 0.4768
Epoch 4/5, Loss: 0.4702
Epoch 5/5, Loss: 0.4682
新模型评估：
Accuracy: 78.32%
DONE
========================= literation: 3 =========================
----- literation 3: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.2911
DataOwner1的最优x_1 = 0.1077
DataOwner2的最优x_2 = 0.1846
DataOwner3的最优x_3 = 0.1121
DataOwner4的最优x_4 = 0.1077
DataOwner5的最优x_5 = 0.1228
DataOwner6的最优x_6 = 0.1059
DataOwner7的最优x_7 = 0.1008
DataOwner8的最优x_8 = 0.1043
DataOwner9的最优x_9 = 0.0960
DataOwner10的最优x_10 = 0.1120
每个DataOwner应该贡献数据比例 xn_list = [0.10773713368985087, 0.1845524759813923, 0.1121182399576983, 0.10768712569034947, 0.12280278037600495, 0.10592139500671763, 0.10076365209395839, 0.10427228628819651, 0.09599198246262075, 0.11198865716275275]
ModelOwner的最大效用 U(Eta) = 0.5253
xn开始变化：
DONE
----- literation 3: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.2910956996974834
DataOwner1的分配到的支付 ： 0.5634
DataOwner2的分配到的支付 ： 0.1131
DataOwner3的分配到的支付 ： 0.0673
DataOwner4的分配到的支付 ： 0.0601
DataOwner5的分配到的支付 ： 0.0749
DataOwner6的分配到的支付 ： 0.0789
DataOwner7的分配到的支付 ： 0.0850
DataOwner8的分配到的支付 ： 0.0847
DataOwner9的分配到的支付 ： 0.0848
DataOwner10的分配到的支付 ： 0.0789
DONE
----- literation 3: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC2
DataOwner3 把数据交给 CPC3
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC8
DataOwner9 把数据交给 CPC9
DataOwner10 把数据交给 CPC10
DONE
----- literation 3: 模型训练 -----
CPC1调整模型中, 本轮训练的数据量为：6670.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.77%
Epoch 1/5, Loss: 0.4326
Epoch 2/5, Loss: 0.4098
Epoch 3/5, Loss: 0.3925
Epoch 4/5, Loss: 0.3814
Epoch 5/5, Loss: 0.3664
新模型评估：
Accuracy: 78.96%
Model saved to ../../../data/model/mnist_cnn_model
CPC2调整模型中, 本轮训练的数据量为：410.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.96%
Epoch 1/5, Loss: 0.3337
Epoch 2/5, Loss: 0.3212
Epoch 3/5, Loss: 0.3185
Epoch 4/5, Loss: 0.3133
Epoch 5/5, Loss: 0.3057
新模型评估：
Accuracy: 77.65%
CPC3调整模型中, 本轮训练的数据量为：533.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.96%
Epoch 1/5, Loss: 0.3079
Epoch 2/5, Loss: 0.3039
Epoch 3/5, Loss: 0.2903
Epoch 4/5, Loss: 0.3159
Epoch 5/5, Loss: 0.3199
新模型评估：
Accuracy: 78.05%
CPC4调整模型中, 本轮训练的数据量为：717.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 45.28%
Epoch 1/5, Loss: 2.2291
Epoch 2/5, Loss: 2.2164
Epoch 3/5, Loss: 2.2094
Epoch 4/5, Loss: 2.1958
Epoch 5/5, Loss: 2.1740
新模型评估：
Accuracy: 46.77%
Model saved to ../../../data/model/mnist_cnn_model
CPC5调整模型中, 本轮训练的数据量为：293.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 46.77%
Epoch 1/5, Loss: 2.1364
Epoch 2/5, Loss: 2.1362
Epoch 3/5, Loss: 2.1250
Epoch 4/5, Loss: 2.1201
Epoch 5/5, Loss: 2.1048
新模型评估：
Accuracy: 47.94%
Model saved to ../../../data/model/mnist_cnn_model
CPC6调整模型中, 本轮训练的数据量为：943.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 47.94%
Epoch 1/5, Loss: 2.1282
Epoch 2/5, Loss: 2.1087
Epoch 3/5, Loss: 2.0853
Epoch 4/5, Loss: 2.0610
Epoch 5/5, Loss: 2.0363
新模型评估：
Accuracy: 48.07%
Model saved to ../../../data/model/mnist_cnn_model
CPC7调整模型中, 本轮训练的数据量为：3066.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 48.07%
Epoch 1/5, Loss: 1.9835
Epoch 2/5, Loss: 1.8854
Epoch 3/5, Loss: 1.7720
Epoch 4/5, Loss: 1.6462
Epoch 5/5, Loss: 1.5115
新模型评估：
Accuracy: 65.62%
Model saved to ../../../data/model/mnist_cnn_model
CPC8调整模型中, 本轮训练的数据量为：1014.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 65.62%
Epoch 1/5, Loss: 1.4196
Epoch 2/5, Loss: 1.3767
Epoch 3/5, Loss: 1.3387
Epoch 4/5, Loss: 1.2985
Epoch 5/5, Loss: 1.2602
新模型评估：
Accuracy: 68.72%
Model saved to ../../../data/model/mnist_cnn_model
CPC9调整模型中, 本轮训练的数据量为：341.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 68.72%
Epoch 1/5, Loss: 1.2938
Epoch 2/5, Loss: 1.2676
Epoch 3/5, Loss: 1.2744
Epoch 4/5, Loss: 1.2395
Epoch 5/5, Loss: 1.2568
新模型评估：
Accuracy: 70.24%
Model saved to ../../../data/model/mnist_cnn_model
CPC10调整模型中, 本轮训练的数据量为：312.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 70.24%
Epoch 1/5, Loss: 1.1943
Epoch 2/5, Loss: 1.1788
Epoch 3/5, Loss: 1.1639
Epoch 4/5, Loss: 1.1535
Epoch 5/5, Loss: 1.1395
新模型评估：
Accuracy: 70.38%
Model saved to ../../../data/model/mnist_cnn_model
DONE
最终的列表：
[-0.001045737676195901, -0.0011578050891925176, -0.0012712225537129217, -0.001385986993080025, -0.0015020953399583793, -0.0016195445363187226, -0.0017383315334027274, -0.001858453291687899, -0.0019799067808526083, -0.002102688979741321, -0.002226796876329956, -0.002352227467691436, -0.0024789777599613164, -0.0026070447683036874, -0.0027364255168771216, -0.0028671170388008364, -0.0029991163761209916, -0.0031324205797771364, -0.0032670267095688023, -0.0034029318341222796, -0.0035401330308574945, -0.0036786273859550737, -0.003818411994323541, -0.003959483959566641, -0.004101840393950871, -0.0042454784183730936, -0.004390395162328323, -0.004536587763877636, -0.0046840533696162895, -0.004832789134641885, -0.004982792222522736, -0.0051340598052663985, -0.005286589063288251, -0.005440377185380305, -0.005595421368680145, -0.005751718818639916, -0.005909266748995556, -0.0060680623817361695, -0.00622810294707337, -0.006389385683411027, -0.006551907837314873, -0.006715666663482445, -0.006880659424713088, -0.007046883391878048, -0.007214335843890758, -0.007383014067677281, -0.007552915358146742, -0.0077240370181621, -0.007896376358510843, -0.008069930697875968, -0.008244697362806946, -0.00842067368769104, -0.008597857014724437, -0.008776244693883765, -0.008955834082897657, -0.009136622547218358, -0.009318607459993598, -0.009501786202038454, -0.009686156161807416, -0.00987171473536659, -0.010058459326365933, -0.010246387346011708, -0.01043549621303895, -0.010625783353684193, -0.010817246201658248, -0.011009882198118964, -0.011203688791644378, -0.011398663438205767, -0.011594803601140916, -0.011792106751127393, -0.011990570366156209, -0.012190191931505168, -0.012390968939712754, -0.012592898890551832, -0.01279597929100365, -0.013000207655231827, -0.013205581504556599, -0.013412098367428862, -0.013619755779404885, -0.013828551283120452, -0.014038482428265706, -0.014249546771559735, -0.014461741876725442, -0.0146750653144644, -0.014889514662431935, -0.015105087505212236, -0.015321781434293663, -0.015539594048043975, -0.015758522951685935, -0.015978565757272717, -0.016199720083663688, -0.016421983556500097, -0.016645353808181024, -0.01686982847783927, -0.017095405211317555, -0.01732208166114453, -0.017549855486511196, -0.01777872435324722, -0.018008685933797453, -0.01823973790719849, -0.018471877959055333, -0.018705103781518226, -0.01893941307325947, -0.019174803539450433, -0.019411272891738676, -0.019648818848225003, -0.01988743913344082, -0.02012713147832547, -0.020367893620203706, -0.020609723302763228, -0.02085261827603234, -0.02109657629635768, -0.021341595126382104, -0.021587672535022542, -0.021834806297448092, -0.02208299419505806, -0.02233223401546025, -0.02258252355244922, -0.02283386060598462, -0.023086242982169797, -0.023339668493230215, -0.02359413495749231, -0.02384964019936199, -0.024106182049303662, -0.024363758343819097, -0.024622366925426464, -0.024882005642639343, -0.025142672349946077, -0.0254043649077889, -0.025667081182543364, -0.025930819046497716, -0.026195576377832597, -0.02646135106060045, -0.02672814098470537, -0.026995944045882833, -0.027264758145679566, -0.027534581191433496, -0.027805411096253813, -0.028077245779001114, -0.028350083164267473, -0.028623921182356876, -0.028898757769265496, -0.029174590866662173, -0.02945141842186895, -0.029729238387841556, -0.030008048723150355, -0.030287847391960804, -0.030568632364014497, -0.030850401614610085, -0.03113315312458409, -0.031416884880292234, -0.03170159487359048, -0.03198728110181606, -0.03227394156776933, -0.0325615742796945, -0.0328501772512616, -0.03313974850154769, -0.03343028605501877, -0.03372178794151112, -0.034014252196213374, -0.03430767685964822, -0.034602059977654365, -0.03489739960136834, -0.035193693787206826, -0.0354909405968486, -0.035789138097216844, -0.036088284360461326, -0.0363883774639408, -0.03668941549020541, -0.03699139652697919, -0.03729431866714275, -0.03759818000871551, -0.03790297865483891, -0.03820871271375886, -0.038515380298808605, -0.03882297952839181, -0.039131508525965336, -0.0394409654200224, -0.03975134834407565, -0.04006265543664034, -0.040374884841217734, -0.0406880347062781, -0.041002103185244415, -0.04131708843647569, -0.041632988623250405, -0.04194980191375028, -0.04226752648104373, -0.04258616050306971, -0.04290570216262146, -0.04322614964733035, -0.04354750114964975, -0.043869754866839084, -0.04419290900094783, -0.04451696175879952, -0.04484191135197613, -0.04516775599680209, -0.045494493914328665, -0.045822123330318376, -0.04615064247522918, -0.04648004958419927, -0.0468103428970314, -0.047141520658177505, -0.04747358111672337, -0.04780652252637346, -0.048140343145435555, -0.048475041236805655, -0.04881061506795284, -0.049147062910904316, -0.04948438304223038, -0.049822573743029536, -0.05016163329891343, -0.05050155999999248, -0.05084235214086069, -0.05118400802058115, -0.05152652594267146, -0.05186990421508908, -0.052214141150216775, -0.05255923506484836, -0.052905184280173906, -0.053251987121765904, -0.05359964191956451, -0.05394814700786363, -0.054297500725296544, -0.05464770141482195, -0.054998747423709776, -0.055350637103527195, -0.05570336881012464, -0.05605694090362204, -0.056411351748394734, -0.05676659971306, -0.0571226831704629, -0.057479600497663014, -0.057837350075920446, -0.05819593029068254, -0.05855533953157013, -0.058915576192364205, -0.05927663867099228, -0.05963852536951536, -0.060001234694114364, -0.06036476505507685, -0.06072911486678398, -0.06109428254769711, -0.06146026652034489, -0.06182706521131012, -0.062194677051216674, -0.06256310047471655, -0.06293233392047715, -0.06330237583116805, -0.06367322465344849, -0.06404487883795451, -0.06441733683928619, -0.06479059711599514, -0.06516465813057154, -0.06553951834943186, -0.06591517624290641, -0.06629163028522647, -0.06666887895451226, -0.06704692073276033, -0.0674257541058314, -0.06780537756343782, -0.06818578959913157, -0.06856698871029193, -0.0689489733981134, -0.06933174216759364, -0.06971529352752118, -0.07009962599046377, -0.07048473807275615, -0.0708706282944882, -0.07125729517949309, -0.07164473725533538, -0.0720329530532994, -0.07242194110837741, -0.07281169995925782, -0.07320222814831359, -0.07359352422159085, -0.07398558672879701, -0.07437841422328934, -0.07477200526206365, -0.07516635840574265, -0.07556147221856471, -0.07595734526837244, -0.0763539761266015, -0.07675136336826899, -0.07714950557196279, -0.07754840131982987, -0.07794804919756551, -0.07834844779440198, -0.07874959570309756, -0.07915149151992557, -0.07955413384466345, -0.0799575212805817, -0.08036165243443302, -0.08076652591644154, -0.08117214034029205, -0.08157849432311898, -0.08198558648549606, -0.08239341545142528, -0.08280197984832646, -0.08321127830702651, -0.08362130946174917, -0.08403207195010395, -0.0844435644130761, -0.08485578549501602, -0.08526873384362876, -0.08568240810996378, -0.08609680694840452, -0.08651192901665827, -0.08692777297574567, -0.0873443374899906, -0.08776162122701012, -0.08817962285770412, -0.08859834105624542, -0.08901777450006948, -0.0894379218698646, -0.08985878184956175, -0.09028035312632479, -0.09070263439054027, -0.09112562433580784, -0.09154932165893018, -0.0919737250599034, -0.09239883324190709, -0.09282464491129458, -0.09325115877758347, -0.09367837355344569, -0.09410628795469791, -0.09453490070029211, -0.0949642105123058, -0.09539421611593277, -0.09582491623947326, -0.09625630961432474, -0.09668839497497247, -0.09712117105898005, -0.09755463660697997, -0.0979887903626645, -0.09842363107277627, -0.09885915748709906, -0.09929536835844854, -0.09973226244266314, -0.10016983849859487, -0.10060809528810027, -0.10104703157603112, -0.10148664613022562, -0.10192693772149919, -0.10236790512363575, -0.10280954711337831, -0.10325186247042056, -0.1036948499773977, -0.10413850841987754, -0.10458283658635165, -0.10502783326822707, -0.1054734972598167, -0.10591982735833133, -0.1063668223638704, -0.10681448107941383, -0.10726280231081281, -0.10771178486678179, -0.10816142755888941, -0.10861172920155027, -0.10906268861201612, -0.10951430461036793, -0.10996657601950677, -0.11041950166514569, -0.11087308037580151, -0.1113273109827862, -0.11178219232019859, -0.11223772322491599, -0.11269390253658634, -0.11315072909761942, -0.11360820175317876, -0.114066319351174, -0.11452508074225187, -0.11498448477978873, -0.11544453031988217, -0.11590521622134303, -0.11636654134568736, -0.11682850455712823, -0.1172911047225682, -0.11775434071159085, -0.11821821139645317, -0.1186827156520775, -0.11914785235604386, -0.11961362038858181, -0.1200800186325629, -0.1205470459734928, -0.12101470129950359, -0.12148298350134568, -0.12195189147238078, -0.1224214241085736, -0.1228915803084844, -0.12336235897326153, -0.12383375900663374, -0.12430577931490228, -0.12477841880693408, -0.12525167639415352, -0.12572555099053528, -0.12620004151259673, -0.1266751468793909, -0.12715086601249842, -0.1276271978360205, -0.12810414127657177, -0.1285816952632725, -0.12905985872774162, -0.12953863060408916, -0.13001800982890938, -0.13049799534127315, -0.130978586082721, -0.13145978099725564, -0.1319415790313354, -0.13242397913386633, -0.13290698025619568, -0.13339058135210446, -0.13387478137780073, -0.13435957929191222, -0.13484497405547946, -0.13533096463194882, -0.13581754998716566, -0.13630472908936714, -0.1367925009091755, -0.13728086441959103, -0.13776981859598547, -0.13825936241609477, -0.1387494948600126, -0.13924021491018346, -0.13973152155139584, -0.14022341377077563, -0.14071589055777922, -0.14120895090418695, -0.14170259380409633, -0.14219681825391534, -0.14269162325235601, -0.1431870078004276, -0.14368297090143023, -0.14417951156094805, -0.14467662878684284, -0.1451743215892476, -0.14567258898055985, -0.1461714299754353, -0.1466708435907813, -0.1471708288457506, -0.14767138476173464, -0.14817251036235746, -0.14867420467346915, -0.1491764667231395, -0.149679295541652, -0.15018269016149705, -0.150686649617366, -0.15119117294614476, -0.15169625918690788, -0.15220190738091183, -0.15270811657158917, -0.15321488580454223, -0.15372221412753712, -0.1542301005904974, -0.15473854424549793, -0.1552475441467593, -0.15575709935064097, -0.15626720891563595, -0.15677787190236425, -0.15728908737356717, -0.15780085439410135, -0.15831317203093237, -0.15882603935312944, -0.15933945543185907, -0.15985341934037928, -0.16036793015403356, -0.16088298695024522, -0.1613985888085117, -0.1619147348103981, -0.16243142403953198, -0.16294865558159755, -0.16346642852432958, -0.16398474195750767, -0.16450359497295097, -0.16502298666451193, -0.16554291612807098, -0.16606338246153063, -0.16658438476481002, -0.1671059221398391, -0.1676279936905533, -0.16815059852288744, -0.16867373574477068, -0.16919740446612058, -0.16972160379883788, -0.17024633285680085, -0.1707715907558594, -0.17129737661383054, -0.17182368955049182, -0.17235052868757667, -0.1728778931487686, -0.17340578205969603, -0.17393419454792647, -0.1744631297429617, -0.17499258677623208, -0.1755225647810913, -0.17605306289281097, -0.17658408024857547, -0.17711561598747644, -0.17764766925050784, -0.17818023918056036, -0.17871332492241637, -0.1792469256227448, -0.17978104043009563, -0.1803156684948951, -0.1808508089694401, -0.18138646100789352, -0.1819226237662786, -0.18245929640247427, -0.18299647807620983, -0.18353416794905986, -0.18407236518443926, -0.18461106894759793, -0.18515027840561643, -0.18568999272739994, -0.1862302110836742, -0.18677093264697991, -0.18731215659166817, -0.1878538820938953, -0.18839610833161796, -0.1889388344845883, -0.18948205973434884, -0.1900257832642281, -0.1905700042593349, -0.19111472190655443, -0.19165993539454268, -0.19220564391372225, -0.19275184665627698, -0.19329854281614733, -0.19384573158902596, -0.19439341217235234, -0.19494158376530862, -0.1954902455688146, -0.19603939678552285, -0.1965890366198147, -0.19713916427779454, -0.19768977896728607, -0.19824087989782713, -0.19879246628066521, -0.1993445373287529, -0.19989709225674318, -0.20045013028098496, -0.20100365061951814, -0.20155765249206986, -0.20211213512004883, -0.2026670977265418, -0.20322253953630837, -0.2037784597757769, -0.20433485767303988, -0.20489173245784958, -0.20544908336161305, -0.20600690961738855, -0.2065652104598804, -0.2071239851254349, -0.20768323285203583, -0.20824295287930011, -0.20880314444847348, -0.2093638068024259, -0.20992493918564753, -0.21048654084424429, -0.2110486110259332, -0.2116111489800387, -0.21217415395748795, -0.2127376252108064, -0.21330156199411415, -0.213865963563121, -0.2144308291751228, -0.2149961580889967, -0.21556194956519736, -0.21612820286575263, -0.2166949172542592, -0.21726209199587876, -0.21782972635733344, -0.21839781960690213, -0.21896637101441596, -0.21953537985125426, -0.2201048453903407, -0.22067476690613902, -0.22124514367464881, -0.22181597497340189, -0.22238726008145754, -0.2229589982793994, -0.2235311888493306, -0.22410383107487025, -0.22467692424114927, -0.22525046763480633, -0.22582446054398403, -0.22639890225832487, -0.22697379206896734, -0.22754912926854193, -0.22812491315116717, -0.22870114301244593, -0.2292778181494612, -0.2298549378607725, -0.23043250144641164, -0.23101050820787944, -0.23158895744814134, -0.23216784847162386, -0.2327471805842104, -0.23332695309323798, -0.23390716530749317, -0.23448781653720813, -0.23506890609405706, -0.23565043329115226, -0.23623239744304075, -0.23681479786569987, -0.23739763387653434, -0.23798090479437167, -0.23856460993945944, -0.2391487486334608, -0.23973332019945087, -0.24031832396191383, -0.24090375924673813, -0.24148962538121366, -0.2420759216940278, -0.242662647515262, -0.2432498021763878, -0.24383738501026347, -0.24442539535113056, -0.2450138325346099, -0.2456026958976984, -0.24619198477876525, -0.24678169851754878, -0.24737183645515232, -0.247962397934041, -0.24855338229803836, -0.24914478889232256, -0.24973661706342315, -0.2503288661592172, -0.2509215355289265, -0.2515146245231132, -0.2521081324936772, -0.25270205879385227, -0.2532964027782024, -0.25389116380261934, -0.2544863412243176, -0.255081934401833, -0.2556779426950174, -0.25627436546503674, -0.2568712020743668, -0.2574684518867903, -0.25806611426739345, -0.25866418858256246, -0.25926267419998045, -0.25986157048862385, -0.26046087681875946, -0.2610605925619408, -0.2616607170910049, -0.2622612497800696, -0.2628621900045291, -0.26346353714105186, -0.2640652905675767, -0.2646674496633098, -0.2652700138087214, -0.2658729823855425, -0.2664763547767622, -0.2670801303666235, -0.26768430854062103, -0.2682888886854974, -0.2688938701892402, -0.2694992524410787, -0.27010503483148096, -0.2707112167521506, -0.27131779759602315, -0.271924776757264, -0.2725321536312642, -0.2731399276146381, -0.2737480981052198, -0.2743566645020604, -0.27496562620542475, -0.2755749826167884, -0.27618473313883474, -0.2767948771754514, -0.27740541413172815, -0.27801634341395276, -0.27862766442960907, -0.2792393765873729, -0.2798514792971102, -0.2804639719698731, -0.2810768540178974, -0.2816901248545996, -0.2823037838945735, -0.2829178305535881, -0.28353226424858374, -0.2841470843976697, -0.28476229042012124, -0.2853778817363763, -0.28599385776803316, -0.2866102179378469, -0.2872269616697275, -0.28784408838873554, -0.2884615975210806, -0.2890794884941177, -0.2896977607363446, -0.29031641367739924, -0.29093544674805627, -0.29155485938022496, -0.2921746510069456, -0.29279482106238763, -0.29341536898184567, -0.29403629420173777, -0.2946575961596022, -0.2952792742940943, -0.2959013280449846, -0.2965237568531551, -0.29714656016059704, -0.29776973741040813, -0.2983932880467897, -0.29901721151504396, -0.29964150726157135, -0.30026617473386785, -0.300891213380522, -0.3015166226512126, -0.30214240199670567, -0.3027685508688522, -0.3033950687205847, -0.3040219550059155, -0.3046492091799335, -0.3052768306988014, -0.30590481901975364, -0.30653317360109295, -0.3071618939021888, -0.3077909793834735, -0.3084204295064407, -0.3090502437336422, -0.30968042152868536, -0.31031096235623085, -0.3109418656819896, -0.31157313097272077, -0.3122047576962285, -0.3128367453213602, -0.31346909331800304, -0.31410180115708225, -0.3147348683105581, -0.3153682942514234, -0.3160020784537013, -0.3166362203924423, -0.3172707195437223, -0.31790557538463937, -0.31854078739331204, -0.3191763550488763, -0.31981227783148325, -0.3204485552222967, -0.3210851867034905, -0.3217221717582466, -0.3223595098707518, -0.32299720052619596, -0.3236352432107695, -0.3242736374116604, -0.3249123826170526, -0.32555147831612286, -0.326190923999039, -0.32683071915695694, -0.3274708632820186, -0.3281113558673493, -0.32875219640705594, -0.3293933843962239, -0.33003491933091494, -0.3306768007081651, -0.33131902802598195, -0.3319616007833427, -0.3326045184801912, -0.33324778061743643, -0.33389138669694945, -0.3345353362215615, -0.3351796286950616, -0.33582426362219414, -0.3364692405086567, -0.33711455886109754, -0.3377602181871139, -0.33840621799524884, -0.3390525577949898, -0.3396992370967657, -0.3403462554119452, -0.340993612252834, -0.3416413071326727, -0.3422893395656352, -0.34293770906682525, -0.3435864151522753, -0.3442354573389438, -0.34488483514471285, -0.34553454808838663, -0.34618459568968823, -0.34683497746925857, -0.34748569294865317, -0.3481367416503407, -0.34878812309770035, -0.34943983681502, -0.350091882327494, -0.3507442591612205, -0.35139696684320026, -0.3520500049013336, -0.35270337286441866, -0.3533570702621493, -0.3540110966251129, -0.35466545148478834, -0.3553201343735434, -0.35597514482463355, -0.35663048237219896, -0.35728614655126306, -0.3579421368977298, -0.35859845294838216, -0.3592550942408801, -0.35991206031375755, -0.36056935070642177, -0.3612269649591501, -0.36188490261308837, -0.3625431632102489, -0.3632017462935084, -0.3638606514066058, -0.3645198780941405, -0.36517942590157015, -0.3658392943752085, -0.36649948306222363, -0.36715999151063583, -0.36782081926931565, -0.36848196588798177, -0.3691434309171992, -0.3698052139083773, -0.3704673144137674, -0.3711297319864614, -0.3717924661803892, -0.37245551655031744, -0.3731188826518468, -0.3737825640414107, -0.37444656027627293, -0.37511087091452566, -0.375775495515088, -0.37644043363770335, -0.3771056848429383, -0.3777712486921798, -0.37843712474763425, -0.37910331257232455, -0.379769811730089, -0.38043662178557913, -0.3811037423042575, -0.3817711728523963, -0.38243891299707516, -0.38310696230617963, -0.3837753203483986, -0.3844439866932232, -0.3851129609109446, -0.3857822425726519, -0.3864518312502309, -0.3871217265163617, -0.38779192794451695, -0.3884624351089603, -0.3891332475847443, -0.3898043649477087, -0.39047578677447853, -0.3911475126424625, -0.3918195421298507, -0.3924918748156135, -0.393164510279499, -0.3938374481020319, -0.39451068786451116, -0.3951842291490088, -0.39585807153836744, -0.396532214616199, -0.39720665796688287, -0.39788140117556386, -0.3985564438281509, -0.3992317855113148, -0.39990742581248695, -0.400583364319857, -0.4012596006223719, -0.40193613430973335, -0.4026129649723967, -0.4032900922015688, -0.4039675155892065, -0.40464523472801495, -0.40532324921144547, -0.40600155863369475, -0.40668016258970213, -0.40735906067514843, -0.40803825248645426, -0.40871773762077807, -0.40939751567601484, -0.410077586250794, -0.41075794894447815, -0.411438603357161, -0.41211954908966586, -0.41280078574354423, -0.4134823129210735, -0.4141641302252563, -0.41484623725981756, -0.41552863362920417, -0.4162113189385824, -0.41689429279383666, -0.4175775548015678, -0.41826110456909144, -0.4189449417044365, -0.41962906581634346, -0.42031347651426276, -0.4209981734083529, -0.4216831561094796, -0.4223684242292134, -0.4230539773798285, -0.4237398151743011, -0.42442593722630767, -0.4251123431502234, -0.42579903256112095, -0.42648600507476825, -0.4271732603076276, -0.4278607978768536, -0.4285486174002921, -0.4292367184964777, -0.42992510078463353, -0.4306137638846684, -0.43130270741717625, -0.4319919310034338, -0.43268143426540007, -0.4333712168257135, -0.43406127830769153, -0.4347516183353284, -0.43544223653329406, -0.4361331325269326, -0.4368243059422602, -0.43751575640596435, -0.4382074835454023, -0.43889948698859854, -0.43959176636424485, -0.44028432130169737, -0.4409771514309764, -0.44167025638276364, -0.44236363578840193, -0.4430572892798926, -0.44375121648989513, -0.4444454170517247, -0.44513989059935166, -0.4458346367673991, -0.44652965519114185, -0.4472249455065057, -0.4479205073500646, -0.44861634035904013, -0.4493124441713, -0.4500088184253563, -0.45070546276036416, -0.45140237681612044, -0.4520995602330624, -0.45279701265226585, -0.45349473371544435, -0.454192723064947, -0.454890980343758, -0.4555895051954944, -0.45628829726440495, -0.4569873561953689, -0.4576866816338947, -0.45838627322611786, -0.45908613061880066, -0.45978625345932966, -0.4604866413957155, -0.46118729407659, -0.4618882111512065, -0.46258939226943707, -0.46329083708177216, -0.4639925452393183, -0.4646945163937979, -0.46539675019754667, -0.46609924630351296, -0.46680200436525643, -0.4675050240369465, -0.4682083049733611, -0.4689118468298852, -0.4696156492625094, -0.47031971192782973, -0.47102403448304375, -0.47172861658595255, -0.4724334578949564, -0.47313855806905547, -0.47384391676784765, -0.4745495336515275, -0.4752554083808844, -0.4759615406173022, -0.4766679300227573, -0.4773745762598173, -0.4780814789916398, -0.47878863788197157, -0.4794960525951465, -0.4802037227960848, -0.4809116481502915, -0.48161982832385597, -0.4823282629834489, -0.4830369517963228, -0.4837458944303097, -0.48445509055382074, -0.4851645398358434, -0.4858742419459424, -0.48658419655425655, -0.4872944033314983, -0.48800486194895276, -0.4887155720784756, -0.48942653339249287, -0.4901377455639988, -0.4908492082665551, -0.49156092117428996, -0.49227288396189606, -0.49298509630462994, -0.4936975578783106, -0.49441026835931823, -0.49512322742459336, -0.49583643475163486]
**** log-parameter_analysis 运行时间： 2025-01-17 23:04:47 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.060844997898674216
DataOwner2: noise random: 0.08859484346216961
DataOwner3: noise random: 0.07297759892194641
DataOwner4: noise random: 0.03258939539398142
DataOwner5: noise random: 0.014190322344247663
DataOwner6: noise random: 0.09687091208096475
DataOwner7: noise random: 0.012757541076017865
DataOwner8: noise random: 0.0956735099567791
DataOwner9: noise random: 0.06761704240284887
DataOwner10: noise random: 0.010326141665297817
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9984967969145012, 0.9968164249254796, 0.9978449775306767, 0.9995706045057651, 0.9999185989480246, 0.9962084998221096, 0.9999341436382053, 0.9962994461648321, 0.9981477762483458, 0.9999568230947059]
归一化后的数据质量列表avg_f_list: [0.9610485522719235, 0.916218587863393, 0.9436589266601221, 0.9896962305315434, 0.9989802334563627, 0.9, 0.999394943956238, 0.9024263206801679, 0.9517371711350017, 1.0]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3363
DataOwner1的最优x_1 = 0.1265
DataOwner2的最优x_2 = 0.0779
DataOwner3的最优x_3 = 0.1088
DataOwner4的最优x_4 = 0.1529
DataOwner5的最优x_5 = 0.1607
DataOwner6的最优x_6 = 0.0577
DataOwner7的最优x_7 = 0.1611
DataOwner8的最优x_8 = 0.0608
DataOwner9的最优x_9 = 0.1172
DataOwner10的最优x_10 = 0.1616
每个DataOwner应该贡献数据比例 xn_list = [0.12649655758900466, 0.077857812360099, 0.10877858731403715, 0.15286247751951798, 0.16071651715685614, 0.05769794233110358, 0.1610599285227827, 0.06080914759933146, 0.11718023593075123, 0.1615598395336469]
ModelOwner的最大效用 U(Eta) = 0.5754
DONE
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.336258710303773
DataOwner1的分配到的支付 ： 0.1415
DataOwner2的分配到的支付 ： 0.0830
DataOwner3的分配到的支付 ： 0.1195
DataOwner4的分配到的支付 ： 0.1761
DataOwner5的分配到的支付 ： 0.1868
DataOwner6的分配到的支付 ： 0.0604
DataOwner7的分配到的支付 ： 0.1873
DataOwner8的分配到的支付 ： 0.0639
DataOwner9的分配到的支付 ： 0.1298
DataOwner10的分配到的支付 ： 0.1880
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC1', 'DataOwner2': 'CPC2', 'DataOwner3': 'CPC3', 'DataOwner4': 'CPC4', 'DataOwner5': 'CPC5', 'DataOwner6': 'CPC6', 'DataOwner7': 'CPC7', 'DataOwner8': 'CPC8', 'DataOwner9': 'CPC9', 'DataOwner10': 'CPC10'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC2
DataOwner3 把数据交给 CPC3
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC8
DataOwner9 把数据交给 CPC9
DataOwner10 把数据交给 CPC10
DONE
----- literation 1: 模型训练 -----
CPC1调整模型中, 本轮训练的数据量为：220.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 26.82%
Epoch 1/5, Loss: 2.2972
Epoch 2/5, Loss: 2.2912
Epoch 3/5, Loss: 2.2942
Epoch 4/5, Loss: 2.2929
Epoch 5/5, Loss: 2.2885
新模型评估：
Accuracy: 28.87%
Model saved to ../../../data/model/mnist_cnn_model
CPC2调整模型中, 本轮训练的数据量为：203.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 28.87%
Epoch 1/5, Loss: 2.2938
Epoch 2/5, Loss: 2.2862
Epoch 3/5, Loss: 2.2840
Epoch 4/5, Loss: 2.2838
Epoch 5/5, Loss: 2.2756
新模型评估：
Accuracy: 31.28%
Model saved to ../../../data/model/mnist_cnn_model
CPC3调整模型中, 本轮训练的数据量为：756.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 31.28%
Epoch 1/5, Loss: 2.2799
Epoch 2/5, Loss: 2.2749
Epoch 3/5, Loss: 2.2686
Epoch 4/5, Loss: 2.2616
Epoch 5/5, Loss: 2.2536
新模型评估：
Accuracy: 37.18%
Model saved to ../../../data/model/mnist_cnn_model
CPC4调整模型中, 本轮训练的数据量为：1594.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 37.18%
Epoch 1/5, Loss: 2.2503
Epoch 2/5, Loss: 2.2317
Epoch 3/5, Loss: 2.2080
Epoch 4/5, Loss: 2.1781
Epoch 5/5, Loss: 2.1407
新模型评估：
Accuracy: 44.13%
Model saved to ../../../data/model/mnist_cnn_model
CPC5调整模型中, 本轮训练的数据量为：1956.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 44.13%
Epoch 1/5, Loss: 2.1038
Epoch 2/5, Loss: 2.0556
Epoch 3/5, Loss: 2.0001
Epoch 4/5, Loss: 1.9323
Epoch 5/5, Loss: 1.8542
新模型评估：
Accuracy: 55.15%
Model saved to ../../../data/model/mnist_cnn_model
CPC6调整模型中, 本轮训练的数据量为：200.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 55.15%
Epoch 1/5, Loss: 1.8438
Epoch 2/5, Loss: 1.7266
Epoch 3/5, Loss: 1.7231
Epoch 4/5, Loss: 1.7262
Epoch 5/5, Loss: 1.7169
新模型评估：
Accuracy: 56.34%
Model saved to ../../../data/model/mnist_cnn_model
CPC7调整模型中, 本轮训练的数据量为：840.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 56.34%
Epoch 1/5, Loss: 1.7357
Epoch 2/5, Loss: 1.7123
Epoch 3/5, Loss: 1.6747
Epoch 4/5, Loss: 1.6395
Epoch 5/5, Loss: 1.6260
新模型评估：
Accuracy: 60.36%
Model saved to ../../../data/model/mnist_cnn_model
CPC8调整模型中, 本轮训练的数据量为：52.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 60.36%
Epoch 1/5, Loss: 1.5703
Epoch 2/5, Loss: 1.5641
Epoch 3/5, Loss: 1.5580
Epoch 4/5, Loss: 1.5518
Epoch 5/5, Loss: 1.5458
新模型评估：
Accuracy: 60.51%
Model saved to ../../../data/model/mnist_cnn_model
CPC9调整模型中, 本轮训练的数据量为：713.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 60.51%
Epoch 1/5, Loss: 1.6000
Epoch 2/5, Loss: 1.5826
Epoch 3/5, Loss: 1.5531
Epoch 4/5, Loss: 1.5498
Epoch 5/5, Loss: 1.5127
新模型评估：
Accuracy: 65.34%
Model saved to ../../../data/model/mnist_cnn_model
CPC10调整模型中, 本轮训练的数据量为：1685.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 65.34%
Epoch 1/5, Loss: 1.4290
Epoch 2/5, Loss: 1.3617
Epoch 3/5, Loss: 1.3012
Epoch 4/5, Loss: 1.2434
Epoch 5/5, Loss: 1.1781
新模型评估：
Accuracy: 71.18%
Model saved to ../../../data/model/mnist_cnn_model
DONE
========================= literation: 2 =========================
----- literation 2: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3363
DataOwner1的最优x_1 = 0.1265
DataOwner2的最优x_2 = 0.0779
DataOwner3的最优x_3 = 0.1088
DataOwner4的最优x_4 = 0.1529
DataOwner5的最优x_5 = 0.1607
DataOwner6的最优x_6 = 0.0577
DataOwner7的最优x_7 = 0.1611
DataOwner8的最优x_8 = 0.0608
DataOwner9的最优x_9 = 0.1172
DataOwner10的最优x_10 = 0.1616
每个DataOwner应该贡献数据比例 xn_list = [0.12649655758900466, 0.077857812360099, 0.10877858731403715, 0.15286247751951798, 0.16071651715685614, 0.05769794233110358, 0.1610599285227827, 0.06080914759933146, 0.11718023593075123, 0.1615598395336469]
ModelOwner的最大效用 U(Eta) = 0.5754
DONE
----- literation 2: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.336258710303773
DataOwner1的分配到的支付 ： 0.1415
DataOwner2的分配到的支付 ： 0.0830
DataOwner3的分配到的支付 ： 0.1195
DataOwner4的分配到的支付 ： 0.1761
DataOwner5的分配到的支付 ： 0.1868
DataOwner6的分配到的支付 ： 0.0604
DataOwner7的分配到的支付 ： 0.1873
DataOwner8的分配到的支付 ： 0.0639
DataOwner9的分配到的支付 ： 0.1298
DataOwner10的分配到的支付 ： 0.1880
DONE
----- literation 2: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC2
DataOwner3 把数据交给 CPC3
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC8
DataOwner9 把数据交给 CPC9
DataOwner10 把数据交给 CPC10
DONE
----- literation 2: 模型训练 -----
重新调整fn，进而调整xn、Eta
正在评估DataOwner1的数据质量, 本轮评估的样本数据量为：220.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 71.18%
Epoch 1/5, Loss: 1.1288
Epoch 2/5, Loss: 1.0814
Epoch 3/5, Loss: 1.0971
Epoch 4/5, Loss: 1.0759
Epoch 5/5, Loss: 1.0609
新模型评估：
Accuracy: 71.52%
loss差为：
0.067889004945755
单位数据loss差为：
0.0003085863861170682
正在评估DataOwner2的数据质量, 本轮评估的样本数据量为：203.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 66.75%
Epoch 1/5, Loss: 1.2954
Epoch 2/5, Loss: 1.3029
Epoch 3/5, Loss: 1.3381
Epoch 4/5, Loss: 1.3007
Epoch 5/5, Loss: 1.2733
新模型评估：
Accuracy: 67.61%
loss差为：
0.022067010402679443
单位数据loss差为：
0.00010870448474226326
正在评估DataOwner3的数据质量, 本轮评估的样本数据量为：756.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 69.48%
Epoch 1/5, Loss: 1.2581
Epoch 2/5, Loss: 1.2259
Epoch 3/5, Loss: 1.1980
Epoch 4/5, Loss: 1.1706
Epoch 5/5, Loss: 1.1469
新模型评估：
Accuracy: 72.00%
loss差为：
0.11124676465988159
单位数据loss差为：
0.0001471518051056635
正在评估DataOwner4的数据质量, 本轮评估的样本数据量为：1594.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 69.48%
Epoch 1/5, Loss: 1.2490
Epoch 2/5, Loss: 1.1938
Epoch 3/5, Loss: 1.1398
Epoch 4/5, Loss: 1.0878
Epoch 5/5, Loss: 1.0375
新模型评估：
Accuracy: 72.50%
loss差为：
0.21151057243347182
单位数据loss差为：
0.0001326917016521153
正在评估DataOwner5的数据质量, 本轮评估的样本数据量为：1956.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 72.00%
Epoch 1/5, Loss: 1.0400
Epoch 2/5, Loss: 0.9815
Epoch 3/5, Loss: 0.9275
Epoch 4/5, Loss: 0.8808
Epoch 5/5, Loss: 0.8354
新模型评估：
Accuracy: 75.55%
loss差为：
0.20451863542679816
单位数据loss差为：
0.00010455962956380274
正在评估DataOwner6的数据质量, 本轮评估的样本数据量为：200.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 75.03%
Epoch 1/5, Loss: 0.9916
Epoch 2/5, Loss: 0.9052
Epoch 3/5, Loss: 0.8538
Epoch 4/5, Loss: 0.9147
Epoch 5/5, Loss: 0.9569
新模型评估：
Accuracy: 75.75%
loss差为：
0.03463219106197357
单位数据loss差为：
0.00017316095530986786
正在评估DataOwner7的数据质量, 本轮评估的样本数据量为：840.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 75.03%
Epoch 1/5, Loss: 0.8684
Epoch 2/5, Loss: 0.8402
Epoch 3/5, Loss: 0.8238
Epoch 4/5, Loss: 0.8230
Epoch 5/5, Loss: 0.7862
新模型评估：
Accuracy: 76.05%
loss差为：
0.08214651686804642
单位数据loss差为：
9.779347246196002e-05
正在评估DataOwner8的数据质量, 本轮评估的样本数据量为：52.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 76.48%
Epoch 1/5, Loss: 1.0612
Epoch 2/5, Loss: 1.0523
Epoch 3/5, Loss: 1.0436
Epoch 4/5, Loss: 1.0355
Epoch 5/5, Loss: 1.0277
新模型评估：
Accuracy: 76.23%
loss差为：
0.03354454040527344
单位数据loss差为：
0.0006450873154860276
正在评估DataOwner9的数据质量, 本轮评估的样本数据量为：713.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 76.48%
Epoch 1/5, Loss: 0.8851
Epoch 2/5, Loss: 0.8418
Epoch 3/5, Loss: 0.8542
Epoch 4/5, Loss: 0.8131
Epoch 5/5, Loss: 0.8067
新模型评估：
Accuracy: 75.53%
loss差为：
0.07837816576162981
单位数据loss差为：
0.00010992730120845696
正在评估DataOwner10的数据质量, 本轮评估的样本数据量为：1685.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 76.48%
Epoch 1/5, Loss: 0.8653
Epoch 2/5, Loss: 0.8224
Epoch 3/5, Loss: 0.7881
Epoch 4/5, Loss: 0.7615
Epoch 5/5, Loss: 0.7254
新模型评估：
Accuracy: 76.18%
loss差为：
0.13983362471615823
单位数据loss差为：
8.298731437160726e-05
经过服务器调节后的真实数据质量：
数据质量列表avg_f_list: [0.0003085863861170682, 0.00010870448474226326, 0.0001471518051056635, 0.0001326917016521153, 0.00010455962956380274, 0.00017316095530986786, 9.779347246196002e-05, 0.0006450873154860276, 0.00010992730120845696, 8.298731437160726e-05]
归一化后的数据质量列表avg_f_list:[0.9401350420384608, 0.9045751948620653, 0.9114151379837829, 0.9088426235868998, 0.9038378073562402, 0.9160422773099951, 0.9026340790003555, 1.0, 0.9047927391537873, 0.9]
CPC1调整模型中, 本轮训练的数据量为：220.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 76.48%
Epoch 1/5, Loss: 0.7802
Epoch 2/5, Loss: 0.7421
Epoch 3/5, Loss: 0.7607
Epoch 4/5, Loss: 0.7402
Epoch 5/5, Loss: 0.7439
新模型评估：
Accuracy: 75.36%
CPC2调整模型中, 本轮训练的数据量为：203.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 76.48%
Epoch 1/5, Loss: 0.8609
Epoch 2/5, Loss: 0.8407
Epoch 3/5, Loss: 0.8096
Epoch 4/5, Loss: 0.7613
Epoch 5/5, Loss: 0.9122
新模型评估：
Accuracy: 74.70%
CPC3调整模型中, 本轮训练的数据量为：756.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 76.48%
Epoch 1/5, Loss: 0.8584
Epoch 2/5, Loss: 0.8274
Epoch 3/5, Loss: 0.8069
Epoch 4/5, Loss: 0.7877
Epoch 5/5, Loss: 0.7661
新模型评估：
Accuracy: 75.73%
CPC4调整模型中, 本轮训练的数据量为：1594.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 26.82%
Epoch 1/5, Loss: 2.2923
Epoch 2/5, Loss: 2.2848
Epoch 3/5, Loss: 2.2735
Epoch 4/5, Loss: 2.2568
Epoch 5/5, Loss: 2.2324
新模型评估：
Accuracy: 41.16%
Model saved to ../../../data/model/mnist_cnn_model
CPC5调整模型中, 本轮训练的数据量为：1956.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 41.16%
Epoch 1/5, Loss: 2.2136
Epoch 2/5, Loss: 2.1798
Epoch 3/5, Loss: 2.1414
Epoch 4/5, Loss: 2.0894
Epoch 5/5, Loss: 2.0297
新模型评估：
Accuracy: 48.84%
Model saved to ../../../data/model/mnist_cnn_model
CPC6调整模型中, 本轮训练的数据量为：200.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 48.84%
Epoch 1/5, Loss: 1.9802
Epoch 2/5, Loss: 2.0141
Epoch 3/5, Loss: 1.9765
Epoch 4/5, Loss: 1.9809
Epoch 5/5, Loss: 1.9607
新模型评估：
Accuracy: 51.78%
Model saved to ../../../data/model/mnist_cnn_model
CPC7调整模型中, 本轮训练的数据量为：840.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 51.78%
Epoch 1/5, Loss: 1.9473
Epoch 2/5, Loss: 1.9109
Epoch 3/5, Loss: 1.8896
Epoch 4/5, Loss: 1.8604
Epoch 5/5, Loss: 1.8278
新模型评估：
Accuracy: 56.17%
Model saved to ../../../data/model/mnist_cnn_model
CPC8调整模型中, 本轮训练的数据量为：52.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 56.17%
Epoch 1/5, Loss: 1.8958
Epoch 2/5, Loss: 1.8910
Epoch 3/5, Loss: 1.8862
Epoch 4/5, Loss: 1.8814
Epoch 5/5, Loss: 1.8767
新模型评估：
Accuracy: 57.70%
Model saved to ../../../data/model/mnist_cnn_model
CPC9调整模型中, 本轮训练的数据量为：713.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 57.70%
Epoch 1/5, Loss: 1.7988
Epoch 2/5, Loss: 1.7739
Epoch 3/5, Loss: 1.7274
Epoch 4/5, Loss: 1.6807
Epoch 5/5, Loss: 1.7009
新模型评估：
Accuracy: 60.47%
Model saved to ../../../data/model/mnist_cnn_model
CPC10调整模型中, 本轮训练的数据量为：1685.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 60.47%
Epoch 1/5, Loss: 1.6540
Epoch 2/5, Loss: 1.5851
Epoch 3/5, Loss: 1.5227
Epoch 4/5, Loss: 1.4579
Epoch 5/5, Loss: 1.3945
新模型评估：
Accuracy: 66.41%
Model saved to ../../../data/model/mnist_cnn_model
DONE
========================= literation: 3 =========================
----- literation 3: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.2901
DataOwner1的最优x_1 = 0.1371
DataOwner2的最优x_2 = 0.1017
DataOwner3的最优x_3 = 0.1090
DataOwner4的最优x_4 = 0.1063
DataOwner5的最优x_5 = 0.1009
DataOwner6的最优x_6 = 0.1137
DataOwner7的最优x_7 = 0.0996
DataOwner8的最优x_8 = 0.1850
DataOwner9的最优x_9 = 0.1019
DataOwner10的最优x_10 = 0.0967
每个DataOwner应该贡献数据比例 xn_list = [0.13705976587204313, 0.10170727855035702, 0.10896669811019864, 0.10626344575678817, 0.10091079626258781, 0.11374853736794928, 0.09960471266608664, 0.18497603084924624, 0.10194173545609242, 0.09672098315226577]
ModelOwner的最大效用 U(Eta) = 0.5243
xn开始变化：
DONE
----- literation 3: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.290119037327765
DataOwner1的分配到的支付 ： 0.5800
DataOwner2的分配到的支付 ： 0.0563
DataOwner3的分配到的支付 ： 0.0607
DataOwner4的分配到的支付 ： 0.0849
DataOwner5的分配到的支付 ： 0.0888
DataOwner6的分配到的支付 ： 0.0637
DataOwner7的分配到的支付 ： 0.0889
DataOwner8的分配到的支付 ： 0.1131
DataOwner9的分配到的支付 ： 0.0648
DataOwner10的分配到的支付 ： 0.0889
DONE
----- literation 3: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC2
DataOwner3 把数据交给 CPC3
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC8
DataOwner9 把数据交给 CPC9
DataOwner10 把数据交给 CPC10
DONE
----- literation 3: 模型训练 -----
CPC1调整模型中, 本轮训练的数据量为：1745.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 66.41%
Epoch 1/5, Loss: 1.3111
Epoch 2/5, Loss: 1.2520
Epoch 3/5, Loss: 1.1924
Epoch 4/5, Loss: 1.1413
Epoch 5/5, Loss: 1.0688
新模型评估：
Accuracy: 71.92%
Model saved to ../../../data/model/mnist_cnn_model
CPC2调整模型中, 本轮训练的数据量为：265.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 71.92%
Epoch 1/5, Loss: 1.1286
Epoch 2/5, Loss: 1.0371
Epoch 3/5, Loss: 1.0645
Epoch 4/5, Loss: 1.0201
Epoch 5/5, Loss: 1.0380
新模型评估：
Accuracy: 70.77%
CPC3调整模型中, 本轮训练的数据量为：757.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 71.92%
Epoch 1/5, Loss: 1.0207
Epoch 2/5, Loss: 0.9927
Epoch 3/5, Loss: 0.9644
Epoch 4/5, Loss: 0.9426
Epoch 5/5, Loss: 0.9170
新模型评估：
Accuracy: 74.09%
Model saved to ../../../data/model/mnist_cnn_model
CPC4调整模型中, 本轮训练的数据量为：1594.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 74.09%
Epoch 1/5, Loss: 0.9186
Epoch 2/5, Loss: 0.8726
Epoch 3/5, Loss: 0.8338
Epoch 4/5, Loss: 0.7968
Epoch 5/5, Loss: 0.7632
新模型评估：
Accuracy: 76.71%
Model saved to ../../../data/model/mnist_cnn_model
CPC5调整模型中, 本轮训练的数据量为：1956.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 76.71%
Epoch 1/5, Loss: 0.7505
Epoch 2/5, Loss: 0.7109
Epoch 3/5, Loss: 0.6790
Epoch 4/5, Loss: 0.6490
Epoch 5/5, Loss: 0.6239
新模型评估：
Accuracy: 78.30%
Model saved to ../../../data/model/mnist_cnn_model
CPC6调整模型中, 本轮训练的数据量为：395.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.30%
Epoch 1/5, Loss: 0.6495
Epoch 2/5, Loss: 0.5869
Epoch 3/5, Loss: 0.6227
Epoch 4/5, Loss: 0.5882
Epoch 5/5, Loss: 0.6021
新模型评估：
Accuracy: 75.62%
CPC7调整模型中, 本轮训练的数据量为：840.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.10%
Epoch 1/5, Loss: 0.7089
Epoch 2/5, Loss: 0.6868
Epoch 3/5, Loss: 0.7119
Epoch 4/5, Loss: 0.6570
Epoch 5/5, Loss: 0.6426
新模型评估：
Accuracy: 76.89%
CPC8调整模型中, 本轮训练的数据量为：160.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.10%
Epoch 1/5, Loss: 0.8095
Epoch 2/5, Loss: 0.8168
Epoch 3/5, Loss: 0.7830
Epoch 4/5, Loss: 0.7715
Epoch 5/5, Loss: 0.7396
新模型评估：
Accuracy: 77.62%
CPC9调整模型中, 本轮训练的数据量为：713.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.10%
Epoch 1/5, Loss: 0.7301
Epoch 2/5, Loss: 0.6864
Epoch 3/5, Loss: 0.6919
Epoch 4/5, Loss: 0.7069
Epoch 5/5, Loss: 0.6617
新模型评估：
Accuracy: 77.03%
CPC10调整模型中, 本轮训练的数据量为：1685.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.10%
Epoch 1/5, Loss: 0.7096
Epoch 2/5, Loss: 0.6762
Epoch 3/5, Loss: 0.6475
Epoch 4/5, Loss: 0.6260
Epoch 5/5, Loss: 0.5967
新模型评估：
Accuracy: 76.48%
DONE
最终的列表：
[0.00035869405027408145, 0.0003854218104024625, 0.00041050344227540163, 0.000433942901519482, 0.0004557441310976528, 0.0004759110613598884, 0.0004944476100935714, 0.0005113576825736439, 0.0005266451716125185, 0.0005403139576097594, 0.0005523679086015081, 0.0005628108803096797, 0.0005716467161909732, 0.000578879247485533, 0.0005845122932654906, 0.0005885496604832348, 0.0005909951440194183, 0.000591852526730826, 0.0005911255794979059, 0.000588818061272145, 0.0005849337191232012, 0.0005794762882858281, 0.0005724494922065212, 0.0005638570425900644, 0.000553702639445687, 0.0005419899711331458, 0.0005287227144085277, 0.000513904534469839, 0.0004975390850023784, 0.0004796300082239441, 0.0004601809349297631, 0.0004391954845371984, 0.0004166772651303316, 0.00039262987350427436, 0.00036705689520930007, 0.00033996190459472525, 0.00031134846485261075, 0.0002812201280613033, 0.00024958043522868567, 0.00021643291633526018, 0.00018178109037717688, 0.00014562846540872032, 0.00010797853858494216, 6.883479620391209e-05, 2.820071374884381e-05, -1.3920244070056431e-05, -5.752462327378999e-05, -0.00010260898057314061, -0.00014916988332732556, -0.0001972039095028133, -0.00024670764763248115, -0.0002976776967747313, -0.00035011066647306494, -0.00040400317671562824, -0.0004593518578950434, -0.000516153350768378, -0.0005744043064174059, -0.0006341013862088407, -0.0006952412617550058, -0.0007578206148745326, -0.0008218361375531835, -0.0008872845319051054, -0.0009541625101338741, -0.0010224667944941923, -0.0010921941172532673, -0.0011633412206528276, -0.0012359048568709174, -0.0013098817879841768, -0.0013852687859300106, -0.0014620626324692299, -0.001540260119148651, -0.001619858047263903, -0.0017008532278224575, -0.0017832424815068387, -0.0018670226386378053, -0.00195219053913806, -0.0020387430324957795, -0.002126676977728517, -0.0022159892433471773, -0.0023066767073201405, -0.002398736257037584, -0.002492164789276108, -0.002586959210163206, -0.00268311643514213, -0.002780633388936929, -0.0028795070055173955, -0.002979734228064551, -0.003081312008935952, -0.0031842373096312993, -0.0032885071007582034, -0.0033941183619981136, -0.003501068082072248, -0.0036093532587079674, -0.003718970898604998, -0.003829918017402026, -0.003942191639643336, -0.004055788798745477, -0.004170706536964441, -0.004286941905362676, -0.004404491963776264, -0.0045233537807824326, -0.004643524433666987, -0.004765001008392028, -0.00488778059956381, -0.005011860310400754, -0.005137237252701418, -0.00526390854681294, -0.005391871321599281, -0.005521122714409715, -0.005651659871047779, -0.005783479945739772, -0.005916580101103977, -0.00605095750811957, -0.006186609346095759, -0.006323532802641502, -0.006461725073634655, -0.006601183363191737, -0.006741904883637795, -0.00688388685547614, -0.007027126507358317, -0.0071716210760545684, -0.007317367806423863, -0.0074643639513844035, -0.007612606771884217, -0.007762093536871795, -0.007912821523266833, -0.008064788015931301, -0.008217990307640505, -0.008372425699054109, -0.008528091498687745, -0.008684985022884006, -0.00884310359578458, -0.009002444549301464, -0.009163005223089077, -0.00932478296451597, -0.00948777512863716, -0.009651979078165979, -0.009817392183446516, -0.009984011822426309, -0.010151835380628443, -0.01032086025112447, -0.01049108383450717, -0.01066250353886325, -0.01083511677974669, -0.011008920980151576, -0.011183913570485537, -0.01136009198854307, -0.011537453679478726, -0.011715996095781223, -0.011895716697246761, -0.012076612950952997, -0.012258682331232867, -0.012441922319648774, -0.012626330404966496, -0.012811904083129794, -0.012998640857234539, -0.013186538237503154, -0.013375593741259434, -0.013565804892903127, -0.013757169223884591, -0.013949684272679952, -0.01414334758476607, -0.014338156712595557, -0.014534109215572188, -0.014731202660025922, -0.014929434619188775, -0.015128802673170072, -0.015329304408932237, -0.015530937420266427, -0.015733699307768606, -0.01593758767881512, -0.016142600147539293, -0.016348734334807158, -0.0165559878681939, -0.01676435838196022, -0.016973843517028797, -0.017184440920960747, -0.017396148247932453, -0.017608963158712326, -0.017822883320637417, -0.018037906407590842, -0.01825403009997814, -0.01847125208470518, -0.018689570055155008, -0.01890898171116509, -0.019129484759005028, -0.019351076911354043, -0.019573755887278332, -0.01979751941220914, -0.02002236521792039, -0.02024829104250664, -0.02047529463036102, -0.020703373732153474, -0.02093252610480864, -0.021162749511484447, -0.021394041721550405, -0.021626400510565885, -0.021859823660258904, -0.022094308958504505, -0.02232985419930375, -0.022566457182762234, -0.022804115715069295, -0.023042827608476757, -0.023282590681278137, -0.02352340275778758, -0.02376526166831952, -0.024008165249167712, -0.024252111342584565, -0.024497097796760903, -0.024743122465805484, -0.02499018320972446, -0.025238277894401417, -0.02548740439157693, -0.02573756057882884, -0.025988744339552006, -0.026240953562938235, -0.026494186143956877, -0.026748439983334787, -0.02700371298753662, -0.02726000306874543, -0.027517308144843128, -0.027775626139390774, -0.028034954981609517, -0.028295292606361433, -0.028556636954129827, -0.028818985971000688, -0.029082337608643344, -0.029346689824291394, -0.02961204058072414, -0.029878387846247606, -0.03014572959467582, -0.030414063805312225, -0.03068338846293084, -0.0309537015577582, -0.031225001085454673, -0.03149728504709595, -0.03177055144915511, -0.032044798303484334, -0.03232002362729641, -0.03259622544314714, -0.03287340177891726, -0.03315155066779424, -0.03343067014825479, -0.033710758264047014, -0.0339918130641724, -0.034273832602868703, -0.03455681493959206, -0.034840758138999534, -0.035125660270931836, -0.035411519410395775, -0.03569833363754718, -0.03598610103767372, -0.03627481970117749, -0.03656448772355825, -0.03685510320539637, -0.03714666425233559, -0.037439168975066606, -0.03773261548931006, -0.03802700191579966, -0.038322326380266, -0.03861858701341936, -0.03891578195093348, -0.039213909333429164, -0.0395129673064579, -0.039812954020485236, -0.04011386763087485, -0.040415706297872, -0.04071846818658781, -0.04102215146698293, -0.041326754313851355, -0.041632274906805045, -0.04193871143025743, -0.04224606207340781, -0.0425543250302258, -0.0428634984994353, -0.043173580684499197, -0.043484569793603406, -0.04379646403964188, -0.04410926164020043, -0.04442296081754202, -0.04473755979859115, -0.04505305681491861, -0.045369450102726194, -0.04568673790283162, -0.04600491846065349, -0.04632399002619619, -0.04664395085403503, -0.04696479920330099, -0.047286533337666525, -0.04760915152532991, -0.04793265203900121, -0.04825703315588731, -0.0485822931576772, -0.04890843033052772, -0.049235442965048726, -0.049563329356288766, -0.049892087803720564, -0.050221716611227096, -0.05055221408708682, -0.05088357854395964, -0.05121580829887257, -0.05154890167320608, -0.05188285699267953, -0.05221767258733717, -0.05255334679153478, -0.05288987794392502, -0.05322726438744391, -0.05356550446929714, -0.05390459654094626, -0.05424453895809467, -0.05458533008067468, -0.05492696827283322, -0.05526945190291849, -0.05561277934346687, -0.05595694897118908, -0.05630195916695696, -0.05664780831579003, -0.05699449480684243, -0.05734201703338937, -0.05769037339281452, -0.058039562286596125, -0.05838958212029455, -0.058740431303539176, -0.059092108250014874, -0.05944461137744994, -0.05979793910760256, -0.060152089866248315, -0.06050706208316736, -0.060862854192131444, -0.06121946463089162, -0.06157689184116533, -0.061935134268624015, -0.062294190362880475, -0.06265405857747647, -0.06301473736986996, -0.06337622520142328, -0.06373852053739026, -0.06410162184690443, -0.06446552760296614, -0.06483023628243112, -0.06519574636599756, -0.06556205633819445, -0.06592916468736959, -0.06629706990567719, -0.06666577048906625, -0.06703526493726847, -0.06740555175378626, -0.06777662944588103, -0.06814849652456162, -0.06852115150457211, -0.06889459290438038, -0.06926881924616629, -0.06964382905581029, -0.07001962086288172, -0.07039619320062707, -0.07077354460595897, -0.0711516736194443, -0.07153057878529301, -0.07191025865134659, -0.07229071176906693, -0.07267193669352517, -0.07305393198339011, -0.07343669620091703, -0.07382022791193693, -0.07420452568584518, -0.07458958809559035, -0.07497541371766347, -0.07536200113208669, -0.07574934892240248, -0.07613745567566299, -0.07652631998241888, -0.07691594043670852, -0.07730631563604723, -0.07769744418141694, -0.07808932467725449, -0.07848195573144223, -0.07887533595529633, -0.079269463963557, -0.07966433837437731, -0.08005995780931313, -0.08045632089331228, -0.08085342625470449, -0.08125127252519093, -0.08164985833983351, -0.08204918233704506, -0.0824492431585786, -0.08285003944951741, -0.08325156985826482, -0.08365383303653379, -0.08405682763933714, -0.08446055232497729, -0.08486500575503597, -0.08527018659436453, -0.08567609351107408, -0.0860827251765251, -0.08649008026531801, -0.08689815745528262, -0.08730695542746914, -0.08771647286613798, -0.08812670845874981, -0.08853766089595616, -0.08894932887158946, -0.08936171108265362, -0.08977480622931427, -0.09018861301488923, -0.09060313014583898, -0.09101835633175687, -0.09143429028536043, -0.09185093072248063, -0.09226827636205381, -0.09268632592611148, -0.09310507813977109, -0.0935245317312271, -0.09394468543174112, -0.09436553797563324, -0.09478708810027237, -0.0952093345460675, -0.09563227605645802, -0.09605591137790531, -0.09648023925988264, -0.0969052584548673, -0.09733096771833089, -0.09775736580873029, -0.0981844514874991, -0.09861222351903853, -0.09904068067070837, -0.09946982171281837, -0.09989964541861945, -0.10033015056429467, -0.10076133592895059, -0.10119320029460865, -0.10162574244619638, -0.10205896117153856, -0.10249285526134905, -0.1029274235092218, -0.10336266471162248, -0.10379857766787948, -0.10423516118017617, -0.104672414053542, -0.10511033509584383, -0.10554892311777808, -0.10598817693286178, -0.10642809535742465, -0.10686867721060034, -0.10730992131431843, -0.10775182649329629, -0.10819439157503041, -0.10863761538978861, -0.10908149677060158, -0.10952603455325483, -0.10997122757628047, -0.11041707468094941, -0.11086357471126296, -0.11131072651394491, -0.11175852893843347, -0.11220698083687347, -0.11265608106410824, -0.11310582847767159, -0.11355622193778031, -0.11400726030732589, -0.11445894245186672, -0.11491126723962058, -0.11536423354145653, -0.11581784023088737, -0.11627208618406176, -0.11672697027975643, -0.11718249139936893, -0.11763864842690935, -0.11809544024899338, -0.11855286575483415, -0.1190109238362349, -0.1194696133875815, -0.1199289333058346, -0.12038888249052265, -0.12084945984373408, -0.12131066427010984, -0.12177249467683637, -0.12223494997363749, -0.12269802907276778, -0.12316173088900484, -0.12362605433964219, -0.12409099834448178, -0.12455656182582686, -0.12502274370847455, -0.12548954291970887, -0.12595695838929372, -0.1264249890494652, -0.12689363383492475, -0.12736289168283205, -0.127832761532798, -0.1283032423268775, -0.12877433300956248, -0.129246032527775, -0.12971833983086012, -0.13019125387057873, -0.13066477360110113, -0.1311388979789998, -0.1316136259632424, -0.1320889565151852, -0.13256488859856602, -0.13304142117949724, -0.13351855322645956, -0.13399628371029465, -0.13447461160419882, -0.13495353588371595, -0.13543305552673096, -0.13591316951346305, -0.13639387682645937, -0.13687517645058778, -0.13735706737303072, -0.13783954858327863, -0.1383226190731227, -0.13880627783664934, -0.13929052387023283, -0.1397753561725294, -0.14026077374447038, -0.14074677558925563, -0.14123336071234766, -0.14172052812146463, -0.1422082768265744, -0.1426966058398878, -0.1431855141758525, -0.1436750008511467, -0.1441650648846725, -0.14465570529755034, -0.14514692111311167, -0.14563871135689377, -0.14613107505663292, -0.14662401124225805, -0.14711751894588526, -0.1476115972018111, -0.14810624504650666, -0.1486014615186112, -0.14909724565892618, -0.14959359651040943, -0.150090513118169, -0.15058799452945665, -0.15108603979366264, -0.15158464796230897, -0.15208381808904364, -0.15258354922963535, -0.15308384044196643, -0.153584690786028, -0.1540860993239132, -0.1545880651198117, -0.15509058724000413, -0.1555936647528559, -0.15609729672881112, -0.1566014822403876, -0.1571062203621703, -0.157611510170806, -0.1581173507449971, -0.15862374116549677, -0.15913068051510249, -0.15963816787865037, -0.16014620234301014, -0.16065478299707875, -0.161163908931775, -0.16167357924003445, -0.16218379301680297, -0.162694549359032, -0.16320584736567206, -0.16371768613766813, -0.16423006477795365, -0.16474298239144536, -0.16525643808503715, -0.16577043096759553, -0.16628496014995325, -0.16680002474490457, -0.16731562386719978, -0.16783175663353933, -0.16834842216256918, -0.16886561957487461, -0.1693833479929755, -0.16990160654132108, -0.17042039434628398, -0.1709397105361557, -0.17145955424114112, -0.1719799245933526, -0.17250082072680573, -0.1730222417774135, -0.17354418688298145, -0.1740666551832024, -0.17458964581965092, -0.17511315793577859, -0.17563719067690892, -0.17616174319023187, -0.17668681462479924, -0.17721240413151917, -0.17773851086315123, -0.17826513397430138, -0.17879227262141695, -0.17931992596278185, -0.17984809315851097, -0.18037677337054597, -0.1809059657626496, -0.1814356695004014, -0.1819658837511921, -0.1824966076842195, -0.18302784047048282, -0.18355958128277822, -0.18409182929569373, -0.18462458368560475, -0.18515784363066906, -0.1856916083108215, -0.18622587690777004, -0.18676064860499025, -0.187295922587721, -0.1878316980429594, -0.18836797415945628, -0.1889047501277114, -0.18944202513996877, -0.18997979839021167, -0.1905180690741583, -0.19105683638925708, -0.19159609953468182, -0.1921358577113274, -0.1926761101218049, -0.1932168559704368, -0.1937580944632527, -0.19429982480798497, -0.19484204621406376, -0.1953847578926124, -0.19592795905644328, -0.19647164892005292, -0.19701582669961792, -0.19756049161299, -0.19810564287969168, -0.1986512797209123, -0.19919740135950265, -0.1997440070199712, -0.20029109592847966, -0.2008386673128385, -0.20138672040250216, -0.20193525442856547, -0.20248426862375835, -0.20303376222244235, -0.20358373446060574, -0.20413418457585936, -0.2046851118074326, -0.2052365153961685, -0.20578839458451975, -0.20634074861654478, -0.20689357673790332, -0.20744687819585145, -0.20800065223923875, -0.20855489811850259, -0.20910961508566528, -0.20966480239432872, -0.2102204592996712, -0.21077658505844288, -0.21133317892896125, -0.2118902401711073, -0.21244776804632182, -0.21300576181760073, -0.21356422074949116, -0.21412314410808736, -0.21468253116102654, -0.2152423811774854, -0.21580269342817499, -0.21636346718533778, -0.21692470172274314, -0.21748639631568312, -0.21804855024096886, -0.2186111627769266, -0.2191742332033934, -0.21973776080171348, -0.22030174485473414, -0.22086618464680186, -0.22143107946375828, -0.2219964285929366, -0.22256223132315744, -0.2231284869447251, -0.2236951947494233, -0.22426235403051203, -0.22482996408272293, -0.2253980242022563, -0.22596653368677627, -0.22653549183540794, -0.22710489794873334, -0.227674751328787, -0.22824505127905298, -0.2288157971044607, -0.22938698811138147, -0.22995862360762448, -0.23053070290243322, -0.23110322530648153, -0.2316761901318704, -0.23224959669212375, -0.2328234443021851, -0.23339773227841382, -0.23397245993858135, -0.2345476266018675, -0.23512323158885717, -0.23569927422153658, -0.23627575382328925, -0.23685266971889324, -0.23743002123451667, -0.2380078076977145, -0.23858602843742538, -0.23916468278396752, -0.23974377006903524, -0.24032328962569577, -0.2409032407883851, -0.24148362289290531, -0.24206443527642058, -0.24264567727745345, -0.2432273482358817, -0.24380944749293515, -0.24439197439119142, -0.2449749282745733, -0.24555830848834492, -0.246142114379108, -0.24672634529479903, -0.24731100058468564, -0.24789607959936322, -0.2484815816907513, -0.24906750621209067, -0.2496538525179393, -0.25024061996416996, -0.25082780790796577, -0.2514154157078178, -0.2520034427235212, -0.2525918883161721, -0.2531807518481642, -0.25377003268318554, -0.25435973018621555, -0.2549498437235208, -0.2555403726626531, -0.2561313163724449, -0.2567226742230071, -0.2573144455857251, -0.25790662983325613, -0.25849922633952566, -0.25909223447972435, -0.2596856536303045, -0.2602794831689776, -0.2608737224747108, -0.26146837092772324, -0.2620634279094836, -0.2626588928027068, -0.2632547649913506, -0.26385104386061237, -0.2644477287969267, -0.26504481918796174, -0.2656423144226159, -0.26624021389101526, -0.2668385169845103, -0.2674372230956726, -0.26803633161829216, -0.26863584194737405, -0.26923575347913575, -0.2698360656110035, -0.2704367777416096, -0.27103788927078987, -0.2716393995995799, -0.27224130813021213, -0.27284361426611353, -0.27344631741190184, -0.274049416973383, -0.2746529123575481, -0.27525680297257044, -0.2758610882278028, -0.276465767533774, -0.2770708403021862, -0.27767630594591236, -0.2782821638789929, -0.2788884135166327, -0.2794950542751988, -0.2801020855722166, -0.28070950682636814, -0.28131731745748817, -0.28192551688656187, -0.28253410453572203, -0.2831430798282457, -0.2837524421885519, -0.28436219104219873, -0.2849723258158803, -0.2855828459374238, -0.2861937508357876, -0.2868050399410569, -0.2874167126844427, -0.28802876849827763, -0.288641206816014, -0.2892540270722207, -0.28986722870258064, -0.29048081114388746, -0.2910947738340438, -0.29170911621205764, -0.29232383771804027, -0.2929389377932028, -0.2935544158798543, -0.2941702714213985, -0.2947865038623314, -0.2954031126482386, -0.2960200972257926, -0.2966374570427498, -0.29725519154794844, -0.2978733001913052, -0.2984917824238137, -0.29911063769754065, -0.29972986546562386, -0.3003494651822697, -0.3009694363027501, -0.30158977828340006, -0.30221049058161553, -0.30283157265585026, -0.30345302396561347, -0.3040748439714671, -0.3046970321350233, -0.3053195879189423, -0.30594251078692913, -0.3065658002037318, -0.3071894556351382, -0.3078134765479741, -0.30843786241009974, -0.30906261269040836, -0.3096877268588234, -0.3103132043862953, -0.3109390447448003, -0.3115652474073366, -0.31219181184792266, -0.31281873754159484, -0.31344602396440435, -0.3140736705934155, -0.3147016769067027, -0.31533004238334783, -0.3159587665034389, -0.31658784874806634, -0.3172172885993215, -0.31784708554029395, -0.3184772390550683, -0.3191077486287236, -0.3197386137473289, -0.3203698338979425, -0.3210014085686088, -0.32163333724835586, -0.322265619427193, -0.32289825459610944, -0.3235312422470704, -0.3241645818730159, -0.3247982729678581, -0.32543231502647896, -0.32606670754472733, -0.3267014500194181, -0.3273365419483283, -0.3279719828301958, -0.3286077721647167, -0.3292439094525428, -0.3298803941952799, -0.3305172258954848, -0.33115440405666396, -0.3317919281832701, -0.3324297977807009, -0.33306801235529615, -0.333706571414336, -0.33434547446603846, -0.334984721019557, -0.33562431058497855, -0.3362642426733212, -0.3369045167965323, -0.33754513246748563, -0.3381860891999796, -0.33882738650873534, -0.3394690239093937, -0.3401110009185141, -0.34075331705357115, -0.3413959718329538, -0.34203896477596196, -0.34268229540280526, -0.3433259632346005, -0.3439699677933695, -0.3446143086020369, -0.3452589851844282, -0.34590399706526787, -0.3465493437701763, -0.3471950248256692, -0.3478410397591539, -0.3484873880989281, -0.3491340693741779, -0.34978108311497513, -0.350428428852276, -0.3510761061179182, -0.35172411444461926, -0.3523724533659749, -0.3530211224164562, -0.35367012113140783, -0.35431944904704615, -0.35496910570045714, -0.3556190906295942, -0.35626940337327595, -0.3569200434711851, -0.3575710104638655, -0.35822230389272014, -0.35887392330000945, -0.3595258682288499, -0.36017813822321065, -0.36083073282791256, -0.36148365158862616, -0.362136894051869, -0.36279045976500424, -0.3634443482762387, -0.36409855913462075, -0.36475309189003824, -0.3654079460932167, -0.36606312129571705, -0.3667186170499346, -0.3673744329090959, -0.36803056842725745, -0.36868702315930413, -0.3693437966609463, -0.3700008884887187, -0.3706582981999783, -0.3713160253529023, -0.3719740695064865, -0.3726324302205427, -0.3732911070556978, -0.3739500995733913, -0.37460940733587345, -0.3752690299062036, -0.3759289668482483, -0.37658921772667897, -0.3772497821069709, -0.3779106595554005, -0.37857184963904433, -0.37923335192577645, -0.379895165984267, -0.3805572913839803, -0.3812197276951731, -0.3818824744888929, -0.3825455313369752, -0.38320889781204315, -0.3838725734875048, -0.38453655793755137, -0.3852008507371554, -0.3858654514620694, -0.3865303596888241, -0.3871955749947258, -0.3878610969578554, -0.3885269251570663, -0.389193059171983, -0.3898594985829986, -0.39052624297127403, -0.3911932919187352, -0.3918606450080724, -0.39252830182273757, -0.393196261946943, -0.3938645249656597, -0.3945330904646155, -0.39520195803029345, -0.39587112724992957, -0.3965405977115123, -0.3972103690037794, -0.3978804407162171, -0.39855081243905854, -0.3992214837632815, -0.3998924542806065, -0.40056372358349634, -0.40123529126515334, -0.4019071569195177, -0.4025793201412664, -0.4032517805258111, -0.4039245376692966, -0.40459759116859906, -0.40527094062132485, -0.40594458562580793, -0.40661852578110946, -0.4072927606870147, -0.40796728994403275, -0.40864211315339427, -0.40931722991704944, -0.4099926398376674, -0.4106683425186337, -0.41134433756404887, -0.41202062457872723, -0.4126972031681949, -0.41337407293868855, -0.414051233497153, -0.41472868445124056, -0.41540642540930905, -0.4160844559804202, -0.41676277577433785, -0.41744138440152734, -0.41812028147315206, -0.4187994666010739, -0.41947893939785097, -0.420158699476735, -0.4208387464516715, -0.4215190799372972, -0.4221996995489382, -0.4228806049026096, -0.42356179561501284, -0.4242432713035348, -0.42492503158624584, -0.4256070760818992, -0.42628940440992746, -0.426972016190444, -0.42765491104423836, -0.4283380885927771, -0.42902154845820084]
**** log-parameter_analysis 运行时间： 2025-01-17 23:21:39 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.07551143441603081
DataOwner2: noise random: 0.04070712554427178
DataOwner3: noise random: 0.02823653622747249
DataOwner4: noise random: 0.019764649839168216
DataOwner5: noise random: 0.08515412716727835
DataOwner6: noise random: 0.020087710822993522
DataOwner7: noise random: 0.009880279141453775
DataOwner8: noise random: 0.049396118774037226
DataOwner9: noise random: 0.07307698522038925
DataOwner10: noise random: 0.03084922818151108
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9977008621642304, 0.9993295748813182, 0.9996777088468973, 0.999842078254062, 0.9970687507188313, 0.9998365332926609, 0.9999604666760351, 0.999011726033934, 0.9978424636474736, 0.9996158289167915]
归一化后的数据质量列表avg_f_list: [0.9218593891915415, 0.9781827882110898, 0.9902217979454941, 0.9959059456832833, 0.9, 0.995714192361615, 1.0, 0.9671910845967546, 0.9267561869869998, 0.9880818944756629]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3487
DataOwner1的最优x_1 = 0.0734
DataOwner2的最优x_2 = 0.1342
DataOwner3的最优x_3 = 0.1453
DataOwner4的最优x_4 = 0.1504
DataOwner5的最优x_5 = 0.0454
DataOwner6的最优x_6 = 0.1502
DataOwner7的最优x_7 = 0.1539
DataOwner8的最优x_8 = 0.1235
DataOwner9的最优x_9 = 0.0793
DataOwner10的最优x_10 = 0.1434
每个DataOwner应该贡献数据比例 xn_list = [0.0734177478124918, 0.1341659241550416, 0.14530712131282325, 0.15036705600219624, 0.04541205870108592, 0.15019839819440925, 0.15393457224031828, 0.12346742500970169, 0.07932317853509877, 0.14336943550656742]
ModelOwner的最大效用 U(Eta) = 0.5898
DONE
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.348729244257586
DataOwner1的分配到的支付 ： 0.0779
DataOwner2的分配到的支付 ： 0.1511
DataOwner3的分配到的支付 ： 0.1657
DataOwner4的分配到的支付 ： 0.1724
DataOwner5的分配到的支付 ： 0.0471
DataOwner6的分配到的支付 ： 0.1722
DataOwner7的分配到的支付 ： 0.1772
DataOwner8的分配到的支付 ： 0.1375
DataOwner9的分配到的支付 ： 0.0846
DataOwner10的分配到的支付 ： 0.1631
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC1', 'DataOwner2': 'CPC2', 'DataOwner3': 'CPC3', 'DataOwner4': 'CPC10', 'DataOwner5': 'CPC4', 'DataOwner6': 'CPC5', 'DataOwner7': 'CPC6', 'DataOwner8': 'CPC7', 'DataOwner9': 'CPC8', 'DataOwner10': 'CPC9'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC2
DataOwner3 把数据交给 CPC3
DataOwner4 把数据交给 CPC10
DataOwner5 把数据交给 CPC4
DataOwner6 把数据交给 CPC5
DataOwner7 把数据交给 CPC6
DataOwner8 把数据交给 CPC7
DataOwner9 把数据交给 CPC8
DataOwner10 把数据交给 CPC9
DONE
----- literation 1: 模型训练 -----
CPC1调整模型中, 本轮训练的数据量为：440.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 26.82%
Epoch 1/5, Loss: 2.2953
Epoch 2/5, Loss: 2.2939
Epoch 3/5, Loss: 2.2922
Epoch 4/5, Loss: 2.2906
Epoch 5/5, Loss: 2.2884
新模型评估：
Accuracy: 34.56%
Model saved to ../../../data/model/mnist_cnn_model
CPC2调整模型中, 本轮训练的数据量为：1006.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 34.56%
Epoch 1/5, Loss: 2.2897
Epoch 2/5, Loss: 2.2836
Epoch 3/5, Loss: 2.2767
Epoch 4/5, Loss: 2.2680
Epoch 5/5, Loss: 2.2571
新模型评估：
Accuracy: 45.51%
Model saved to ../../../data/model/mnist_cnn_model
CPC3调整模型中, 本轮训练的数据量为：1307.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 45.51%
Epoch 1/5, Loss: 2.2383
Epoch 2/5, Loss: 2.2214
Epoch 3/5, Loss: 2.1992
Epoch 4/5, Loss: 2.1735
Epoch 5/5, Loss: 2.1435
新模型评估：
Accuracy: 50.19%
Model saved to ../../../data/model/mnist_cnn_model
CPC10调整模型中, 本轮训练的数据量为：1127.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 50.19%
Epoch 1/5, Loss: 2.1211
Epoch 2/5, Loss: 2.0934
Epoch 3/5, Loss: 2.0661
Epoch 4/5, Loss: 2.0373
Epoch 5/5, Loss: 2.0011
新模型评估：
Accuracy: 53.05%
Model saved to ../../../data/model/mnist_cnn_model
CPC4调整模型中, 本轮训练的数据量为：136.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 53.05%
Epoch 1/5, Loss: 2.0714
Epoch 2/5, Loss: 2.0280
Epoch 3/5, Loss: 2.0310
Epoch 4/5, Loss: 2.0262
Epoch 5/5, Loss: 1.9987
新模型评估：
Accuracy: 54.05%
Model saved to ../../../data/model/mnist_cnn_model
CPC5调整模型中, 本轮训练的数据量为：1126.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 54.05%
Epoch 1/5, Loss: 1.9483
Epoch 2/5, Loss: 1.9105
Epoch 3/5, Loss: 1.8734
Epoch 4/5, Loss: 1.8332
Epoch 5/5, Loss: 1.7916
新模型评估：
Accuracy: 57.10%
Model saved to ../../../data/model/mnist_cnn_model
CPC6调整模型中, 本轮训练的数据量为：1385.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 57.10%
Epoch 1/5, Loss: 1.7618
Epoch 2/5, Loss: 1.7137
Epoch 3/5, Loss: 1.6660
Epoch 4/5, Loss: 1.6110
Epoch 5/5, Loss: 1.5606
新模型评估：
Accuracy: 64.97%
Model saved to ../../../data/model/mnist_cnn_model
CPC7调整模型中, 本轮训练的数据量为：370.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 64.97%
Epoch 1/5, Loss: 1.4975
Epoch 2/5, Loss: 1.4781
Epoch 3/5, Loss: 1.4657
Epoch 4/5, Loss: 1.4401
Epoch 5/5, Loss: 1.4294
新模型评估：
Accuracy: 65.48%
Model saved to ../../../data/model/mnist_cnn_model
CPC8调整模型中, 本轮训练的数据量为：237.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 65.48%
Epoch 1/5, Loss: 1.4629
Epoch 2/5, Loss: 1.4399
Epoch 3/5, Loss: 1.4234
Epoch 4/5, Loss: 1.4250
Epoch 5/5, Loss: 1.3986
新模型评估：
Accuracy: 67.28%
Model saved to ../../../data/model/mnist_cnn_model
CPC9调整模型中, 本轮训练的数据量为：645.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 67.28%
Epoch 1/5, Loss: 1.4006
Epoch 2/5, Loss: 1.3435
Epoch 3/5, Loss: 1.3282
Epoch 4/5, Loss: 1.2704
Epoch 5/5, Loss: 1.2457
新模型评估：
Accuracy: 69.72%
Model saved to ../../../data/model/mnist_cnn_model
DONE
========================= literation: 2 =========================
----- literation 2: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3487
DataOwner1的最优x_1 = 0.0734
DataOwner2的最优x_2 = 0.1342
DataOwner3的最优x_3 = 0.1453
DataOwner4的最优x_4 = 0.1504
DataOwner5的最优x_5 = 0.0454
DataOwner6的最优x_6 = 0.1502
DataOwner7的最优x_7 = 0.1539
DataOwner8的最优x_8 = 0.1235
DataOwner9的最优x_9 = 0.0793
DataOwner10的最优x_10 = 0.1434
每个DataOwner应该贡献数据比例 xn_list = [0.0734177478124918, 0.1341659241550416, 0.14530712131282325, 0.15036705600219624, 0.04541205870108592, 0.15019839819440925, 0.15393457224031828, 0.12346742500970169, 0.07932317853509877, 0.14336943550656742]
ModelOwner的最大效用 U(Eta) = 0.5898
DONE
----- literation 2: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.348729244257586
DataOwner1的分配到的支付 ： 0.0779
DataOwner2的分配到的支付 ： 0.1511
DataOwner3的分配到的支付 ： 0.1657
DataOwner4的分配到的支付 ： 0.1724
DataOwner5的分配到的支付 ： 0.0471
DataOwner6的分配到的支付 ： 0.1722
DataOwner7的分配到的支付 ： 0.1772
DataOwner8的分配到的支付 ： 0.1375
DataOwner9的分配到的支付 ： 0.0846
DataOwner10的分配到的支付 ： 0.1631
DONE
----- literation 2: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC2
DataOwner3 把数据交给 CPC3
DataOwner4 把数据交给 CPC10
DataOwner5 把数据交给 CPC4
DataOwner6 把数据交给 CPC5
DataOwner7 把数据交给 CPC6
DataOwner8 把数据交给 CPC7
DataOwner9 把数据交给 CPC8
DataOwner10 把数据交给 CPC9
DONE
----- literation 2: 模型训练 -----
重新调整fn，进而调整xn、Eta
正在评估DataOwner1的数据质量, 本轮评估的样本数据量为：440.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 69.72%
Epoch 1/5, Loss: 1.3210
Epoch 2/5, Loss: 1.2941
Epoch 3/5, Loss: 1.2751
Epoch 4/5, Loss: 1.2572
Epoch 5/5, Loss: 1.2402
新模型评估：
Accuracy: 68.34%
loss差为：
0.0807795354298182
单位数据loss差为：
0.0001835898532495868
正在评估DataOwner2的数据质量, 本轮评估的样本数据量为：1006.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 69.72%
Epoch 1/5, Loss: 1.2855
Epoch 2/5, Loss: 1.2469
Epoch 3/5, Loss: 1.2118
Epoch 4/5, Loss: 1.1751
Epoch 5/5, Loss: 1.1425
新模型评估：
Accuracy: 71.50%
loss差为：
0.14302995055913925
单位数据loss差为：
0.00014217688922379647
正在评估DataOwner3的数据质量, 本轮评估的样本数据量为：1307.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 69.72%
Epoch 1/5, Loss: 1.2111
Epoch 2/5, Loss: 1.1683
Epoch 3/5, Loss: 1.1118
Epoch 4/5, Loss: 1.0661
Epoch 5/5, Loss: 1.0312
新模型评估：
Accuracy: 72.71%
loss差为：
0.17994001649674907
单位数据loss差为：
0.00013767407536094037
正在评估DataOwner4的数据质量, 本轮评估的样本数据量为：1127.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 69.72%
Epoch 1/5, Loss: 1.3004
Epoch 2/5, Loss: 1.2575
Epoch 3/5, Loss: 1.2222
Epoch 4/5, Loss: 1.1838
Epoch 5/5, Loss: 1.1414
新模型评估：
Accuracy: 74.52%
loss差为：
0.15907244549857258
单位数据loss差为：
0.00014114680168462519
正在评估DataOwner5的数据质量, 本轮评估的样本数据量为：136.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 69.72%
Epoch 1/5, Loss: 1.2964
Epoch 2/5, Loss: 1.2764
Epoch 3/5, Loss: 1.2712
Epoch 4/5, Loss: 1.2412
Epoch 5/5, Loss: 1.2548
新模型评估：
Accuracy: 70.05%
loss差为：
0.0415571928024292
单位数据loss差为：
0.00030556759413550883
正在评估DataOwner6的数据质量, 本轮评估的样本数据量为：1126.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 69.72%
Epoch 1/5, Loss: 1.3085
Epoch 2/5, Loss: 1.2639
Epoch 3/5, Loss: 1.2279
Epoch 4/5, Loss: 1.1903
Epoch 5/5, Loss: 1.1517
新模型评估：
Accuracy: 74.38%
loss差为：
0.15677164660559773
单位数据loss差为：
0.00013922881581314184
正在评估DataOwner7的数据质量, 本轮评估的样本数据量为：1385.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 69.72%
Epoch 1/5, Loss: 1.2718
Epoch 2/5, Loss: 1.2212
Epoch 3/5, Loss: 1.1769
Epoch 4/5, Loss: 1.1290
Epoch 5/5, Loss: 1.0830
新模型评估：
Accuracy: 74.07%
loss差为：
0.18882001800970594
单位数据loss差为：
0.00013633214296729671
正在评估DataOwner8的数据质量, 本轮评估的样本数据量为：370.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 69.72%
Epoch 1/5, Loss: 1.2820
Epoch 2/5, Loss: 1.2566
Epoch 3/5, Loss: 1.2437
Epoch 4/5, Loss: 1.2288
Epoch 5/5, Loss: 1.2178
新模型评估：
Accuracy: 70.99%
loss差为：
0.06415724754333496
单位数据loss差为：
0.00017339796633333772
正在评估DataOwner9的数据质量, 本轮评估的样本数据量为：237.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 69.72%
Epoch 1/5, Loss: 1.3422
Epoch 2/5, Loss: 1.3410
Epoch 3/5, Loss: 1.3269
Epoch 4/5, Loss: 1.3086
Epoch 5/5, Loss: 1.2919
新模型评估：
Accuracy: 68.84%
loss差为：
0.050275593996047974
单位数据loss差为：
0.00021213330800020242
正在评估DataOwner10的数据质量, 本轮评估的样本数据量为：645.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 69.72%
Epoch 1/5, Loss: 1.2823
Epoch 2/5, Loss: 1.2752
Epoch 3/5, Loss: 1.2705
Epoch 4/5, Loss: 1.2100
Epoch 5/5, Loss: 1.2285
新模型评估：
Accuracy: 70.02%
loss差为：
0.05386860804124316
单位数据loss差为：
8.351722176936925e-05
经过服务器调节后的真实数据质量：
数据质量列表avg_f_list: [0.0001835898532495868, 0.00014217688922379647, 0.00013767407536094037, 0.00014114680168462519, 0.00030556759413550883, 0.00013922881581314184, 0.00013633214296729671, 0.00017339796633333772, 0.00021213330800020242, 8.351722176936925e-05]
归一化后的数据质量列表avg_f_list:[0.9450675359891798, 0.9264172794800376, 0.924389445068019, 0.9259533813436847, 1.0, 0.9250896197336295, 0.9237851081424178, 0.940477637396511, 0.9579220313212345, 0.9]
CPC1调整模型中, 本轮训练的数据量为：440.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 69.72%
Epoch 1/5, Loss: 1.3213
Epoch 2/5, Loss: 1.2984
Epoch 3/5, Loss: 1.2765
Epoch 4/5, Loss: 1.2597
Epoch 5/5, Loss: 1.2430
新模型评估：
Accuracy: 68.10%
CPC2调整模型中, 本轮训练的数据量为：1006.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 69.72%
Epoch 1/5, Loss: 1.2880
Epoch 2/5, Loss: 1.2480
Epoch 3/5, Loss: 1.2112
Epoch 4/5, Loss: 1.1762
Epoch 5/5, Loss: 1.1460
新模型评估：
Accuracy: 71.98%
Model saved to ../../../data/model/mnist_cnn_model
CPC3调整模型中, 本轮训练的数据量为：1307.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 71.98%
Epoch 1/5, Loss: 1.0434
Epoch 2/5, Loss: 0.9942
Epoch 3/5, Loss: 0.9593
Epoch 4/5, Loss: 0.9176
Epoch 5/5, Loss: 0.8761
新模型评估：
Accuracy: 74.31%
Model saved to ../../../data/model/mnist_cnn_model
CPC10调整模型中, 本轮训练的数据量为：1127.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 74.31%
Epoch 1/5, Loss: 0.9407
Epoch 2/5, Loss: 0.9072
Epoch 3/5, Loss: 0.8756
Epoch 4/5, Loss: 0.8491
Epoch 5/5, Loss: 0.8215
新模型评估：
Accuracy: 76.92%
Model saved to ../../../data/model/mnist_cnn_model
CPC4调整模型中, 本轮训练的数据量为：136.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 76.92%
Epoch 1/5, Loss: 0.8637
Epoch 2/5, Loss: 0.8577
Epoch 3/5, Loss: 0.8629
Epoch 4/5, Loss: 0.6564
Epoch 5/5, Loss: 0.7263
新模型评估：
Accuracy: 76.70%
CPC5调整模型中, 本轮训练的数据量为：1126.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 76.92%
Epoch 1/5, Loss: 0.8222
Epoch 2/5, Loss: 0.7969
Epoch 3/5, Loss: 0.7703
Epoch 4/5, Loss: 0.7499
Epoch 5/5, Loss: 0.7316
新模型评估：
Accuracy: 78.42%
Model saved to ../../../data/model/mnist_cnn_model
CPC6调整模型中, 本轮训练的数据量为：1385.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.42%
Epoch 1/5, Loss: 0.6894
Epoch 2/5, Loss: 0.6640
Epoch 3/5, Loss: 0.6435
Epoch 4/5, Loss: 0.6192
Epoch 5/5, Loss: 0.6014
新模型评估：
Accuracy: 78.12%
CPC7调整模型中, 本轮训练的数据量为：370.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.42%
Epoch 1/5, Loss: 0.6607
Epoch 2/5, Loss: 0.6402
Epoch 3/5, Loss: 0.6333
Epoch 4/5, Loss: 0.6249
Epoch 5/5, Loss: 0.6130
新模型评估：
Accuracy: 76.60%
CPC8调整模型中, 本轮训练的数据量为：237.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.42%
Epoch 1/5, Loss: 0.7149
Epoch 2/5, Loss: 0.7142
Epoch 3/5, Loss: 0.6970
Epoch 4/5, Loss: 0.6873
Epoch 5/5, Loss: 0.6726
新模型评估：
Accuracy: 74.87%
CPC9调整模型中, 本轮训练的数据量为：645.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.42%
Epoch 1/5, Loss: 0.7437
Epoch 2/5, Loss: 0.6877
Epoch 3/5, Loss: 0.6787
Epoch 4/5, Loss: 0.6935
Epoch 5/5, Loss: 0.6586
新模型评估：
Accuracy: 76.25%
DONE
========================= literation: 3 =========================
----- literation 3: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3132
DataOwner1的最优x_1 = 0.1269
DataOwner2的最优x_2 = 0.1081
DataOwner3的最优x_3 = 0.1059
DataOwner4的最优x_4 = 0.1076
DataOwner5的最优x_5 = 0.1742
DataOwner6的最优x_6 = 0.1066
DataOwner7的最优x_7 = 0.1053
DataOwner8的最优x_8 = 0.1224
DataOwner9的最优x_9 = 0.1391
DataOwner10的最优x_10 = 0.0784
每个DataOwner应该贡献数据比例 xn_list = [0.12693609243944445, 0.10805307459901223, 0.10590174703339068, 0.10756267802204363, 0.17415713178396075, 0.10664681035408176, 0.10525675031969639, 0.12243608624519058, 0.13905298594901133, 0.07840149914229286]
ModelOwner的最大效用 U(Eta) = 0.5494
xn开始变化：
DONE
----- literation 3: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3132073461145717
DataOwner1的分配到的支付 ： 0.5702
DataOwner2的分配到的支付 ： 0.0750
DataOwner3的分配到的支付 ： 0.0810
DataOwner4的分配到的支付 ： 0.0840
DataOwner5的分配到的支付 ： 0.1051
DataOwner6的分配到的支付 ： 0.0838
DataOwner7的分配到的支付 ： 0.0858
DataOwner8的分配到的支付 ： 0.0701
DataOwner9的分配到的支付 ： 0.0804
DataOwner10的分配到的支付 ： 0.0779
DONE
----- literation 3: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC2
DataOwner3 把数据交给 CPC3
DataOwner4 把数据交给 CPC10
DataOwner5 把数据交给 CPC4
DataOwner6 把数据交给 CPC5
DataOwner7 把数据交给 CPC6
DataOwner8 把数据交给 CPC7
DataOwner9 把数据交给 CPC8
DataOwner10 把数据交给 CPC9
DONE
----- literation 3: 模型训练 -----
CPC1调整模型中, 本轮训练的数据量为：6000.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.42%
Epoch 1/5, Loss: 0.6606
Epoch 2/5, Loss: 0.5874
Epoch 3/5, Loss: 0.5346
Epoch 4/5, Loss: 0.4954
Epoch 5/5, Loss: 0.4630
新模型评估：
Accuracy: 75.68%
CPC2调整模型中, 本轮训练的数据量为：1006.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.42%
Epoch 1/5, Loss: 0.6975
Epoch 2/5, Loss: 0.6777
Epoch 3/5, Loss: 0.6600
Epoch 4/5, Loss: 0.6441
Epoch 5/5, Loss: 0.6285
新模型评估：
Accuracy: 77.90%
CPC3调整模型中, 本轮训练的数据量为：1307.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.42%
Epoch 1/5, Loss: 0.6560
Epoch 2/5, Loss: 0.6212
Epoch 3/5, Loss: 0.6050
Epoch 4/5, Loss: 0.5861
Epoch 5/5, Loss: 0.5619
新模型评估：
Accuracy: 77.93%
CPC10调整模型中, 本轮训练的数据量为：1127.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.42%
Epoch 1/5, Loss: 0.6991
Epoch 2/5, Loss: 0.6769
Epoch 3/5, Loss: 0.6650
Epoch 4/5, Loss: 0.6402
Epoch 5/5, Loss: 0.6241
新模型评估：
Accuracy: 78.57%
Model saved to ../../../data/model/mnist_cnn_model
CPC4调整模型中, 本轮训练的数据量为：522.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.57%
Epoch 1/5, Loss: 0.6259
Epoch 2/5, Loss: 0.6237
Epoch 3/5, Loss: 0.6111
Epoch 4/5, Loss: 0.5995
Epoch 5/5, Loss: 0.6075
新模型评估：
Accuracy: 75.93%
CPC5调整模型中, 本轮训练的数据量为：1126.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.57%
Epoch 1/5, Loss: 0.6059
Epoch 2/5, Loss: 0.5817
Epoch 3/5, Loss: 0.5728
Epoch 4/5, Loss: 0.5569
Epoch 5/5, Loss: 0.5402
新模型评估：
Accuracy: 77.72%
CPC6调整模型中, 本轮训练的数据量为：1385.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.57%
Epoch 1/5, Loss: 0.6053
Epoch 2/5, Loss: 0.5855
Epoch 3/5, Loss: 0.5657
Epoch 4/5, Loss: 0.5510
Epoch 5/5, Loss: 0.5380
新模型评估：
Accuracy: 77.61%
CPC7调整模型中, 本轮训练的数据量为：370.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.57%
Epoch 1/5, Loss: 0.5927
Epoch 2/5, Loss: 0.5796
Epoch 3/5, Loss: 0.5684
Epoch 4/5, Loss: 0.5621
Epoch 5/5, Loss: 0.5552
新模型评估：
Accuracy: 76.70%
CPC8调整模型中, 本轮训练的数据量为：417.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.57%
Epoch 1/5, Loss: 0.6327
Epoch 2/5, Loss: 0.6122
Epoch 3/5, Loss: 0.6099
Epoch 4/5, Loss: 0.5992
Epoch 5/5, Loss: 0.5893
新模型评估：
Accuracy: 76.07%
CPC9调整模型中, 本轮训练的数据量为：645.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 78.57%
Epoch 1/5, Loss: 0.6285
Epoch 2/5, Loss: 0.5637
Epoch 3/5, Loss: 0.5787
Epoch 4/5, Loss: 0.5366
Epoch 5/5, Loss: 0.5277
新模型评估：
Accuracy: 76.28%
DONE
最终的列表：
[9.084331145450819e-07, -0.00014935838399141727, -0.00044739838236248705, -0.000889910813844319, -0.0014736925070257534, -0.002195634287985644, -0.00305271755744288, -0.004042011016393676, -0.005160667532774915, -0.006405921142117771, -0.007775084175554281, -0.009265544508912288, -0.010874762926984416, -0.012600270597384, -0.014439666648709104, -0.016390615848023732, -0.018450846372937646, -0.02061814767381817, -0.022890368421910362, -0.02526541453936318, -0.027741247307373262, -0.030315881548855672, -0.03298738388223907, -0.035753871043158414, -0.0386135082709855, -0.041564507757293506, -0.044605127153498986, -0.04773366813506552, -0.0509484750197835, -0.05424793343776402, -0.057630469050904076, -0.061094546319687804, -0.0646386673152971, -0.06826137057509823, -0.07196122999967047, -0.07573685378962713, -0.0795868834205638, -0.08350999265454845, -0.08750488658664224, -0.0915703007250126, -0.09570500010326372, -0.09990777842367721, -0.1041774572301139, -0.108512885109386, -0.11291293691996246, -0.11737651304692204, -0.12190253868211859, -0.12648996312856858, -0.13113775912811432, -0.13584492221146122, -0.14061047006972288, -0.14543344194664992, -0.15031289805075237, -0.1552479189865596, -0.16023760520429486, -0.16528107646727375, -0.17037747133636189, -0.17552594667086086, -0.1807256771452112, -0.18597585478093492, -0.19127568849325532, -0.1966244036518638, -0.2020212416553192, -0.20746545951858947, -0.21295632947326387, -0.21849313857998387, -0.22407518835266155, -0.22970179439406535, -0.23537228604237725, -0.24108600602833757, -0.24684231014260716, -0.2526405669129953, -0.25848015729121426, -0.2643604743488309, -0.2702809229821063, -0.27624091962541714, -0.2822398919729732, -0.28827727870854913, -0.294352529242964, -0.3004651034590513, -0.30661447146386833, -0.31280011334791047, -0.3190215189510974, -0.3252781876353108, -0.3315696280632706, -0.33789535798354653, -0.3442549040215024, -0.35064780147598884, -0.3570735941215981, -0.36353183401630196, -0.3700220813143077, -0.3765439040839629, -0.3830968781305548, -0.389680586823847, -0.39629462093021184, -0.4029385784492111, -0.40961206445449116, -0.41631469093885975, -0.42304607666341587, -0.42980584701060964]
**** log-parameter_analysis 运行时间： 2025-01-17 23:45:20 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.05865967695000649
DataOwner2: noise random: 0.017118920390866776
DataOwner3: noise random: 0.0556531704516965
DataOwner4: noise random: 0.003336871706305178
DataOwner5: noise random: 0.08199438610118182
DataOwner6: noise random: 0.01958503040917018
DataOwner7: noise random: 0.0873878437757346
DataOwner8: noise random: 0.03196876253454185
DataOwner9: noise random: 0.027561147637308294
DataOwner10: noise random: 0.036046981169154016
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9986089767412821, 0.9998814855991429, 0.9987455181226688, 0.9999955010011535, 0.9972687806704239, 0.9998446133750168, 0.99692381698804, 0.9995861679373431, 0.9996929183539068, 0.9994728275897179]
归一化后的数据质量列表avg_f_list: [0.9548611037479083, 0.9962881793334255, 0.9593062674041867, 1.0, 0.9112304417027011, 0.9950877881483733, 0.9, 0.986673985277688, 0.9901492912045992, 0.9829841412982502]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3503
DataOwner1的最优x_1 = 0.1096
DataOwner2的最优x_2 = 0.1497
DataOwner3的最优x_3 = 0.1143
DataOwner4的最优x_4 = 0.1530
DataOwner5的最优x_5 = 0.0587
DataOwner6的最优x_6 = 0.1486
DataOwner7的最优x_7 = 0.0438
DataOwner8的最优x_8 = 0.1410
DataOwner9的最优x_9 = 0.1442
DataOwner10的最优x_10 = 0.1376
每个DataOwner应该贡献数据比例 xn_list = [0.10961210378669053, 0.1497013727018006, 0.11427135324392541, 0.15295129020735582, 0.058650358807870465, 0.14863906590895334, 0.043840364457304576, 0.14103496181834013, 0.144209701129785, 0.13761106142760743]
ModelOwner的最大效用 U(Eta) = 0.5916
DONE
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.350259967138263
DataOwner1的分配到的支付 ： 0.1203
DataOwner2的分配到的支付 ： 0.1715
DataOwner3的分配到的支付 ： 0.1260
DataOwner4的分配到的支付 ： 0.1759
DataOwner5的分配到的支付 ： 0.0614
DataOwner6的分配到的支付 ： 0.1701
DataOwner7的分配到的支付 ： 0.0454
DataOwner8的分配到的支付 ： 0.1600
DataOwner9的分配到的支付 ： 0.1642
DataOwner10的分配到的支付 ： 0.1555
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC2', 'DataOwner3': 'CPC1', 'DataOwner2': 'CPC3', 'DataOwner4': 'CPC4', 'DataOwner5': 'CPC5', 'DataOwner6': 'CPC6', 'DataOwner7': 'CPC7', 'DataOwner8': 'CPC8', 'DataOwner9': 'CPC9', 'DataOwner10': 'CPC10'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC2
DataOwner3 把数据交给 CPC1
DataOwner2 把数据交给 CPC3
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC8
DataOwner9 把数据交给 CPC9
DataOwner10 把数据交给 CPC10
DONE
----- literation 1: 模型训练 -----
CPC2调整模型中, 本轮训练的数据量为：212.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 26.82%
Epoch 1/5, Loss: 2.2869
Epoch 2/5, Loss: 2.2989
Epoch 3/5, Loss: 2.2937
Epoch 4/5, Loss: 2.2919
Epoch 5/5, Loss: 2.2836
新模型评估：
Accuracy: 28.91%
Model saved to ../../../data/model/mnist_cnn_model
CPC1调整模型中, 本轮训练的数据量为：442.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 28.91%
Epoch 1/5, Loss: 2.2951
Epoch 2/5, Loss: 2.2930
Epoch 3/5, Loss: 2.2905
Epoch 4/5, Loss: 2.2886
Epoch 5/5, Loss: 2.2859
新模型评估：
Accuracy: 34.50%
Model saved to ../../../data/model/mnist_cnn_model
CPC3调整模型中, 本轮训练的数据量为：1158.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 34.50%
Epoch 1/5, Loss: 2.2802
Epoch 2/5, Loss: 2.2793
Epoch 3/5, Loss: 2.2668
Epoch 4/5, Loss: 2.2487
Epoch 5/5, Loss: 2.2326
新模型评估：
Accuracy: 43.94%
Model saved to ../../../data/model/mnist_cnn_model
CPC4调整模型中, 本轮训练的数据量为：2072.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 43.94%
Epoch 1/5, Loss: 2.2021
Epoch 2/5, Loss: 2.1649
Epoch 3/5, Loss: 2.1142
Epoch 4/5, Loss: 2.0547
Epoch 5/5, Loss: 1.9768
新模型评估：
Accuracy: 53.05%
Model saved to ../../../data/model/mnist_cnn_model
CPC5调整模型中, 本轮训练的数据量为：113.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 53.05%
Epoch 1/5, Loss: 1.9571
Epoch 2/5, Loss: 1.9578
Epoch 3/5, Loss: 1.9534
Epoch 4/5, Loss: 1.9514
Epoch 5/5, Loss: 1.9341
新模型评估：
Accuracy: 52.85%
CPC6调整模型中, 本轮训练的数据量为：575.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 53.05%
Epoch 1/5, Loss: 1.9284
Epoch 2/5, Loss: 1.9068
Epoch 3/5, Loss: 1.8855
Epoch 4/5, Loss: 1.8639
Epoch 5/5, Loss: 1.8414
新模型评估：
Accuracy: 52.87%
CPC7调整模型中, 本轮训练的数据量为：84.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 53.05%
Epoch 1/5, Loss: 1.9428
Epoch 2/5, Loss: 1.9090
Epoch 3/5, Loss: 1.9262
Epoch 4/5, Loss: 1.9631
Epoch 5/5, Loss: 1.9381
新模型评估：
Accuracy: 52.56%
CPC8调整模型中, 本轮训练的数据量为：818.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 53.05%
Epoch 1/5, Loss: 1.9337
Epoch 2/5, Loss: 1.9021
Epoch 3/5, Loss: 1.8736
Epoch 4/5, Loss: 1.8408
Epoch 5/5, Loss: 1.8109
新模型评估：
Accuracy: 54.89%
Model saved to ../../../data/model/mnist_cnn_model
CPC9调整模型中, 本轮训练的数据量为：2511.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 54.89%
Epoch 1/5, Loss: 1.7731
Epoch 2/5, Loss: 1.6762
Epoch 3/5, Loss: 1.5705
Epoch 4/5, Loss: 1.4650
Epoch 5/5, Loss: 1.3668
新模型评估：
Accuracy: 69.21%
Model saved to ../../../data/model/mnist_cnn_model
CPC10调整模型中, 本轮训练的数据量为：266.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 69.21%
Epoch 1/5, Loss: 1.2425
Epoch 2/5, Loss: 1.1991
Epoch 3/5, Loss: 1.2384
Epoch 4/5, Loss: 1.2053
Epoch 5/5, Loss: 1.2473
新模型评估：
Accuracy: 71.20%
Model saved to ../../../data/model/mnist_cnn_model
DONE
========================= literation: 2 =========================
----- literation 2: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3503
DataOwner1的最优x_1 = 0.1096
DataOwner2的最优x_2 = 0.1497
DataOwner3的最优x_3 = 0.1143
DataOwner4的最优x_4 = 0.1530
DataOwner5的最优x_5 = 0.0587
DataOwner6的最优x_6 = 0.1486
DataOwner7的最优x_7 = 0.0438
DataOwner8的最优x_8 = 0.1410
DataOwner9的最优x_9 = 0.1442
DataOwner10的最优x_10 = 0.1376
每个DataOwner应该贡献数据比例 xn_list = [0.10961210378669053, 0.1497013727018006, 0.11427135324392541, 0.15295129020735582, 0.058650358807870465, 0.14863906590895334, 0.043840364457304576, 0.14103496181834013, 0.144209701129785, 0.13761106142760743]
ModelOwner的最大效用 U(Eta) = 0.5916
DONE
----- literation 2: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.350259967138263
DataOwner1的分配到的支付 ： 0.1203
DataOwner2的分配到的支付 ： 0.1715
DataOwner3的分配到的支付 ： 0.1260
DataOwner4的分配到的支付 ： 0.1759
DataOwner5的分配到的支付 ： 0.0614
DataOwner6的分配到的支付 ： 0.1701
DataOwner7的分配到的支付 ： 0.0454
DataOwner8的分配到的支付 ： 0.1600
DataOwner9的分配到的支付 ： 0.1642
DataOwner10的分配到的支付 ： 0.1555
DONE
----- literation 2: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC2
DataOwner3 把数据交给 CPC1
DataOwner2 把数据交给 CPC3
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC8
DataOwner9 把数据交给 CPC9
DataOwner10 把数据交给 CPC10
DONE
----- literation 2: 模型训练 -----
重新调整fn，进而调整xn、Eta
正在评估DataOwner1的数据质量, 本轮评估的样本数据量为：212.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 71.20%
Epoch 1/5, Loss: 1.2719
Epoch 2/5, Loss: 1.2810
Epoch 3/5, Loss: 1.2235
Epoch 4/5, Loss: 1.2057
Epoch 5/5, Loss: 1.1953
新模型评估：
Accuracy: 72.70%
loss差为：
0.07656246423721313
单位数据loss差为：
0.00036114369923213744
正在评估DataOwner3的数据质量, 本轮评估的样本数据量为：442.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 71.20%
Epoch 1/5, Loss: 1.2649
Epoch 2/5, Loss: 1.2407
Epoch 3/5, Loss: 1.2206
Epoch 4/5, Loss: 1.2023
Epoch 5/5, Loss: 1.1859
新模型评估：
Accuracy: 72.67%
loss差为：
0.07893712180001389
单位数据loss差为：
0.00017859077330319883
正在评估DataOwner2的数据质量, 本轮评估的样本数据量为：1158.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 71.20%
Epoch 1/5, Loss: 1.2660
Epoch 2/5, Loss: 1.2088
Epoch 3/5, Loss: 1.1809
Epoch 4/5, Loss: 1.1319
Epoch 5/5, Loss: 1.1020
新模型评估：
Accuracy: 73.84%
loss差为：
0.16403943927664488
单位数据loss差为：
0.0001416575468710232
正在评估DataOwner4的数据质量, 本轮评估的样本数据量为：2072.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 71.20%
Epoch 1/5, Loss: 1.2480
Epoch 2/5, Loss: 1.1691
Epoch 3/5, Loss: 1.1060
Epoch 4/5, Loss: 1.0380
Epoch 5/5, Loss: 0.9788
新模型评估：
Accuracy: 74.40%
loss差为：
0.26918480432394765
单位数据loss差为：
0.00012991544610229134
正在评估DataOwner5的数据质量, 本轮评估的样本数据量为：113.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 71.20%
Epoch 1/5, Loss: 1.1950
Epoch 2/5, Loss: 1.1885
Epoch 3/5, Loss: 1.1614
Epoch 4/5, Loss: 1.1627
Epoch 5/5, Loss: 1.1538
新模型评估：
Accuracy: 69.91%
loss差为：
0.0411759614944458
单位数据loss差为：
0.00036438903977385664
正在评估DataOwner6的数据质量, 本轮评估的样本数据量为：575.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 71.20%
Epoch 1/5, Loss: 1.2404
Epoch 2/5, Loss: 1.2165
Epoch 3/5, Loss: 1.1935
Epoch 4/5, Loss: 1.1726
Epoch 5/5, Loss: 1.1512
新模型评估：
Accuracy: 73.96%
loss差为：
0.0892343521118164
单位数据loss差为：
0.00015519017758576766
正在评估DataOwner7的数据质量, 本轮评估的样本数据量为：84.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 71.20%
Epoch 1/5, Loss: 1.2154
Epoch 2/5, Loss: 1.2156
Epoch 3/5, Loss: 1.2911
Epoch 4/5, Loss: 1.2667
Epoch 5/5, Loss: 1.1843
新模型评估：
Accuracy: 71.00%
loss差为：
0.031066536903381348
单位数据loss差为：
0.00036983972504025413
正在评估DataOwner8的数据质量, 本轮评估的样本数据量为：818.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 71.20%
Epoch 1/5, Loss: 1.2257
Epoch 2/5, Loss: 1.1875
Epoch 3/5, Loss: 1.1588
Epoch 4/5, Loss: 1.1261
Epoch 5/5, Loss: 1.1010
新模型评估：
Accuracy: 73.75%
loss差为：
0.12476970599247861
单位数据loss差为：
0.00015253020292479047
正在评估DataOwner9的数据质量, 本轮评估的样本数据量为：2511.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 71.20%
Epoch 1/5, Loss: 1.2289
Epoch 2/5, Loss: 1.1455
Epoch 3/5, Loss: 1.0595
Epoch 4/5, Loss: 0.9817
Epoch 5/5, Loss: 0.9129
新模型评估：
Accuracy: 75.84%
loss差为：
0.31598547101020813
单位数据loss差为：
0.00012584049024699646
正在评估DataOwner10的数据质量, 本轮评估的样本数据量为：266.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 71.20%
Epoch 1/5, Loss: 1.2330
Epoch 2/5, Loss: 1.2950
Epoch 3/5, Loss: 1.1854
Epoch 4/5, Loss: 1.2043
Epoch 5/5, Loss: 1.2240
新模型评估：
Accuracy: 70.31%
loss差为：
0.008962392807006836
单位数据loss差为：
3.3693206041379085e-05
经过服务器调节后的真实数据质量：
数据质量列表avg_f_list: [0.00036114369923213744, 0.0001416575468710232, 0.00017859077330319883, 0.00012991544610229134, 0.00036438903977385664, 0.00015519017758576766, 0.00036983972504025413, 0.00015253020292479047, 0.00012584049024699646, 3.3693206041379085e-05]
归一化后的数据质量列表avg_f_list:[0.9974130251790155, 0.9321182385440694, 0.9431054790313936, 0.9286250889485589, 0.9983784793361445, 0.936144051679082, 1.0, 0.9353527376208852, 0.9274128330943465, 0.9]
CPC2调整模型中, 本轮训练的数据量为：212.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 71.20%
Epoch 1/5, Loss: 1.2589
Epoch 2/5, Loss: 1.2900
Epoch 3/5, Loss: 1.2188
Epoch 4/5, Loss: 1.2243
Epoch 5/5, Loss: 1.1768
新模型评估：
Accuracy: 72.93%
Model saved to ../../../data/model/mnist_cnn_model
CPC1调整模型中, 本轮训练的数据量为：442.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 72.93%
Epoch 1/5, Loss: 1.2262
Epoch 2/5, Loss: 1.2013
Epoch 3/5, Loss: 1.1801
Epoch 4/5, Loss: 1.1618
Epoch 5/5, Loss: 1.1445
新模型评估：
Accuracy: 73.01%
Model saved to ../../../data/model/mnist_cnn_model
CPC3调整模型中, 本轮训练的数据量为：1158.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 73.01%
Epoch 1/5, Loss: 1.1524
Epoch 2/5, Loss: 1.0770
Epoch 3/5, Loss: 1.0737
Epoch 4/5, Loss: 1.0217
Epoch 5/5, Loss: 0.9931
新模型评估：
Accuracy: 73.85%
Model saved to ../../../data/model/mnist_cnn_model
CPC4调整模型中, 本轮训练的数据量为：2072.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 73.85%
Epoch 1/5, Loss: 0.9638
Epoch 2/5, Loss: 0.9066
Epoch 3/5, Loss: 0.8595
Epoch 4/5, Loss: 0.8069
Epoch 5/5, Loss: 0.7620
新模型评估：
Accuracy: 75.59%
Model saved to ../../../data/model/mnist_cnn_model
CPC5调整模型中, 本轮训练的数据量为：113.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 75.59%
Epoch 1/5, Loss: 0.6744
Epoch 2/5, Loss: 0.6608
Epoch 3/5, Loss: 0.6517
Epoch 4/5, Loss: 0.6359
Epoch 5/5, Loss: 0.6227
新模型评估：
Accuracy: 74.71%
CPC6调整模型中, 本轮训练的数据量为：575.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 75.59%
Epoch 1/5, Loss: 0.7220
Epoch 2/5, Loss: 0.7052
Epoch 3/5, Loss: 0.6933
Epoch 4/5, Loss: 0.6800
Epoch 5/5, Loss: 0.6694
新模型评估：
Accuracy: 77.58%
Model saved to ../../../data/model/mnist_cnn_model
CPC7调整模型中, 本轮训练的数据量为：84.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.58%
Epoch 1/5, Loss: 0.6066
Epoch 2/5, Loss: 0.7220
Epoch 3/5, Loss: 0.7781
Epoch 4/5, Loss: 0.6397
Epoch 5/5, Loss: 0.6149
新模型评估：
Accuracy: 76.12%
CPC8调整模型中, 本轮训练的数据量为：818.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.58%
Epoch 1/5, Loss: 0.6538
Epoch 2/5, Loss: 0.6345
Epoch 3/5, Loss: 0.6243
Epoch 4/5, Loss: 0.6117
Epoch 5/5, Loss: 0.5953
新模型评估：
Accuracy: 76.93%
CPC9调整模型中, 本轮训练的数据量为：2511.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.58%
Epoch 1/5, Loss: 0.6756
Epoch 2/5, Loss: 0.6414
Epoch 3/5, Loss: 0.6057
Epoch 4/5, Loss: 0.5767
Epoch 5/5, Loss: 0.5487
新模型评估：
Accuracy: 76.83%
CPC10调整模型中, 本轮训练的数据量为：266.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.58%
Epoch 1/5, Loss: 0.6840
Epoch 2/5, Loss: 0.6332
Epoch 3/5, Loss: 0.6590
Epoch 4/5, Loss: 0.6757
Epoch 5/5, Loss: 0.6435
新模型评估：
Accuracy: 75.16%
DONE
========================= literation: 3 =========================
----- literation 3: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3288
DataOwner1的最优x_1 = 0.1638
DataOwner2的最优x_2 = 0.1022
DataOwner3的最优x_3 = 0.1139
DataOwner4的最优x_4 = 0.0984
DataOwner5的最优x_5 = 0.1645
DataOwner6的最优x_6 = 0.1066
DataOwner7的最优x_7 = 0.1659
DataOwner8的最优x_8 = 0.1057
DataOwner9的最优x_9 = 0.0971
DataOwner10的最优x_10 = 0.0647
每个DataOwner应该贡献数据比例 xn_list = [0.16376416538514993, 0.10224938090024879, 0.11389559198737591, 0.09842435460550782, 0.16454649274820438, 0.10658361711274231, 0.16585296452623385, 0.10573787311444983, 0.09708276804931043, 0.06469082933254318]
ModelOwner的最大效用 U(Eta) = 0.5668
xn开始变化：
new_U_qn_list: []
DONE
----- literation 3: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3287834509116727
DataOwner1的分配到的支付 ： 0.5902
DataOwner2的分配到的支付 ： 0.0826
DataOwner3的分配到的支付 ： 0.0638
DataOwner4的分配到的支付 ： 0.0840
DataOwner5的分配到的支付 ： 0.0972
DataOwner6的分配到的支付 ： 0.0823
DataOwner7的分配到的支付 ： 0.0981
DataOwner8的分配到的支付 ： 0.0781
DataOwner9的分配到的支付 ： 0.0791
DataOwner10的分配到的支付 ： 0.0733
DONE
----- literation 3: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC2
DataOwner3 把数据交给 CPC1
DataOwner2 把数据交给 CPC3
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC8
DataOwner9 把数据交给 CPC9
DataOwner10 把数据交给 CPC10
DONE
----- literation 3: 模型训练 -----
CPC2调整模型中, 本轮训练的数据量为：1941.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.58%
Epoch 1/5, Loss: 0.7006
Epoch 2/5, Loss: 0.6747
Epoch 3/5, Loss: 0.6439
Epoch 4/5, Loss: 0.6182
Epoch 5/5, Loss: 0.6000
新模型评估：
Accuracy: 76.24%
CPC1调整模型中, 本轮训练的数据量为：442.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.58%
Epoch 1/5, Loss: 0.7250
Epoch 2/5, Loss: 0.7086
Epoch 3/5, Loss: 0.6995
Epoch 4/5, Loss: 0.6878
Epoch 5/5, Loss: 0.6793
新模型评估：
Accuracy: 76.21%
CPC3调整模型中, 本轮训练的数据量为：1158.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.58%
Epoch 1/5, Loss: 0.6432
Epoch 2/5, Loss: 0.6309
Epoch 3/5, Loss: 0.6125
Epoch 4/5, Loss: 0.5904
Epoch 5/5, Loss: 0.5689
新模型评估：
Accuracy: 75.50%
CPC4调整模型中, 本轮训练的数据量为：2072.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.58%
Epoch 1/5, Loss: 0.6452
Epoch 2/5, Loss: 0.6226
Epoch 3/5, Loss: 0.5886
Epoch 4/5, Loss: 0.5658
Epoch 5/5, Loss: 0.5454
新模型评估：
Accuracy: 76.35%
CPC5调整模型中, 本轮训练的数据量为：318.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.58%
Epoch 1/5, Loss: 0.7859
Epoch 2/5, Loss: 0.7698
Epoch 3/5, Loss: 0.7578
Epoch 4/5, Loss: 0.7486
Epoch 5/5, Loss: 0.7409
新模型评估：
Accuracy: 76.30%
CPC6调整模型中, 本轮训练的数据量为：575.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.58%
Epoch 1/5, Loss: 0.6588
Epoch 2/5, Loss: 0.6390
Epoch 3/5, Loss: 0.6262
Epoch 4/5, Loss: 0.6146
Epoch 5/5, Loss: 0.6049
新模型评估：
Accuracy: 75.32%
CPC7调整模型中, 本轮训练的数据量为：320.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.58%
Epoch 1/5, Loss: 0.6914
Epoch 2/5, Loss: 0.6742
Epoch 3/5, Loss: 0.6617
Epoch 4/5, Loss: 0.6520
Epoch 5/5, Loss: 0.6435
新模型评估：
Accuracy: 76.31%
CPC8调整模型中, 本轮训练的数据量为：818.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.58%
Epoch 1/5, Loss: 0.6757
Epoch 2/5, Loss: 0.6519
Epoch 3/5, Loss: 0.6350
Epoch 4/5, Loss: 0.6198
Epoch 5/5, Loss: 0.6082
新模型评估：
Accuracy: 75.48%
CPC9调整模型中, 本轮训练的数据量为：2511.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.58%
Epoch 1/5, Loss: 0.6686
Epoch 2/5, Loss: 0.6306
Epoch 3/5, Loss: 0.6020
Epoch 4/5, Loss: 0.5808
Epoch 5/5, Loss: 0.5495
新模型评估：
Accuracy: 76.68%
CPC10调整模型中, 本轮训练的数据量为：266.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 77.58%
Epoch 1/5, Loss: 0.7344
Epoch 2/5, Loss: 0.6814
Epoch 3/5, Loss: 0.7017
Epoch 4/5, Loss: 0.6539
Epoch 5/5, Loss: 0.6621
新模型评估：
Accuracy: 76.48%
DONE
最终的列表：
[0.0005344418723075596, 0.0009031658576833627, 0.0011100518148399428, 0.0011588594244734615, 0.0010532328066272037, 0.0007967049267938731, 0.00039270180194816984, -0.000155453482971038, -0.0008445369382374945, -0.0016714200028665194, -0.002633066039737564, -0.0037265269840614096, -0.00494894013742167, -0.0062975251000667865, -0.007769580834546375, -0.009362482854178233, -0.011073680530197721, -0.01290069451178566, -0.014841114253493187, -0.01689259564488299, -0.019052858737491574, -0.02131968556448191, -0.023690918048606985, -0.02616445599433867, -0.028738255160238646, -0.03141032540785388, -0.0341787289236164, -0.03704157851041018, -0.039997035945641135, -0.04304331040280934, -0.04617865693373696, -0.04940137500874958, -0.052709807112245655, -0.05610233739121723, -0.05957739035440884, -0.06313342961991247, -0.06676895670910943, -0.0704825098849699, -0.07427266303281721, -0.078138024581759, -0.08207723646507004, -0.08608897311789582, -0.09017194051072341, -0.09432487521713973, -0.09854654351446468, -0.10283574051591704, -0.1071912893330289, -0.1116120402670861, -0.11609687002842894, -0.1206446809824977, -0.12525440042156194, -0.12992497986111756, -0.13465539435998258, -0.13944464186316569, -0.14429174256662203, -0.14919573830305177, -0.15415569194793094, -0.15917068684500274, -0.164239826250488, -0.16936223279530954, -0.17453704796464925, -0.17976343159419367, -0.1850405613824433, -0.19036763241849441, -0.19574385672472155, -0.20116846281381578, -0.20664069525965556, -0.21215981428150937, -0.21772509534108836, -0.22333582875198926, -0.2289913193010843, -0.23469088588143444, -0.24043386113631954, -0.24621959111399228, -0.2520474349327843, -0.2579167644562005, -0.26382696397765903, -0.2697774299145419, -0.2757675705112389, -0.2817968055508766, -0.28786456607543787, -0.2939702941139889, -0.30011344241874083, -0.3062934742086826, -0.3125098629205354, -0.31876209196678307, -0.325049654500546, -0.3313720531870762, -0.33772879998165184, -0.34411941591366924, -0.3505434308767257, -0.35700038342450524, -0.36348982057227697, -0.3700112976038292, -0.37656437788366537, -0.38314863267429633, -0.38976364095846594, -0.3964089892661592, -0.4030842715062396, -0.4097890888025729]
**** log-parameter_analysis 运行时间： 2025-01-17 23:58:00 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.03429654041700715
DataOwner2: noise random: 0.051581109733914776
DataOwner3: noise random: 0.052654130700702254
DataOwner4: noise random: 0.06735070894685188
DataOwner5: noise random: 0.05885984624075233
DataOwner6: noise random: 0.048427533207979594
DataOwner7: noise random: 0.042945622102513674
DataOwner8: noise random: 0.0830848920039093
DataOwner9: noise random: 0.07171997801542394
DataOwner10: noise random: 0.0010146959564973025
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9995225731125665, 0.998923936426997, 0.9988701950424489, 0.9981613522805676, 0.9986033593622502, 0.9990519610466909, 0.9992520979116599, 0.9972037442077079, 0.9979194571094367, 0.999999582633949]
归一化后的数据质量列表avg_f_list: [0.9829385876914307, 0.9615268823528488, 0.9596046902818174, 0.9342511950573335, 0.9500606594932626, 0.9661059960273859, 0.9732643805424052, 0.9, 0.9255992225806534, 1.0]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3359
DataOwner1的最优x_1 = 0.1471
DataOwner2的最优x_2 = 0.1272
DataOwner3的最优x_3 = 0.1253
DataOwner4的最优x_4 = 0.0989
DataOwner5的最优x_5 = 0.1157
DataOwner6的最优x_6 = 0.1316
DataOwner7的最优x_7 = 0.1384
DataOwner8的最优x_8 = 0.0580
DataOwner9的最优x_9 = 0.0891
DataOwner10的最优x_10 = 0.1617
每个DataOwner应该贡献数据比例 xn_list = [0.1471391414137425, 0.12718574779746322, 0.12530010474241382, 0.09885595895445935, 0.11569499889951142, 0.13161348239225265, 0.1383576018712863, 0.057993608198503833, 0.08912259426348496, 0.16174208514360713]
ModelOwner的最大效用 U(Eta) = 0.5750
DONE
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3359485287806874
DataOwner1的分配到的支付 ： 0.1684
DataOwner2的分配到的支付 ： 0.1424
DataOwner3的分配到的支付 ： 0.1400
DataOwner4的分配到的支付 ： 0.1075
DataOwner5的分配到的支付 ： 0.1279
DataOwner6的分配到的支付 ： 0.1480
DataOwner7的分配到的支付 ： 0.1567
DataOwner8的分配到的支付 ： 0.0608
DataOwner9的分配到的支付 ： 0.0960
DataOwner10的分配到的支付 ： 0.1883
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC1', 'DataOwner2': 'CPC2', 'DataOwner3': 'CPC3', 'DataOwner4': 'CPC4', 'DataOwner5': 'CPC5', 'DataOwner6': 'CPC6', 'DataOwner7': 'CPC7', 'DataOwner8': 'CPC8', 'DataOwner9': 'CPC9', 'DataOwner10': 'CPC10'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC2
DataOwner3 把数据交给 CPC3
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC8
DataOwner9 把数据交给 CPC9
DataOwner10 把数据交给 CPC10
DONE
----- literation 1: 模型训练 -----
CPC1调整模型中, 本轮训练的数据量为：201.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 26.82%
Epoch 1/5, Loss: 2.3061
Epoch 2/5, Loss: 2.2920
Epoch 3/5, Loss: 2.2899
Epoch 4/5, Loss: 2.2834
Epoch 5/5, Loss: 2.2983
新模型评估：
Accuracy: 28.25%
Model saved to ../../../data/model/mnist_cnn_model
CPC2调整模型中, 本轮训练的数据量为：693.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 28.25%
Epoch 1/5, Loss: 2.2914
Epoch 2/5, Loss: 2.2878
Epoch 3/5, Loss: 2.2839
Epoch 4/5, Loss: 2.2791
Epoch 5/5, Loss: 2.2733
新模型评估：
Accuracy: 36.93%
Model saved to ../../../data/model/mnist_cnn_model
CPC3调整模型中, 本轮训练的数据量为：341.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 36.93%
Epoch 1/5, Loss: 2.2676
Epoch 2/5, Loss: 2.2636
Epoch 3/5, Loss: 2.2630
Epoch 4/5, Loss: 2.2623
Epoch 5/5, Loss: 2.2518
新模型评估：
Accuracy: 38.74%
Model saved to ../../../data/model/mnist_cnn_model
CPC4调整模型中, 本轮训练的数据量为：539.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 38.74%
Epoch 1/5, Loss: 2.2460
Epoch 2/5, Loss: 2.2375
Epoch 3/5, Loss: 2.2273
Epoch 4/5, Loss: 2.2211
Epoch 5/5, Loss: 2.2131
新模型评估：
Accuracy: 43.77%
Model saved to ../../../data/model/mnist_cnn_model
CPC5调整模型中, 本轮训练的数据量为：157.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 43.77%
Epoch 1/5, Loss: 2.2461
Epoch 2/5, Loss: 2.2403
Epoch 3/5, Loss: 2.2466
Epoch 4/5, Loss: 2.2389
Epoch 5/5, Loss: 2.2276
新模型评估：
Accuracy: 42.30%
CPC6调整模型中, 本轮训练的数据量为：538.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 43.77%
Epoch 1/5, Loss: 2.2276
Epoch 2/5, Loss: 2.2172
Epoch 3/5, Loss: 2.2121
Epoch 4/5, Loss: 2.2056
Epoch 5/5, Loss: 2.1883
新模型评估：
Accuracy: 44.88%
Model saved to ../../../data/model/mnist_cnn_model
CPC7调整模型中, 本轮训练的数据量为：188.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 44.88%
Epoch 1/5, Loss: 2.1831
Epoch 2/5, Loss: 2.1792
Epoch 3/5, Loss: 2.1748
Epoch 4/5, Loss: 2.1713
Epoch 5/5, Loss: 2.1656
新模型评估：
Accuracy: 44.76%
CPC8调整模型中, 本轮训练的数据量为：79.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 44.88%
Epoch 1/5, Loss: 2.1552
Epoch 2/5, Loss: 2.1643
Epoch 3/5, Loss: 2.1502
Epoch 4/5, Loss: 2.1397
Epoch 5/5, Loss: 2.1738
新模型评估：
Accuracy: 46.00%
Model saved to ../../../data/model/mnist_cnn_model
CPC9调整模型中, 本轮训练的数据量为：2795.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 46.00%
Epoch 1/5, Loss: 2.1530
Epoch 2/5, Loss: 2.0916
Epoch 3/5, Loss: 2.0093
Epoch 4/5, Loss: 1.9059
Epoch 5/5, Loss: 1.7832
新模型评估：
Accuracy: 57.05%
Model saved to ../../../data/model/mnist_cnn_model
CPC10调整模型中, 本轮训练的数据量为：882.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 57.05%
Epoch 1/5, Loss: 1.6878
Epoch 2/5, Loss: 1.6537
Epoch 3/5, Loss: 1.6193
Epoch 4/5, Loss: 1.5862
Epoch 5/5, Loss: 1.5506
新模型评估：
Accuracy: 62.83%
Model saved to ../../../data/model/mnist_cnn_model
DONE
========================= literation: 2 =========================
----- literation 2: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3359
DataOwner1的最优x_1 = 0.1471
DataOwner2的最优x_2 = 0.1272
DataOwner3的最优x_3 = 0.1253
DataOwner4的最优x_4 = 0.0989
DataOwner5的最优x_5 = 0.1157
DataOwner6的最优x_6 = 0.1316
DataOwner7的最优x_7 = 0.1384
DataOwner8的最优x_8 = 0.0580
DataOwner9的最优x_9 = 0.0891
DataOwner10的最优x_10 = 0.1617
每个DataOwner应该贡献数据比例 xn_list = [0.1471391414137425, 0.12718574779746322, 0.12530010474241382, 0.09885595895445935, 0.11569499889951142, 0.13161348239225265, 0.1383576018712863, 0.057993608198503833, 0.08912259426348496, 0.16174208514360713]
ModelOwner的最大效用 U(Eta) = 0.5750
DONE
最终的列表：
**** log-parameter_analysis 运行时间： 2025-01-18 00:01:25 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.09762852919849112
DataOwner2: noise random: 0.07762487588949622
DataOwner3: noise random: 0.07316004837419555
DataOwner4: noise random: 0.03435927126038773
DataOwner5: noise random: 0.02381142276240572
DataOwner6: noise random: 0.06198857748847456
DataOwner7: noise random: 0.07184154092387193
DataOwner8: noise random: 0.07250891078545167
DataOwner9: noise random: 0.02580131155617642
DataOwner10: noise random: 0.05778103545140544
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9961401461043454, 0.9975671550252484, 0.9978320947369989, 0.9995213018151529, 0.9997705500985905, 0.9984353901037903, 0.9979085778914053, 0.9978773006365703, 0.9997307749076578, 0.9986459328410289]
归一化后的数据质量列表avg_f_list: [0.9, 0.939307165901235, 0.9466049683543625, 0.9931344201958584, 1.0, 0.9632228259742812, 0.9487117078392157, 0.9478501713577506, 0.9989043866468931, 0.969022255943295]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3423
DataOwner1的最优x_1 = 0.0519
DataOwner2的最优x_2 = 0.0993
DataOwner3的最优x_3 = 0.1072
DataOwner4的最优x_4 = 0.1521
DataOwner5的最优x_5 = 0.1579
DataOwner6的最优x_6 = 0.1243
DataOwner7的最优x_7 = 0.1094
DataOwner8的最优x_8 = 0.1085
DataOwner9的最优x_9 = 0.1570
DataOwner10的最优x_10 = 0.1300
每个DataOwner应该贡献数据比例 xn_list = [0.05185166758054109, 0.09925680527899347, 0.10717510277140546, 0.15206429619181722, 0.15794365952033243, 0.12427573328346891, 0.1094135136019216, 0.10850066520684538, 0.15701723555063024, 0.12995351028079152]
ModelOwner的最大效用 U(Eta) = 0.5823
DONE
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3422907221722253
DataOwner1的分配到的支付 ： 0.0540
DataOwner2的分配到的支付 ： 0.1079
DataOwner3的分配到的支付 ： 0.1175
DataOwner4的分配到的支付 ： 0.1748
DataOwner5的分配到的支付 ： 0.1829
DataOwner6的分配到的支付 ： 0.1386
DataOwner7的分配到的支付 ： 0.1202
DataOwner8的分配到的支付 ： 0.1191
DataOwner9的分配到的支付 ： 0.1816
DataOwner10的分配到的支付 ： 0.1458
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC2', 'DataOwner9': 'CPC1', 'DataOwner2': 'CPC3', 'DataOwner3': 'CPC4', 'DataOwner4': 'CPC5', 'DataOwner5': 'CPC10', 'DataOwner6': 'CPC6', 'DataOwner7': 'CPC7', 'DataOwner8': 'CPC8', 'DataOwner10': 'CPC9'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC2
DataOwner9 把数据交给 CPC1
DataOwner2 把数据交给 CPC3
DataOwner3 把数据交给 CPC4
DataOwner4 把数据交给 CPC5
DataOwner5 把数据交给 CPC10
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC8
DataOwner10 把数据交给 CPC9
DONE
----- literation 1: 模型训练 -----
CPC2调整模型中, 本轮训练的数据量为：259.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 26.82%
Epoch 1/5, Loss: 2.3074
Epoch 2/5, Loss: 2.3178
Epoch 3/5, Loss: 2.3005
Epoch 4/5, Loss: 2.2967
Epoch 5/5, Loss: 2.3095
新模型评估：
Accuracy: 29.44%
Model saved to ../../../data/model/mnist_cnn_model
CPC1调整模型中, 本轮训练的数据量为：1099.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 29.44%
Epoch 1/5, Loss: 2.2915
Epoch 2/5, Loss: 2.2852
Epoch 3/5, Loss: 2.2786
Epoch 4/5, Loss: 2.2697
Epoch 5/5, Loss: 2.2551
新模型评估：
Accuracy: 43.47%
Model saved to ../../../data/model/mnist_cnn_model
CPC3调整模型中, 本轮训练的数据量为：496.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 43.47%
Epoch 1/5, Loss: 2.2526
Epoch 2/5, Loss: 2.2453
Epoch 3/5, Loss: 2.2397
Epoch 4/5, Loss: 2.2335
Epoch 5/5, Loss: 2.2269
新模型评估：
Accuracy: 43.30%
CPC4调整模型中, 本轮训练的数据量为：1071.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 43.47%
Epoch 1/5, Loss: 2.2477
Epoch 2/5, Loss: 2.2347
Epoch 3/5, Loss: 2.2198
Epoch 4/5, Loss: 2.2012
Epoch 5/5, Loss: 2.1813
新模型评估：
Accuracy: 49.64%
Model saved to ../../../data/model/mnist_cnn_model
CPC5调整模型中, 本轮训练的数据量为：304.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 49.64%
Epoch 1/5, Loss: 2.1789
Epoch 2/5, Loss: 2.1730
Epoch 3/5, Loss: 2.1659
Epoch 4/5, Loss: 2.1609
Epoch 5/5, Loss: 2.1550
新模型评估：
Accuracy: 50.67%
Model saved to ../../../data/model/mnist_cnn_model
CPC10调整模型中, 本轮训练的数据量为：789.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 50.67%
Epoch 1/5, Loss: 2.1451
Epoch 2/5, Loss: 2.1254
Epoch 3/5, Loss: 2.1094
Epoch 4/5, Loss: 2.0886
Epoch 5/5, Loss: 2.0644
新模型评估：
Accuracy: 53.87%
Model saved to ../../../data/model/mnist_cnn_model
CPC6调整模型中, 本轮训练的数据量为：124.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 53.87%
Epoch 1/5, Loss: 2.0557
Epoch 2/5, Loss: 2.0471
Epoch 3/5, Loss: 2.0439
Epoch 4/5, Loss: 2.0406
Epoch 5/5, Loss: 2.0359
新模型评估：
Accuracy: 53.59%
CPC7调整模型中, 本轮训练的数据量为：1422.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 53.87%
Epoch 1/5, Loss: 2.0343
Epoch 2/5, Loss: 1.9901
Epoch 3/5, Loss: 1.9500
Epoch 4/5, Loss: 1.8888
Epoch 5/5, Loss: 1.8349
新模型评估：
Accuracy: 56.96%
Model saved to ../../../data/model/mnist_cnn_model
CPC8调整模型中, 本轮训练的数据量为：1193.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 56.96%
Epoch 1/5, Loss: 1.7950
Epoch 2/5, Loss: 1.7487
Epoch 3/5, Loss: 1.7028
Epoch 4/5, Loss: 1.6609
Epoch 5/5, Loss: 1.6135
新模型评估：
Accuracy: 62.18%
Model saved to ../../../data/model/mnist_cnn_model
CPC9调整模型中, 本轮训练的数据量为：129.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 62.18%
Epoch 1/5, Loss: 1.7141
Epoch 2/5, Loss: 1.4350
Epoch 3/5, Loss: 1.5308
Epoch 4/5, Loss: 1.3926
Epoch 5/5, Loss: 1.7408
新模型评估：
Accuracy: 61.34%
DONE
========================= literation: 2 =========================
----- literation 2: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3423
DataOwner1的最优x_1 = 0.0519
DataOwner2的最优x_2 = 0.0993
DataOwner3的最优x_3 = 0.1072
DataOwner4的最优x_4 = 0.1521
DataOwner5的最优x_5 = 0.1579
DataOwner6的最优x_6 = 0.1243
DataOwner7的最优x_7 = 0.1094
DataOwner8的最优x_8 = 0.1085
DataOwner9的最优x_9 = 0.1570
DataOwner10的最优x_10 = 0.1300
每个DataOwner应该贡献数据比例 xn_list = [0.05185166758054109, 0.09925680527899347, 0.10717510277140546, 0.15206429619181722, 0.15794365952033243, 0.12427573328346891, 0.1094135136019216, 0.10850066520684538, 0.15701723555063024, 0.12995351028079152]
ModelOwner的最大效用 U(Eta) = 0.5823
DONE
----- literation 2: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3422907221722253
DataOwner1的分配到的支付 ： 0.0540
DataOwner2的分配到的支付 ： 0.1079
DataOwner3的分配到的支付 ： 0.1175
DataOwner4的分配到的支付 ： 0.1748
DataOwner5的分配到的支付 ： 0.1829
DataOwner6的分配到的支付 ： 0.1386
DataOwner7的分配到的支付 ： 0.1202
DataOwner8的分配到的支付 ： 0.1191
DataOwner9的分配到的支付 ： 0.1816
DataOwner10的分配到的支付 ： 0.1458
DONE
----- literation 2: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC2
DataOwner9 把数据交给 CPC1
DataOwner2 把数据交给 CPC3
DataOwner3 把数据交给 CPC4
DataOwner4 把数据交给 CPC5
DataOwner5 把数据交给 CPC10
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC8
DataOwner10 把数据交给 CPC9
DONE
----- literation 2: 模型训练 -----
重新调整fn，进而调整xn、Eta
正在评估DataOwner1的数据质量, 本轮评估的样本数据量为：259.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 62.18%
Epoch 1/5, Loss: 1.5828
Epoch 2/5, Loss: 1.6244
Epoch 3/5, Loss: 1.5924
Epoch 4/5, Loss: 1.6171
Epoch 5/5, Loss: 1.6916
新模型评估：
Accuracy: 61.38%
loss差为：
-0.1087210893630981
单位数据loss差为：
-0.0004197725458034676
正在评估DataOwner9的数据质量, 本轮评估的样本数据量为：1099.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 62.18%
Epoch 1/5, Loss: 1.5566
Epoch 2/5, Loss: 1.5148
Epoch 3/5, Loss: 1.4846
Epoch 4/5, Loss: 1.4484
Epoch 5/5, Loss: 1.4038
新模型评估：
Accuracy: 65.49%
loss差为：
0.15274850527445483
单位数据loss差为：
0.00013898863082297984
正在评估DataOwner2的数据质量, 本轮评估的样本数据量为：496.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 62.18%
Epoch 1/5, Loss: 1.6212
Epoch 2/5, Loss: 1.5934
Epoch 3/5, Loss: 1.5709
Epoch 4/5, Loss: 1.5605
Epoch 5/5, Loss: 1.5355
新模型评估：
Accuracy: 63.56%
loss差为：
0.08576446771621704
单位数据loss差为：
0.00017291223329882468
正在评估DataOwner3的数据质量, 本轮评估的样本数据量为：1071.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 62.18%
Epoch 1/5, Loss: 1.5790
Epoch 2/5, Loss: 1.5361
Epoch 3/5, Loss: 1.4962
Epoch 4/5, Loss: 1.4558
Epoch 5/5, Loss: 1.4169
新模型评估：
Accuracy: 67.57%
loss差为：
0.16213330801795522
单位数据loss差为：
0.0001513849748066809
正在评估DataOwner4的数据质量, 本轮评估的样本数据量为：304.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 62.18%
Epoch 1/5, Loss: 1.5782
Epoch 2/5, Loss: 1.5657
Epoch 3/5, Loss: 1.5478
Epoch 4/5, Loss: 1.5343
Epoch 5/5, Loss: 1.5255
新模型评估：
Accuracy: 64.43%
loss差为：
0.05271513462066646
单位数据loss差为：
0.00017340504809429757
正在评估DataOwner5的数据质量, 本轮评估的样本数据量为：789.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 62.18%
Epoch 1/5, Loss: 1.5945
Epoch 2/5, Loss: 1.5566
Epoch 3/5, Loss: 1.5330
Epoch 4/5, Loss: 1.5034
Epoch 5/5, Loss: 1.4716
新模型评估：
Accuracy: 64.63%
loss差为：
0.12298402419457055
单位数据loss差为：
0.00015587328795256091
正在评估DataOwner6的数据质量, 本轮评估的样本数据量为：124.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 62.18%
Epoch 1/5, Loss: 1.6618
Epoch 2/5, Loss: 1.6530
Epoch 3/5, Loss: 1.6421
Epoch 4/5, Loss: 1.6340
Epoch 5/5, Loss: 1.6312
新模型评估：
Accuracy: 61.74%
loss差为：
0.03065413236618042
单位数据loss差为：
0.00024721074488855176
正在评估DataOwner7的数据质量, 本轮评估的样本数据量为：1422.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 62.18%
Epoch 1/5, Loss: 1.5811
Epoch 2/5, Loss: 1.5230
Epoch 3/5, Loss: 1.4741
Epoch 4/5, Loss: 1.4200
Epoch 5/5, Loss: 1.3620
新模型评估：
Accuracy: 68.73%
loss差为：
0.21917538020921778
单位数据loss差为：
0.00015413177229902798
正在评估DataOwner8的数据质量, 本轮评估的样本数据量为：1193.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 62.18%
Epoch 1/5, Loss: 1.5616
Epoch 2/5, Loss: 1.5133
Epoch 3/5, Loss: 1.4659
Epoch 4/5, Loss: 1.4250
Epoch 5/5, Loss: 1.3818
新模型评估：
Accuracy: 65.30%
loss差为：
0.17979364018691202
单位数据loss差为：
0.00015070715858081477
正在评估DataOwner10的数据质量, 本轮评估的样本数据量为：129.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 62.18%
Epoch 1/5, Loss: 1.4502
Epoch 2/5, Loss: 1.6423
Epoch 3/5, Loss: 1.3549
Epoch 4/5, Loss: 1.6101
Epoch 5/5, Loss: 1.3449
新模型评估：
Accuracy: 63.98%
loss差为：
0.1053011417388916
单位数据loss差为：
0.000816287920456524
经过服务器调节后的真实数据质量：
数据质量列表avg_f_list: [-0.0004197725458034676, 0.00017291223329882468, 0.0001513849748066809, 0.00017340504809429757, 0.00015587328795256091, 0.00024721074488855176, 0.00015413177229902798, 0.00015070715858081477, 0.00013898863082297984, 0.000816287920456524]
归一化后的数据质量列表avg_f_list:[0.9, 0.9479494972358115, 0.9462078948563356, 0.9479893670325508, 0.9465710092239895, 0.9539604096157321, 0.9464301167918577, 0.9461530580385287, 0.9452050034669517, 1.0]
CPC2调整模型中, 本轮训练的数据量为：259.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 62.18%
Epoch 1/5, Loss: 1.5541
Epoch 2/5, Loss: 1.7054
Epoch 3/5, Loss: 1.5911
Epoch 4/5, Loss: 1.6238
Epoch 5/5, Loss: 1.5827
新模型评估：
Accuracy: 62.24%
Model saved to ../../../data/model/mnist_cnn_model
CPC1调整模型中, 本轮训练的数据量为：1099.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 62.24%
Epoch 1/5, Loss: 1.5190
Epoch 2/5, Loss: 1.4759
Epoch 3/5, Loss: 1.4388
Epoch 4/5, Loss: 1.3951
Epoch 5/5, Loss: 1.3629
新模型评估：
Accuracy: 66.61%
Model saved to ../../../data/model/mnist_cnn_model
CPC3调整模型中, 本轮训练的数据量为：496.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 66.61%
Epoch 1/5, Loss: 1.3904
Epoch 2/5, Loss: 1.3633
Epoch 3/5, Loss: 1.3407
Epoch 4/5, Loss: 1.3305
Epoch 5/5, Loss: 1.3052
新模型评估：
Accuracy: 67.38%
Model saved to ../../../data/model/mnist_cnn_model
CPC4调整模型中, 本轮训练的数据量为：1071.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 67.38%
Epoch 1/5, Loss: 1.2578
Epoch 2/5, Loss: 1.2142
Epoch 3/5, Loss: 1.1758
Epoch 4/5, Loss: 1.1408
Epoch 5/5, Loss: 1.1051
新模型评估：
Accuracy: 72.49%
Model saved to ../../../data/model/mnist_cnn_model
CPC5调整模型中, 本轮训练的数据量为：304.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 72.49%
Epoch 1/5, Loss: 1.0755
Epoch 2/5, Loss: 1.0616
Epoch 3/5, Loss: 1.0537
Epoch 4/5, Loss: 1.0361
Epoch 5/5, Loss: 1.0242
新模型评估：
Accuracy: 72.99%
Model saved to ../../../data/model/mnist_cnn_model
CPC10调整模型中, 本轮训练的数据量为：789.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 72.99%
Epoch 1/5, Loss: 1.0777
Epoch 2/5, Loss: 1.0553
Epoch 3/5, Loss: 1.0224
Epoch 4/5, Loss: 0.9826
Epoch 5/5, Loss: 0.9633
新模型评估：
Accuracy: 75.30%
Model saved to ../../../data/model/mnist_cnn_model
CPC6调整模型中, 本轮训练的数据量为：124.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 75.30%
Epoch 1/5, Loss: 1.0369
Epoch 2/5, Loss: 1.0282
Epoch 3/5, Loss: 1.0195
Epoch 4/5, Loss: 1.0151
Epoch 5/5, Loss: 1.0060
新模型评估：
Accuracy: 73.89%
CPC7调整模型中, 本轮训练的数据量为：1422.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 75.30%
Epoch 1/5, Loss: 0.9287
Epoch 2/5, Loss: 0.8790
Epoch 3/5, Loss: 0.8542
Epoch 4/5, Loss: 0.8089
Epoch 5/5, Loss: 0.7978
新模型评估：
Accuracy: 75.37%
Model saved to ../../../data/model/mnist_cnn_model
CPC8调整模型中, 本轮训练的数据量为：1193.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 75.37%
Epoch 1/5, Loss: 0.7580
Epoch 2/5, Loss: 0.7300
Epoch 3/5, Loss: 0.7103
Epoch 4/5, Loss: 0.6849
Epoch 5/5, Loss: 0.6633
新模型评估：
Accuracy: 75.46%
Model saved to ../../../data/model/mnist_cnn_model
CPC9调整模型中, 本轮训练的数据量为：129.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 75.46%
Epoch 1/5, Loss: 0.4976
Epoch 2/5, Loss: 0.5123
Epoch 3/5, Loss: 0.4806
Epoch 4/5, Loss: 0.5299
Epoch 5/5, Loss: 0.5799
新模型评估：
Accuracy: 76.92%
Model saved to ../../../data/model/mnist_cnn_model
DONE
========================= literation: 3 =========================
----- literation 3: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3273
DataOwner1的最优x_1 = 0.0660
DataOwner2的最优x_2 = 0.1199
DataOwner3的最优x_3 = 0.1181
DataOwner4的最优x_4 = 0.1199
DataOwner5的最优x_5 = 0.1185
DataOwner6的最优x_6 = 0.1259
DataOwner7的最优x_7 = 0.1184
DataOwner8的最优x_8 = 0.1181
DataOwner9的最优x_9 = 0.1171
DataOwner10的最优x_10 = 0.1667
每个DataOwner应该贡献数据比例 xn_list = [0.06600935461483472, 0.11989869202643164, 0.11813862778701073, 0.11993882369399153, 0.11850672694028846, 0.12586885177046955, 0.1183639715621538, 0.1180829860287108, 0.11711884701615327, 0.16665828752058792]
ModelOwner的最大效用 U(Eta) = 0.5652
xn开始变化：
new_xn_list: [0.06600935461483472, 0.11989869202643164, 0.11813862778701073, 0.15206429619181722, 0.15794365952033243, 0.12586885177046955, 0.1183639715621538, 0.1180829860287108, 0.15701723555063024, 0.16665828752058792]
avg_f_list: [0.9, 0.9479494972358115, 0.9462078948563356, 0.9479893670325508, 0.9465710092239895, 0.9539604096157321, 0.9464301167918577, 0.9461530580385287, 0.9452050034669517, 1.0]
============= xn0: 0.01 =============
new_qn: 0.009000000000000001
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.1869953035623877
new_xn: 0.01
Lambda: 1
Rho: 1
============= xn0: 0.02 =============
new_qn: 0.018000000000000002
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.1959953035623878
new_xn: 0.02
Lambda: 1
Rho: 1
============= xn0: 0.03 =============
new_qn: 0.027
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.2049953035623877
new_xn: 0.03
Lambda: 1
Rho: 1
============= xn0: 0.04 =============
new_qn: 0.036000000000000004
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.2139953035623878
new_xn: 0.04
Lambda: 1
Rho: 1
============= xn0: 0.05 =============
new_qn: 0.045000000000000005
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.2229953035623877
new_xn: 0.05
Lambda: 1
Rho: 1
============= xn0: 0.060000000000000005 =============
new_qn: 0.054000000000000006
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.2319953035623878
new_xn: 0.060000000000000005
Lambda: 1
Rho: 1
============= xn0: 0.06999999999999999 =============
new_qn: 0.063
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.2409953035623877
new_xn: 0.06999999999999999
Lambda: 1
Rho: 1
============= xn0: 0.08 =============
new_qn: 0.07200000000000001
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.2499953035623879
new_xn: 0.08
Lambda: 1
Rho: 1
============= xn0: 0.09 =============
new_qn: 0.081
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.2589953035623878
new_xn: 0.09
Lambda: 1
Rho: 1
============= xn0: 0.09999999999999999 =============
new_qn: 0.09
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.2679953035623877
new_xn: 0.09999999999999999
Lambda: 1
Rho: 1
============= xn0: 0.11 =============
new_qn: 0.099
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.2769953035623878
new_xn: 0.11
Lambda: 1
Rho: 1
============= xn0: 0.12 =============
new_qn: 0.108
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.2859953035623877
new_xn: 0.12
Lambda: 1
Rho: 1
============= xn0: 0.13 =============
new_qn: 0.117
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.2949953035623878
new_xn: 0.13
Lambda: 1
Rho: 1
============= xn0: 0.14 =============
new_qn: 0.12600000000000003
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.3039953035623877
new_xn: 0.14
Lambda: 1
Rho: 1
============= xn0: 0.15000000000000002 =============
new_qn: 0.13500000000000004
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.3129953035623878
new_xn: 0.15000000000000002
Lambda: 1
Rho: 1
============= xn0: 0.16 =============
new_qn: 0.14400000000000002
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.3219953035623877
new_xn: 0.16
Lambda: 1
Rho: 1
============= xn0: 0.17 =============
new_qn: 0.15300000000000002
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.3309953035623878
new_xn: 0.17
Lambda: 1
Rho: 1
============= xn0: 0.18000000000000002 =============
new_qn: 0.16200000000000003
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.3399953035623877
new_xn: 0.18000000000000002
Lambda: 1
Rho: 1
============= xn0: 0.19 =============
new_qn: 0.171
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.3489953035623878
new_xn: 0.19
Lambda: 1
Rho: 1
============= xn0: 0.2 =============
new_qn: 0.18000000000000002
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.3579953035623877
new_xn: 0.2
Lambda: 1
Rho: 1
============= xn0: 0.21000000000000002 =============
new_qn: 0.18900000000000003
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.3669953035623879
new_xn: 0.21000000000000002
Lambda: 1
Rho: 1
============= xn0: 0.22 =============
new_qn: 0.198
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.3759953035623878
new_xn: 0.22
Lambda: 1
Rho: 1
============= xn0: 0.23 =============
new_qn: 0.20700000000000002
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.3849953035623879
new_xn: 0.23
Lambda: 1
Rho: 1
============= xn0: 0.24000000000000002 =============
new_qn: 0.21600000000000003
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.3939953035623878
new_xn: 0.24000000000000002
Lambda: 1
Rho: 1
============= xn0: 0.25 =============
new_qn: 0.225
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.4029953035623877
new_xn: 0.25
Lambda: 1
Rho: 1
============= xn0: 0.26 =============
new_qn: 0.234
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.4119953035623878
new_xn: 0.26
Lambda: 1
Rho: 1
============= xn0: 0.27 =============
new_qn: 0.24300000000000002
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.4209953035623877
new_xn: 0.27
Lambda: 1
Rho: 1
============= xn0: 0.28 =============
new_qn: 0.25200000000000006
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.4299953035623878
new_xn: 0.28
Lambda: 1
Rho: 1
============= xn0: 0.29000000000000004 =============
new_qn: 0.26100000000000007
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.438995303562388
new_xn: 0.29000000000000004
Lambda: 1
Rho: 1
============= xn0: 0.3 =============
new_qn: 0.27
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.4479953035623878
new_xn: 0.3
Lambda: 1
Rho: 1
============= xn0: 0.31 =============
new_qn: 0.279
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.4569953035623877
new_xn: 0.31
Lambda: 1
Rho: 1
============= xn0: 0.32 =============
new_qn: 0.28800000000000003
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.4659953035623878
new_xn: 0.32
Lambda: 1
Rho: 1
============= xn0: 0.33 =============
new_qn: 0.29700000000000004
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.4749953035623877
new_xn: 0.33
Lambda: 1
Rho: 1
============= xn0: 0.34 =============
new_qn: 0.30600000000000005
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.4839953035623878
new_xn: 0.34
Lambda: 1
Rho: 1
============= xn0: 0.35000000000000003 =============
new_qn: 0.31500000000000006
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.4929953035623877
new_xn: 0.35000000000000003
Lambda: 1
Rho: 1
============= xn0: 0.36000000000000004 =============
new_qn: 0.32400000000000007
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.5019953035623879
new_xn: 0.36000000000000004
Lambda: 1
Rho: 1
============= xn0: 0.37 =============
new_qn: 0.333
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.5109953035623878
new_xn: 0.37
Lambda: 1
Rho: 1
============= xn0: 0.38 =============
new_qn: 0.342
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.5199953035623879
new_xn: 0.38
Lambda: 1
Rho: 1
============= xn0: 0.39 =============
new_qn: 0.35100000000000003
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.5289953035623878
new_xn: 0.39
Lambda: 1
Rho: 1
============= xn0: 0.4 =============
new_qn: 0.36000000000000004
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.537995303562388
new_xn: 0.4
Lambda: 1
Rho: 1
============= xn0: 0.41000000000000003 =============
new_qn: 0.36900000000000005
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.5469953035623878
new_xn: 0.41000000000000003
Lambda: 1
Rho: 1
============= xn0: 0.42000000000000004 =============
new_qn: 0.37800000000000006
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.555995303562388
new_xn: 0.42000000000000004
Lambda: 1
Rho: 1
============= xn0: 0.43 =============
new_qn: 0.387
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.5649953035623878
new_xn: 0.43
Lambda: 1
Rho: 1
============= xn0: 0.44 =============
new_qn: 0.396
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.5739953035623877
new_xn: 0.44
Lambda: 1
Rho: 1
============= xn0: 0.45 =============
new_qn: 0.405
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.5829953035623878
new_xn: 0.45
Lambda: 1
Rho: 1
============= xn0: 0.46 =============
new_qn: 0.41400000000000003
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.5919953035623877
new_xn: 0.46
Lambda: 1
Rho: 1
============= xn0: 0.47000000000000003 =============
new_qn: 0.42300000000000004
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.6009953035623878
new_xn: 0.47000000000000003
Lambda: 1
Rho: 1
============= xn0: 0.48000000000000004 =============
new_qn: 0.43200000000000005
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.6099953035623877
new_xn: 0.48000000000000004
Lambda: 1
Rho: 1
============= xn0: 0.49 =============
new_qn: 0.441
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.6189953035623876
new_xn: 0.49
Lambda: 1
Rho: 1
============= xn0: 0.5 =============
new_qn: 0.45
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.6279953035623878
new_xn: 0.5
Lambda: 1
Rho: 1
============= xn0: 0.51 =============
new_qn: 0.459
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.6369953035623876
new_xn: 0.51
Lambda: 1
Rho: 1
============= xn0: 0.52 =============
new_qn: 0.468
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.6459953035623878
new_xn: 0.52
Lambda: 1
Rho: 1
============= xn0: 0.53 =============
new_qn: 0.47700000000000004
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.6549953035623877
new_xn: 0.53
Lambda: 1
Rho: 1
============= xn0: 0.54 =============
new_qn: 0.48600000000000004
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.6639953035623878
new_xn: 0.54
Lambda: 1
Rho: 1
============= xn0: 0.55 =============
new_qn: 0.49500000000000005
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.6729953035623877
new_xn: 0.55
Lambda: 1
Rho: 1
============= xn0: 0.56 =============
new_qn: 0.5040000000000001
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.6819953035623878
new_xn: 0.56
Lambda: 1
Rho: 1
============= xn0: 0.5700000000000001 =============
new_qn: 0.5130000000000001
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.690995303562388
new_xn: 0.5700000000000001
Lambda: 1
Rho: 1
============= xn0: 0.5800000000000001 =============
new_qn: 0.5220000000000001
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.6999953035623878
new_xn: 0.5800000000000001
Lambda: 1
Rho: 1
============= xn0: 0.59 =============
new_qn: 0.531
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.7089953035623877
new_xn: 0.59
Lambda: 1
Rho: 1
============= xn0: 0.6 =============
new_qn: 0.54
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.7179953035623878
new_xn: 0.6
Lambda: 1
Rho: 1
============= xn0: 0.61 =============
new_qn: 0.549
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.7269953035623877
new_xn: 0.61
Lambda: 1
Rho: 1
============= xn0: 0.62 =============
new_qn: 0.558
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.7359953035623878
new_xn: 0.62
Lambda: 1
Rho: 1
============= xn0: 0.63 =============
new_qn: 0.5670000000000001
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.7449953035623877
new_xn: 0.63
Lambda: 1
Rho: 1
============= xn0: 0.64 =============
new_qn: 0.5760000000000001
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.7539953035623879
new_xn: 0.64
Lambda: 1
Rho: 1
============= xn0: 0.65 =============
new_qn: 0.5850000000000001
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.7629953035623878
new_xn: 0.65
Lambda: 1
Rho: 1
============= xn0: 0.66 =============
new_qn: 0.5940000000000001
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.7719953035623879
new_xn: 0.66
Lambda: 1
Rho: 1
============= xn0: 0.67 =============
new_qn: 0.6030000000000001
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.7809953035623878
new_xn: 0.67
Lambda: 1
Rho: 1
============= xn0: 0.68 =============
new_qn: 0.6120000000000001
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.789995303562388
new_xn: 0.68
Lambda: 1
Rho: 1
============= xn0: 0.6900000000000001 =============
new_qn: 0.6210000000000001
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.7989953035623878
new_xn: 0.6900000000000001
Lambda: 1
Rho: 1
============= xn0: 0.7000000000000001 =============
new_qn: 0.6300000000000001
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.807995303562388
new_xn: 0.7000000000000001
Lambda: 1
Rho: 1
============= xn0: 0.7100000000000001 =============
new_qn: 0.6390000000000001
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.8169953035623878
new_xn: 0.7100000000000001
Lambda: 1
Rho: 1
============= xn0: 0.72 =============
new_qn: 0.648
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.8259953035623877
new_xn: 0.72
Lambda: 1
Rho: 1
============= xn0: 0.73 =============
new_qn: 0.657
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.8349953035623878
new_xn: 0.73
Lambda: 1
Rho: 1
============= xn0: 0.74 =============
new_qn: 0.666
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.8439953035623877
new_xn: 0.74
Lambda: 1
Rho: 1
============= xn0: 0.75 =============
new_qn: 0.675
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.8529953035623878
new_xn: 0.75
Lambda: 1
Rho: 1
============= xn0: 0.76 =============
new_qn: 0.684
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.8619953035623877
new_xn: 0.76
Lambda: 1
Rho: 1
============= xn0: 0.77 =============
new_qn: 0.6930000000000001
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.8709953035623879
new_xn: 0.77
Lambda: 1
Rho: 1
============= xn0: 0.78 =============
new_qn: 0.7020000000000001
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.8799953035623878
new_xn: 0.78
Lambda: 1
Rho: 1
============= xn0: 0.79 =============
new_qn: 0.7110000000000001
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.8889953035623879
new_xn: 0.79
Lambda: 1
Rho: 1
============= xn0: 0.8 =============
new_qn: 0.7200000000000001
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.8979953035623878
new_xn: 0.8
Lambda: 1
Rho: 1
============= xn0: 0.81 =============
new_qn: 0.7290000000000001
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.9069953035623879
new_xn: 0.81
Lambda: 1
Rho: 1
============= xn0: 0.8200000000000001 =============
new_qn: 0.7380000000000001
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.9159953035623878
new_xn: 0.8200000000000001
Lambda: 1
Rho: 1
============= xn0: 0.8300000000000001 =============
new_qn: 0.7470000000000001
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.924995303562388
new_xn: 0.8300000000000001
Lambda: 1
Rho: 1
============= xn0: 0.8400000000000001 =============
new_qn: 0.7560000000000001
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.9339953035623878
new_xn: 0.8400000000000001
Lambda: 1
Rho: 1
============= xn0: 0.85 =============
new_qn: 0.765
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.9429953035623877
new_xn: 0.85
Lambda: 1
Rho: 1
============= xn0: 0.86 =============
new_qn: 0.774
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.9519953035623878
new_xn: 0.86
Lambda: 1
Rho: 1
============= xn0: 0.87 =============
new_qn: 0.783
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.9609953035623877
new_xn: 0.87
Lambda: 1
Rho: 1
============= xn0: 0.88 =============
new_qn: 0.792
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.9699953035623878
new_xn: 0.88
Lambda: 1
Rho: 1
============= xn0: 0.89 =============
new_qn: 0.801
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.9789953035623877
new_xn: 0.89
Lambda: 1
Rho: 1
============= xn0: 0.9 =============
new_qn: 0.81
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.9879953035623878
new_xn: 0.9
Lambda: 1
Rho: 1
============= xn0: 0.91 =============
new_qn: 0.8190000000000001
best_Eta: 1.3273403386954499
sum(new_qn_list): 1.9969953035623877
new_xn: 0.91
Lambda: 1
Rho: 1
============= xn0: 0.92 =============
new_qn: 0.8280000000000001
best_Eta: 1.3273403386954499
sum(new_qn_list): 2.0059953035623876
new_xn: 0.92
Lambda: 1
Rho: 1
============= xn0: 0.93 =============
new_qn: 0.8370000000000001
best_Eta: 1.3273403386954499
sum(new_qn_list): 2.0149953035623875
new_xn: 0.93
Lambda: 1
Rho: 1
============= xn0: 0.9400000000000001 =============
new_qn: 0.8460000000000001
best_Eta: 1.3273403386954499
sum(new_qn_list): 2.023995303562388
new_xn: 0.9400000000000001
Lambda: 1
Rho: 1
============= xn0: 0.9500000000000001 =============
new_qn: 0.8550000000000001
best_Eta: 1.3273403386954499
sum(new_qn_list): 2.032995303562388
new_xn: 0.9500000000000001
Lambda: 1
Rho: 1
============= xn0: 0.9600000000000001 =============
new_qn: 0.8640000000000001
best_Eta: 1.3273403386954499
sum(new_qn_list): 2.0419953035623877
new_xn: 0.9600000000000001
Lambda: 1
Rho: 1
============= xn0: 0.97 =============
new_qn: 0.873
best_Eta: 1.3273403386954499
sum(new_qn_list): 2.0509953035623876
new_xn: 0.97
Lambda: 1
Rho: 1
============= xn0: 0.98 =============
new_qn: 0.882
best_Eta: 1.3273403386954499
sum(new_qn_list): 2.0599953035623875
new_xn: 0.98
Lambda: 1
Rho: 1
============= xn0: 0.99 =============
new_qn: 0.891
best_Eta: 1.3273403386954499
sum(new_qn_list): 2.0689953035623874
new_xn: 0.99
Lambda: 1
Rho: 1
============= xn0: 1.0 =============
new_qn: 0.9
best_Eta: 1.3273403386954499
sum(new_qn_list): 2.0779953035623877
new_xn: 1.0
Lambda: 1
Rho: 1
DONE
最终的列表：
[6.411989365649046e-05, -2.322749482954259e-05, -0.00025864827951866537, -0.000638849217277429, -0.0011606340046355912, -0.0018208997369569754, -0.0026166335216840364, -0.003544909237890956, -0.004602884434823706, -0.0057877973625216905, -0.007096964128004993, -0.008527775970875381, -0.010077696652522311, -0.01174425995344458, -0.013525067273501196, -0.015417785330184675, -0.017420143950278993, -0.019529933950509892, -0.021745005103033066, -0.02406326418182292, -0.02648267308623506, -0.029001247038207556, -0.031617052849752525, -0.03432820725756072, -0.037132875321705616, -0.0400292688855875, -0.04301564509440106, -0.04609030496955033, -0.049251592036559655, -0.05249789100415381, -0.055827626492295446, -0.05923926180707484, -0.06273129776045128, -0.06630227153294205, -0.06995075557744707, -0.07367535656248303, -0.0774747143531841, -0.08134750102850458, -0.08529241993313103, -0.08930820476268209, -0.09339361868084134, -0.09754745346712868, -0.10176852869407893, -0.10605569093264822, -0.11040781298472702, -0.11482379314168528, -0.11930255446792698, -0.12384304410847341, -0.12844423261964277, -0.13310511332192992, -0.13782470167423283, -0.14260203466860893, -0.14743617024478023, -0.1523261867236379, -0.15727118225903242, -0.16227027430716406, -0.16732259911291725, -0.17242731121251165, -0.17758358295186838, -0.182790604020115, -0.18804758099767616, -0.19335373691842273, -0.19870831084536905, -0.20411055745943446, -0.2095597466607998, -0.21505516318241297, -0.2205961062152127, -0.2261818890446588, -0.23181183869817196, -0.23748529560310433, -0.24320161325487433, -0.24896015789491627, -0.2547603081981068, -0.260601454969345, -0.2664830008489748, -0.27240436002675034, -0.2783649579640568, -0.2843642311241102, -0.2904016267098697, -0.2964766024094062, -0.3025886261484825, -0.30873717585010485, -0.31492173920082167, -0.32114181342354553, -0.3273969050566904, -0.33368652973941815, -0.3400102120027987, -0.3463674850666951, -0.35275789064219065, -0.3591809787393826, -0.3656363074803737, -0.37212344291729704, -0.3786419588552189, -0.3851914366797654, -0.3917714651893278, -0.3983816404317061, -0.4050215655450523, -0.4116908506029846, -0.4183891124637419, -0.42511597462326056]
**** log-parameter_analysis 运行时间： 2025-01-18 00:18:45 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.02784850326365579
DataOwner2: noise random: 0.06629090374061021
DataOwner3: noise random: 0.08276295014073162
DataOwner4: noise random: 0.08906825438273014
DataOwner5: noise random: 0.0850626027297014
DataOwner6: noise random: 0.037112498415870325
DataOwner7: noise random: 0.013223230663772158
DataOwner8: noise random: 0.030552238669615796
DataOwner9: noise random: 0.015201265796472197
DataOwner10: noise random: 0.09195623362875928
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9996861425865886, 0.9982218453159829, 0.9972287400907122, 0.9968014650309558, 0.9970694251847845, 0.999440235129353, 0.9999293431665475, 0.9996223425617536, 0.99990633953505, 0.9965845841479302]
归一化后的数据质量列表avg_f_list: [0.9927289057715316, 0.9489500486863047, 0.9192586652490226, 0.9064842005602928, 0.9144955446462849, 0.9853768826252638, 1.0, 0.9908214432464334, 0.9993122484648553, 0.9]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3354
DataOwner1的最优x_1 = 0.1560
DataOwner2的最优x_2 = 0.1150
DataOwner3的最优x_3 = 0.0822
DataOwner4的最优x_4 = 0.0667
DataOwner5的最优x_5 = 0.0766
DataOwner6的最优x_6 = 0.1496
DataOwner7的最优x_7 = 0.1621
DataOwner8的最优x_8 = 0.1544
DataOwner9的最优x_9 = 0.1615
DataOwner10的最优x_10 = 0.0585
每个DataOwner应该贡献数据比例 xn_list = [0.15599495209799633, 0.11497572308425803, 0.08223454473181768, 0.06674290310425791, 0.07656280385233347, 0.14964928708103348, 0.16207197763123637, 0.15436828051260218, 0.1615054529648605, 0.05852906742201197]
ModelOwner的最大效用 U(Eta) = 0.5744
DONE
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.335385497747093
DataOwner1的分配到的支付 ： 0.1804
DataOwner2的分配到的支付 ： 0.1271
DataOwner3的分配到的支付 ： 0.0880
DataOwner4的分配到的支付 ： 0.0705
DataOwner5的分配到的支付 ： 0.0815
DataOwner6的分配到的支付 ： 0.1717
DataOwner7的分配到的支付 ： 0.1888
DataOwner8的分配到的支付 ： 0.1781
DataOwner9的分配到的支付 ： 0.1880
DataOwner10的分配到的支付 ： 0.0613
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC1', 'DataOwner2': 'CPC8', 'DataOwner4': 'CPC2', 'DataOwner3': 'CPC9', 'DataOwner5': 'CPC3', 'DataOwner6': 'CPC4', 'DataOwner8': 'CPC5', 'DataOwner9': 'CPC6', 'DataOwner7': 'CPC10', 'DataOwner10': 'CPC7'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC8
DataOwner4 把数据交给 CPC2
DataOwner3 把数据交给 CPC9
DataOwner5 把数据交给 CPC3
DataOwner6 把数据交给 CPC4
DataOwner8 把数据交给 CPC5
DataOwner9 把数据交给 CPC6
DataOwner7 把数据交给 CPC10
DataOwner10 把数据交给 CPC7
DONE
----- literation 1: 模型训练 -----
CPC1调整模型中, 本轮训练的数据量为：918.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 26.82%
Epoch 1/5, Loss: 2.2957
Epoch 2/5, Loss: 2.2922
Epoch 3/5, Loss: 2.2857
Epoch 4/5, Loss: 2.2797
Epoch 5/5, Loss: 2.2726
新模型评估：
Accuracy: 39.91%
Model saved to ../../../data/model/mnist_cnn_model
CPC8调整模型中, 本轮训练的数据量为：676.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 39.91%
Epoch 1/5, Loss: 2.2795
Epoch 2/5, Loss: 2.2734
Epoch 3/5, Loss: 2.2665
Epoch 4/5, Loss: 2.2609
Epoch 5/5, Loss: 2.2508
新模型评估：
Accuracy: 43.10%
Model saved to ../../../data/model/mnist_cnn_model
CPC2调整模型中, 本轮训练的数据量为：78.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 43.10%
Epoch 1/5, Loss: 2.2514
Epoch 2/5, Loss: 2.2810
Epoch 3/5, Loss: 2.2355
Epoch 4/5, Loss: 2.2382
Epoch 5/5, Loss: 2.2484
新模型评估：
Accuracy: 44.28%
Model saved to ../../../data/model/mnist_cnn_model
CPC9调整模型中, 本轮训练的数据量为：193.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 44.28%
Epoch 1/5, Loss: 2.2218
Epoch 2/5, Loss: 2.2399
Epoch 3/5, Loss: 2.2451
Epoch 4/5, Loss: 2.2204
Epoch 5/5, Loss: 2.2249
新模型评估：
Accuracy: 45.97%
Model saved to ../../../data/model/mnist_cnn_model
CPC3调整模型中, 本轮训练的数据量为：540.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 45.97%
Epoch 1/5, Loss: 2.2161
Epoch 2/5, Loss: 2.2065
Epoch 3/5, Loss: 2.2005
Epoch 4/5, Loss: 2.1888
Epoch 5/5, Loss: 2.1759
新模型评估：
Accuracy: 41.09%
CPC4调整模型中, 本轮训练的数据量为：175.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 45.97%
Epoch 1/5, Loss: 2.2153
Epoch 2/5, Loss: 2.2092
Epoch 3/5, Loss: 2.2093
Epoch 4/5, Loss: 2.2067
Epoch 5/5, Loss: 2.1974
新模型评估：
Accuracy: 48.19%
Model saved to ../../../data/model/mnist_cnn_model
CPC5调整模型中, 本轮训练的数据量为：544.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 48.19%
Epoch 1/5, Loss: 2.1981
Epoch 2/5, Loss: 2.1842
Epoch 3/5, Loss: 2.1774
Epoch 4/5, Loss: 2.1627
Epoch 5/5, Loss: 2.1535
新模型评估：
Accuracy: 50.22%
Model saved to ../../../data/model/mnist_cnn_model
CPC6调整模型中, 本轮训练的数据量为：2090.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 50.22%
Epoch 1/5, Loss: 2.1391
Epoch 2/5, Loss: 2.0928
Epoch 3/5, Loss: 2.0380
Epoch 4/5, Loss: 1.9698
Epoch 5/5, Loss: 1.8894
新模型评估：
Accuracy: 59.26%
Model saved to ../../../data/model/mnist_cnn_model
CPC10调整模型中, 本轮训练的数据量为：1334.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 59.26%
Epoch 1/5, Loss: 1.8170
Epoch 2/5, Loss: 1.7685
Epoch 3/5, Loss: 1.7198
Epoch 4/5, Loss: 1.6698
Epoch 5/5, Loss: 1.6190
新模型评估：
Accuracy: 63.85%
Model saved to ../../../data/model/mnist_cnn_model
CPC7调整模型中, 本轮训练的数据量为：688.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 63.85%
Epoch 1/5, Loss: 1.6167
Epoch 2/5, Loss: 1.5877
Epoch 3/5, Loss: 1.5628
Epoch 4/5, Loss: 1.5358
Epoch 5/5, Loss: 1.5117
新模型评估：
Accuracy: 65.29%
Model saved to ../../../data/model/mnist_cnn_model
DONE
========================= literation: 2 =========================
----- literation 2: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3354
DataOwner1的最优x_1 = 0.1560
DataOwner2的最优x_2 = 0.1150
DataOwner3的最优x_3 = 0.0822
DataOwner4的最优x_4 = 0.0667
DataOwner5的最优x_5 = 0.0766
DataOwner6的最优x_6 = 0.1496
DataOwner7的最优x_7 = 0.1621
DataOwner8的最优x_8 = 0.1544
DataOwner9的最优x_9 = 0.1615
DataOwner10的最优x_10 = 0.0585
每个DataOwner应该贡献数据比例 xn_list = [0.15599495209799633, 0.11497572308425803, 0.08223454473181768, 0.06674290310425791, 0.07656280385233347, 0.14964928708103348, 0.16207197763123637, 0.15436828051260218, 0.1615054529648605, 0.05852906742201197]
ModelOwner的最大效用 U(Eta) = 0.5744
DONE
----- literation 2: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.335385497747093
DataOwner1的分配到的支付 ： 0.1804
DataOwner2的分配到的支付 ： 0.1271
DataOwner3的分配到的支付 ： 0.0880
DataOwner4的分配到的支付 ： 0.0705
DataOwner5的分配到的支付 ： 0.0815
DataOwner6的分配到的支付 ： 0.1717
DataOwner7的分配到的支付 ： 0.1888
DataOwner8的分配到的支付 ： 0.1781
DataOwner9的分配到的支付 ： 0.1880
DataOwner10的分配到的支付 ： 0.0613
DONE
----- literation 2: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC8
DataOwner4 把数据交给 CPC2
DataOwner3 把数据交给 CPC9
DataOwner5 把数据交给 CPC3
DataOwner6 把数据交给 CPC4
DataOwner8 把数据交给 CPC5
DataOwner9 把数据交给 CPC6
DataOwner7 把数据交给 CPC10
DataOwner10 把数据交给 CPC7
DONE
----- literation 2: 模型训练 -----
重新调整fn，进而调整xn、Eta
正在评估DataOwner1的数据质量, 本轮评估的样本数据量为：918.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 65.29%
Epoch 1/5, Loss: 1.4579
Epoch 2/5, Loss: 1.4133
Epoch 3/5, Loss: 1.3857
Epoch 4/5, Loss: 1.3510
Epoch 5/5, Loss: 1.3126
新模型评估：
Accuracy: 68.58%
loss差为：
0.14524223009745274
单位数据loss差为：
0.0001582159369253298
正在评估DataOwner2的数据质量, 本轮评估的样本数据量为：676.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 65.29%
Epoch 1/5, Loss: 1.5257
Epoch 2/5, Loss: 1.5042
Epoch 3/5, Loss: 1.4732
Epoch 4/5, Loss: 1.4454
Epoch 5/5, Loss: 1.4273
新模型评估：
Accuracy: 69.49%
loss差为：
0.09843862056732178
单位数据loss差为：
0.00014561926119426298
正在评估DataOwner4的数据质量, 本轮评估的样本数据量为：78.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 65.29%
Epoch 1/5, Loss: 1.4073
Epoch 2/5, Loss: 1.3786
Epoch 3/5, Loss: 1.4111
Epoch 4/5, Loss: 1.4209
Epoch 5/5, Loss: 1.3300
新模型评估：
Accuracy: 62.88%
loss差为：
0.07731783390045166
单位数据loss差为：
0.0009912542807750213
正在评估DataOwner3的数据质量, 本轮评估的样本数据量为：193.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 65.29%
Epoch 1/5, Loss: 1.4975
Epoch 2/5, Loss: 1.7071
Epoch 3/5, Loss: 1.5980
Epoch 4/5, Loss: 1.4481
Epoch 5/5, Loss: 1.4999
新模型评估：
Accuracy: 66.67%
loss差为：
-0.0023751556873321533
单位数据loss差为：
-1.2306506151980069e-05
正在评估DataOwner5的数据质量, 本轮评估的样本数据量为：540.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 65.29%
Epoch 1/5, Loss: 1.4762
Epoch 2/5, Loss: 1.4592
Epoch 3/5, Loss: 1.4256
Epoch 4/5, Loss: 1.4088
Epoch 5/5, Loss: 1.3875
新模型评估：
Accuracy: 65.78%
loss差为：
0.08864871660868334
单位数据loss差为：
0.00016416429001608025
正在评估DataOwner6的数据质量, 本轮评估的样本数据量为：175.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 65.29%
Epoch 1/5, Loss: 1.4504
Epoch 2/5, Loss: 1.4403
Epoch 3/5, Loss: 1.4208
Epoch 4/5, Loss: 1.4242
Epoch 5/5, Loss: 1.4223
新模型评估：
Accuracy: 66.20%
loss差为：
0.028066913286844963
单位数据loss差为：
0.00016038236163911406
正在评估DataOwner8的数据质量, 本轮评估的样本数据量为：544.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 65.29%
Epoch 1/5, Loss: 1.4443
Epoch 2/5, Loss: 1.4242
Epoch 3/5, Loss: 1.3954
Epoch 4/5, Loss: 1.3768
Epoch 5/5, Loss: 1.3461
新模型评估：
Accuracy: 67.47%
loss差为：
0.09818904929690886
单位数据loss差为：
0.00018049457591343541
正在评估DataOwner9的数据质量, 本轮评估的样本数据量为：2090.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 65.29%
Epoch 1/5, Loss: 1.4373
Epoch 2/5, Loss: 1.3596
Epoch 3/5, Loss: 1.2850
Epoch 4/5, Loss: 1.2103
Epoch 5/5, Loss: 1.1388
新模型评估：
Accuracy: 72.98%
loss差为：
0.2985326412952307
单位数据loss差为：
0.00014283858435178504
正在评估DataOwner7的数据质量, 本轮评估的样本数据量为：1334.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 65.29%
Epoch 1/5, Loss: 1.4620
Epoch 2/5, Loss: 1.4068
Epoch 3/5, Loss: 1.3558
Epoch 4/5, Loss: 1.3060
Epoch 5/5, Loss: 1.2555
新模型评估：
Accuracy: 70.96%
loss差为：
0.2064365602674938
单位数据loss差为：
0.00015475004517803134
正在评估DataOwner10的数据质量, 本轮评估的样本数据量为：688.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 65.29%
Epoch 1/5, Loss: 1.4912
Epoch 2/5, Loss: 1.4573
Epoch 3/5, Loss: 1.4321
Epoch 4/5, Loss: 1.4059
Epoch 5/5, Loss: 1.3768
新模型评估：
Accuracy: 66.49%
loss差为：
0.11432087421417236
单位数据loss差为：
0.00016616406135780868
经过服务器调节后的真实数据质量：
数据质量列表avg_f_list: [0.0001582159369253298, 0.00014561926119426298, -1.2306506151980069e-05, 0.0009912542807750213, 0.00016416429001608025, 0.00016038236163911406, 0.00015475004517803134, 0.00018049457591343541, 0.00014283858435178504, 0.00016616406135780868]
归一化后的数据质量列表avg_f_list:[0.9169917403408583, 0.9157365422606664, 0.9, 1.0, 0.9175844650834187, 0.9172076141316645, 0.9166463809174484, 0.919211699438336, 0.9154594612030263, 0.9177837326681808]
CPC1调整模型中, 本轮训练的数据量为：918.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 65.29%
Epoch 1/5, Loss: 1.4594
Epoch 2/5, Loss: 1.4207
Epoch 3/5, Loss: 1.3796
Epoch 4/5, Loss: 1.3521
Epoch 5/5, Loss: 1.3271
新模型评估：
Accuracy: 68.77%
Model saved to ../../../data/model/mnist_cnn_model
CPC8调整模型中, 本轮训练的数据量为：676.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 68.77%
Epoch 1/5, Loss: 1.3615
Epoch 2/5, Loss: 1.3456
Epoch 3/5, Loss: 1.3127
Epoch 4/5, Loss: 1.2884
Epoch 5/5, Loss: 1.2627
新模型评估：
Accuracy: 72.51%
Model saved to ../../../data/model/mnist_cnn_model
CPC2调整模型中, 本轮训练的数据量为：78.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 72.51%
Epoch 1/5, Loss: 1.0456
Epoch 2/5, Loss: 1.1548
Epoch 3/5, Loss: 1.1003
Epoch 4/5, Loss: 1.0377
Epoch 5/5, Loss: 1.0148
新模型评估：
Accuracy: 68.70%
CPC9调整模型中, 本轮训练的数据量为：193.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 72.51%
Epoch 1/5, Loss: 1.2494
Epoch 2/5, Loss: 1.1597
Epoch 3/5, Loss: 1.2309
Epoch 4/5, Loss: 1.2119
Epoch 5/5, Loss: 1.3607
新模型评估：
Accuracy: 72.03%
CPC3调整模型中, 本轮训练的数据量为：540.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 72.51%
Epoch 1/5, Loss: 1.1864
Epoch 2/5, Loss: 1.1837
Epoch 3/5, Loss: 1.1487
Epoch 4/5, Loss: 1.1420
Epoch 5/5, Loss: 1.1180
新模型评估：
Accuracy: 71.06%
CPC4调整模型中, 本轮训练的数据量为：175.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 72.51%
Epoch 1/5, Loss: 1.1799
Epoch 2/5, Loss: 1.1602
Epoch 3/5, Loss: 1.1570
Epoch 4/5, Loss: 1.1497
Epoch 5/5, Loss: 1.1388
新模型评估：
Accuracy: 72.84%
Model saved to ../../../data/model/mnist_cnn_model
CPC5调整模型中, 本轮训练的数据量为：544.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 72.84%
Epoch 1/5, Loss: 1.1199
Epoch 2/5, Loss: 1.0980
Epoch 3/5, Loss: 1.0734
Epoch 4/5, Loss: 1.0482
Epoch 5/5, Loss: 1.0421
新模型评估：
Accuracy: 73.77%
Model saved to ../../../data/model/mnist_cnn_model
CPC6调整模型中, 本轮训练的数据量为：2090.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 73.77%
Epoch 1/5, Loss: 1.0412
Epoch 2/5, Loss: 0.9762
Epoch 3/5, Loss: 0.9189
Epoch 4/5, Loss: 0.8656
Epoch 5/5, Loss: 0.8167
新模型评估：
Accuracy: 76.70%
Model saved to ../../../data/model/mnist_cnn_model
CPC10调整模型中, 本轮训练的数据量为：1334.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 76.70%
Epoch 1/5, Loss: 0.7798
Epoch 2/5, Loss: 0.7488
Epoch 3/5, Loss: 0.7204
Epoch 4/5, Loss: 0.6942
Epoch 5/5, Loss: 0.6688
新模型评估：
Accuracy: 76.52%
CPC7调整模型中, 本轮训练的数据量为：688.00 :
Model loaded from ../../../data/model/mnist_cnn_model
原模型评估：
Accuracy: 76.70%
Epoch 1/5, Loss: 0.8114
Epoch 2/5, Loss: 0.7852
Epoch 3/5, Loss: 0.7683
Epoch 4/5, Loss: 0.7529
Epoch 5/5, Loss: 0.7358
新模型评估：
Accuracy: 75.71%
DONE
========================= literation: 3 =========================
----- literation 3: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.2962
DataOwner1的最优x_1 = 0.1105
DataOwner2的最优x_2 = 0.1092
DataOwner3的最优x_3 = 0.0921
DataOwner4的最优x_4 = 0.1823
DataOwner5的最优x_5 = 0.1111
DataOwner6的最优x_6 = 0.1107
DataOwner7的最优x_7 = 0.1102
DataOwner8的最优x_8 = 0.1128
DataOwner9的最优x_9 = 0.1089
DataOwner10的最优x_10 = 0.1113
每个DataOwner应该贡献数据比例 xn_list = [0.11051712151933754, 0.10920866671235967, 0.0921431271183382, 0.18230603817484353, 0.11113236500981065, 0.11074139162433874, 0.11015786451367077, 0.11281279747184338, 0.10891880636308894, 0.11133882550614851]
ModelOwner的最大效用 U(Eta) = 0.5308
xn开始变化：
new_xn_list: [0.15599495209799633, 0.11497572308425803, 0.0921431271183382, 0.18230603817484353, 0.11113236500981065, 0.14964928708103348, 0.16207197763123637, 0.15436828051260218, 0.1615054529648605, 0.11133882550614851]
avg_f_list: [0.9169917403408583, 0.9157365422606664, 0.9, 1.0, 0.9175844650834187, 0.9172076141316645, 0.9166463809174484, 0.919211699438336, 0.9154594612030263, 0.9177837326681808]
============= xn0: -1.01 =============
new_qn: -0.9261616577442668
best_Eta: 1.296166836280543
sum(new_qn_list): 0.22408994222891637
============= xn0: -1.0 =============
new_qn: -0.9169917403408583
best_Eta: 1.296166836280543
sum(new_qn_list): 0.23325985963232493
============= xn0: -0.99 =============
new_qn: -0.9078218229374497
best_Eta: 1.296166836280543
sum(new_qn_list): 0.24242977703573348
============= xn0: -0.98 =============
new_qn: -0.8986519055340411
best_Eta: 1.296166836280543
sum(new_qn_list): 0.25159969443914226
============= xn0: -0.97 =============
new_qn: -0.8894819881306325
best_Eta: 1.296166836280543
sum(new_qn_list): 0.2607696118425508
============= xn0: -0.96 =============
new_qn: -0.880312070727224
best_Eta: 1.296166836280543
sum(new_qn_list): 0.2699395292459594
============= xn0: -0.95 =============
new_qn: -0.8711421533238153
best_Eta: 1.296166836280543
sum(new_qn_list): 0.27910944664936793
============= xn0: -0.94 =============
new_qn: -0.8619722359204067
best_Eta: 1.296166836280543
sum(new_qn_list): 0.2882793640527765
============= xn0: -0.9299999999999999 =============
new_qn: -0.8528023185169982
best_Eta: 1.296166836280543
sum(new_qn_list): 0.2974492814561851
============= xn0: -0.9199999999999999 =============
new_qn: -0.8436324011135895
best_Eta: 1.296166836280543
sum(new_qn_list): 0.30661919885959377
============= xn0: -0.9099999999999999 =============
new_qn: -0.834462483710181
best_Eta: 1.296166836280543
sum(new_qn_list): 0.3157891162630023
============= xn0: -0.8999999999999999 =============
new_qn: -0.8252925663067724
best_Eta: 1.296166836280543
sum(new_qn_list): 0.3249590336664109
============= xn0: -0.8899999999999999 =============
new_qn: -0.8161226489033638
best_Eta: 1.296166836280543
sum(new_qn_list): 0.33412895106981944
============= xn0: -0.8799999999999999 =============
new_qn: -0.8069527314999552
best_Eta: 1.296166836280543
sum(new_qn_list): 0.3432988684732281
============= xn0: -0.8699999999999999 =============
new_qn: -0.7977828140965466
best_Eta: 1.296166836280543
sum(new_qn_list): 0.35246878587663666
============= xn0: -0.8599999999999999 =============
new_qn: -0.788612896693138
best_Eta: 1.296166836280543
sum(new_qn_list): 0.3616387032800452
============= xn0: -0.8499999999999999 =============
new_qn: -0.7794429792897294
best_Eta: 1.296166836280543
sum(new_qn_list): 0.3708086206834539
============= xn0: -0.8399999999999999 =============
new_qn: -0.7702730618863208
best_Eta: 1.296166836280543
sum(new_qn_list): 0.37997853808686244
============= xn0: -0.8299999999999998 =============
new_qn: -0.7611031444829123
best_Eta: 1.296166836280543
sum(new_qn_list): 0.389148455490271
============= xn0: -0.8199999999999998 =============
new_qn: -0.7519332270795036
best_Eta: 1.296166836280543
sum(new_qn_list): 0.39831837289367966
============= xn0: -0.8099999999999998 =============
new_qn: -0.742763309676095
best_Eta: 1.296166836280543
sum(new_qn_list): 0.4074882902970882
============= xn0: -0.7999999999999998 =============
new_qn: -0.7335933922726865
best_Eta: 1.296166836280543
sum(new_qn_list): 0.4166582077004968
============= xn0: -0.7899999999999998 =============
new_qn: -0.7244234748692778
best_Eta: 1.296166836280543
sum(new_qn_list): 0.42582812510390544
============= xn0: -0.7799999999999998 =============
new_qn: -0.7152535574658693
best_Eta: 1.296166836280543
sum(new_qn_list): 0.434998042507314
============= xn0: -0.7699999999999998 =============
new_qn: -0.7060836400624607
best_Eta: 1.296166836280543
sum(new_qn_list): 0.44416795991072255
============= xn0: -0.7599999999999998 =============
new_qn: -0.696913722659052
best_Eta: 1.296166836280543
sum(new_qn_list): 0.4533378773141312
============= xn0: -0.7499999999999998 =============
new_qn: -0.6877438052556435
best_Eta: 1.296166836280543
sum(new_qn_list): 0.46250779471753983
============= xn0: -0.7399999999999998 =============
new_qn: -0.6785738878522349
best_Eta: 1.296166836280543
sum(new_qn_list): 0.4716777121209484
============= xn0: -0.7299999999999998 =============
new_qn: -0.6694039704488264
best_Eta: 1.296166836280543
sum(new_qn_list): 0.48084762952435695
============= xn0: -0.7199999999999998 =============
new_qn: -0.6602340530454177
best_Eta: 1.296166836280543
sum(new_qn_list): 0.4900175469277656
============= xn0: -0.7099999999999997 =============
new_qn: -0.6510641356420092
best_Eta: 1.296166836280543
sum(new_qn_list): 0.49918746433117417
============= xn0: -0.6999999999999997 =============
new_qn: -0.6418942182386006
best_Eta: 1.296166836280543
sum(new_qn_list): 0.5083573817345827
============= xn0: -0.6899999999999997 =============
new_qn: -0.6327243008351919
best_Eta: 1.296166836280543
sum(new_qn_list): 0.5175272991379913
============= xn0: -0.6799999999999997 =============
new_qn: -0.6235543834317834
best_Eta: 1.296166836280543
sum(new_qn_list): 0.5266972165413999
============= xn0: -0.6699999999999997 =============
new_qn: -0.6143844660283748
best_Eta: 1.296166836280543
sum(new_qn_list): 0.5358671339448084
============= xn0: -0.6599999999999997 =============
new_qn: -0.6052145486249662
best_Eta: 1.296166836280543
sum(new_qn_list): 0.5450370513482171
============= xn0: -0.6499999999999997 =============
new_qn: -0.5960446312215576
best_Eta: 1.296166836280543
sum(new_qn_list): 0.5542069687516257
============= xn0: -0.6399999999999997 =============
new_qn: -0.586874713818149
best_Eta: 1.296166836280543
sum(new_qn_list): 0.5633768861550342
============= xn0: -0.6299999999999997 =============
new_qn: -0.5777047964147404
best_Eta: 1.296166836280543
sum(new_qn_list): 0.5725468035584429
============= xn0: -0.6199999999999997 =============
new_qn: -0.5685348790113318
best_Eta: 1.296166836280543
sum(new_qn_list): 0.5817167209618515
============= xn0: -0.6099999999999997 =============
new_qn: -0.5593649616079233
best_Eta: 1.296166836280543
sum(new_qn_list): 0.59088663836526
============= xn0: -0.5999999999999996 =============
new_qn: -0.5501950442045146
best_Eta: 1.296166836280543
sum(new_qn_list): 0.6000565557686687
============= xn0: -0.5899999999999996 =============
new_qn: -0.541025126801106
best_Eta: 1.296166836280543
sum(new_qn_list): 0.6092264731720772
============= xn0: -0.5799999999999996 =============
new_qn: -0.5318552093976975
best_Eta: 1.296166836280543
sum(new_qn_list): 0.6183963905754858
============= xn0: -0.5699999999999996 =============
new_qn: -0.5226852919942889
best_Eta: 1.296166836280543
sum(new_qn_list): 0.6275663079788943
============= xn0: -0.5599999999999996 =============
new_qn: -0.5135153745908803
best_Eta: 1.296166836280543
sum(new_qn_list): 0.636736225382303
============= xn0: -0.5499999999999996 =============
new_qn: -0.5043454571874717
best_Eta: 1.296166836280543
sum(new_qn_list): 0.6459061427857116
============= xn0: -0.5399999999999996 =============
new_qn: -0.4951755397840631
best_Eta: 1.296166836280543
sum(new_qn_list): 0.6550760601891201
============= xn0: -0.5299999999999996 =============
new_qn: -0.48600562238065453
best_Eta: 1.296166836280543
sum(new_qn_list): 0.6642459775925287
============= xn0: -0.5199999999999996 =============
new_qn: -0.4768357049772459
best_Eta: 1.296166836280543
sum(new_qn_list): 0.6734158949959373
============= xn0: -0.5099999999999996 =============
new_qn: -0.4676657875738373
best_Eta: 1.296166836280543
sum(new_qn_list): 0.6825858123993459
============= xn0: -0.49999999999999956 =============
new_qn: -0.45849587017042875
best_Eta: 1.296166836280543
sum(new_qn_list): 0.6917557298027545
============= xn0: -0.48999999999999955 =============
new_qn: -0.44932595276702014
best_Eta: 1.296166836280543
sum(new_qn_list): 0.7009256472061631
============= xn0: -0.47999999999999954 =============
new_qn: -0.44015603536361153
best_Eta: 1.296166836280543
sum(new_qn_list): 0.7100955646095717
============= xn0: -0.46999999999999953 =============
new_qn: -0.430986117960203
best_Eta: 1.296166836280543
sum(new_qn_list): 0.7192654820129802
============= xn0: -0.4599999999999995 =============
new_qn: -0.42181620055679436
best_Eta: 1.296166836280543
sum(new_qn_list): 0.7284353994163889
============= xn0: -0.4499999999999995 =============
new_qn: -0.4126462831533858
best_Eta: 1.296166836280543
sum(new_qn_list): 0.7376053168197975
============= xn0: -0.4399999999999995 =============
new_qn: -0.4034763657499772
best_Eta: 1.296166836280543
sum(new_qn_list): 0.746775234223206
============= xn0: -0.4299999999999995 =============
new_qn: -0.3943064483465686
best_Eta: 1.296166836280543
sum(new_qn_list): 0.7559451516266147
============= xn0: -0.4199999999999995 =============
new_qn: -0.38513653094316
best_Eta: 1.296166836280543
sum(new_qn_list): 0.7651150690300232
============= xn0: -0.4099999999999995 =============
new_qn: -0.3759666135397514
best_Eta: 1.296166836280543
sum(new_qn_list): 0.7742849864334318
============= xn0: -0.39999999999999947 =============
new_qn: -0.3667966961363428
best_Eta: 1.296166836280543
sum(new_qn_list): 0.7834549038368405
============= xn0: -0.38999999999999946 =============
new_qn: -0.35762677873293425
best_Eta: 1.296166836280543
sum(new_qn_list): 0.792624821240249
============= xn0: -0.37999999999999945 =============
new_qn: -0.34845686132952564
best_Eta: 1.296166836280543
sum(new_qn_list): 0.8017947386436576
============= xn0: -0.36999999999999944 =============
new_qn: -0.3392869439261171
best_Eta: 1.296166836280543
sum(new_qn_list): 0.8109646560470661
============= xn0: -0.35999999999999943 =============
new_qn: -0.33011702652270847
best_Eta: 1.296166836280543
sum(new_qn_list): 0.8201345734504747
============= xn0: -0.3499999999999994 =============
new_qn: -0.32094710911929986
best_Eta: 1.296166836280543
sum(new_qn_list): 0.8293044908538834
============= xn0: -0.3399999999999994 =============
new_qn: -0.3117771917158913
best_Eta: 1.296166836280543
sum(new_qn_list): 0.8384744082572919
============= xn0: -0.3299999999999994 =============
new_qn: -0.3026072743124827
best_Eta: 1.296166836280543
sum(new_qn_list): 0.8476443256607005
============= xn0: -0.3199999999999994 =============
new_qn: -0.2934373569090741
best_Eta: 1.296166836280543
sum(new_qn_list): 0.8568142430641091
============= xn0: -0.3099999999999994 =============
new_qn: -0.2842674395056655
best_Eta: 1.296166836280543
sum(new_qn_list): 0.8659841604675177
============= xn0: -0.2999999999999994 =============
new_qn: -0.2750975221022569
best_Eta: 1.296166836280543
sum(new_qn_list): 0.8751540778709263
============= xn0: -0.28999999999999937 =============
new_qn: -0.26592760469884835
best_Eta: 1.296166836280543
sum(new_qn_list): 0.8843239952743348
============= xn0: -0.27999999999999936 =============
new_qn: -0.25675768729543974
best_Eta: 1.296166836280543
sum(new_qn_list): 0.8934939126777436
============= xn0: -0.26999999999999935 =============
new_qn: -0.24758776989203113
best_Eta: 1.296166836280543
sum(new_qn_list): 0.9026638300811521
============= xn0: -0.25999999999999934 =============
new_qn: -0.23841785248862254
best_Eta: 1.296166836280543
sum(new_qn_list): 0.9118337474845607
============= xn0: -0.24999999999999933 =============
new_qn: -0.22924793508521396
best_Eta: 1.296166836280543
sum(new_qn_list): 0.9210036648879693
============= xn0: -0.23999999999999932 =============
new_qn: -0.22007801768180538
best_Eta: 1.296166836280543
sum(new_qn_list): 0.9301735822913778
============= xn0: -0.22999999999999932 =============
new_qn: -0.21090810027839677
best_Eta: 1.296166836280543
sum(new_qn_list): 0.9393434996947866
============= xn0: -0.2199999999999993 =============
new_qn: -0.20173818287498818
best_Eta: 1.296166836280543
sum(new_qn_list): 0.9485134170981951
============= xn0: -0.2099999999999993 =============
new_qn: -0.1925682654715796
best_Eta: 1.296166836280543
sum(new_qn_list): 0.9576833345016037
============= xn0: -0.1999999999999993 =============
new_qn: -0.183398348068171
best_Eta: 1.296166836280543
sum(new_qn_list): 0.9668532519050123
============= xn0: -0.18999999999999928 =============
new_qn: -0.1742284306647624
best_Eta: 1.296166836280543
sum(new_qn_list): 0.9760231693084208
============= xn0: -0.17999999999999927 =============
new_qn: -0.16505851326135382
best_Eta: 1.296166836280543
sum(new_qn_list): 0.9851930867118294
============= xn0: -0.16999999999999926 =============
new_qn: -0.15588859585794523
best_Eta: 1.296166836280543
sum(new_qn_list): 0.9943630041152379
============= xn0: -0.15999999999999925 =============
new_qn: -0.14671867845453665
best_Eta: 1.296166836280543
sum(new_qn_list): 1.0035329215186468
============= xn0: -0.14999999999999925 =============
new_qn: -0.13754876105112804
best_Eta: 1.296166836280543
sum(new_qn_list): 1.0127028389220554
============= xn0: -0.13999999999999924 =============
new_qn: -0.12837884364771945
best_Eta: 1.296166836280543
sum(new_qn_list): 1.021872756325464
============= xn0: -0.12999999999999923 =============
new_qn: -0.11920892624431087
best_Eta: 1.296166836280543
sum(new_qn_list): 1.0310426737288725
============= xn0: -0.11999999999999922 =============
new_qn: -0.11003900884090227
best_Eta: 1.296166836280543
sum(new_qn_list): 1.040212591132281
============= xn0: -0.10999999999999921 =============
new_qn: -0.10086909143749369
best_Eta: 1.296166836280543
sum(new_qn_list): 1.0493825085356896
============= xn0: -0.0999999999999992 =============
new_qn: -0.09169917403408509
best_Eta: 1.296166836280543
sum(new_qn_list): 1.0585524259390984
============= xn0: -0.08999999999999919 =============
new_qn: -0.0825292566306765
best_Eta: 1.296166836280543
sum(new_qn_list): 1.067722343342507
============= xn0: -0.07999999999999918 =============
new_qn: -0.07335933922726791
best_Eta: 1.296166836280543
sum(new_qn_list): 1.0768922607459155
============= xn0: -0.06999999999999917 =============
new_qn: -0.06418942182385932
best_Eta: 1.296166836280543
sum(new_qn_list): 1.086062178149324
============= xn0: -0.059999999999999165 =============
new_qn: -0.055019504420450734
best_Eta: 1.296166836280543
sum(new_qn_list): 1.0952320955527326
============= xn0: -0.049999999999999156 =============
new_qn: -0.04584958701704214
best_Eta: 1.296166836280543
sum(new_qn_list): 1.1044020129561412
============= xn0: -0.03999999999999915 =============
new_qn: -0.03667966961363355
best_Eta: 1.296166836280543
sum(new_qn_list): 1.1135719303595495
============= xn0: -0.02999999999999914 =============
new_qn: -0.027509752210224957
best_Eta: 1.296166836280543
sum(new_qn_list): 1.1227418477629583
============= xn0: -0.01999999999999913 =============
new_qn: -0.018339834806816366
best_Eta: 1.296166836280543
sum(new_qn_list): 1.131911765166367
============= xn0: -0.00999999999999912 =============
new_qn: -0.009169917403407777
best_Eta: 1.296166836280543
sum(new_qn_list): 1.1410816825697754
============= xn0: 8.881784197001252e-16 =============
new_qn: 8.144522748140111e-16
best_Eta: 1.296166836280543
sum(new_qn_list): 1.1502515999731842
============= xn0: 0.010000000000000897 =============
new_qn: 0.009169917403409406
best_Eta: 1.296166836280543
sum(new_qn_list): 1.1594215173765925
============= xn0: 0.020000000000000906 =============
new_qn: 0.018339834806817997
best_Eta: 1.296166836280543
sum(new_qn_list): 1.1685914347800013
============= xn0: 0.030000000000000915 =============
new_qn: 0.027509752210226588
best_Eta: 1.296166836280543
sum(new_qn_list): 1.17776135218341
============= xn0: 0.040000000000000924 =============
new_qn: 0.036679669613635175
best_Eta: 1.296166836280543
sum(new_qn_list): 1.1869312695868184
============= xn0: 0.05000000000000093 =============
new_qn: 0.045849587017043766
best_Eta: 1.296166836280543
sum(new_qn_list): 1.1961011869902272
============= xn0: 0.06000000000000094 =============
new_qn: 0.05501950442045236
best_Eta: 1.296166836280543
sum(new_qn_list): 1.2052711043936355
============= xn0: 0.07000000000000095 =============
new_qn: 0.06418942182386095
best_Eta: 1.296166836280543
sum(new_qn_list): 1.2144410217970443
============= xn0: 0.08000000000000096 =============
new_qn: 0.07335933922726955
best_Eta: 1.296166836280543
sum(new_qn_list): 1.223610939200453
============= xn0: 0.09000000000000097 =============
new_qn: 0.08252925663067813
best_Eta: 1.296166836280543
sum(new_qn_list): 1.2327808566038614
============= xn0: 0.10000000000000098 =============
new_qn: 0.09169917403408673
best_Eta: 1.296166836280543
sum(new_qn_list): 1.2419507740072702
============= xn0: 0.11000000000000099 =============
new_qn: 0.10086909143749531
best_Eta: 1.296166836280543
sum(new_qn_list): 1.2511206914106785
============= xn0: 0.120000000000001 =============
new_qn: 0.11003900884090391
best_Eta: 1.296166836280543
sum(new_qn_list): 1.260290608814087
============= xn0: 0.130000000000001 =============
new_qn: 0.1192089262443125
best_Eta: 1.296166836280543
sum(new_qn_list): 1.2694605262174956
============= xn0: 0.140000000000001 =============
new_qn: 0.1283788436477211
best_Eta: 1.296166836280543
sum(new_qn_list): 1.2786304436209044
============= xn0: 0.15000000000000102 =============
new_qn: 0.13754876105112968
best_Eta: 1.296166836280543
sum(new_qn_list): 1.2878003610243127
============= xn0: 0.16000000000000103 =============
new_qn: 0.14671867845453826
best_Eta: 1.296166836280543
sum(new_qn_list): 1.2969702784277215
============= xn0: 0.17000000000000104 =============
new_qn: 0.15588859585794687
best_Eta: 1.296166836280543
sum(new_qn_list): 1.30614019583113
============= xn0: 0.18000000000000105 =============
new_qn: 0.16505851326135546
best_Eta: 1.296166836280543
sum(new_qn_list): 1.3153101132345386
============= xn0: 0.19000000000000106 =============
new_qn: 0.17422843066476404
best_Eta: 1.296166836280543
sum(new_qn_list): 1.3244800306379472
============= xn0: 0.20000000000000107 =============
new_qn: 0.18339834806817262
best_Eta: 1.296166836280543
sum(new_qn_list): 1.3336499480413557
============= xn0: 0.21000000000000107 =============
new_qn: 0.19256826547158123
best_Eta: 1.296166836280543
sum(new_qn_list): 1.3428198654447643
============= xn0: 0.22000000000000108 =============
new_qn: 0.20173818287498982
best_Eta: 1.296166836280543
sum(new_qn_list): 1.351989782848173
============= xn0: 0.2300000000000011 =============
new_qn: 0.2109081002783984
best_Eta: 1.296166836280543
sum(new_qn_list): 1.3611597002515816
============= xn0: 0.2400000000000011 =============
new_qn: 0.220078017681807
best_Eta: 1.296166836280543
sum(new_qn_list): 1.3703296176549902
============= xn0: 0.2500000000000011 =============
new_qn: 0.2292479350852156
best_Eta: 1.296166836280543
sum(new_qn_list): 1.3794995350583987
============= xn0: 0.2600000000000011 =============
new_qn: 0.23841785248862418
best_Eta: 1.296166836280543
sum(new_qn_list): 1.3886694524618073
============= xn0: 0.27000000000000113 =============
new_qn: 0.24758776989203277
best_Eta: 1.296166836280543
sum(new_qn_list): 1.3978393698652158
============= xn0: 0.28000000000000114 =============
new_qn: 0.25675768729544135
best_Eta: 1.296166836280543
sum(new_qn_list): 1.4070092872686244
============= xn0: 0.29000000000000115 =============
new_qn: 0.26592760469884996
best_Eta: 1.296166836280543
sum(new_qn_list): 1.4161792046720332
============= xn0: 0.30000000000000115 =============
new_qn: 0.2750975221022585
best_Eta: 1.296166836280543
sum(new_qn_list): 1.4253491220754417
============= xn0: 0.31000000000000116 =============
new_qn: 0.28426743950566713
best_Eta: 1.296166836280543
sum(new_qn_list): 1.4345190394788503
============= xn0: 0.3200000000000012 =============
new_qn: 0.29343735690907574
best_Eta: 1.296166836280543
sum(new_qn_list): 1.4436889568822588
============= xn0: 0.3300000000000012 =============
new_qn: 0.3026072743124843
best_Eta: 1.296166836280543
sum(new_qn_list): 1.4528588742856674
============= xn0: 0.3400000000000012 =============
new_qn: 0.3117771917158929
best_Eta: 1.296166836280543
sum(new_qn_list): 1.462028791689076
============= xn0: 0.3500000000000012 =============
new_qn: 0.3209471091193015
best_Eta: 1.296166836280543
sum(new_qn_list): 1.4711987090924845
============= xn0: 0.3600000000000012 =============
new_qn: 0.3301170265227101
best_Eta: 1.296166836280543
sum(new_qn_list): 1.480368626495893
============= xn0: 0.3700000000000012 =============
new_qn: 0.3392869439261187
best_Eta: 1.296166836280543
sum(new_qn_list): 1.4895385438993018
============= xn0: 0.3800000000000012 =============
new_qn: 0.34845686132952725
best_Eta: 1.296166836280543
sum(new_qn_list): 1.4987084613027104
============= xn0: 0.39000000000000123 =============
new_qn: 0.35762677873293586
best_Eta: 1.296166836280543
sum(new_qn_list): 1.507878378706119
============= xn0: 0.40000000000000124 =============
new_qn: 0.36679669613634447
best_Eta: 1.296166836280543
sum(new_qn_list): 1.5170482961095275
============= xn0: 0.41000000000000125 =============
new_qn: 0.375966613539753
best_Eta: 1.296166836280543
sum(new_qn_list): 1.526218213512936
============= xn0: 0.42000000000000126 =============
new_qn: 0.38513653094316164
best_Eta: 1.296166836280543
sum(new_qn_list): 1.5353881309163446
============= xn0: 0.43000000000000127 =============
new_qn: 0.39430644834657025
best_Eta: 1.296166836280543
sum(new_qn_list): 1.5445580483197534
============= xn0: 0.4400000000000013 =============
new_qn: 0.4034763657499788
best_Eta: 1.296166836280543
sum(new_qn_list): 1.553727965723162
============= xn0: 0.4500000000000013 =============
new_qn: 0.4126462831533874
best_Eta: 1.296166836280543
sum(new_qn_list): 1.5628978831265705
============= xn0: 0.4600000000000013 =============
new_qn: 0.42181620055679603
best_Eta: 1.296166836280543
sum(new_qn_list): 1.572067800529979
============= xn0: 0.4700000000000013 =============
new_qn: 0.4309861179602046
best_Eta: 1.296166836280543
sum(new_qn_list): 1.5812377179333876
============= xn0: 0.4800000000000013 =============
new_qn: 0.4401560353636132
best_Eta: 1.296166836280543
sum(new_qn_list): 1.5904076353367962
============= xn0: 0.4900000000000013 =============
new_qn: 0.44932595276702175
best_Eta: 1.296166836280543
sum(new_qn_list): 1.5995775527402047
============= xn0: 0.5000000000000013 =============
new_qn: 0.45849587017043036
best_Eta: 1.296166836280543
sum(new_qn_list): 1.6087474701436135
============= xn0: 0.5100000000000013 =============
new_qn: 0.467665787573839
best_Eta: 1.296166836280543
sum(new_qn_list): 1.617917387547022
============= xn0: 0.5200000000000014 =============
new_qn: 0.47683570497724753
best_Eta: 1.296166836280543
sum(new_qn_list): 1.6270873049504306
============= xn0: 0.5300000000000014 =============
new_qn: 0.48600562238065614
best_Eta: 1.296166836280543
sum(new_qn_list): 1.6362572223538392
============= xn0: 0.5400000000000014 =============
new_qn: 0.49517553978406476
best_Eta: 1.296166836280543
sum(new_qn_list): 1.6454271397572477
============= xn0: 0.5500000000000014 =============
new_qn: 0.5043454571874734
best_Eta: 1.296166836280543
sum(new_qn_list): 1.6545970571606565
============= xn0: 0.5600000000000014 =============
new_qn: 0.5135153745908819
best_Eta: 1.296166836280543
sum(new_qn_list): 1.663766974564065
============= xn0: 0.5700000000000014 =============
new_qn: 0.5226852919942905
best_Eta: 1.296166836280543
sum(new_qn_list): 1.6729368919674736
============= xn0: 0.5800000000000014 =============
new_qn: 0.5318552093976991
best_Eta: 1.296166836280543
sum(new_qn_list): 1.6821068093708822
============= xn0: 0.5900000000000014 =============
new_qn: 0.5410251268011077
best_Eta: 1.296166836280543
sum(new_qn_list): 1.6912767267742908
============= xn0: 0.6000000000000014 =============
new_qn: 0.5501950442045163
best_Eta: 1.296166836280543
sum(new_qn_list): 1.7004466441776993
============= xn0: 0.6100000000000014 =============
new_qn: 0.5593649616079248
best_Eta: 1.296166836280543
sum(new_qn_list): 1.7096165615811079
============= xn0: 0.6200000000000014 =============
new_qn: 0.5685348790113335
best_Eta: 1.296166836280543
sum(new_qn_list): 1.7187864789845166
============= xn0: 0.6300000000000014 =============
new_qn: 0.577704796414742
best_Eta: 1.296166836280543
sum(new_qn_list): 1.7279563963879252
============= xn0: 0.6400000000000015 =============
new_qn: 0.5868747138181506
best_Eta: 1.296166836280543
sum(new_qn_list): 1.7371263137913338
============= xn0: 0.6500000000000015 =============
new_qn: 0.5960446312215593
best_Eta: 1.296166836280543
sum(new_qn_list): 1.7462962311947423
============= xn0: 0.6600000000000015 =============
new_qn: 0.6052145486249678
best_Eta: 1.296166836280543
sum(new_qn_list): 1.7554661485981509
============= xn0: 0.6700000000000015 =============
new_qn: 0.6143844660283764
best_Eta: 1.296166836280543
sum(new_qn_list): 1.7646360660015594
============= xn0: 0.6800000000000015 =============
new_qn: 0.623554383431785
best_Eta: 1.296166836280543
sum(new_qn_list): 1.7738059834049682
============= xn0: 0.6900000000000015 =============
new_qn: 0.6327243008351936
best_Eta: 1.296166836280543
sum(new_qn_list): 1.7829759008083768
============= xn0: 0.7000000000000015 =============
new_qn: 0.6418942182386022
best_Eta: 1.296166836280543
sum(new_qn_list): 1.7921458182117853
============= xn0: 0.7100000000000015 =============
new_qn: 0.6510641356420108
best_Eta: 1.296166836280543
sum(new_qn_list): 1.8013157356151939
============= xn0: 0.7200000000000015 =============
new_qn: 0.6602340530454194
best_Eta: 1.296166836280543
sum(new_qn_list): 1.8104856530186024
============= xn0: 0.7300000000000015 =============
new_qn: 0.6694039704488279
best_Eta: 1.296166836280543
sum(new_qn_list): 1.819655570422011
============= xn0: 0.7400000000000015 =============
new_qn: 0.6785738878522366
best_Eta: 1.296166836280543
sum(new_qn_list): 1.8288254878254198
============= xn0: 0.7500000000000016 =============
new_qn: 0.6877438052556452
best_Eta: 1.296166836280543
sum(new_qn_list): 1.8379954052288283
============= xn0: 0.7600000000000016 =============
new_qn: 0.6969137226590537
best_Eta: 1.296166836280543
sum(new_qn_list): 1.8471653226322369
============= xn0: 0.7700000000000016 =============
new_qn: 0.7060836400624623
best_Eta: 1.296166836280543
sum(new_qn_list): 1.8563352400356454
============= xn0: 0.7800000000000016 =============
new_qn: 0.7152535574658709
best_Eta: 1.296166836280543
sum(new_qn_list): 1.865505157439054
============= xn0: 0.7900000000000016 =============
new_qn: 0.7244234748692795
best_Eta: 1.296166836280543
sum(new_qn_list): 1.8746750748424625
============= xn0: 0.8000000000000016 =============
new_qn: 0.733593392272688
best_Eta: 1.296166836280543
sum(new_qn_list): 1.883844992245871
============= xn0: 0.8100000000000016 =============
new_qn: 0.7427633096760967
best_Eta: 1.296166836280543
sum(new_qn_list): 1.8930149096492799
============= xn0: 0.8200000000000016 =============
new_qn: 0.7519332270795053
best_Eta: 1.296166836280543
sum(new_qn_list): 1.9021848270526884
============= xn0: 0.8300000000000016 =============
new_qn: 0.7611031444829138
best_Eta: 1.296166836280543
sum(new_qn_list): 1.911354744456097
============= xn0: 0.8400000000000016 =============
new_qn: 0.7702730618863225
best_Eta: 1.296166836280543
sum(new_qn_list): 1.9205246618595055
============= xn0: 0.8500000000000016 =============
new_qn: 0.779442979289731
best_Eta: 1.296166836280543
sum(new_qn_list): 1.929694579262914
============= xn0: 0.8600000000000017 =============
new_qn: 0.7886128966931396
best_Eta: 1.296166836280543
sum(new_qn_list): 1.9388644966663227
============= xn0: 0.8700000000000017 =============
new_qn: 0.7977828140965483
best_Eta: 1.296166836280543
sum(new_qn_list): 1.9480344140697314
============= xn0: 0.8800000000000017 =============
new_qn: 0.8069527314999568
best_Eta: 1.296166836280543
sum(new_qn_list): 1.95720433147314
============= xn0: 0.8900000000000017 =============
new_qn: 0.8161226489033654
best_Eta: 1.296166836280543
sum(new_qn_list): 1.9663742488765485
============= xn0: 0.9000000000000017 =============
new_qn: 0.8252925663067741
best_Eta: 1.296166836280543
sum(new_qn_list): 1.9755441662799573
============= xn0: 0.9100000000000017 =============
new_qn: 0.8344624837101826
best_Eta: 1.296166836280543
sum(new_qn_list): 1.9847140836833659
============= xn0: 0.9200000000000017 =============
new_qn: 0.8436324011135912
best_Eta: 1.296166836280543
sum(new_qn_list): 1.9938840010867744
============= xn0: 0.9300000000000017 =============
new_qn: 0.8528023185169997
best_Eta: 1.296166836280543
sum(new_qn_list): 2.003053918490183
============= xn0: 0.9400000000000017 =============
new_qn: 0.8619722359204084
best_Eta: 1.296166836280543
sum(new_qn_list): 2.0122238358935918
============= xn0: 0.9500000000000017 =============
new_qn: 0.871142153323817
best_Eta: 1.296166836280543
sum(new_qn_list): 2.021393753297
============= xn0: 0.9600000000000017 =============
new_qn: 0.8803120707272255
best_Eta: 1.296166836280543
sum(new_qn_list): 2.030563670700409
============= xn0: 0.9700000000000017 =============
new_qn: 0.8894819881306342
best_Eta: 1.296166836280543
sum(new_qn_list): 2.0397335881038177
============= xn0: 0.9800000000000018 =============
new_qn: 0.8986519055340427
best_Eta: 1.296166836280543
sum(new_qn_list): 2.048903505507226
============= xn0: 0.9900000000000018 =============
new_qn: 0.9078218229374513
best_Eta: 1.296166836280543
sum(new_qn_list): 2.0580734229106343
============= xn0: 1.0000000000000016 =============
new_qn: 0.9169917403408597
best_Eta: 1.296166836280543
sum(new_qn_list): 2.067243340314043
DONE
最终的列表：
[-4.347045541010555, -4.095494290558546, -3.863729416126257, -3.6495874878150363, -3.4512093822493597, -3.2669885957103073, -3.0955297460527795, -2.935614994731562, -2.7861766797803256, -2.6462748595070416, -2.5150787687205334, -2.3918514146427, -2.2759367093422727, -2.1667486644157306, -2.0637622723517657, -1.9665057751997028, -1.8745540803904246, -1.7875231299223704, -1.7050650656565518, -1.6268640624267934, -1.552632723767888, -1.482108953586001, -1.4150532320277163, -1.3512462359031225, -1.2904867538693343, -1.2325898546384746, -1.1773852730939571, -1.124715984660486, -1.0744369427973626, -1.0264139582469085, -0.9805227018100198, -0.9366478150512902, -0.8946821155471008, -0.854525885154569, -0.8160862313566126, -0.7792765130769039, -0.7440158234976781, -0.7102285233856538, -0.6778438192635114, -0.6467953814784462, -0.6170209978336787, -0.5884622589786819, -0.5610642722119488, -0.5347754007470873, -0.5095470258377766, -0.4853333294571789, -0.4620910954891304, -0.43977952761718, -0.4183600822978849, -0.3977963153805393, -0.37805374109004897, -0.3590997022257453, -0.34090325054900994, -0.32343503643868443, -0.30666720698718064, -0.2905733117934993, -0.27512821578334623, -0.26030801845231866, -0.2460899789867712, -0.23245244676924648, -0.21937479682208183, -0.20683736978460587, -0.19482141605677372, -0.1833090437756919, -0.1722831703216502, -0.16172747707741686, -0.15162636718899136, -0.14196492609803535, -0.13272888463609356, -0.12390458448868397, -0.1154789458535947, -0.10743943713245124, -0.09977404650796695, -0.09247125527140654, -0.08552001277580651, -0.0789097129005002, -0.07263017192162219, -0.06667160769156566, -0.061024620037945954, -0.05568017229954403, -0.0506295739230197, -0.04586446404997335, -0.04137679602922245, -0.03715882279401189, -0.033203083048324145, -0.0295023882105358, -0.026049810066417628, -0.022838669086918756, -0.019862523369349572, -0.01711515816349321, -0.014590575946867163, -0.012282987015834454, -0.010186800561553092, -0.008296616201864102, -0.006607215942171021, -0.005113556540168318, -0.003810762250946731, -0.002694117930549189, -0.001759062477484273, -0.001001182593031915, -0.0004162068424081976, 2.959124771157755e-17, 0.00025144233705764844, 0.0003419988817008751, 0.0002754275499754835, 5.537012726113122e-05, -0.00031464328014622206, -0.0008311899966623598, -0.0014909507215129097, -0.0022907056552042976, -0.0032273307988721495, -0.004297794417571554, -0.0054991536590984075, -0.006828551320420179, -0.008283212754252284, -0.00986044290874466, -0.011557623493643449, -0.013372210266668932, -0.01530173043420091, -0.01734378016069385, -0.01949602218155211, -0.021756183514485217, -0.024122053264636623, -0.02659148051903265, -0.029162372326140246, -0.03183269175654685, -0.03460045604098691, -0.0374637347821411, -0.04042064823681904, -0.04346936566531587, -0.046608103744897156, -0.049835125044524586, -0.05314873655808183, -0.056547288293499975, -0.060029171915311874, -0.06359281943829109, -0.06723670196994558, -0.07095932849974751, -0.07475924473308587, -0.07863503196802657, -0.08258530601305691, -0.08660871614408089, -0.09070394409901406, -0.09486970310840692, -0.09910473696059857, -0.10340781909997443, -0.10777775175696874, -0.11221336510851526, -0.11671351646771028, -0.1212770895015084, -0.12590299347532802, -0.13059016252348982, -0.1353375549444661, -0.14014415251996065, -0.14500895985688478, -0.14993100375133644, -0.15490933257372813, -0.15994301567424862, -0.16503114280787756, -0.1701728235782075, -0.17536718689935865, -0.18061338047530395, -0.18591057029595126, -0.19125794014935504, -0.1966546911494616, -0.20210004127880954, -0.2075932249456401, -0.21313349255488734, -0.21872011009254472, -0.22435235872292353, -0.23002953439833967, -0.2357509474807828, -0.2415159223751409, -0.2473237971735721, -0.2531739233106281, -0.25906566522875346, -0.26499840005379854, -0.2709715172801968, -0.2769844184654736, -0.28303651693376436, -0.28912723748803526, -0.29525601613070696, -0.3014222997924, -0.307625546068525, -0.31386522296345587, -0.32014080864203154, -0.32645179118814327, -0.332797668370172, -0.3391779474130515, -0.3455921447767385, -0.3520397859408806, -0.3585204051954831, -0.3650335454373774, -0.37157875797230544, -0.3781556023224417, -0.38476364603917756, -0.391402464521001, -0.3980716408363111, -0.4047707655510121, -0.4114994365607376, -0.4182572589275587, -0.4250438447210394]
**** log-parameter_analysis 运行时间： 2025-01-19 17:07:38 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.08490037573755677
DataOwner2: noise random: 0.0014809403931679843
DataOwner3: noise random: 0.0886004948189753
DataOwner4: noise random: 0.03527401286726519
DataOwner5: noise random: 0.08311964205127076
DataOwner6: noise random: 0.005148114543310923
DataOwner7: noise random: 0.07455536087139293
DataOwner8: noise random: 0.0494763307910655
DataOwner9: noise random: 0.020053704766235192
DataOwner10: noise random: 0.040329045555689697
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9970826722205202, 0.9999991135169196, 0.9968239918920992, 0.9994966263748225, 0.9971901007437024, 0.9999892950415576, 0.9977442069230315, 0.999005989736478, 0.9998372681629639, 0.9993418562888727]
归一化后的数据质量列表avg_f_list: [0.9081470998275711, 1.0, 0.9, 0.9841742395576568, 0.9115305457511054, 0.9996907685272522, 0.9289820403646512, 0.9687217090306652, 0.9949027037991073, 0.9792997779074348]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3377
DataOwner1的最优x_1 = 0.0667
DataOwner2的最优x_2 = 0.1607
DataOwner3的最优x_3 = 0.0563
DataOwner4的最优x_4 = 0.1471
DataOwner5的最优x_5 = 0.0709
DataOwner6的最优x_6 = 0.1605
DataOwner7的最优x_7 = 0.0915
DataOwner8的最优x_8 = 0.1329
DataOwner9的最优x_9 = 0.1564
DataOwner10的最优x_10 = 0.1427
每个DataOwner应该贡献数据比例 xn_list = [0.06668189453625943, 0.16070881246443253, 0.056318615975122915, 0.14711437697537558, 0.07087438159011955, 0.1604521340519029, 0.09150987653176934, 0.13289376340188125, 0.15643300307546212, 0.14273281610877922]
ModelOwner的最大效用 U(Eta) = 0.5770
xn开始变化：
new_xn_list: [0.06668189453625943, 0.16070881246443253, 0.056318615975122915, 0.14711437697537558, 0.07087438159011955, 0.1604521340519029, 0.09150987653176934, 0.13289376340188125, 0.15643300307546212, 0.14273281610877922]
avg_f_list: [0.9081470998275711, 1.0, 0.9, 0.9841742395576568, 0.9115305457511054, 0.9996907685272522, 0.9289820403646512, 0.9687217090306652, 0.9949027037991073, 0.9792997779074348]
============= xn0: 0.0 =============
new_qn: 0.0
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.0903503661173894
============= xn0: 0.001 =============
new_qn: 0.0009081470998275711
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.091258513217217
============= xn0: 0.002 =============
new_qn: 0.0018162941996551422
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.0921666603170446
============= xn0: 0.003 =============
new_qn: 0.0027244412994827136
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.0930748074168721
============= xn0: 0.004 =============
new_qn: 0.0036325883993102845
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.0939829545166997
============= xn0: 0.005 =============
new_qn: 0.004540735499137856
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.0948911016165273
============= xn0: 0.006 =============
new_qn: 0.005448882598965427
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.0957992487163548
============= xn0: 0.007 =============
new_qn: 0.0063570296987929984
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.0967073958161824
============= xn0: 0.008 =============
new_qn: 0.007265176798620569
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.09761554291601
============= xn0: 0.009000000000000001 =============
new_qn: 0.008173323898448142
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.0985236900158375
============= xn0: 0.01 =============
new_qn: 0.009081470998275712
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.099431837115665
============= xn0: 0.011 =============
new_qn: 0.009989618098103281
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1003399842154926
============= xn0: 0.012 =============
new_qn: 0.010897765197930854
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1012481313153202
============= xn0: 0.013000000000000001 =============
new_qn: 0.011805912297758426
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1021562784151477
============= xn0: 0.014 =============
new_qn: 0.012714059397585997
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1030644255149753
============= xn0: 0.015 =============
new_qn: 0.013622206497413566
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1039725726148029
============= xn0: 0.016 =============
new_qn: 0.014530353597241138
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1048807197146304
============= xn0: 0.017 =============
new_qn: 0.015438500697068711
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.105788866814458
============= xn0: 0.018000000000000002 =============
new_qn: 0.016346647796896284
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1066970139142858
============= xn0: 0.019 =============
new_qn: 0.017254794896723852
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1076051610141133
============= xn0: 0.02 =============
new_qn: 0.018162941996551423
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1085133081139409
============= xn0: 0.021 =============
new_qn: 0.019071089096378994
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1094214552137684
============= xn0: 0.022 =============
new_qn: 0.019979236196206562
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.110329602313596
============= xn0: 0.023 =============
new_qn: 0.020887383296034137
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1112377494134236
============= xn0: 0.024 =============
new_qn: 0.02179553039586171
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1121458965132511
============= xn0: 0.025 =============
new_qn: 0.02270367749568928
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1130540436130787
============= xn0: 0.026000000000000002 =============
new_qn: 0.02361182459551685
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1139621907129063
============= xn0: 0.027 =============
new_qn: 0.02451997169534442
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1148703378127338
============= xn0: 0.028 =============
new_qn: 0.025428118795171994
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1157784849125614
============= xn0: 0.029 =============
new_qn: 0.026336265894999565
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.116686632012389
============= xn0: 0.03 =============
new_qn: 0.027244412994827133
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1175947791122165
============= xn0: 0.031 =============
new_qn: 0.028152560094654704
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.118502926212044
============= xn0: 0.032 =============
new_qn: 0.029060707194482276
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1194110733118716
============= xn0: 0.033 =============
new_qn: 0.02996885429430985
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1203192204116992
============= xn0: 0.034 =============
new_qn: 0.030877001394137422
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1212273675115267
============= xn0: 0.035 =============
new_qn: 0.03178514849396499
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1221355146113543
============= xn0: 0.036000000000000004 =============
new_qn: 0.03269329559379257
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1230436617111819
============= xn0: 0.037 =============
new_qn: 0.03360144269362013
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1239518088110094
============= xn0: 0.038 =============
new_qn: 0.034509589793447704
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.124859955910837
============= xn0: 0.039 =============
new_qn: 0.03541773689327527
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1257681030106645
============= xn0: 0.04 =============
new_qn: 0.036325883993102846
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1266762501104923
============= xn0: 0.041 =============
new_qn: 0.03723403109293042
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1275843972103199
============= xn0: 0.042 =============
new_qn: 0.03814217819275799
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1284925443101474
============= xn0: 0.043000000000000003 =============
new_qn: 0.039050325292585564
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.129400691409975
============= xn0: 0.044 =============
new_qn: 0.039958472392413125
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1303088385098026
============= xn0: 0.045 =============
new_qn: 0.0408666194922407
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1312169856096301
============= xn0: 0.046 =============
new_qn: 0.041774766592068274
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1321251327094577
============= xn0: 0.047 =============
new_qn: 0.04268291369189584
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1330332798092853
============= xn0: 0.048 =============
new_qn: 0.04359106079172342
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1339414269091128
============= xn0: 0.049 =============
new_qn: 0.044499207891550985
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1348495740089404
============= xn0: 0.05 =============
new_qn: 0.04540735499137856
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.135757721108768
============= xn0: 0.051000000000000004 =============
new_qn: 0.046315502091206134
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1366658682085955
============= xn0: 0.052000000000000005 =============
new_qn: 0.0472236491910337
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.137574015308423
============= xn0: 0.053 =============
new_qn: 0.04813179629086127
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1384821624082506
============= xn0: 0.054 =============
new_qn: 0.04903994339068884
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1393903095080782
============= xn0: 0.055 =============
new_qn: 0.04994809049051641
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.140298456607906
============= xn0: 0.056 =============
new_qn: 0.05085623759034399
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1412066037077333
============= xn0: 0.057 =============
new_qn: 0.051764384690171555
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.142114750807561
============= xn0: 0.058 =============
new_qn: 0.05267253178999913
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1430228979073884
============= xn0: 0.059000000000000004 =============
new_qn: 0.0535806788898267
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1439310450072162
============= xn0: 0.06 =============
new_qn: 0.054488825989654266
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1448391921070435
============= xn0: 0.061 =============
new_qn: 0.05539697308948184
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1457473392068713
============= xn0: 0.062 =============
new_qn: 0.05630512018930941
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1466554863066991
============= xn0: 0.063 =============
new_qn: 0.05721326728913698
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1475636334065265
============= xn0: 0.064 =============
new_qn: 0.05812141438896455
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1484717805063542
============= xn0: 0.065 =============
new_qn: 0.059029561488792126
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1493799276061816
============= xn0: 0.066 =============
new_qn: 0.0599377085886197
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1502880747060094
============= xn0: 0.067 =============
new_qn: 0.06084585568844727
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1511962218058367
============= xn0: 0.068 =============
new_qn: 0.061754002788274844
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1521043689056645
============= xn0: 0.069 =============
new_qn: 0.06266214988810241
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1530125160054918
============= xn0: 0.07 =============
new_qn: 0.06357029698792999
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1539206631053196
============= xn0: 0.07100000000000001 =============
new_qn: 0.06447844408775756
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.154828810205147
============= xn0: 0.07200000000000001 =============
new_qn: 0.06538659118758514
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1557369573049747
============= xn0: 0.073 =============
new_qn: 0.06629473828741268
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.156645104404802
============= xn0: 0.074 =============
new_qn: 0.06720288538724026
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1575532515046298
============= xn0: 0.075 =============
new_qn: 0.06811103248706783
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1584613986044572
============= xn0: 0.076 =============
new_qn: 0.06901917958689541
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.159369545704285
============= xn0: 0.077 =============
new_qn: 0.06992732668672298
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1602776928041123
============= xn0: 0.078 =============
new_qn: 0.07083547378655054
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.16118583990394
============= xn0: 0.079 =============
new_qn: 0.07174362088637812
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1620939870037674
============= xn0: 0.08 =============
new_qn: 0.07265176798620569
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1630021341035952
============= xn0: 0.081 =============
new_qn: 0.07355991508603327
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1639102812034225
============= xn0: 0.082 =============
new_qn: 0.07446806218586084
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1648184283032503
============= xn0: 0.083 =============
new_qn: 0.0753762092856884
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1657265754030777
============= xn0: 0.084 =============
new_qn: 0.07628435638551598
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1666347225029055
============= xn0: 0.085 =============
new_qn: 0.07719250348534355
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1675428696027332
============= xn0: 0.08600000000000001 =============
new_qn: 0.07810065058517113
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1684510167025606
============= xn0: 0.08700000000000001 =============
new_qn: 0.0790087976849987
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1693591638023884
============= xn0: 0.088 =============
new_qn: 0.07991694478482625
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1702673109022157
============= xn0: 0.089 =============
new_qn: 0.08082509188465382
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.171175458002043
============= xn0: 0.09 =============
new_qn: 0.0817332389844814
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1720836051018708
============= xn0: 0.091 =============
new_qn: 0.08264138608430897
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1729917522016982
============= xn0: 0.092 =============
new_qn: 0.08354953318413655
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.173899899301526
============= xn0: 0.093 =============
new_qn: 0.08445768028396411
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1748080464013533
============= xn0: 0.094 =============
new_qn: 0.08536582738379168
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.175716193501181
============= xn0: 0.095 =============
new_qn: 0.08627397448361926
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1766243406010088
============= xn0: 0.096 =============
new_qn: 0.08718212158344683
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1775324877008362
============= xn0: 0.097 =============
new_qn: 0.08809026868327441
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.178440634800664
============= xn0: 0.098 =============
new_qn: 0.08899841578310197
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1793487819004913
============= xn0: 0.099 =============
new_qn: 0.08990656288292954
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.180256929000319
============= xn0: 0.1 =============
new_qn: 0.09081470998275712
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1811650761001464
============= xn0: 0.101 =============
new_qn: 0.0917228570825847
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1820732231999742
============= xn0: 0.10200000000000001 =============
new_qn: 0.09263100418241227
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1829813702998015
============= xn0: 0.10300000000000001 =============
new_qn: 0.09353915128223983
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1838895173996293
============= xn0: 0.10400000000000001 =============
new_qn: 0.0944472983820674
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1847976644994567
============= xn0: 0.105 =============
new_qn: 0.09535544548189497
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1857058115992845
============= xn0: 0.106 =============
new_qn: 0.09626359258172254
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1866139586991118
============= xn0: 0.107 =============
new_qn: 0.09717173968155011
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1875221057989396
============= xn0: 0.108 =============
new_qn: 0.09807988678137768
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.188430252898767
============= xn0: 0.109 =============
new_qn: 0.09898803388120525
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1893383999985947
============= xn0: 0.11 =============
new_qn: 0.09989618098103283
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.190246547098422
============= xn0: 0.111 =============
new_qn: 0.1008043280808604
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1911546941982498
============= xn0: 0.112 =============
new_qn: 0.10171247518068798
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1920628412980772
============= xn0: 0.113 =============
new_qn: 0.10262062228051554
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.192970988397905
============= xn0: 0.114 =============
new_qn: 0.10352876938034311
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1938791354977323
============= xn0: 0.115 =============
new_qn: 0.10443691648017069
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.19478728259756
============= xn0: 0.116 =============
new_qn: 0.10534506357999826
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1956954296973874
============= xn0: 0.117 =============
new_qn: 0.10625321067982584
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1966035767972152
============= xn0: 0.11800000000000001 =============
new_qn: 0.1071613577796534
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.197511723897043
============= xn0: 0.11900000000000001 =============
new_qn: 0.10806950487948097
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.1984198709968703
============= xn0: 0.12 =============
new_qn: 0.10897765197930853
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.199328018096698
============= xn0: 0.121 =============
new_qn: 0.1098857990791361
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2002361651965254
============= xn0: 0.122 =============
new_qn: 0.11079394617896368
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2011443122963532
============= xn0: 0.123 =============
new_qn: 0.11170209327879124
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2020524593961806
============= xn0: 0.124 =============
new_qn: 0.11261024037861882
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2029606064960083
============= xn0: 0.125 =============
new_qn: 0.11351838747844639
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2038687535958357
============= xn0: 0.126 =============
new_qn: 0.11442653457827397
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2047769006956635
============= xn0: 0.127 =============
new_qn: 0.11533468167810154
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2056850477954908
============= xn0: 0.128 =============
new_qn: 0.1162428287779291
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2065931948953186
============= xn0: 0.129 =============
new_qn: 0.11715097587775668
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.207501341995146
============= xn0: 0.13 =============
new_qn: 0.11805912297758425
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2084094890949737
============= xn0: 0.131 =============
new_qn: 0.11896727007741183
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.209317636194801
============= xn0: 0.132 =============
new_qn: 0.1198754171772394
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2102257832946288
============= xn0: 0.133 =============
new_qn: 0.12078356427706696
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2111339303944562
============= xn0: 0.134 =============
new_qn: 0.12169171137689454
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.212042077494284
============= xn0: 0.135 =============
new_qn: 0.12259985847672211
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2129502245941113
============= xn0: 0.136 =============
new_qn: 0.12350800557654969
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.213858371693939
============= xn0: 0.137 =============
new_qn: 0.12441615267637726
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2147665187937664
============= xn0: 0.138 =============
new_qn: 0.12532429977620482
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2156746658935942
============= xn0: 0.139 =============
new_qn: 0.1262324468760324
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.216582812993422
============= xn0: 0.14 =============
new_qn: 0.12714059397585997
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2174909600932493
============= xn0: 0.14100000000000001 =============
new_qn: 0.12804874107568753
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.218399107193077
============= xn0: 0.14200000000000002 =============
new_qn: 0.12895688817551512
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2193072542929044
============= xn0: 0.14300000000000002 =============
new_qn: 0.12986503527534268
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2202154013927322
============= xn0: 0.14400000000000002 =============
new_qn: 0.13077318237517027
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2211235484925596
============= xn0: 0.145 =============
new_qn: 0.1316813294749978
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2220316955923873
============= xn0: 0.146 =============
new_qn: 0.13258947657482537
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2229398426922147
============= xn0: 0.147 =============
new_qn: 0.13349762367465295
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2238479897920425
============= xn0: 0.148 =============
new_qn: 0.13440577077448052
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2247561368918698
============= xn0: 0.149 =============
new_qn: 0.1353139178743081
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2256642839916976
============= xn0: 0.15 =============
new_qn: 0.13622206497413566
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.226572431091525
============= xn0: 0.151 =============
new_qn: 0.13713021207396323
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2274805781913527
============= xn0: 0.152 =============
new_qn: 0.13803835917379081
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.22838872529118
============= xn0: 0.153 =============
new_qn: 0.13894650627361838
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2292968723910078
============= xn0: 0.154 =============
new_qn: 0.13985465337344596
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2302050194908352
============= xn0: 0.155 =============
new_qn: 0.14076280047327352
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.231113166590663
============= xn0: 0.156 =============
new_qn: 0.14167094757310109
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2320213136904903
============= xn0: 0.157 =============
new_qn: 0.14257909467292867
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.232929460790318
============= xn0: 0.158 =============
new_qn: 0.14348724177275624
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2338376078901454
============= xn0: 0.159 =============
new_qn: 0.14439538887258382
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2347457549899732
============= xn0: 0.16 =============
new_qn: 0.14530353597241139
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2356539020898005
============= xn0: 0.161 =============
new_qn: 0.14621168307223895
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2365620491896283
============= xn0: 0.162 =============
new_qn: 0.14711983017206653
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.237470196289456
============= xn0: 0.163 =============
new_qn: 0.1480279772718941
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2383783433892834
============= xn0: 0.164 =============
new_qn: 0.14893612437172168
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2392864904891112
============= xn0: 0.165 =============
new_qn: 0.14984427147154925
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2401946375889386
============= xn0: 0.166 =============
new_qn: 0.1507524185713768
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2411027846887663
============= xn0: 0.167 =============
new_qn: 0.1516605656712044
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2420109317885937
============= xn0: 0.168 =============
new_qn: 0.15256871277103196
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2429190788884215
============= xn0: 0.169 =============
new_qn: 0.15347685987085954
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2438272259882488
============= xn0: 0.17 =============
new_qn: 0.1543850069706871
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2447353730880766
============= xn0: 0.171 =============
new_qn: 0.15529315407051467
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.245643520187904
============= xn0: 0.17200000000000001 =============
new_qn: 0.15620130117034225
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2465516672877317
============= xn0: 0.17300000000000001 =============
new_qn: 0.15710944827016982
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.247459814387559
============= xn0: 0.17400000000000002 =============
new_qn: 0.1580175953699974
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2483679614873868
============= xn0: 0.17500000000000002 =============
new_qn: 0.15892574246982497
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2492761085872142
============= xn0: 0.176 =============
new_qn: 0.1598338895696525
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.250184255687042
============= xn0: 0.177 =============
new_qn: 0.1607420366694801
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2510924027868693
============= xn0: 0.178 =============
new_qn: 0.16165018376930765
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.252000549886697
============= xn0: 0.179 =============
new_qn: 0.16255833086913524
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2529086969865244
============= xn0: 0.18 =============
new_qn: 0.1634664779689628
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2538168440863522
============= xn0: 0.181 =============
new_qn: 0.16437462506879036
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2547249911861795
============= xn0: 0.182 =============
new_qn: 0.16528277216861795
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2556331382860073
============= xn0: 0.183 =============
new_qn: 0.1661909192684455
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2565412853858346
============= xn0: 0.184 =============
new_qn: 0.1670990663682731
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2574494324856624
============= xn0: 0.185 =============
new_qn: 0.16800721346810066
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2583575795854902
============= xn0: 0.186 =============
new_qn: 0.16891536056792822
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2592657266853176
============= xn0: 0.187 =============
new_qn: 0.1698235076677558
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2601738737851453
============= xn0: 0.188 =============
new_qn: 0.17073165476758337
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2610820208849727
============= xn0: 0.189 =============
new_qn: 0.17163980186741096
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2619901679848005
============= xn0: 0.19 =============
new_qn: 0.17254794896723852
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2628983150846278
============= xn0: 0.191 =============
new_qn: 0.17345609606706608
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2638064621844556
============= xn0: 0.192 =============
new_qn: 0.17436424316689367
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.264714609284283
============= xn0: 0.193 =============
new_qn: 0.17527239026672123
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2656227563841107
============= xn0: 0.194 =============
new_qn: 0.17618053736654882
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.266530903483938
============= xn0: 0.195 =============
new_qn: 0.17708868446637638
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2674390505837658
============= xn0: 0.196 =============
new_qn: 0.17799683156620394
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2683471976835932
============= xn0: 0.197 =============
new_qn: 0.17890497866603153
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.269255344783421
============= xn0: 0.198 =============
new_qn: 0.1798131257658591
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2701634918832483
============= xn0: 0.199 =============
new_qn: 0.18072127286568668
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.271071638983076
============= xn0: 0.2 =============
new_qn: 0.18162941996551424
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2719797860829034
============= xn0: 0.201 =============
new_qn: 0.1825375670653418
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2728879331827312
============= xn0: 0.202 =============
new_qn: 0.1834457141651694
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2737960802825585
============= xn0: 0.203 =============
new_qn: 0.18435386126499695
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2747042273823863
============= xn0: 0.20400000000000001 =============
new_qn: 0.18526200836482454
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.275612374482214
============= xn0: 0.20500000000000002 =============
new_qn: 0.1861701554646521
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2765205215820414
============= xn0: 0.20600000000000002 =============
new_qn: 0.18707830256447966
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2774286686818692
============= xn0: 0.20700000000000002 =============
new_qn: 0.18798644966430725
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2783368157816966
============= xn0: 0.20800000000000002 =============
new_qn: 0.1888945967641348
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2792449628815243
============= xn0: 0.209 =============
new_qn: 0.18980274386396237
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2801531099813517
============= xn0: 0.21 =============
new_qn: 0.19071089096378993
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2810612570811795
============= xn0: 0.211 =============
new_qn: 0.1916190380636175
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2819694041810068
============= xn0: 0.212 =============
new_qn: 0.19252718516344508
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2828775512808346
============= xn0: 0.213 =============
new_qn: 0.19343533226327264
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.283785698380662
============= xn0: 0.214 =============
new_qn: 0.19434347936310023
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2846938454804897
============= xn0: 0.215 =============
new_qn: 0.1952516264629278
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.285601992580317
============= xn0: 0.216 =============
new_qn: 0.19615977356275535
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2865101396801448
============= xn0: 0.217 =============
new_qn: 0.19706792066258294
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2874182867799722
============= xn0: 0.218 =============
new_qn: 0.1979760677624105
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2883264338798
============= xn0: 0.219 =============
new_qn: 0.1988842148622381
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2892345809796273
============= xn0: 0.22 =============
new_qn: 0.19979236196206565
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.290142728079455
============= xn0: 0.221 =============
new_qn: 0.2007005090618932
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2910508751792824
============= xn0: 0.222 =============
new_qn: 0.2016086561617208
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2919590222791102
============= xn0: 0.223 =============
new_qn: 0.20251680326154836
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2928671693789375
============= xn0: 0.224 =============
new_qn: 0.20342495036137595
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2937753164787653
============= xn0: 0.225 =============
new_qn: 0.2043330974612035
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2946834635785927
============= xn0: 0.226 =============
new_qn: 0.20524124456103107
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2955916106784204
============= xn0: 0.227 =============
new_qn: 0.20614939166085866
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2964997577782482
============= xn0: 0.228 =============
new_qn: 0.20705753876068622
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2974079048780756
============= xn0: 0.229 =============
new_qn: 0.2079656858605138
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2983160519779033
============= xn0: 0.23 =============
new_qn: 0.20887383296034137
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.2992241990777307
============= xn0: 0.231 =============
new_qn: 0.20978198006016893
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3001323461775585
============= xn0: 0.232 =============
new_qn: 0.21069012715999652
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3010404932773858
============= xn0: 0.233 =============
new_qn: 0.21159827425982408
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3019486403772136
============= xn0: 0.234 =============
new_qn: 0.21250642135965167
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.302856787477041
============= xn0: 0.23500000000000001 =============
new_qn: 0.21341456845947923
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3037649345768687
============= xn0: 0.23600000000000002 =============
new_qn: 0.2143227155593068
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.304673081676696
============= xn0: 0.23700000000000002 =============
new_qn: 0.21523086265913438
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3055812287765238
============= xn0: 0.23800000000000002 =============
new_qn: 0.21613900975896194
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3064893758763512
============= xn0: 0.23900000000000002 =============
new_qn: 0.21704715685878953
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.307397522976179
============= xn0: 0.24 =============
new_qn: 0.21795530395861706
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3083056700760063
============= xn0: 0.241 =============
new_qn: 0.21886345105844462
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.309213817175834
============= xn0: 0.242 =============
new_qn: 0.2197715981582722
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3101219642756614
============= xn0: 0.243 =============
new_qn: 0.22067974525809977
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3110301113754892
============= xn0: 0.244 =============
new_qn: 0.22158789235792736
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3119382584753165
============= xn0: 0.245 =============
new_qn: 0.22249603945775492
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3128464055751443
============= xn0: 0.246 =============
new_qn: 0.22340418655758248
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3137545526749717
============= xn0: 0.247 =============
new_qn: 0.22431233365741007
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3146626997747994
============= xn0: 0.248 =============
new_qn: 0.22522048075723763
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3155708468746268
============= xn0: 0.249 =============
new_qn: 0.22612862785706522
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3164789939744546
============= xn0: 0.25 =============
new_qn: 0.22703677495689278
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3173871410742823
============= xn0: 0.251 =============
new_qn: 0.22794492205672034
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3182952881741097
============= xn0: 0.252 =============
new_qn: 0.22885306915654793
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3192034352739375
============= xn0: 0.253 =============
new_qn: 0.2297612162563755
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3201115823737648
============= xn0: 0.254 =============
new_qn: 0.23066936335620308
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3210197294735926
============= xn0: 0.255 =============
new_qn: 0.23157751045603064
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.32192787657342
============= xn0: 0.256 =============
new_qn: 0.2324856575558582
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3228360236732477
============= xn0: 0.257 =============
new_qn: 0.2333938046556858
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.323744170773075
============= xn0: 0.258 =============
new_qn: 0.23430195175551335
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3246523178729028
============= xn0: 0.259 =============
new_qn: 0.23521009885534094
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3255604649727302
============= xn0: 0.26 =============
new_qn: 0.2361182459551685
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.326468612072558
============= xn0: 0.261 =============
new_qn: 0.23702639305499607
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3273767591723853
============= xn0: 0.262 =============
new_qn: 0.23793454015482365
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.328284906272213
============= xn0: 0.263 =============
new_qn: 0.23884268725465121
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3291930533720404
============= xn0: 0.264 =============
new_qn: 0.2397508343544788
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3301012004718682
============= xn0: 0.265 =============
new_qn: 0.24065898145430636
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3310093475716955
============= xn0: 0.266 =============
new_qn: 0.24156712855413393
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3319174946715233
============= xn0: 0.267 =============
new_qn: 0.24247527565396151
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3328256417713507
============= xn0: 0.268 =============
new_qn: 0.24338342275378907
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3337337888711784
============= xn0: 0.269 =============
new_qn: 0.24429156985361666
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3346419359710062
============= xn0: 0.27 =============
new_qn: 0.24519971695344422
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3355500830708336
============= xn0: 0.271 =============
new_qn: 0.24610786405327179
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3364582301706613
============= xn0: 0.272 =============
new_qn: 0.24701601115309937
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3373663772704887
============= xn0: 0.273 =============
new_qn: 0.24792415825292693
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3382745243703165
============= xn0: 0.274 =============
new_qn: 0.24883230535275452
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3391826714701438
============= xn0: 0.275 =============
new_qn: 0.24974045245258208
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3400908185699716
============= xn0: 0.276 =============
new_qn: 0.25064859955240965
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.340998965669799
============= xn0: 0.277 =============
new_qn: 0.2515567466522372
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3419071127696267
============= xn0: 0.278 =============
new_qn: 0.2524648937520648
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.342815259869454
============= xn0: 0.279 =============
new_qn: 0.2533730408518924
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3437234069692818
============= xn0: 0.28 =============
new_qn: 0.25428118795171994
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3446315540691092
============= xn0: 0.281 =============
new_qn: 0.2551893350515475
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.345539701168937
============= xn0: 0.28200000000000003 =============
new_qn: 0.25609748215137507
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3464478482687643
============= xn0: 0.28300000000000003 =============
new_qn: 0.2570056292512027
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.347355995368592
============= xn0: 0.28400000000000003 =============
new_qn: 0.25791377635103024
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3482641424684194
============= xn0: 0.28500000000000003 =============
new_qn: 0.2588219234508578
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3491722895682472
============= xn0: 0.28600000000000003 =============
new_qn: 0.25973007055068537
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3500804366680745
============= xn0: 0.28700000000000003 =============
new_qn: 0.2606382176505129
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3509885837679023
============= xn0: 0.28800000000000003 =============
new_qn: 0.26154636475034054
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.35189673086773
============= xn0: 0.289 =============
new_qn: 0.26245451185016805
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3528048779675574
============= xn0: 0.29 =============
new_qn: 0.2633626589499956
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3537130250673848
============= xn0: 0.291 =============
new_qn: 0.26427080604982317
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3546211721672126
============= xn0: 0.292 =============
new_qn: 0.26517895314965073
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.35552931926704
============= xn0: 0.293 =============
new_qn: 0.26608710024947835
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3564374663668677
============= xn0: 0.294 =============
new_qn: 0.2669952473493059
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3573456134666955
============= xn0: 0.295 =============
new_qn: 0.26790339444913347
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3582537605665228
============= xn0: 0.296 =============
new_qn: 0.26881154154896103
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3591619076663506
============= xn0: 0.297 =============
new_qn: 0.2697196886487886
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.360070054766178
============= xn0: 0.298 =============
new_qn: 0.2706278357486162
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3609782018660057
============= xn0: 0.299 =============
new_qn: 0.27153598284844377
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.361886348965833
============= xn0: 0.3 =============
new_qn: 0.27244412994827133
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3627944960656608
============= xn0: 0.301 =============
new_qn: 0.2733522770480989
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3637026431654882
============= xn0: 0.302 =============
new_qn: 0.27426042414792645
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.364610790265316
============= xn0: 0.303 =============
new_qn: 0.27516857124775407
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3655189373651433
============= xn0: 0.304 =============
new_qn: 0.27607671834758163
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.366427084464971
============= xn0: 0.305 =============
new_qn: 0.2769848654474092
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3673352315647984
============= xn0: 0.306 =============
new_qn: 0.27789301254723675
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3682433786646262
============= xn0: 0.307 =============
new_qn: 0.2788011596470643
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3691515257644535
============= xn0: 0.308 =============
new_qn: 0.27970930674689193
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3700596728642813
============= xn0: 0.309 =============
new_qn: 0.2806174538467195
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3709678199641087
============= xn0: 0.31 =============
new_qn: 0.28152560094654705
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3718759670639364
============= xn0: 0.311 =============
new_qn: 0.2824337480463746
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3727841141637638
============= xn0: 0.312 =============
new_qn: 0.28334189514620217
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3736922612635916
============= xn0: 0.313 =============
new_qn: 0.2842500422460298
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3746004083634193
============= xn0: 0.314 =============
new_qn: 0.28515818934585735
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3755085554632467
============= xn0: 0.315 =============
new_qn: 0.2860663364456849
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3764167025630745
============= xn0: 0.316 =============
new_qn: 0.28697448354551247
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3773248496629018
============= xn0: 0.317 =============
new_qn: 0.28788263064534003
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3782329967627296
============= xn0: 0.318 =============
new_qn: 0.28879077774516765
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.379141143862557
============= xn0: 0.319 =============
new_qn: 0.2896989248449952
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3800492909623847
============= xn0: 0.32 =============
new_qn: 0.29060707194482277
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.380957438062212
============= xn0: 0.321 =============
new_qn: 0.29151521904465033
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3818655851620398
============= xn0: 0.322 =============
new_qn: 0.2924233661444779
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3827737322618672
============= xn0: 0.323 =============
new_qn: 0.2933315132443055
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.383681879361695
============= xn0: 0.324 =============
new_qn: 0.29423966034413307
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3845900264615223
============= xn0: 0.325 =============
new_qn: 0.29514780744396063
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.38549817356135
============= xn0: 0.326 =============
new_qn: 0.2960559545437882
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3864063206611774
============= xn0: 0.327 =============
new_qn: 0.29696410164361575
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3873144677610052
============= xn0: 0.328 =============
new_qn: 0.29787224874344337
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3882226148608325
============= xn0: 0.329 =============
new_qn: 0.29878039584327093
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3891307619606603
============= xn0: 0.33 =============
new_qn: 0.2996885429430985
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3900389090604877
============= xn0: 0.331 =============
new_qn: 0.30059669004292605
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3909470561603154
============= xn0: 0.332 =============
new_qn: 0.3015048371427536
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3918552032601428
============= xn0: 0.333 =============
new_qn: 0.30241298424258123
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3927633503599706
============= xn0: 0.334 =============
new_qn: 0.3033211313424088
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3936714974597983
============= xn0: 0.335 =============
new_qn: 0.30422927844223635
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3945796445596257
============= xn0: 0.336 =============
new_qn: 0.3051374255420639
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3954877916594535
============= xn0: 0.337 =============
new_qn: 0.3060455726418915
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3963959387592808
============= xn0: 0.338 =============
new_qn: 0.3069537197417191
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3973040858591086
============= xn0: 0.339 =============
new_qn: 0.30786186684154665
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.398212232958936
============= xn0: 0.34 =============
new_qn: 0.3087700139413742
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.3991203800587637
============= xn0: 0.341 =============
new_qn: 0.30967816104120177
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.400028527158591
============= xn0: 0.342 =============
new_qn: 0.31058630814102933
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4009366742584188
============= xn0: 0.343 =============
new_qn: 0.31149445524085695
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4018448213582462
============= xn0: 0.34400000000000003 =============
new_qn: 0.3124026023406845
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.402752968458074
============= xn0: 0.34500000000000003 =============
new_qn: 0.31331074944051207
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4036611155579013
============= xn0: 0.34600000000000003 =============
new_qn: 0.31421889654033963
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.404569262657729
============= xn0: 0.34700000000000003 =============
new_qn: 0.3151270436401672
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4054774097575564
============= xn0: 0.34800000000000003 =============
new_qn: 0.3160351907399948
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4063855568573842
============= xn0: 0.34900000000000003 =============
new_qn: 0.31694333783982237
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4072937039572115
============= xn0: 0.35000000000000003 =============
new_qn: 0.31785148493964993
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4082018510570393
============= xn0: 0.35100000000000003 =============
new_qn: 0.3187596320394775
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4091099981568667
============= xn0: 0.352 =============
new_qn: 0.319667779139305
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4100181452566944
============= xn0: 0.353 =============
new_qn: 0.3205759262391326
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4109262923565218
============= xn0: 0.354 =============
new_qn: 0.3214840733389602
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4118344394563496
============= xn0: 0.355 =============
new_qn: 0.32239222043878774
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.412742586556177
============= xn0: 0.356 =============
new_qn: 0.3233003675386153
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4136507336560047
============= xn0: 0.357 =============
new_qn: 0.32420851463844286
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.414558880755832
============= xn0: 0.358 =============
new_qn: 0.3251166617382705
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4154670278556598
============= xn0: 0.359 =============
new_qn: 0.32602480883809803
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4163751749554876
============= xn0: 0.36 =============
new_qn: 0.3269329559379256
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.417283322055315
============= xn0: 0.361 =============
new_qn: 0.32784110303775316
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4181914691551427
============= xn0: 0.362 =============
new_qn: 0.3287492501375807
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.41909961625497
============= xn0: 0.363 =============
new_qn: 0.32965739723740833
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4200077633547978
============= xn0: 0.364 =============
new_qn: 0.3305655443372359
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4209159104546252
============= xn0: 0.365 =============
new_qn: 0.33147369143706346
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.421824057554453
============= xn0: 0.366 =============
new_qn: 0.332381838536891
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4227322046542803
============= xn0: 0.367 =============
new_qn: 0.3332899856367186
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.423640351754108
============= xn0: 0.368 =============
new_qn: 0.3341981327365462
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4245484988539359
============= xn0: 0.369 =============
new_qn: 0.33510627983637375
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4254566459537632
============= xn0: 0.37 =============
new_qn: 0.3360144269362013
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.426364793053591
============= xn0: 0.371 =============
new_qn: 0.3369225740360289
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4272729401534183
============= xn0: 0.372 =============
new_qn: 0.33783072113585644
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.428181087253246
============= xn0: 0.373 =============
new_qn: 0.33873886823568405
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4290892343530734
============= xn0: 0.374 =============
new_qn: 0.3396470153355116
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4299973814529012
============= xn0: 0.375 =============
new_qn: 0.3405551624353392
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4309055285527286
============= xn0: 0.376 =============
new_qn: 0.34146330953516674
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4318136756525564
============= xn0: 0.377 =============
new_qn: 0.3423714566349943
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4327218227523837
============= xn0: 0.378 =============
new_qn: 0.3432796037348219
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4336299698522115
============= xn0: 0.379 =============
new_qn: 0.3441877508346495
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4345381169520393
============= xn0: 0.38 =============
new_qn: 0.34509589793447704
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4354462640518666
============= xn0: 0.381 =============
new_qn: 0.3460040450343046
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4363544111516944
============= xn0: 0.382 =============
new_qn: 0.34691219213413216
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4372625582515217
============= xn0: 0.383 =============
new_qn: 0.3478203392339598
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4381707053513495
============= xn0: 0.384 =============
new_qn: 0.34872848633378734
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4390788524511768
============= xn0: 0.385 =============
new_qn: 0.3496366334336149
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4399869995510046
============= xn0: 0.386 =============
new_qn: 0.35054478053344246
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.440895146650832
============= xn0: 0.387 =============
new_qn: 0.35145292763327
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4418032937506597
============= xn0: 0.388 =============
new_qn: 0.35236107473309763
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.442711440850487
============= xn0: 0.389 =============
new_qn: 0.3532692218329252
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4436195879503149
============= xn0: 0.39 =============
new_qn: 0.35417736893275276
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4445277350501422
============= xn0: 0.391 =============
new_qn: 0.3550855160325803
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.44543588214997
============= xn0: 0.392 =============
new_qn: 0.3559936631324079
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4463440292497973
============= xn0: 0.393 =============
new_qn: 0.3569018102322355
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.447252176349625
============= xn0: 0.394 =============
new_qn: 0.35780995733206306
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4481603234494524
============= xn0: 0.395 =============
new_qn: 0.3587181044318906
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4490684705492802
============= xn0: 0.396 =============
new_qn: 0.3596262515317182
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4499766176491076
============= xn0: 0.397 =============
new_qn: 0.36053439863154574
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4508847647489354
============= xn0: 0.398 =============
new_qn: 0.36144254573137335
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4517929118487631
============= xn0: 0.399 =============
new_qn: 0.3623506928312009
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4527010589485905
============= xn0: 0.4 =============
new_qn: 0.3632588399310285
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4536092060484183
============= xn0: 0.401 =============
new_qn: 0.36416698703085604
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4545173531482456
============= xn0: 0.402 =============
new_qn: 0.3650751341306836
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4554255002480734
============= xn0: 0.403 =============
new_qn: 0.3659832812305112
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4563336473479007
============= xn0: 0.404 =============
new_qn: 0.3668914283303388
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4572417944477285
============= xn0: 0.405 =============
new_qn: 0.36779957543016634
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4581499415475558
============= xn0: 0.406 =============
new_qn: 0.3687077225299939
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4590580886473836
============= xn0: 0.40700000000000003 =============
new_qn: 0.36961586962982146
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.459966235747211
============= xn0: 0.40800000000000003 =============
new_qn: 0.3705240167296491
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4608743828470387
============= xn0: 0.40900000000000003 =============
new_qn: 0.37143216382947664
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.461782529946866
============= xn0: 0.41000000000000003 =============
new_qn: 0.3723403109293042
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4626906770466939
============= xn0: 0.41100000000000003 =============
new_qn: 0.37324845802913176
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4635988241465212
============= xn0: 0.41200000000000003 =============
new_qn: 0.3741566051289593
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.464506971246349
============= xn0: 0.41300000000000003 =============
new_qn: 0.37506475222878694
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4654151183461763
============= xn0: 0.41400000000000003 =============
new_qn: 0.3759728993286145
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.466323265446004
============= xn0: 0.41500000000000004 =============
new_qn: 0.37688104642844206
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4672314125458314
============= xn0: 0.41600000000000004 =============
new_qn: 0.3777891935282696
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4681395596456592
============= xn0: 0.417 =============
new_qn: 0.3786973406280971
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4690477067454866
============= xn0: 0.418 =============
new_qn: 0.37960548772792474
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4699558538453144
============= xn0: 0.419 =============
new_qn: 0.3805136348277523
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4708640009451417
============= xn0: 0.42 =============
new_qn: 0.38142178192757986
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4717721480449695
============= xn0: 0.421 =============
new_qn: 0.3823299290274074
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4726802951447968
============= xn0: 0.422 =============
new_qn: 0.383238076127235
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4735884422446246
============= xn0: 0.423 =============
new_qn: 0.3841462232270626
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4744965893444524
============= xn0: 0.424 =============
new_qn: 0.38505437032689016
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4754047364442797
============= xn0: 0.425 =============
new_qn: 0.3859625174267177
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4763128835441075
============= xn0: 0.426 =============
new_qn: 0.3868706645265453
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4772210306439348
============= xn0: 0.427 =============
new_qn: 0.38777881162637284
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4781291777437626
============= xn0: 0.428 =============
new_qn: 0.38868695872620046
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.47903732484359
============= xn0: 0.429 =============
new_qn: 0.389595105826028
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4799454719434177
============= xn0: 0.43 =============
new_qn: 0.3905032529258556
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.480853619043245
============= xn0: 0.431 =============
new_qn: 0.39141140002568314
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4817617661430729
============= xn0: 0.432 =============
new_qn: 0.3923195471255107
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4826699132429002
============= xn0: 0.433 =============
new_qn: 0.3932276942253383
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.483578060342728
============= xn0: 0.434 =============
new_qn: 0.3941358413251659
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4844862074425553
============= xn0: 0.435 =============
new_qn: 0.39504398842499344
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.485394354542383
============= xn0: 0.436 =============
new_qn: 0.395952135524821
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4863025016422104
============= xn0: 0.437 =============
new_qn: 0.39686028262464856
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4872106487420382
============= xn0: 0.438 =============
new_qn: 0.3977684297244762
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4881187958418656
============= xn0: 0.439 =============
new_qn: 0.39867657682430374
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4890269429416934
============= xn0: 0.44 =============
new_qn: 0.3995847239241313
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4899350900415207
============= xn0: 0.441 =============
new_qn: 0.40049287102395886
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4908432371413485
============= xn0: 0.442 =============
new_qn: 0.4014010181237864
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4917513842411758
============= xn0: 0.443 =============
new_qn: 0.40230916522361404
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4926595313410036
============= xn0: 0.444 =============
new_qn: 0.4032173123234416
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4935676784408314
============= xn0: 0.445 =============
new_qn: 0.40412545942326916
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4944758255406587
============= xn0: 0.446 =============
new_qn: 0.4050336065230967
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4953839726404865
============= xn0: 0.447 =============
new_qn: 0.4059417536229243
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4962921197403138
============= xn0: 0.448 =============
new_qn: 0.4068499007227519
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4972002668401416
============= xn0: 0.449 =============
new_qn: 0.40775804782257946
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.498108413939969
============= xn0: 0.45 =============
new_qn: 0.408666194922407
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.4990165610397967
============= xn0: 0.451 =============
new_qn: 0.4095743420222346
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.499924708139624
============= xn0: 0.452 =============
new_qn: 0.41048248912206214
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5008328552394519
============= xn0: 0.453 =============
new_qn: 0.41139063622188976
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5017410023392792
============= xn0: 0.454 =============
new_qn: 0.4122987833217173
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.502649149439107
============= xn0: 0.455 =============
new_qn: 0.4132069304215449
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5035572965389343
============= xn0: 0.456 =============
new_qn: 0.41411507752137244
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5044654436387621
============= xn0: 0.457 =============
new_qn: 0.4150232246212
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5053735907385895
============= xn0: 0.458 =============
new_qn: 0.4159313717210276
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5062817378384172
============= xn0: 0.459 =============
new_qn: 0.4168395188208552
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5071898849382446
============= xn0: 0.46 =============
new_qn: 0.41774766592068274
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5080980320380724
============= xn0: 0.461 =============
new_qn: 0.4186558130205103
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5090061791378997
============= xn0: 0.462 =============
new_qn: 0.41956396012033786
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5099143262377275
============= xn0: 0.463 =============
new_qn: 0.4204721072201655
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5108224733375553
============= xn0: 0.464 =============
new_qn: 0.42138025431999304
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5117306204373826
============= xn0: 0.465 =============
new_qn: 0.4222884014198206
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5126387675372104
============= xn0: 0.466 =============
new_qn: 0.42319654851964816
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5135469146370377
============= xn0: 0.467 =============
new_qn: 0.4241046956194757
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5144550617368655
============= xn0: 0.468 =============
new_qn: 0.42501284271930334
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5153632088366928
============= xn0: 0.46900000000000003 =============
new_qn: 0.4259209898191309
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5162713559365206
============= xn0: 0.47000000000000003 =============
new_qn: 0.42682913691895846
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.517179503036348
============= xn0: 0.47100000000000003 =============
new_qn: 0.427737284018786
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5180876501361757
============= xn0: 0.47200000000000003 =============
new_qn: 0.4286454311186136
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.518995797236003
============= xn0: 0.47300000000000003 =============
new_qn: 0.4295535782184412
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5199039443358309
============= xn0: 0.47400000000000003 =============
new_qn: 0.43046172531826876
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5208120914356582
============= xn0: 0.47500000000000003 =============
new_qn: 0.4313698724180963
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.521720238535486
============= xn0: 0.47600000000000003 =============
new_qn: 0.4322780195179239
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5226283856353133
============= xn0: 0.47700000000000004 =============
new_qn: 0.43318616661775144
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5235365327351411
============= xn0: 0.47800000000000004 =============
new_qn: 0.43409431371757906
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5244446798349685
============= xn0: 0.47900000000000004 =============
new_qn: 0.4350024608174066
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5253528269347962
============= xn0: 0.48 =============
new_qn: 0.4359106079172341
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5262609740346236
============= xn0: 0.481 =============
new_qn: 0.4368187550170617
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5271691211344514
============= xn0: 0.482 =============
new_qn: 0.43772690211688925
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5280772682342787
============= xn0: 0.483 =============
new_qn: 0.43863504921671687
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5289854153341065
============= xn0: 0.484 =============
new_qn: 0.4395431963165444
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5298935624339338
============= xn0: 0.485 =============
new_qn: 0.440451343416372
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5308017095337616
============= xn0: 0.486 =============
new_qn: 0.44135949051619955
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.531709856633589
============= xn0: 0.487 =============
new_qn: 0.4422676376160271
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5326180037334167
============= xn0: 0.488 =============
new_qn: 0.4431757847158547
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5335261508332445
============= xn0: 0.489 =============
new_qn: 0.4440839318156823
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5344342979330718
============= xn0: 0.49 =============
new_qn: 0.44499207891550985
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5353424450328996
============= xn0: 0.491 =============
new_qn: 0.4459002260153374
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.536250592132727
============= xn0: 0.492 =============
new_qn: 0.44680837311516497
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5371587392325547
============= xn0: 0.493 =============
new_qn: 0.4477165202149926
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.538066886332382
============= xn0: 0.494 =============
new_qn: 0.44862466731482015
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5389750334322099
============= xn0: 0.495 =============
new_qn: 0.4495328144146477
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5398831805320372
============= xn0: 0.496 =============
new_qn: 0.45044096151447527
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.540791327631865
============= xn0: 0.497 =============
new_qn: 0.45134910861430283
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5416994747316923
============= xn0: 0.498 =============
new_qn: 0.45225725571413045
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5426076218315201
============= xn0: 0.499 =============
new_qn: 0.453165402813958
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5435157689313475
============= xn0: 0.5 =============
new_qn: 0.45407354991378557
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5444239160311752
============= xn0: 0.501 =============
new_qn: 0.45498169701361313
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5453320631310026
============= xn0: 0.502 =============
new_qn: 0.4558898441134407
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5462402102308304
============= xn0: 0.503 =============
new_qn: 0.4567979912132683
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5471483573306577
============= xn0: 0.504 =============
new_qn: 0.45770613831309587
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5480565044304855
============= xn0: 0.505 =============
new_qn: 0.45861428541292343
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5489646515303128
============= xn0: 0.506 =============
new_qn: 0.459522432512751
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5498727986301406
============= xn0: 0.507 =============
new_qn: 0.46043057961257855
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.550780945729968
============= xn0: 0.508 =============
new_qn: 0.46133872671240617
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5516890928297957
============= xn0: 0.509 =============
new_qn: 0.4622468738122337
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5525972399296235
============= xn0: 0.51 =============
new_qn: 0.4631550209120613
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5535053870294508
============= xn0: 0.511 =============
new_qn: 0.46406316801188885
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5544135341292786
============= xn0: 0.512 =============
new_qn: 0.4649713151117164
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.555321681229106
============= xn0: 0.513 =============
new_qn: 0.465879462211544
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5562298283289338
============= xn0: 0.514 =============
new_qn: 0.4667876093113716
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.557137975428761
============= xn0: 0.515 =============
new_qn: 0.46769575641119915
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5580461225285889
============= xn0: 0.516 =============
new_qn: 0.4686039035110267
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5589542696284162
============= xn0: 0.517 =============
new_qn: 0.46951205061085427
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.559862416728244
============= xn0: 0.518 =============
new_qn: 0.4704201977106819
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5607705638280713
============= xn0: 0.519 =============
new_qn: 0.47132834481050945
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5616787109278991
============= xn0: 0.52 =============
new_qn: 0.472236491910337
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5625868580277265
============= xn0: 0.521 =============
new_qn: 0.47314463901016457
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5634950051275542
============= xn0: 0.522 =============
new_qn: 0.47405278610999213
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5644031522273816
============= xn0: 0.523 =============
new_qn: 0.47496093320981975
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5653112993272094
============= xn0: 0.524 =============
new_qn: 0.4758690803096473
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5662194464270367
============= xn0: 0.525 =============
new_qn: 0.47677722740947487
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5671275935268645
============= xn0: 0.526 =============
new_qn: 0.47768537450930243
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5680357406266918
============= xn0: 0.527 =============
new_qn: 0.47859352160913
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5689438877265196
============= xn0: 0.528 =============
new_qn: 0.4795016687089576
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5698520348263474
============= xn0: 0.529 =============
new_qn: 0.48040981580878517
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5707601819261747
============= xn0: 0.53 =============
new_qn: 0.48131796290861273
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5716683290260025
============= xn0: 0.531 =============
new_qn: 0.4822261100084403
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5725764761258298
============= xn0: 0.532 =============
new_qn: 0.48313425710826785
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5734846232256576
============= xn0: 0.533 =============
new_qn: 0.48404240420809547
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.574392770325485
============= xn0: 0.534 =============
new_qn: 0.48495055130792303
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5753009174253128
============= xn0: 0.535 =============
new_qn: 0.4858586984077506
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.57620906452514
============= xn0: 0.536 =============
new_qn: 0.48676684550757815
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5771172116249679
============= xn0: 0.537 =============
new_qn: 0.4876749926074057
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5780253587247952
============= xn0: 0.538 =============
new_qn: 0.4885831397072333
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.578933505824623
============= xn0: 0.539 =============
new_qn: 0.4894912868070609
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5798416529244503
============= xn0: 0.54 =============
new_qn: 0.49039943390688845
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5807498000242781
============= xn0: 0.541 =============
new_qn: 0.491307581006716
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5816579471241055
============= xn0: 0.542 =============
new_qn: 0.49221572810654357
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5825660942239332
============= xn0: 0.543 =============
new_qn: 0.4931238752063712
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5834742413237606
============= xn0: 0.544 =============
new_qn: 0.49403202230619875
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5843823884235884
============= xn0: 0.545 =============
new_qn: 0.4949401694060263
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5852905355234157
============= xn0: 0.546 =============
new_qn: 0.49584831650585387
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5861986826232435
============= xn0: 0.547 =============
new_qn: 0.49675646360568143
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5871068297230708
============= xn0: 0.548 =============
new_qn: 0.49766461070550905
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5880149768228986
============= xn0: 0.549 =============
new_qn: 0.4985727578053366
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5889231239227264
============= xn0: 0.55 =============
new_qn: 0.49948090490516417
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5898312710225537
============= xn0: 0.551 =============
new_qn: 0.5003890520049917
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5907394181223815
============= xn0: 0.552 =============
new_qn: 0.5012971991048193
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5916475652222088
============= xn0: 0.553 =============
new_qn: 0.5022053462046469
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5925557123220366
============= xn0: 0.554 =============
new_qn: 0.5031134933044744
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.593463859421864
============= xn0: 0.555 =============
new_qn: 0.504021640404302
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5943720065216918
============= xn0: 0.556 =============
new_qn: 0.5049297875041296
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.595280153621519
============= xn0: 0.557 =============
new_qn: 0.5058379346039572
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5961883007213469
============= xn0: 0.558 =============
new_qn: 0.5067460817037848
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5970964478211742
============= xn0: 0.559 =============
new_qn: 0.5076542288036123
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.598004594921002
============= xn0: 0.56 =============
new_qn: 0.5085623759034399
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5989127420208293
============= xn0: 0.561 =============
new_qn: 0.5094705230032675
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.5998208891206571
============= xn0: 0.562 =============
new_qn: 0.510378670103095
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6007290362204845
============= xn0: 0.5630000000000001 =============
new_qn: 0.5112868172029226
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6016371833203122
============= xn0: 0.5640000000000001 =============
new_qn: 0.5121949643027501
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6025453304201396
============= xn0: 0.5650000000000001 =============
new_qn: 0.5131031114025777
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6034534775199674
============= xn0: 0.5660000000000001 =============
new_qn: 0.5140112585024054
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6043616246197951
============= xn0: 0.5670000000000001 =============
new_qn: 0.5149194056022329
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6052697717196225
============= xn0: 0.5680000000000001 =============
new_qn: 0.5158275527020605
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6061779188194503
============= xn0: 0.5690000000000001 =============
new_qn: 0.516735699801888
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6070860659192776
============= xn0: 0.5700000000000001 =============
new_qn: 0.5176438469017156
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6079942130191054
============= xn0: 0.5710000000000001 =============
new_qn: 0.5185519940015432
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6089023601189327
============= xn0: 0.5720000000000001 =============
new_qn: 0.5194601411013707
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6098105072187605
============= xn0: 0.5730000000000001 =============
new_qn: 0.5203682882011983
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6107186543185879
============= xn0: 0.5740000000000001 =============
new_qn: 0.5212764353010259
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6116268014184156
============= xn0: 0.5750000000000001 =============
new_qn: 0.5221845824008534
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.612534948518243
============= xn0: 0.5760000000000001 =============
new_qn: 0.5230927295006811
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6134430956180708
============= xn0: 0.577 =============
new_qn: 0.5240008766005085
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.614351242717898
============= xn0: 0.578 =============
new_qn: 0.5249090237003361
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6152593898177259
============= xn0: 0.579 =============
new_qn: 0.5258171708001637
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6161675369175532
============= xn0: 0.58 =============
new_qn: 0.5267253178999912
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.617075684017381
============= xn0: 0.581 =============
new_qn: 0.5276334649998188
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6179838311172083
============= xn0: 0.582 =============
new_qn: 0.5285416120996463
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6188919782170361
============= xn0: 0.583 =============
new_qn: 0.5294497591994739
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6198001253168635
============= xn0: 0.584 =============
new_qn: 0.5303579062993015
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6207082724166912
============= xn0: 0.585 =============
new_qn: 0.5312660533991291
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6216164195165186
============= xn0: 0.586 =============
new_qn: 0.5321742004989567
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6225245666163464
============= xn0: 0.587 =============
new_qn: 0.5330823475987843
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6234327137161737
============= xn0: 0.588 =============
new_qn: 0.5339904946986118
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6243408608160015
============= xn0: 0.589 =============
new_qn: 0.5348986417984394
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6252490079158288
============= xn0: 0.59 =============
new_qn: 0.5358067888982669
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6261571550156566
============= xn0: 0.591 =============
new_qn: 0.5367149359980945
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.627065302115484
============= xn0: 0.592 =============
new_qn: 0.5376230830979221
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6279734492153117
============= xn0: 0.593 =============
new_qn: 0.5385312301977496
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.628881596315139
============= xn0: 0.594 =============
new_qn: 0.5394393772975772
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6297897434149669
============= xn0: 0.595 =============
new_qn: 0.5403475243974049
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6306978905147946
============= xn0: 0.596 =============
new_qn: 0.5412556714972324
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.631606037614622
============= xn0: 0.597 =============
new_qn: 0.54216381859706
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6325141847144498
============= xn0: 0.598 =============
new_qn: 0.5430719656968875
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.633422331814277
============= xn0: 0.599 =============
new_qn: 0.5439801127967151
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6343304789141049
============= xn0: 0.6 =============
new_qn: 0.5448882598965427
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6352386260139322
============= xn0: 0.601 =============
new_qn: 0.5457964069963702
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.63614677311376
============= xn0: 0.602 =============
new_qn: 0.5467045540961978
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6370549202135873
============= xn0: 0.603 =============
new_qn: 0.5476127011960253
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6379630673134151
============= xn0: 0.604 =============
new_qn: 0.5485208482958529
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6388712144132425
============= xn0: 0.605 =============
new_qn: 0.5494289953956806
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6397793615130702
============= xn0: 0.606 =============
new_qn: 0.5503371424955081
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6406875086128976
============= xn0: 0.607 =============
new_qn: 0.5512452895953357
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6415956557127254
============= xn0: 0.608 =============
new_qn: 0.5521534366951633
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6425038028125527
============= xn0: 0.609 =============
new_qn: 0.5530615837949908
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6434119499123805
============= xn0: 0.61 =============
new_qn: 0.5539697308948184
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6443200970122078
============= xn0: 0.611 =============
new_qn: 0.5548778779946459
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6452282441120356
============= xn0: 0.612 =============
new_qn: 0.5557860250944735
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.646136391211863
============= xn0: 0.613 =============
new_qn: 0.5566941721943011
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6470445383116907
============= xn0: 0.614 =============
new_qn: 0.5576023192941286
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.647952685411518
============= xn0: 0.615 =============
new_qn: 0.5585104663939563
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6488608325113459
============= xn0: 0.616 =============
new_qn: 0.5594186134937839
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6497689796111736
============= xn0: 0.617 =============
new_qn: 0.5603267605936114
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.650677126711001
============= xn0: 0.618 =============
new_qn: 0.561234907693439
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6515852738108288
============= xn0: 0.619 =============
new_qn: 0.5621430547932665
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.652493420910656
============= xn0: 0.62 =============
new_qn: 0.5630512018930941
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6534015680104839
============= xn0: 0.621 =============
new_qn: 0.5639593489929217
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6543097151103112
============= xn0: 0.622 =============
new_qn: 0.5648674960927492
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.655217862210139
============= xn0: 0.623 =============
new_qn: 0.5657756431925768
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6561260093099663
============= xn0: 0.624 =============
new_qn: 0.5666837902924043
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6570341564097941
============= xn0: 0.625 =============
new_qn: 0.567591937392232
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6579423035096215
============= xn0: 0.626 =============
new_qn: 0.5685000844920596
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6588504506094492
============= xn0: 0.627 =============
new_qn: 0.5694082315918871
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6597585977092766
============= xn0: 0.628 =============
new_qn: 0.5703163786917147
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6606667448091044
============= xn0: 0.629 =============
new_qn: 0.5712245257915423
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6615748919089317
============= xn0: 0.63 =============
new_qn: 0.5721326728913698
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6624830390087595
============= xn0: 0.631 =============
new_qn: 0.5730408199911974
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6633911861085868
============= xn0: 0.632 =============
new_qn: 0.5739489670910249
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6642993332084146
============= xn0: 0.633 =============
new_qn: 0.5748571141908525
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.665207480308242
============= xn0: 0.634 =============
new_qn: 0.5757652612906801
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6661156274080697
============= xn0: 0.635 =============
new_qn: 0.5766734083905076
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.667023774507897
============= xn0: 0.636 =============
new_qn: 0.5775815554903353
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6679319216077249
============= xn0: 0.637 =============
new_qn: 0.5784897025901629
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6688400687075526
============= xn0: 0.638 =============
new_qn: 0.5793978496899904
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.66974821580738
============= xn0: 0.639 =============
new_qn: 0.580305996789818
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6706563629072078
============= xn0: 0.64 =============
new_qn: 0.5812141438896455
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.671564510007035
============= xn0: 0.641 =============
new_qn: 0.5821222909894731
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6724726571068629
============= xn0: 0.642 =============
new_qn: 0.5830304380893007
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6733808042066902
============= xn0: 0.643 =============
new_qn: 0.5839385851891282
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.674288951306518
============= xn0: 0.644 =============
new_qn: 0.5848467322889558
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6751970984063453
============= xn0: 0.645 =============
new_qn: 0.5857548793887833
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6761052455061731
============= xn0: 0.646 =============
new_qn: 0.586663026488611
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6770133926060005
============= xn0: 0.647 =============
new_qn: 0.5875711735884386
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6779215397058282
============= xn0: 0.648 =============
new_qn: 0.5884793206882661
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6788296868056556
============= xn0: 0.649 =============
new_qn: 0.5893874677880937
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6797378339054834
============= xn0: 0.65 =============
new_qn: 0.5902956148879213
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6806459810053107
============= xn0: 0.651 =============
new_qn: 0.5912037619877488
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6815541281051385
============= xn0: 0.652 =============
new_qn: 0.5921119090875764
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6824622752049658
============= xn0: 0.653 =============
new_qn: 0.5930200561874039
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6833704223047936
============= xn0: 0.654 =============
new_qn: 0.5939282032872315
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.684278569404621
============= xn0: 0.655 =============
new_qn: 0.5948363503870591
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6851867165044487
============= xn0: 0.656 =============
new_qn: 0.5957444974868867
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6860948636042765
============= xn0: 0.657 =============
new_qn: 0.5966526445867143
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6870030107041039
============= xn0: 0.658 =============
new_qn: 0.5975607916865419
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6879111578039316
============= xn0: 0.659 =============
new_qn: 0.5984689387863694
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.688819304903759
============= xn0: 0.66 =============
new_qn: 0.599377085886197
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6897274520035868
============= xn0: 0.661 =============
new_qn: 0.6002852329860245
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.690635599103414
============= xn0: 0.662 =============
new_qn: 0.6011933800858521
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6915437462032419
============= xn0: 0.663 =============
new_qn: 0.6021015271856797
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6924518933030692
============= xn0: 0.664 =============
new_qn: 0.6030096742855072
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.693360040402897
============= xn0: 0.665 =============
new_qn: 0.6039178213853348
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6942681875027243
============= xn0: 0.666 =============
new_qn: 0.6048259684851625
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6951763346025521
============= xn0: 0.667 =============
new_qn: 0.60573411558499
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6960844817023795
============= xn0: 0.668 =============
new_qn: 0.6066422626848176
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6969926288022072
============= xn0: 0.669 =============
new_qn: 0.6075504097846451
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6979007759020346
============= xn0: 0.67 =============
new_qn: 0.6084585568844727
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6988089230018624
============= xn0: 0.671 =============
new_qn: 0.6093667039843003
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.6997170701016897
============= xn0: 0.672 =============
new_qn: 0.6102748510841278
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7006252172015175
============= xn0: 0.673 =============
new_qn: 0.6111829981839554
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7015333643013448
============= xn0: 0.674 =============
new_qn: 0.612091145283783
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7024415114011726
============= xn0: 0.675 =============
new_qn: 0.6129992923836105
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.703349658501
============= xn0: 0.676 =============
new_qn: 0.6139074394834382
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7042578056008277
============= xn0: 0.677 =============
new_qn: 0.6148155865832657
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7051659527006555
============= xn0: 0.678 =============
new_qn: 0.6157237336830933
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7060740998004829
============= xn0: 0.679 =============
new_qn: 0.6166318807829209
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7069822469003106
============= xn0: 0.68 =============
new_qn: 0.6175400278827484
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.707890394000138
============= xn0: 0.681 =============
new_qn: 0.618448174982576
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7087985410999658
============= xn0: 0.682 =============
new_qn: 0.6193563220824035
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.709706688199793
============= xn0: 0.683 =============
new_qn: 0.6202644691822311
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7106148352996209
============= xn0: 0.684 =============
new_qn: 0.6211726162820587
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7115229823994482
============= xn0: 0.685 =============
new_qn: 0.6220807633818862
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.712431129499276
============= xn0: 0.686 =============
new_qn: 0.6229889104817139
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7133392765991033
============= xn0: 0.687 =============
new_qn: 0.6238970575815415
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7142474236989311
============= xn0: 0.6880000000000001 =============
new_qn: 0.624805204681369
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7151555707987585
============= xn0: 0.6890000000000001 =============
new_qn: 0.6257133517811966
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7160637178985862
============= xn0: 0.6900000000000001 =============
new_qn: 0.6266214988810241
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7169718649984136
============= xn0: 0.6910000000000001 =============
new_qn: 0.6275296459808517
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7178800120982414
============= xn0: 0.6920000000000001 =============
new_qn: 0.6284377930806793
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7187881591980687
============= xn0: 0.6930000000000001 =============
new_qn: 0.6293459401805068
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7196963062978965
============= xn0: 0.6940000000000001 =============
new_qn: 0.6302540872803344
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7206044533977238
============= xn0: 0.6950000000000001 =============
new_qn: 0.631162234380162
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7215126004975516
============= xn0: 0.6960000000000001 =============
new_qn: 0.6320703814799896
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7224207475973794
============= xn0: 0.6970000000000001 =============
new_qn: 0.6329785285798172
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7233288946972067
============= xn0: 0.6980000000000001 =============
new_qn: 0.6338866756796447
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7242370417970345
============= xn0: 0.6990000000000001 =============
new_qn: 0.6347948227794723
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7251451888968619
============= xn0: 0.7000000000000001 =============
new_qn: 0.6357029698792999
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7260533359966896
============= xn0: 0.7010000000000001 =============
new_qn: 0.6366111169791274
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.726961483096517
============= xn0: 0.7020000000000001 =============
new_qn: 0.637519264078955
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7278696301963448
============= xn0: 0.7030000000000001 =============
new_qn: 0.6384274111787825
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.728777777296172
============= xn0: 0.704 =============
new_qn: 0.63933555827861
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7296859243959994
============= xn0: 0.705 =============
new_qn: 0.6402437053784377
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7305940714958272
============= xn0: 0.706 =============
new_qn: 0.6411518524782652
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.731502218595655
============= xn0: 0.707 =============
new_qn: 0.6420599995780928
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7324103656954823
============= xn0: 0.708 =============
new_qn: 0.6429681466779203
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7333185127953101
============= xn0: 0.709 =============
new_qn: 0.6438762937777479
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.734226659895138
============= xn0: 0.71 =============
new_qn: 0.6447844408775755
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7351348069949653
============= xn0: 0.711 =============
new_qn: 0.645692587977403
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.736042954094793
============= xn0: 0.712 =============
new_qn: 0.6466007350772306
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7369511011946204
============= xn0: 0.713 =============
new_qn: 0.6475088821770582
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7378592482944482
============= xn0: 0.714 =============
new_qn: 0.6484170292768857
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7387673953942755
============= xn0: 0.715 =============
new_qn: 0.6493251763767134
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7396755424941033
============= xn0: 0.716 =============
new_qn: 0.650233323476541
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7405836895939306
============= xn0: 0.717 =============
new_qn: 0.6511414705763685
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7414918366937584
============= xn0: 0.718 =============
new_qn: 0.6520496176761961
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7423999837935857
============= xn0: 0.719 =============
new_qn: 0.6529577647760236
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7433081308934135
============= xn0: 0.72 =============
new_qn: 0.6538659118758512
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7442162779932409
============= xn0: 0.721 =============
new_qn: 0.6547740589756788
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7451244250930686
============= xn0: 0.722 =============
new_qn: 0.6556822060755063
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.746032572192896
============= xn0: 0.723 =============
new_qn: 0.6565903531753339
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7469407192927238
============= xn0: 0.724 =============
new_qn: 0.6574985002751614
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.747848866392551
============= xn0: 0.725 =============
new_qn: 0.6584066473749891
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7487570134923789
============= xn0: 0.726 =============
new_qn: 0.6593147944748167
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7496651605922067
============= xn0: 0.727 =============
new_qn: 0.6602229415746442
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.750573307692034
============= xn0: 0.728 =============
new_qn: 0.6611310886744718
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7514814547918618
============= xn0: 0.729 =============
new_qn: 0.6620392357742994
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7523896018916891
============= xn0: 0.73 =============
new_qn: 0.6629473828741269
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.753297748991517
============= xn0: 0.731 =============
new_qn: 0.6638555299739545
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7542058960913443
============= xn0: 0.732 =============
new_qn: 0.664763677073782
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.755114043191172
============= xn0: 0.733 =============
new_qn: 0.6656718241736096
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7560221902909994
============= xn0: 0.734 =============
new_qn: 0.6665799712734372
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7569303373908272
============= xn0: 0.735 =============
new_qn: 0.6674881183732648
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7578384844906545
============= xn0: 0.736 =============
new_qn: 0.6683962654730924
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7587466315904823
============= xn0: 0.737 =============
new_qn: 0.66930441257292
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7596547786903096
============= xn0: 0.738 =============
new_qn: 0.6702125596727475
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7605629257901374
============= xn0: 0.739 =============
new_qn: 0.6711207067725751
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7614710728899647
============= xn0: 0.74 =============
new_qn: 0.6720288538724026
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7623792199897925
============= xn0: 0.741 =============
new_qn: 0.6729370009722302
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7632873670896199
============= xn0: 0.742 =============
new_qn: 0.6738451480720578
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7641955141894476
============= xn0: 0.743 =============
new_qn: 0.6747532951718853
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.765103661289275
============= xn0: 0.744 =============
new_qn: 0.6756614422717129
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7660118083891028
============= xn0: 0.745 =============
new_qn: 0.6765695893715405
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7669199554889305
============= xn0: 0.746 =============
new_qn: 0.6774777364713681
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7678281025887579
============= xn0: 0.747 =============
new_qn: 0.6783858835711957
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7687362496885857
============= xn0: 0.748 =============
new_qn: 0.6792940306710232
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.769644396788413
============= xn0: 0.749 =============
new_qn: 0.6802021777708508
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7705525438882408
============= xn0: 0.75 =============
new_qn: 0.6811103248706784
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7714606909880681
============= xn0: 0.751 =============
new_qn: 0.6820184719705059
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.772368838087896
============= xn0: 0.752 =============
new_qn: 0.6829266190703335
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7732769851877233
============= xn0: 0.753 =============
new_qn: 0.683834766170161
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.774185132287551
============= xn0: 0.754 =============
new_qn: 0.6847429132699886
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7750932793873784
============= xn0: 0.755 =============
new_qn: 0.6856510603698162
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7760014264872062
============= xn0: 0.756 =============
new_qn: 0.6865592074696438
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7769095735870335
============= xn0: 0.757 =============
new_qn: 0.6874673545694714
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7778177206868613
============= xn0: 0.758 =============
new_qn: 0.688375501669299
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7787258677866886
============= xn0: 0.759 =============
new_qn: 0.6892836487691265
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7796340148865164
============= xn0: 0.76 =============
new_qn: 0.6901917958689541
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7805421619863437
============= xn0: 0.761 =============
new_qn: 0.6910999429687816
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7814503090861715
============= xn0: 0.762 =============
new_qn: 0.6920080900686092
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7823584561859989
============= xn0: 0.763 =============
new_qn: 0.6929162371684368
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7832666032858266
============= xn0: 0.764 =============
new_qn: 0.6938243842682643
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.784174750385654
============= xn0: 0.765 =============
new_qn: 0.6947325313680919
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7850828974854818
============= xn0: 0.766 =============
new_qn: 0.6956406784679195
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7859910445853096
============= xn0: 0.767 =============
new_qn: 0.6965488255677471
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.786899191685137
============= xn0: 0.768 =============
new_qn: 0.6974569726675747
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7878073387849647
============= xn0: 0.769 =============
new_qn: 0.6983651197674022
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.788715485884792
============= xn0: 0.77 =============
new_qn: 0.6992732668672298
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7896236329846198
============= xn0: 0.771 =============
new_qn: 0.7001814139670574
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7905317800844471
============= xn0: 0.772 =============
new_qn: 0.7010895610668849
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.791439927184275
============= xn0: 0.773 =============
new_qn: 0.7019977081667125
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7923480742841023
============= xn0: 0.774 =============
new_qn: 0.70290585526654
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.79325622138393
============= xn0: 0.775 =============
new_qn: 0.7038140023663676
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7941643684837574
============= xn0: 0.776 =============
new_qn: 0.7047221494661953
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7950725155835852
============= xn0: 0.777 =============
new_qn: 0.7056302965660228
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7959806626834125
============= xn0: 0.778 =============
new_qn: 0.7065384436658504
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7968888097832403
============= xn0: 0.779 =============
new_qn: 0.707446590765678
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7977969568830676
============= xn0: 0.78 =============
new_qn: 0.7083547378655055
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7987051039828954
============= xn0: 0.781 =============
new_qn: 0.7092628849653331
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.7996132510827227
============= xn0: 0.782 =============
new_qn: 0.7101710320651606
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8005213981825505
============= xn0: 0.783 =============
new_qn: 0.7110791791649882
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8014295452823779
============= xn0: 0.784 =============
new_qn: 0.7119873262648158
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8023376923822056
============= xn0: 0.785 =============
new_qn: 0.7128954733646433
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.803245839482033
============= xn0: 0.786 =============
new_qn: 0.713803620464471
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8041539865818608
============= xn0: 0.787 =============
new_qn: 0.7147117675642986
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8050621336816886
============= xn0: 0.788 =============
new_qn: 0.7156199146641261
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.805970280781516
============= xn0: 0.789 =============
new_qn: 0.7165280617639537
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8068784278813437
============= xn0: 0.79 =============
new_qn: 0.7174362088637812
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.807786574981171
============= xn0: 0.791 =============
new_qn: 0.7183443559636088
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8086947220809988
============= xn0: 0.792 =============
new_qn: 0.7192525030634364
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8096028691808261
============= xn0: 0.793 =============
new_qn: 0.7201606501632639
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.810511016280654
============= xn0: 0.794 =============
new_qn: 0.7210687972630915
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8114191633804813
============= xn0: 0.795 =============
new_qn: 0.721976944362919
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.812327310480309
============= xn0: 0.796 =============
new_qn: 0.7228850914627467
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8132354575801364
============= xn0: 0.797 =============
new_qn: 0.7237932385625743
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8141436046799642
============= xn0: 0.798 =============
new_qn: 0.7247013856624018
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8150517517797915
============= xn0: 0.799 =============
new_qn: 0.7256095327622294
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8159598988796193
============= xn0: 0.8 =============
new_qn: 0.726517679862057
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8168680459794466
============= xn0: 0.801 =============
new_qn: 0.7274258269618845
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8177761930792744
============= xn0: 0.802 =============
new_qn: 0.7283339740617121
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8186843401791017
============= xn0: 0.803 =============
new_qn: 0.7292421211615396
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8195924872789295
============= xn0: 0.804 =============
new_qn: 0.7301502682613672
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8205006343787569
============= xn0: 0.805 =============
new_qn: 0.7310584153611948
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8214087814785846
============= xn0: 0.806 =============
new_qn: 0.7319665624610224
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8223169285784124
============= xn0: 0.807 =============
new_qn: 0.73287470956085
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8232250756782398
============= xn0: 0.808 =============
new_qn: 0.7337828566606776
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8241332227780676
============= xn0: 0.809 =============
new_qn: 0.7346910037605051
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.825041369877895
============= xn0: 0.81 =============
new_qn: 0.7355991508603327
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8259495169777227
============= xn0: 0.811 =============
new_qn: 0.7365072979601602
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.82685766407755
============= xn0: 0.812 =============
new_qn: 0.7374154450599878
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8277658111773778
============= xn0: 0.8130000000000001 =============
new_qn: 0.7383235921598154
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8286739582772051
============= xn0: 0.8140000000000001 =============
new_qn: 0.7392317392596429
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.829582105377033
============= xn0: 0.8150000000000001 =============
new_qn: 0.7401398863594705
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8304902524768603
============= xn0: 0.8160000000000001 =============
new_qn: 0.7410480334592981
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.831398399576688
============= xn0: 0.8170000000000001 =============
new_qn: 0.7419561805591257
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8323065466765154
============= xn0: 0.8180000000000001 =============
new_qn: 0.7428643276589533
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8332146937763432
============= xn0: 0.8190000000000001 =============
new_qn: 0.7437724747587808
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8341228408761705
============= xn0: 0.8200000000000001 =============
new_qn: 0.7446806218586084
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8350309879759983
============= xn0: 0.8210000000000001 =============
new_qn: 0.745588768958436
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8359391350758256
============= xn0: 0.8220000000000001 =============
new_qn: 0.7464969160582635
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8368472821756534
============= xn0: 0.8230000000000001 =============
new_qn: 0.7474050631580911
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8377554292754807
============= xn0: 0.8240000000000001 =============
new_qn: 0.7483132102579186
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8386635763753085
============= xn0: 0.8250000000000001 =============
new_qn: 0.7492213573577462
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8395717234751359
============= xn0: 0.8260000000000001 =============
new_qn: 0.7501295044575739
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8404798705749637
============= xn0: 0.8270000000000001 =============
new_qn: 0.7510376515574014
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8413880176747914
============= xn0: 0.8280000000000001 =============
new_qn: 0.751945798657229
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8422961647746188
============= xn0: 0.8290000000000001 =============
new_qn: 0.7528539457570566
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8432043118744466
============= xn0: 0.8300000000000001 =============
new_qn: 0.7537620928568841
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.844112458974274
============= xn0: 0.8310000000000001 =============
new_qn: 0.7546702399567117
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8450206060741017
============= xn0: 0.8320000000000001 =============
new_qn: 0.7555783870565392
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.845928753173929
============= xn0: 0.833 =============
new_qn: 0.7564865341563667
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8468369002737564
============= xn0: 0.834 =============
new_qn: 0.7573946812561942
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8477450473735841
============= xn0: 0.835 =============
new_qn: 0.7583028283560219
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.848653194473412
============= xn0: 0.836 =============
new_qn: 0.7592109754558495
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8495613415732393
============= xn0: 0.837 =============
new_qn: 0.760119122555677
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.850469488673067
============= xn0: 0.838 =============
new_qn: 0.7610272696555046
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8513776357728944
============= xn0: 0.839 =============
new_qn: 0.7619354167553322
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8522857828727222
============= xn0: 0.84 =============
new_qn: 0.7628435638551597
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8531939299725495
============= xn0: 0.841 =============
new_qn: 0.7637517109549873
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8541020770723773
============= xn0: 0.842 =============
new_qn: 0.7646598580548148
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8550102241722046
============= xn0: 0.843 =============
new_qn: 0.7655680051546424
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8559183712720324
============= xn0: 0.844 =============
new_qn: 0.76647615225447
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8568265183718597
============= xn0: 0.845 =============
new_qn: 0.7673842993542976
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8577346654716875
============= xn0: 0.846 =============
new_qn: 0.7682924464541252
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8586428125715149
============= xn0: 0.847 =============
new_qn: 0.7692005935539528
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8595509596713427
============= xn0: 0.848 =============
new_qn: 0.7701087406537803
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.86045910677117
============= xn0: 0.849 =============
new_qn: 0.7710168877536079
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8613672538709978
============= xn0: 0.85 =============
new_qn: 0.7719250348534354
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8622754009708251
============= xn0: 0.851 =============
new_qn: 0.772833181953263
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.863183548070653
============= xn0: 0.852 =============
new_qn: 0.7737413290530906
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8640916951704802
============= xn0: 0.853 =============
new_qn: 0.7746494761529181
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.864999842270308
============= xn0: 0.854 =============
new_qn: 0.7755576232527457
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8659079893701354
============= xn0: 0.855 =============
new_qn: 0.7764657703525734
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8668161364699631
============= xn0: 0.856 =============
new_qn: 0.7773739174524009
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.867724283569791
============= xn0: 0.857 =============
new_qn: 0.7782820645522285
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8686324306696183
============= xn0: 0.858 =============
new_qn: 0.779190211652056
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.869540577769446
============= xn0: 0.859 =============
new_qn: 0.7800983587518836
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8704487248692734
============= xn0: 0.86 =============
new_qn: 0.7810065058517112
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8713568719691012
============= xn0: 0.861 =============
new_qn: 0.7819146529515387
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8722650190689285
============= xn0: 0.862 =============
new_qn: 0.7828228000513663
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8731731661687563
============= xn0: 0.863 =============
new_qn: 0.7837309471511938
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8740813132685836
============= xn0: 0.864 =============
new_qn: 0.7846390942510214
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8749894603684114
============= xn0: 0.865 =============
new_qn: 0.7855472413508491
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8758976074682387
============= xn0: 0.866 =============
new_qn: 0.7864553884506766
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8768057545680665
============= xn0: 0.867 =============
new_qn: 0.7873635355505042
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8777139016678939
============= xn0: 0.868 =============
new_qn: 0.7882716826503318
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8786220487677217
============= xn0: 0.869 =============
new_qn: 0.7891798297501593
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.879530195867549
============= xn0: 0.87 =============
new_qn: 0.7900879768499869
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8804383429673768
============= xn0: 0.871 =============
new_qn: 0.7909961239498144
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8813464900672041
============= xn0: 0.872 =============
new_qn: 0.791904271049642
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.882254637167032
============= xn0: 0.873 =============
new_qn: 0.7928124181494696
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8831627842668592
============= xn0: 0.874 =============
new_qn: 0.7937205652492971
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.884070931366687
============= xn0: 0.875 =============
new_qn: 0.7946287123491247
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8849790784665144
============= xn0: 0.876 =============
new_qn: 0.7955368594489524
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8858872255663421
============= xn0: 0.877 =============
new_qn: 0.7964450065487799
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.88679537266617
============= xn0: 0.878 =============
new_qn: 0.7973531536486075
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8877035197659973
============= xn0: 0.879 =============
new_qn: 0.798261300748435
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.888611666865825
============= xn0: 0.88 =============
new_qn: 0.7991694478482626
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8895198139656524
============= xn0: 0.881 =============
new_qn: 0.8000775949480902
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8904279610654802
============= xn0: 0.882 =============
new_qn: 0.8009857420479177
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8913361081653075
============= xn0: 0.883 =============
new_qn: 0.8018938891477453
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8922442552651353
============= xn0: 0.884 =============
new_qn: 0.8028020362475728
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8931524023649626
============= xn0: 0.885 =============
new_qn: 0.8037101833474004
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8940605494647904
============= xn0: 0.886 =============
new_qn: 0.8046183304472281
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8949686965646177
============= xn0: 0.887 =============
new_qn: 0.8055264775470556
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8958768436644455
============= xn0: 0.888 =============
new_qn: 0.8064346246468832
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8967849907642729
============= xn0: 0.889 =============
new_qn: 0.8073427717467108
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8976931378641007
============= xn0: 0.89 =============
new_qn: 0.8082509188465383
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.898601284963928
============= xn0: 0.891 =============
new_qn: 0.8091590659463659
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.8995094320637558
============= xn0: 0.892 =============
new_qn: 0.8100672130461934
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9004175791635831
============= xn0: 0.893 =============
new_qn: 0.810975360146021
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.901325726263411
============= xn0: 0.894 =============
new_qn: 0.8118835072458486
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9022338733632382
============= xn0: 0.895 =============
new_qn: 0.8127916543456761
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.903142020463066
============= xn0: 0.896 =============
new_qn: 0.8136998014455038
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9040501675628938
============= xn0: 0.897 =============
new_qn: 0.8146079485453314
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9049583146627211
============= xn0: 0.898 =============
new_qn: 0.8155160956451589
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.905866461762549
============= xn0: 0.899 =============
new_qn: 0.8164242427449865
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9067746088623763
============= xn0: 0.9 =============
new_qn: 0.817332389844814
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.907682755962204
============= xn0: 0.901 =============
new_qn: 0.8182405369446416
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9085909030620314
============= xn0: 0.902 =============
new_qn: 0.8191486840444692
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9094990501618592
============= xn0: 0.903 =============
new_qn: 0.8200568311442967
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9104071972616865
============= xn0: 0.904 =============
new_qn: 0.8209649782441243
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9113153443615143
============= xn0: 0.905 =============
new_qn: 0.8218731253439518
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9122234914613416
============= xn0: 0.906 =============
new_qn: 0.8227812724437795
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9131316385611694
============= xn0: 0.907 =============
new_qn: 0.8236894195436071
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9140397856609968
============= xn0: 0.908 =============
new_qn: 0.8245975666434346
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9149479327608245
============= xn0: 0.909 =============
new_qn: 0.8255057137432622
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9158560798606519
============= xn0: 0.91 =============
new_qn: 0.8264138608430898
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9167642269604797
============= xn0: 0.911 =============
new_qn: 0.8273220079429173
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.917672374060307
============= xn0: 0.912 =============
new_qn: 0.8282301550427449
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9185805211601348
============= xn0: 0.913 =============
new_qn: 0.8291383021425724
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9194886682599621
============= xn0: 0.914 =============
new_qn: 0.8300464492424
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.92039681535979
============= xn0: 0.915 =============
new_qn: 0.8309545963422276
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9213049624596172
============= xn0: 0.916 =============
new_qn: 0.8318627434420552
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.922213109559445
============= xn0: 0.917 =============
new_qn: 0.8327708905418828
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9231212566592728
============= xn0: 0.918 =============
new_qn: 0.8336790376417104
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9240294037591001
============= xn0: 0.919 =============
new_qn: 0.8345871847415379
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.924937550858928
============= xn0: 0.92 =============
new_qn: 0.8354953318413655
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9258456979587553
============= xn0: 0.921 =============
new_qn: 0.836403478941193
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.926753845058583
============= xn0: 0.922 =============
new_qn: 0.8373116260410206
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9276619921584104
============= xn0: 0.923 =============
new_qn: 0.8382197731408482
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9285701392582382
============= xn0: 0.924 =============
new_qn: 0.8391279202406757
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9294782863580655
============= xn0: 0.925 =============
new_qn: 0.8400360673405033
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9303864334578933
============= xn0: 0.926 =============
new_qn: 0.840944214440331
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9312945805577206
============= xn0: 0.927 =============
new_qn: 0.8418523615401585
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9322027276575484
============= xn0: 0.928 =============
new_qn: 0.8427605086399861
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9331108747573758
============= xn0: 0.929 =============
new_qn: 0.8436686557398136
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9340190218572035
============= xn0: 0.93 =============
new_qn: 0.8445768028396412
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9349271689570309
============= xn0: 0.931 =============
new_qn: 0.8454849499394688
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9358353160568587
============= xn0: 0.932 =============
new_qn: 0.8463930970392963
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.936743463156686
============= xn0: 0.933 =============
new_qn: 0.8473012441391239
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9376516102565138
============= xn0: 0.934 =============
new_qn: 0.8482093912389514
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9385597573563411
============= xn0: 0.935 =============
new_qn: 0.849117538338779
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.939467904456169
============= xn0: 0.936 =============
new_qn: 0.8500256854386067
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9403760515559967
============= xn0: 0.937 =============
new_qn: 0.8509338325384342
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.941284198655824
============= xn0: 0.9380000000000001 =============
new_qn: 0.8518419796382618
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9421923457556518
============= xn0: 0.9390000000000001 =============
new_qn: 0.8527501267380894
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9431004928554791
============= xn0: 0.9400000000000001 =============
new_qn: 0.8536582738379169
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.944008639955307
============= xn0: 0.9410000000000001 =============
new_qn: 0.8545664209377445
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9449167870551343
============= xn0: 0.9420000000000001 =============
new_qn: 0.855474568037572
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.945824934154962
============= xn0: 0.9430000000000001 =============
new_qn: 0.8563827151373996
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9467330812547894
============= xn0: 0.9440000000000001 =============
new_qn: 0.8572908622372272
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9476412283546172
============= xn0: 0.9450000000000001 =============
new_qn: 0.8581990093370547
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9485493754544445
============= xn0: 0.9460000000000001 =============
new_qn: 0.8591071564368824
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9494575225542723
============= xn0: 0.9470000000000001 =============
new_qn: 0.86001530353671
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9503656696540996
============= xn0: 0.9480000000000001 =============
new_qn: 0.8609234506365375
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9512738167539274
============= xn0: 0.9490000000000001 =============
new_qn: 0.8618315977363651
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9521819638537548
============= xn0: 0.9500000000000001 =============
new_qn: 0.8627397448361926
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9530901109535825
============= xn0: 0.9510000000000001 =============
new_qn: 0.8636478919360202
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9539982580534099
============= xn0: 0.9520000000000001 =============
new_qn: 0.8645560390358478
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9549064051532377
============= xn0: 0.9530000000000001 =============
new_qn: 0.8654641861356753
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.955814552253065
============= xn0: 0.9540000000000001 =============
new_qn: 0.8663723332355029
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9567226993528928
============= xn0: 0.9550000000000001 =============
new_qn: 0.8672804803353305
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9576308464527201
============= xn0: 0.9560000000000001 =============
new_qn: 0.8681886274351581
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.958538993552548
============= xn0: 0.9570000000000001 =============
new_qn: 0.8690967745349857
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9594471406523757
============= xn0: 0.9580000000000001 =============
new_qn: 0.8700049216348132
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.960355287752203
============= xn0: 0.9590000000000001 =============
new_qn: 0.8709130687346408
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9612634348520308
============= xn0: 0.96 =============
new_qn: 0.8718212158344683
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9621715819518581
============= xn0: 0.961 =============
new_qn: 0.8727293629342958
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9630797290516855
============= xn0: 0.962 =============
new_qn: 0.8736375100341234
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9639878761515133
============= xn0: 0.963 =============
new_qn: 0.8745456571339509
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9648960232513406
============= xn0: 0.964 =============
new_qn: 0.8754538042337785
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9658041703511684
============= xn0: 0.965 =============
new_qn: 0.8763619513336062
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9667123174509962
============= xn0: 0.966 =============
new_qn: 0.8772700984334337
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9676204645508235
============= xn0: 0.967 =============
new_qn: 0.8781782455332613
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9685286116506513
============= xn0: 0.968 =============
new_qn: 0.8790863926330889
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9694367587504786
============= xn0: 0.969 =============
new_qn: 0.8799945397329164
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9703449058503064
============= xn0: 0.97 =============
new_qn: 0.880902686832744
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9712530529501338
============= xn0: 0.971 =============
new_qn: 0.8818108339325715
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9721612000499615
============= xn0: 0.972 =============
new_qn: 0.8827189810323991
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9730693471497889
============= xn0: 0.973 =============
new_qn: 0.8836271281322267
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9739774942496167
============= xn0: 0.974 =============
new_qn: 0.8845352752320542
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.974885641349444
============= xn0: 0.975 =============
new_qn: 0.8854434223318819
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9757937884492718
============= xn0: 0.976 =============
new_qn: 0.8863515694317095
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9767019355490991
============= xn0: 0.977 =============
new_qn: 0.887259716531537
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.977610082648927
============= xn0: 0.978 =============
new_qn: 0.8881678636313646
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9785182297487542
============= xn0: 0.979 =============
new_qn: 0.8890760107311921
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.979426376848582
============= xn0: 0.98 =============
new_qn: 0.8899841578310197
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9803345239484094
============= xn0: 0.981 =============
new_qn: 0.8908923049308473
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9812426710482371
============= xn0: 0.982 =============
new_qn: 0.8918004520306748
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9821508181480645
============= xn0: 0.983 =============
new_qn: 0.8927085991305024
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9830589652478923
============= xn0: 0.984 =============
new_qn: 0.8936167462303299
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9839671123477196
============= xn0: 0.985 =============
new_qn: 0.8945248933301576
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9848752594475474
============= xn0: 0.986 =============
new_qn: 0.8954330404299852
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9857834065473752
============= xn0: 0.987 =============
new_qn: 0.8963411875298127
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9866915536472025
============= xn0: 0.988 =============
new_qn: 0.8972493346296403
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9875997007470303
============= xn0: 0.989 =============
new_qn: 0.8981574817294679
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9885078478468576
============= xn0: 0.99 =============
new_qn: 0.8990656288292954
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9894159949466854
============= xn0: 0.991 =============
new_qn: 0.899973775929123
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9903241420465128
============= xn0: 0.992 =============
new_qn: 0.9008819230289505
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9912322891463405
============= xn0: 0.993 =============
new_qn: 0.9017900701287781
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9921404362461679
============= xn0: 0.994 =============
new_qn: 0.9026982172286057
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9930485833459957
============= xn0: 0.995 =============
new_qn: 0.9036063643284333
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.993956730445823
============= xn0: 0.996 =============
new_qn: 0.9045145114282609
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9948648775456508
============= xn0: 0.997 =============
new_qn: 0.9054226585280885
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9957730246454781
============= xn0: 0.998 =============
new_qn: 0.906330805627916
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.996681171745306
============= xn0: 0.999 =============
new_qn: 0.9072389527277436
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.9975893188451332
============= xn0: 1.0 =============
new_qn: 0.9081470998275711
best_Eta: 1.3376991217982834
sum(new_qn_list): 1.998497465944961
DONE
最终的列表：
[0.0, 0.00011323537291038415, 0.00022461941394612446, 0.00033415673747120124, 0.00044185194252752136, 0.0005477096128984667, 0.000651734317172116, 0.0007539306088041632, 0.0008543030261805185, 0.0009528560926795968, 0.0010495943167342977, 0.001144522191893687, 0.0012376441968843643, 0.0013289647956715289, 0.0014184884375197382, 0.0015062195570533857, 0.001592162574316859, 0.0016763218948344163, 0.0017587019096697437, 0.0018393069954852695, 0.0019181415146011273, 0.001995209815053873, 0.0020705162306548855, 0.002144065081048512, 0.002215860671769883, 0.0022859072943025027, 0.0023542092261354987, 0.002420770730820636, 0.002485596058029043, 0.002548689443607619, 0.0026100551096352315, 0.002669697264478625, 0.002727620102847987, 0.0027838278058523597, 0.0028383245410546695, 0.0028911144625265867, 0.0029422017109030432, 0.0029915904134365165, 0.00303928468405109, 0.0030852886233961596, 0.003129606318899962, 0.003172241844822815, 0.0032131992623100902, 0.0032524826194449685, 0.0032900959513008643, 0.0033260432799937026, 0.00336032861473385, 0.0033929559518778435, 0.003423929274979892, 0.0034532525548430434, 0.003480929749570233, 0.0035069648046149454, 0.0035313616528317437, 0.003554124214526555, 0.003575256397506589, 0.003594762097130194, 0.003612645196356387, 0.0036289095657940224, 0.0036435590637510645, 0.003656597536283189, 0.0036680288172426107, 0.003677856728326212, 0.0036860850791238797, 0.0036927176671663064, 0.0036977582779726126, 0.0037012106850980303, 0.0037030786501807827, 0.003703365922989449, 0.0037020762414694403, 0.0036992133317898385, 0.0036947809083894556, 0.0036887826740232, 0.003681222319807803, 0.003672103525267645, 0.003661429958380133, 0.0036492052756210286, 0.003635433122009396, 0.003620117131152581, 0.0036032609252906467, 0.003584868115341061, 0.003564942300942564, 0.003543487070499507, 0.0035205060012253597, 0.003496002659186509, 0.003469980599345529, 0.0034424433656044112, 0.003413394490847585, 0.0033828374969845504, 0.0033507758949926497, 0.003317213184959339, 0.0032821528561243485, 0.003245598386921858, 0.0032075532450220046, 0.0031680208873728077, 0.0031270047602412893, 0.003084508299254912, 0.0030405349294425066, 0.0029950880652750284, 0.0029481711107064296, 0.002899787459213876, 0.002849940493838257, 0.0027986335872240142, 0.002745870101659359, 0.002691653389115617, 0.0026359867912872093, 0.0025788736396305384, 0.002520317255403595, 0.0024603209497046086, 0.0023988880235111254, 0.002336021767718452, 0.0022717254631783174, 0.002206002380736885, 0.002138855781273194, 0.0020702889157367815, 0.0020003050251857363, 0.0019289073408240015, 0.001856099084039109, 0.0017818834664390798, 0.0017062636898898392, 0.0016292429465520064, 0.0015508244189174347, 0.0014710112798461822, 0.0013898066926024966, 0.0013072138108915082, 0.0012232357788948545, 0.0011378757313070254, 0.001051136793370766, 0.0009630220809129786, 0.0008735347003798188, 0.0007826777488723902, 0.0006904543141814379, 0.0005968674748226266, 0.0005019203000710681, 0.00040561584999629363, 0.0003079571754963928, 0.00020894731833268065, 0.00010858931116347614, 6.886177578629971e-06, -9.615906786711315e-05, -0.00020054341962003064, -0.00030626388109389335, -0.0004133174646368809, -0.0005217011914977199, -0.0006314120917932375, -0.0007424472044747221, -0.0008548035772957541, -0.0009684782667788716, -0.0010834683381835408, -0.001199770865473515, -0.001317382931284694, -0.0014363016268928996, -0.001556524052182151, -0.0016780473156123854, -0.0018008685341884267, -0.0019249848334277897, -0.0020503933473299263, -0.0021770912183442515, -0.0023050755973396952, -0.002434343643573228, -0.0025648925246594123, -0.0026967194165392616, -0.002829821503450153, -0.0029641959778947957, -0.003099840040611479, -0.00323675090054365, -0.003374925774809856, -0.003514361888673906, -0.0036550564755148673, -0.0037970067767979787, -0.00394021004204434, -0.004084663528802324, -0.004230364502617628, -0.004377310237004689, -0.0045254980134172285, -0.004674925121219781, -0.004825588857658547, -0.004977486527833169, -0.005130615444667808, -0.005284972928883219, -0.005440556308967975, -0.0055973629211509, -0.005755390109372566, -0.0059146352252577306, -0.0060750956280873625, -0.006236768684771188, -0.006399651769819992, -0.006563742265318223, -0.006729037560896878, -0.006895535053706131, -0.00706323214838836, -0.007232126257051247, -0.007402214799240886, -0.007573495201914998, -0.00774596489941673, -0.007919621333447535, -0.008094461953041504, -0.008270484214538493, -0.00844768558155845, -0.008626063524975164, -0.008805615522890414, -0.008986339060608056, -0.009168231630608675, -0.009351290732523332, -0.009535513873108997, -0.00972089856622238, -0.00990744233279503, -0.01009514270080833, -0.010283997205268014, -0.010474003388179831, -0.010665158798524005, -0.01085746099223131, -0.011050907532157594, -0.011245495988060072, -0.01144122393657232, -0.01163808896118032, -0.011836088652198123, -0.012035220606743807, -0.012235482428715194, -0.012436871728766646, -0.012639386124284308, -0.0128430232393634, -0.01304778070478374, -0.01325365615798707, -0.013460647243052792, -0.0136687516106756, -0.013877966918141332, -0.014088290829304795, -0.014299721014565975, -0.014512255150847586, -0.014725890921572171, -0.014940626016639258, -0.015156458132402989, -0.015373384971649112, -0.015591404243573304, -0.015810513663757964, -0.016030710954150845, -0.01625199384304199, -0.016474360065042548, -0.016697807361062134, -0.016922333478287366, -0.017147936170159833, -0.017374613196354582, -0.017602362322758358, -0.017831181321448286, -0.018061067970670114, -0.01829202005481731, -0.018524035364409414, -0.01875711169607122, -0.018991246852511356, -0.019226438642501648, -0.019462684880856035, -0.01969998338840956, -0.01993833199199818, -0.020177728524437555, -0.02041817082450295, -0.02065965673690842, -0.02090218411228667, -0.021145750807168462, -0.021390354683962887, -0.02163599361093657, -0.02188266546219439, -0.022130368117658644, -0.022379099463049945, -0.02262885738986692, -0.022879639795366724, -0.023131444582545208, -0.023384269660117668, -0.023638112942498818, -0.023892972349784103, -0.02414884580772969, -0.024405731247733625, -0.024663626606816674, -0.024922529827602874, -0.025182438858301043, -0.025443351652685325, -0.025705266170076763, -0.025968180375324224, -0.026232092238785865, -0.0264969997363102, -0.02676290084921823, -0.027029793564284144, -0.027297675873717453, -0.02756654577514439, -0.027836401271589872, -0.02810724037145912, -0.02837906108851984, -0.028651861441883586, -0.02892563945598864, -0.02920039316058126, -0.02947612059069865, -0.02975281978665062, -0.030030488794002386, -0.03030912566355637, -0.030588728451335145, -0.03086929521856413, -0.031150824031653424, -0.031433312962181326, -0.031716760086876516, -0.03200116348760129, -0.03228652125133402, -0.03257283147015255, -0.03286009224121672, -0.03314830166675198, -0.033437457854031905, -0.03372755891536244, -0.03401860296806386, -0.034310588134455366, -0.03460351254183752, -0.03489737432247658, -0.03519217161358734, -0.035487902557317264, -0.035784565300729654, -0.03608215799578829, -0.036380678799340094, -0.03668012587309971, -0.03698049738363368, -0.037281791502343564, -0.037584006405450954, -0.03788714027398088, -0.03819119129374654, -0.03849615765533315, -0.038802037554082625, -0.039108829190077465, -0.039416530768125846, -0.03972514049774556, -0.040034656593148976, -0.040345077273227226, -0.04065640076153548, -0.04096862528627698, -0.0412817490802887, -0.0415957703810253, -0.041910687430544824, -0.042226498475493035, -0.042543201767089034, -0.042860795561110165, -0.04317927811787664, -0.043498647702237825, -0.04381890258355642, -0.04414004103569452, -0.044462061336998815, -0.04478496177028596, -0.04510874062282788, -0.04543339618633785, -0.04575892675695559, -0.0460853306352334, -0.04641260612612136, -0.046740751538953684, -0.047069765187434, -0.047399645389621714, -0.04773039046791766, -0.048061998749050316, -0.04839446856406149, -0.048727798248292986, -0.04906198614137208, -0.04939703058719863, -0.0497329299339303, -0.05006968253396982, -0.05040728674395062, -0.05074574092472389, -0.05108504344134457, -0.05142519266305795, -0.05176618696328683, -0.052108024719617174, -0.05245070431378568, -0.052794224131665846, -0.05313858256325554, -0.05348377800266285, -0.05382980884809385, -0.05417667350183886, -0.054524370370260034, -0.05487289786377808, -0.05522225439685918, -0.05557243838800263, -0.05592344825972728, -0.05627528243855989, -0.05662793935502103, -0.056981417443613736, -0.05733571514280966, -0.05769083089503768, -0.05804676314667062, -0.058403510348012666, -0.05876107095328781, -0.059119443420626194, -0.05947862621205324, -0.059838617793475934, -0.060199416634671765, -0.06056102120927548, -0.060923429994768075, -0.06128664147246349, -0.06165065412749765, -0.06201546644881545, -0.06238107692915984, -0.06274748406505881, -0.06311468635681444, -0.06348268230849036, -0.06385147042790074, -0.06422104922659749, -0.06459141721985967, -0.0649625729266809, -0.0653345148697585, -0.06570724157548152, -0.06608075157391907, -0.06645504339880937, -0.06683011558754759, -0.0672059666811754, -0.06758259522436855, -0.0679599997654265, -0.06833817885626026, -0.06871713105238209, -0.06909685491289369, -0.0694773490004752, -0.06985861188137404, -0.07024064212539421, -0.07062343830588463, -0.07100699899972895, -0.07139132278733373, -0.07177640825261844, -0.07216225398300369, -0.07254885856940141, -0.07293622060620308, -0.07332433869126992, -0.07371321142592119, -0.07410283741492474, -0.07449321526648506, -0.07488434359223384, -0.07527622100721876, -0.07566884612989333, -0.07606221758210613, -0.07645633398909052, -0.07685119397945467, -0.07724679618517033, -0.07764313924156335, -0.07804022178730285, -0.07843804246439129, -0.07883659991815439, -0.07923589279723048, -0.07963591975356088, -0.08003667944237952, -0.08043817052220287, -0.08084039165482038, -0.0812433415052839, -0.08164701874189811, -0.08205142203621035, -0.08245655006300134, -0.08286240150027457, -0.08326897502924685, -0.08367626933433886, -0.08408428310316496, -0.08449301502652379, -0.08490246379838828, -0.08531262811589668, -0.08572350667934225, -0.08613509819216414, -0.08654740136093769, -0.08696041489536532, -0.08737413750826645, -0.0877885679155686, -0.08820370483629791, -0.08861954699256963, -0.08903609310957905, -0.08945334191559212, -0.08987129214193601, -0.09028994252299039, -0.09070929179617765, -0.09112933870195417, -0.09155008198380138, -0.09197152038821593, -0.09239365266470162, -0.0928164775657595, -0.09323999384687975, -0.09366420026653172, -0.09408909558615614, -0.09451467857015522, -0.09494094798588465, -0.09536790260364403, -0.09579554119666878, -0.09622386254112092, -0.09665286541608037, -0.09708254860353649, -0.09751291088837938, -0.09794395105839104, -0.09837566790423691, -0.09880806021945726, -0.09924112680045893, -0.09967486644650625, -0.10010927795971314, -0.10054436014503415, -0.10098011181025679, -0.10141653176599214, -0.10185361882566707, -0.10229137180551628, -0.10272978952457323, -0.10316887080466225, -0.10360861447039027, -0.10404901934913885, -0.10449008427105544, -0.10493180806904573, -0.10537418957876526, -0.1058172276386114, -0.10626092108971547, -0.10670526877593434, -0.10715026954384266, -0.1075959222427248, -0.1080422257245669, -0.10848917884404924, -0.10893678045853772, -0.10938502942807649, -0.10983392461537989, -0.11028346488582486, -0.11073364910744299, -0.11118447615091243, -0.11163594488955109, -0.11208805419930767, -0.11254080295875524, -0.11299419004908262, -0.11344821435408758, -0.11390287476016808, -0.11435817015631622, -0.11481409943410925, -0.11527066148770332, -0.11572785521382456, -0.11618567951176334, -0.11664413328336515, -0.11710321543302443, -0.11756292486767628, -0.11802326049679007, -0.11848422123236102, -0.1189458059889037, -0.11940801368344434, -0.11987084323551367, -0.12033429356714004, -0.12079836360284119, -0.12126305226961853, -0.12172835849694846, -0.12219428121677672, -0.12266081936350981, -0.1231279718740092, -0.12359573768758314, -0.12406411574598075, -0.12453310499338388, -0.12500270437640126, -0.1254729128440602, -0.12594372934780118, -0.12641515284146937, -0.12688718228130935, -0.12735981662595647, -0.12783305483643181, -0.12830689587613375, -0.12878133871083247, -0.1292563823086622, -0.129732025640115, -0.13020826767803412, -0.13068510739760653, -0.13116254377635755, -0.1316405757941425, -0.13211920243314196, -0.1325984226778531, -0.1330782355150853, -0.1335586399339515, -0.1340396349258633, -0.13452121948452295, -0.13500339260591854, -0.13548615328831565, -0.1359695005322527, -0.13645343334053295, -0.1369379507182193, -0.13742305167262703, -0.1379087352133181, -0.13839500035209418, -0.1388818461029908, -0.1393692714822708, -0.1398572755084181, -0.14034585720213155, -0.14083501558631828, -0.14132474968608827, -0.1418150585287471, -0.14230594114379075, -0.1427973965628988, -0.14328942381992865, -0.1437820219509091, -0.1442751899940345, -0.14476892698965854, -0.14526323198028823, -0.14575810401057793, -0.14625354212732322, -0.14674954537945478, -0.14724611281803313, -0.14774324349624163, -0.1482409364693812, -0.1487391907948642, -0.14923800553220873, -0.14973737974303247, -0.1502373124910471, -0.15073780284205207, -0.15123884986392955, -0.15174045262663743, -0.1522426102022047, -0.15274532166472538, -0.15324858609035197, -0.15375240255729117, -0.15425677014579658, -0.15476168793816464, -0.1552671550187275, -0.15577317047384848, -0.15627973339191564, -0.15678684286333677, -0.15729449798053347, -0.15780269783793582, -0.15831144153197624, -0.15882072816108495, -0.15933055682568342, -0.15984092662817967, -0.16035183667296232, -0.16086328606639533, -0.16137527391681228, -0.16188779933451175, -0.1624008614317507, -0.1629144593227399, -0.16342859212363875, -0.1639432589525489, -0.16445845892951028, -0.1649741911764943, -0.16549045481740005, -0.1660072489780477, -0.16652457278617422, -0.16704242537142738, -0.16756080586536143, -0.16807971340143052, -0.16859914711498508, -0.1691191061432652, -0.16963958962539677, -0.17016059670238515, -0.17068212651711095, -0.17120417821432438, -0.17172675094064033, -0.17224984384453335, -0.17277345607633243, -0.17329758678821633, -0.1738222351342078, -0.17434740027016965, -0.17487308135379842, -0.17539927754462087, -0.1759259880039875, -0.17645321189506918, -0.1769809483828506, -0.17750919663412684, -0.1780379558174972, -0.1785672251033617, -0.1790970036639144, -0.17962729067314054, -0.18015808530680988, -0.18068938674247353, -0.18122119415945753, -0.1817535067388596, -0.1822863236635432, -0.18281964411813334, -0.1833534672890117, -0.1838877923643119, -0.18442261853391484, -0.18495794498944385, -0.18549377092426028, -0.18603009553345834, -0.18656691801386144, -0.1871042375640159, -0.18764205338418838, -0.18818036467635918, -0.18871917064421956, -0.1892584704931658, -0.1897982634302952, -0.1903385486644013, -0.19087932540596997, -0.19142059286717367, -0.19196235026186848, -0.19250459680558807, -0.19304733171554062, -0.19359055421060312, -0.1941342635113179, -0.19467845883988788, -0.1952231394201716, -0.19576830447767973, -0.19631395323957002, -0.19686008493464324, -0.1974066987933386, -0.19795379404772967, -0.19850136993151962, -0.19904942568003775, -0.19959796053023365, -0.20014697372067491, -0.20069646449154105, -0.20124643208462034, -0.20179687574330496, -0.20234779471258724, -0.2028991882390549, -0.20345105557088755, -0.20400339595785144, -0.2045562086512967, -0.20510949290415154, -0.2056632479709194, -0.20621747310767446, -0.2067721675720568, -0.2073273306232694, -0.20788296152207308, -0.20843905953078312, -0.20899562391326454, -0.20955265393492845, -0.21011014886272816, -0.21066810796515473, -0.21122653051223267, -0.21178541577551702, -0.21234476302808808, -0.21290457154454817, -0.21346484060101772, -0.21402556947513052, -0.21458675744603106, -0.21514840379436895, -0.21571050780229656, -0.21627306875346408, -0.21683608593301607, -0.2173995586275872, -0.21796348612529892, -0.21852786771575494, -0.21909270269003822, -0.21965799034070588, -0.22022372996178685, -0.22078992084877647, -0.22135656229863443, -0.2219236536097795, -0.22249119408208606, -0.2230591830168812, -0.2236276197169399, -0.2241965034864818, -0.2247658336311672, -0.22533560945809383, -0.22590583027579236, -0.22647649539422354, -0.22704760412477354, -0.22761915578025171, -0.22819114967488507, -0.22876358512431627, -0.22933646144559883, -0.22990977795719436, -0.23048353397896792, -0.2310577288321859, -0.23163236183951064, -0.2322074323249984, -0.23278293961409446, -0.23335888303363073, -0.23393526191182168, -0.23451207557826015, -0.2350893233639152, -0.23566700460112733, -0.23624511862360553, -0.2368236647664238, -0.2374026423660175, -0.23798205076017998, -0.238561889288059, -0.2391421572901531, -0.23972285410830907, -0.24030397908571688, -0.24088553156690784, -0.24146751089775031, -0.2420499164254465, -0.24263274749852892, -0.24321600346685746, -0.24379968368161553, -0.24438378749530687, -0.24496831426175214, -0.24555326333608563, -0.24613863407475223, -0.24672442583550303, -0.24731063797739372, -0.2478972698607793, -0.24848432084731298, -0.24907179029994053, -0.2496596775828991, -0.2502479820617125, -0.25083670310318884, -0.2514258400754167, -0.2520153923477624, -0.2526053592908659, -0.25319574027663916, -0.25378653467826107, -0.2543777418701756, -0.2549693612280881, -0.255561392128962, -0.2561538339510159, -0.25674668607372064, -0.25733994787779546, -0.25793361874520504, -0.25852769805915754, -0.25912218520409913, -0.25971707956571366, -0.260312380530917, -0.2609080874878559, -0.26150419982590345, -0.2621007169356574, -0.26269763820893566, -0.26329496303877464, -0.2638926908194247, -0.2644908209463487, -0.2650893528162176, -0.2656882858269085, -0.2662876193775008, -0.2668873528682737, -0.26748748570070313, -0.2680880172774587, -0.26868894700240054, -0.26929027428057684, -0.2698919985182203, -0.27049411912274557, -0.2710966355027463, -0.27169954706799193, -0.27230285322942527, -0.2729065533991586, -0.27351064699047245, -0.2741151334178106, -0.274720012096779, -0.275325282444142, -0.2759309438778198, -0.27653699581688496, -0.2771434376815606, -0.2777502688932165, -0.2783574888743673, -0.27896509704866845, -0.2795730928409147, -0.2801814756770361, -0.2807902449840963, -0.281399400190289, -0.2820089407249349, -0.2826188660184802, -0.2832291755024924, -0.2838398686096586, -0.28445094477378174, -0.28506240342977907, -0.28567424401367847, -0.286286465962616, -0.2868990687148333, -0.28751205170967475, -0.2881254143875843, -0.2887391561901045, -0.2893532765598712, -0.2899677749406131, -0.29058265077714773, -0.29119790351538, -0.2918135326022978, -0.2924295374859712, -0.29304591761554843, -0.29366267244125466, -0.29427980141438725, -0.2948973039873155, -0.29551517961347595, -0.29613342774737184, -0.29675204784456855, -0.2973710393616923, -0.2979904017564271, -0.29861013448751206, -0.2992302370147395, -0.29985070879895115, -0.3004715493020369, -0.3010927579869316, -0.30171433431761263, -0.3023362777590969, -0.3029585877774398, -0.3035812638397306, -0.30420430541409194, -0.3048277119696754, -0.30545148297666114, -0.3060756179062536, -0.30670011623068005, -0.3073249774231871, -0.30795020095804015, -0.30857578631051863, -0.3092017329569152, -0.30982804037453227, -0.31045470804168085, -0.31108173543767637, -0.31170912204283785, -0.31233686733848454, -0.31296497080693386, -0.31359343193149913, -0.31422225019648675, -0.3148514250871941, -0.3154809560899072, -0.3161108426918984, -0.31674108438142334, -0.31737168064772003, -0.31800263098100456, -0.3186339348724707, -0.31926559181428593, -0.3198976012995902, -0.32052996282249335, -0.32116267587807246, -0.32179573996236954, -0.32242915457238996, -0.3230629192060993, -0.3236970333624215, -0.3243314965412363, -0.324966308243377, -0.32560146797062917, -0.3262369752257265, -0.3268728295123501, -0.32750903033512546, -0.3281455771996209, -0.32878246961234425, -0.32941970708074175, -0.3300572891131951, -0.33069521521901957, -0.3313334849084617, -0.331972097692697, -0.3326110530838279, -0.3332503505948814, -0.33388998973980677, -0.33452997003347407, -0.33517029099167095, -0.33581095213110146, -0.3364519529693828, -0.3370932930250443, -0.33773497181752465, -0.3383769888671696, -0.33901934369523035, -0.3396620358238608, -0.3403050647761162, -0.3409484300759501, -0.3415921312482132, -0.3422361678186503, -0.34288053931389906, -0.34352524526148687, -0.3441702851898304, -0.34481565862823116, -0.3454613651068763, -0.3461074041568336, -0.34675377531005147, -0.34740047809935615, -0.34804751205844975, -0.3486948767219077, -0.349342571625178, -0.3499905963045775, -0.3506389502972912, -0.3512876331413697, -0.3519366443757266, -0.35258598354013837, -0.3532356501752396, -0.3538856438225235, -0.3545359640243384, -0.3551866103238863, -0.35583758226522066, -0.3564888793932448, -0.3571405012537092, -0.3577924473932108, -0.35844471735918937, -0.3590973106999269, -0.35975022696454506, -0.3604034657030033, -0.36105702646609683, -0.361710908805455, -0.36236511227353885, -0.36301963642364, -0.36367448080987796, -0.364329644987198, -0.36498512851137055, -0.36564093093898786, -0.36629705182746275, -0.36695349073502703, -0.36761024722072877, -0.36826732084443103, -0.3689247111668099, -0.3695824177493525, -0.3702404401543552, -0.3708987779449213, -0.3715574306849603, -0.3722163979391847, -0.3728756792731093, -0.373535274253048, -0.37419518244611394, -0.3748554034202154, -0.3755159367440557, -0.3761767819871308, -0.37683793871972693, -0.3774994065129199, -0.3781611849385724, -0.378823273569332, -0.3794856719786308, -0.3801483797406817, -0.38081139643047834, -0.3814747216237917, -0.38213835489716996, -0.38280229582793557, -0.38346654399418356, -0.3841310989747806, -0.38479596034936225, -0.3854611276983321, -0.38612660060285875, -0.3867923786448756, -0.3874584614070782, -0.3881248484729227, -0.3887915394266237, -0.38945853385315365, -0.39012583133823986, -0.39079343146836376, -0.391461333830758, -0.39212953801340644]
**** log-parameter_analysis 运行时间： 2025-02-05 19:05:18 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.06670567781178097
DataOwner2: noise random: 0.02055492171805893
DataOwner3: noise random: 0.08936155995070011
DataOwner4: noise random: 0.0468749958765316
DataOwner5: noise random: 0.07893939443076081
DataOwner6: noise random: 0.07882223086564556
DataOwner7: noise random: 0.04141380148807349
DataOwner8: noise random: 0.08978962352165439
DataOwner9: noise random: 0.006603855858819874
DataOwner10: noise random: 0.05820484558742489
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9981990804963059, 0.9998288281119627, 0.9967625420420609, 0.9991118992599607, 0.997481934354745, 0.9974865850034832, 0.9993053830623699, 0.9967377475986557, 0.999982376206486, 0.9986251113113455]
归一化后的数据质量列表avg_f_list: [0.9450385259540499, 0.995267621873495, 0.900764168920454, 0.9731717539435919, 0.9229359611233557, 0.9230792949005099, 0.9791349573112227, 0.9, 1.0, 0.9581688674055004]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3285
DataOwner1的最优x_1 = 0.1161
DataOwner2的最优x_2 = 0.1622
DataOwner3的最优x_3 = 0.0659
DataOwner4的最优x_4 = 0.1432
DataOwner5的最优x_5 = 0.0923
DataOwner6的最优x_6 = 0.0925
DataOwner7的最优x_7 = 0.1485
DataOwner8的最优x_8 = 0.0650
DataOwner9的最优x_9 = 0.1660
DataOwner10的最优x_10 = 0.1292
每个DataOwner应该贡献数据比例 xn_list = [0.11610090776722232, 0.16218081640920706, 0.06591540484849619, 0.1431716934623848, 0.09230345323995953, 0.09246554907961542, 0.14848666314335635, 0.06495754686237785, 0.16601598081325294, 0.12915905344737516]
ModelOwner的最大效用 U(Eta) = 0.5665
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3284924450674411
DataOwner1的分配到的支付 ： 0.1285
DataOwner2的分配到的支付 ： 0.1891
DataOwner3的分配到的支付 ： 0.0696
DataOwner4的分配到的支付 ： 0.1632
DataOwner5的分配到的支付 ： 0.0998
DataOwner6的分配到的支付 ： 0.1000
DataOwner7的分配到的支付 ： 0.1703
DataOwner8的分配到的支付 ： 0.0685
DataOwner9的分配到的支付 ： 0.1945
DataOwner10的分配到的支付 ： 0.1450
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC6', 'DataOwner3': 'CPC9', 'DataOwner8': 'CPC10', 'DataOwner2': 'CPC2', 'DataOwner5': 'CPC8', 'DataOwner4': 'CPC4', 'DataOwner6': 'CPC7', 'DataOwner7': 'CPC3', 'DataOwner10': 'CPC5', 'DataOwner9': 'CPC1'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC6
DataOwner3 把数据交给 CPC9
DataOwner8 把数据交给 CPC10
DataOwner2 把数据交给 CPC2
DataOwner5 把数据交给 CPC8
DataOwner4 把数据交给 CPC4
DataOwner6 把数据交给 CPC7
DataOwner7 把数据交给 CPC3
DataOwner10 把数据交给 CPC5
DataOwner9 把数据交给 CPC1
DONE
最终Um的列表：
[]
**** log-parameter_analysis 运行时间： 2025-02-05 19:21:41 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.049685424597835814
DataOwner2: noise random: 0.038854235051112365
DataOwner3: noise random: 0.027630744102836703
DataOwner4: noise random: 0.03772243377868306
DataOwner5: noise random: 0.014504054621942541
DataOwner6: noise random: 0.09546391117879914
DataOwner7: noise random: 0.004344874714222436
DataOwner8: noise random: 0.02766120494314458
DataOwner9: noise random: 0.050209220340810094
DataOwner10: noise random: 0.0724620934433072
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9990028113028229, 0.9993895693712705, 0.9996909640695376, 0.9994241552607547, 0.9999148920106562, 0.9963052417313247, 0.9999923547601153, 0.999690641653744, 0.9989784790834332, 0.9978723677411121]
归一化后的数据质量列表avg_f_list: [0.9731621067874613, 0.9836515619635763, 0.9918258353290427, 0.9845895828274356, 0.9978990947970888, 0.9, 1.0, 0.9918170909322449, 0.9725021807369255, 0.9425027927690468]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3579
DataOwner1的最优x_1 = 0.1225
DataOwner2的最优x_2 = 0.1328
DataOwner3的最优x_3 = 0.1405
DataOwner4的最优x_4 = 0.1337
DataOwner5的最优x_5 = 0.1460
DataOwner6的最优x_6 = 0.0358
DataOwner7的最优x_7 = 0.1479
DataOwner8的最优x_8 = 0.1405
DataOwner9的最优x_9 = 0.1218
DataOwner10的最优x_10 = 0.0896
每个DataOwner应该贡献数据比例 xn_list = [0.12250226592558476, 0.13279308745728488, 0.14049299985686078, 0.1336906394509335, 0.14603942204563544, 0.03583718941335145, 0.14792433333085517, 0.14048490818691703, 0.12183895336381019, 0.08956580191333005]
ModelOwner的最大效用 U(Eta) = 0.6007
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3579054665875119
DataOwner1的分配到的支付 ： 0.1362
DataOwner2的分配到的支付 ： 0.1492
DataOwner3的分配到的支付 ： 0.1591
DataOwner4的分配到的支付 ： 0.1503
DataOwner5的分配到的支付 ： 0.1664
DataOwner6的分配到的支付 ： 0.0368
DataOwner7的分配到的支付 ： 0.1689
DataOwner8的分配到的支付 ： 0.1591
DataOwner9的分配到的支付 ： 0.1353
DataOwner10的分配到的支付 ： 0.0964
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC7', 'DataOwner6': 'CPC10', 'DataOwner2': 'CPC6', 'DataOwner9': 'CPC8', 'DataOwner10': 'CPC9', 'DataOwner3': 'CPC3', 'DataOwner4': 'CPC5', 'DataOwner5': 'CPC2', 'DataOwner8': 'CPC4', 'DataOwner7': 'CPC1'}
DONE
**** log-parameter_analysis 运行时间： 2025-02-05 19:22:38 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.09957781668418911
DataOwner2: noise random: 0.05356727647792149
DataOwner3: noise random: 0.023562198792409884
DataOwner4: noise random: 0.06163978390727331
DataOwner5: noise random: 0.0731151668480148
DataOwner6: noise random: 0.005010522223586922
DataOwner7: noise random: 0.081089034372364
DataOwner8: noise random: 0.01185876761067718
DataOwner9: noise random: 0.0483820169629308
DataOwner10: noise random: 0.0856624418018942
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
**** log-parameter_analysis 运行时间： 2025-02-05 19:23:30 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.09917938777292258
DataOwner2: noise random: 0.07310588356381119
DataOwner3: noise random: 0.0013311003720126724
DataOwner4: noise random: 0.004408704819603359
DataOwner5: noise random: 0.06708581821442298
DataOwner6: noise random: 0.06342283380878698
DataOwner7: noise random: 0.04444608781406472
DataOwner8: noise random: 0.09880624351526386
DataOwner9: noise random: 0.014222347829028415
DataOwner10: noise random: 0.034467665351604594
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9960178731534961, 0.9978402332073621, 0.9999992836168713, 0.9999921482772297, 0.9981784615749085, 0.9983714616117108, 0.9991995331739049, 0.99602276179036, 0.9999181160418811, 0.9995192952349407]
归一化后的数据质量列表avg_f_list: [0.9, 0.9457717201135089, 1.0, 0.9998207836216029, 0.9542669097117109, 0.9591144389624029, 0.9799128863923151, 0.9001227865579007, 0.9979613361712671, 0.9879442628097246]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3439
DataOwner1的最优x_1 = 0.0503
DataOwner2的最优x_2 = 0.1050
DataOwner3的最优x_3 = 0.1570
DataOwner4的最优x_4 = 0.1568
DataOwner5的最优x_5 = 0.1140
DataOwner6的最优x_6 = 0.1190
DataOwner7的最优x_7 = 0.1391
DataOwner8的最优x_8 = 0.0504
DataOwner9的最优x_9 = 0.1552
DataOwner10的最优x_10 = 0.1465
每个DataOwner应该贡献数据比例 xn_list = [0.05026023582613792, 0.10499669636085496, 0.15695525197786403, 0.15680312120356985, 0.11397999122858152, 0.11895639920182875, 0.13913874521820493, 0.05042268965435084, 0.15521764629370133, 0.14645098923761973]
ModelOwner的最大效用 U(Eta) = 0.5842
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3439003209053342
DataOwner1的分配到的支付 ： 0.0523
DataOwner2的分配到的支付 ： 0.1148
DataOwner3的分配到的支付 ： 0.1815
DataOwner4的分配到的支付 ： 0.1812
DataOwner5的分配到的支付 ： 0.1257
DataOwner6的分配到的支付 ： 0.1319
DataOwner7的分配到的支付 ： 0.1576
DataOwner8的分配到的支付 ： 0.0525
DataOwner9的分配到的支付 ： 0.1791
DataOwner10的分配到的支付 ： 0.1673
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC10', 'DataOwner2': 'CPC8', 'DataOwner8': 'CPC9', 'DataOwner3': 'CPC1', 'DataOwner4': 'CPC2', 'DataOwner5': 'CPC7', 'DataOwner6': 'CPC6', 'DataOwner7': 'CPC5', 'DataOwner9': 'CPC3', 'DataOwner10': 'CPC4'}
DONE
**** log-parameter_analysis 运行时间： 2025-02-05 19:24:00 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.0372712846238419
DataOwner2: noise random: 0.08654697877429401
DataOwner3: noise random: 0.017387955615262973
DataOwner4: noise random: 0.09284181002460434
DataOwner5: noise random: 8.431217626437838e-05
DataOwner6: noise random: 0.07175741261731056
DataOwner7: noise random: 0.0033263151572729744
DataOwner8: noise random: 0.05940685133223153
DataOwner9: noise random: 0.01436819283233002
DataOwner10: noise random: 0.002275534463351603
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9994399327263307, 0.996988415319544, 0.9998778198634083, 0.9965026391171679, 0.9999999971220175, 0.9979159844944799, 0.9999955198880447, 0.9985715251102849, 0.9999164534578857, 0.9999979049495292]
归一化后的数据质量列表avg_f_list: [0.9839860719174254, 0.9138898048670598, 0.9965065841575335, 0.9, 1.0, 0.9404118015757111, 0.9998719823945224, 0.9591556823821932, 0.9976112349946468, 0.9999401784865789]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3518
DataOwner1的最优x_1 = 0.1375
DataOwner2的最优x_2 = 0.0605
DataOwner3的最优x_3 = 0.1489
DataOwner4的最优x_4 = 0.0422
DataOwner5的最优x_5 = 0.1519
DataOwner6的最优x_6 = 0.0925
DataOwner7的最优x_7 = 0.1518
DataOwner8的最优x_8 = 0.1129
DataOwner9的最优x_9 = 0.1499
DataOwner10的最优x_10 = 0.1519
每个DataOwner应该贡献数据比例 xn_list = [0.1374606652725066, 0.06054317424493237, 0.14887273438969145, 0.04223720271745927, 0.15194738436962843, 0.09248466927921918, 0.15183553527650914, 0.1128929622362797, 0.14985002382662904, 0.15189512601020924]
ModelOwner的最大效用 U(Eta) = 0.5935
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3518158723967884
DataOwner1的分配到的支付 ： 0.1553
DataOwner2的分配到的支付 ： 0.0635
DataOwner3的分配到的支付 ： 0.1703
DataOwner4的分配到的支付 ： 0.0436
DataOwner5的分配到的支付 ： 0.1745
DataOwner6的分配到的支付 ： 0.0999
DataOwner7的分配到的支付 ： 0.1743
DataOwner8的分配到的支付 ： 0.1243
DataOwner9的分配到的支付 ： 0.1716
DataOwner10的分配到的支付 ： 0.1744
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC6', 'DataOwner2': 'CPC9', 'DataOwner4': 'CPC10', 'DataOwner3': 'CPC5', 'DataOwner6': 'CPC8', 'DataOwner8': 'CPC7', 'DataOwner5': 'CPC1', 'DataOwner7': 'CPC3', 'DataOwner9': 'CPC4', 'DataOwner10': 'CPC2'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC6
DataOwner2 把数据交给 CPC9
DataOwner4 把数据交给 CPC10
DataOwner3 把数据交给 CPC5
DataOwner6 把数据交给 CPC8
DataOwner8 把数据交给 CPC7
DataOwner5 把数据交给 CPC1
DataOwner7 把数据交给 CPC3
DataOwner9 把数据交给 CPC4
DataOwner10 把数据交给 CPC2
DONE
最终Um的列表：
[('DataOwner1', 'CPC6', 6, 0.1374606652725066, 0.0), ('DataOwner2', 'CPC9', 9, 0.06054317424493237, -8.0), ('DataOwner4', 'CPC10', 10, 0.04223720271745927, -27.0), ('DataOwner3', 'CPC5', 5, 0.14887273438969145, -8.0), ('DataOwner6', 'CPC8', 8, 0.09248466927921918, -35.0), ('DataOwner8', 'CPC7', 7, 0.1128929622362797, -42.0), ('DataOwner5', 'CPC1', 1, 0.15194738436962843, 0.0), ('DataOwner7', 'CPC3', 3, 0.15183553527650914, -12.0), ('DataOwner9', 'CPC4', 4, 0.14985002382662904, -24.0), ('DataOwner10', 'CPC2', 2, 0.15189512601020924, -9.0)]
**** log-parameter_analysis 运行时间： 2025-02-05 19:25:29 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.046166982434603265
DataOwner2: noise random: 0.07312600994371968
DataOwner3: noise random: 0.0672101015029498
DataOwner4: noise random: 0.020939949963954353
DataOwner5: noise random: 0.054859793700634235
DataOwner6: noise random: 0.005124760295172093
DataOwner7: noise random: 0.05260487306831099
DataOwner8: noise random: 0.060665725994835745
DataOwner9: noise random: 0.042193894861631366
DataOwner10: noise random: 0.019081197728965816
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.999141023603611, 0.9978154977328024, 0.998170087935724, 0.9998225678778284, 0.9987833938078955, 0.9999893926305115, 0.9988791231281844, 0.9985096704540035, 0.9992790722235619, 0.9998526288978042]
归一化后的数据质量列表avg_f_list: [0.960974699016291, 0.9, 0.9163112854855724, 0.9923259973212668, 0.9445235910950934, 1.0, 0.9489271765853479, 0.9319322117151408, 0.9673249885402422, 0.9937088157826148]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3359
DataOwner1的最优x_1 = 0.1266
DataOwner2的最优x_2 = 0.0580
DataOwner3的最优x_3 = 0.0782
DataOwner4的最优x_4 = 0.1553
DataOwner5的最优x_5 = 0.1099
DataOwner6的最优x_6 = 0.1617
DataOwner7的最优x_7 = 0.1145
DataOwner8的最优x_8 = 0.0963
DataOwner9的最优x_9 = 0.1328
DataOwner10的最优x_10 = 0.1565
每个DataOwner应该贡献数据比例 xn_list = [0.126648941872109, 0.057997919968430144, 0.07824694050487112, 0.15531214906880192, 0.1099357942036389, 0.16174474245917517, 0.11453031625604841, 0.09628785291154124, 0.13278022637159323, 0.15648736314382783]
ModelOwner的最大效用 U(Eta) = 0.5750
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3359440020952227
DataOwner1的分配到的支付 ： 0.1417
DataOwner2的分配到的支付 ： 0.0608
DataOwner3的分配到的支付 ： 0.0835
DataOwner4的分配到的支付 ： 0.1794
DataOwner5的分配到的支付 ： 0.1209
DataOwner6的分配到的支付 ： 0.1883
DataOwner7的分配到的支付 ： 0.1265
DataOwner8的分配到的支付 ： 0.1045
DataOwner9的分配到的支付 ： 0.1495
DataOwner10的分配到的支付 ： 0.1810
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC5', 'DataOwner2': 'CPC10', 'DataOwner3': 'CPC9', 'DataOwner5': 'CPC7', 'DataOwner8': 'CPC8', 'DataOwner4': 'CPC3', 'DataOwner7': 'CPC6', 'DataOwner6': 'CPC1', 'DataOwner9': 'CPC4', 'DataOwner10': 'CPC2'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC5
DataOwner2 把数据交给 CPC10
DataOwner3 把数据交给 CPC9
DataOwner5 把数据交给 CPC7
DataOwner8 把数据交给 CPC8
DataOwner4 把数据交给 CPC3
DataOwner7 把数据交给 CPC6
DataOwner6 把数据交给 CPC1
DataOwner9 把数据交给 CPC4
DataOwner10 把数据交给 CPC2
DONE
最终Um的列表：
[('DataOwner1', 'CPC5', 5, 0.126648941872109, -0.506595767488436), ('DataOwner2', 'CPC10', 10, 0.057997919968430144, -0.5219812797158713), ('DataOwner3', 'CPC9', 9, 0.07824694050487112, -0.625975524038969), ('DataOwner5', 'CPC7', 7, 0.1099357942036389, -0.6596147652218334), ('DataOwner8', 'CPC8', 8, 0.09628785291154124, -0.6740149703807887), ('DataOwner4', 'CPC3', 3, 0.15531214906880192, -0.31062429813760384), ('DataOwner7', 'CPC6', 6, 0.11453031625604841, -0.5726515812802421), ('DataOwner6', 'CPC1', 1, 0.16174474245917517, 0.0), ('DataOwner9', 'CPC4', 4, 0.13278022637159323, -0.39834067911477966), ('DataOwner10', 'CPC2', 2, 0.15648736314382783, -0.15648736314382786)]
**** log-parameter_analysis 运行时间： 2025-02-05 19:29:33 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.06565372986874579
DataOwner2: noise random: 0.007086408336863715
DataOwner3: noise random: 0.08783998358801451
DataOwner4: noise random: 0.009950579942572025
DataOwner5: noise random: 0.07342173256163229
DataOwner6: noise random: 0.09919121990674465
DataOwner7: noise random: 0.07931676899062927
DataOwner8: noise random: 0.007792725640719256
DataOwner9: noise random: 0.036100131979678776
DataOwner10: noise random: 0.03039654362667582
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9982464111963808, 0.9999796887895152, 0.9968829952721787, 0.9999599782094342, 0.9978167920130957, 0.9960132876390124, 0.9974558702307085, 0.9999753952990075, 0.999473159994953, 0.9996269908602385]
归一化后的数据质量列表avg_f_list: [0.9563010011502577, 1.0, 0.9219268702323784, 0.9995030613563005, 0.9454695404133464, 0.9, 0.9363700628594566, 0.9998917534978242, 0.987229511707409, 0.9911078603526547]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3456
DataOwner1的最优x_1 = 0.1148
DataOwner2的最优x_2 = 0.1559
DataOwner3的最优x_3 = 0.0764
DataOwner4的最优x_4 = 0.1555
DataOwner5的最优x_5 = 0.1033
DataOwner6的最优x_6 = 0.0486
DataOwner7的最优x_7 = 0.0932
DataOwner8的最优x_8 = 0.1558
DataOwner9的最优x_9 = 0.1447
DataOwner10的最优x_10 = 0.1482
每个DataOwner应该贡献数据比例 xn_list = [0.11477981008865779, 0.15590385368934206, 0.07635789307159255, 0.1554791012939498, 0.10330104534987498, 0.048570307471547516, 0.09322151265834161, 0.15581140975918517, 0.1446902630597682, 0.1481622499773309]
ModelOwner的最大效用 U(Eta) = 0.5862
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3455950461254065
DataOwner1的分配到的支付 ： 0.1267
DataOwner2的分配到的支付 ： 0.1800
DataOwner3的分配到的支付 ： 0.0813
DataOwner4的分配到的支付 ： 0.1794
DataOwner5的分配到的支付 ： 0.1127
DataOwner6的分配到的支付 ： 0.0505
DataOwner7的分配到的支付 ： 0.1008
DataOwner8的分配到的支付 ： 0.1798
DataOwner9的分配到的支付 ： 0.1649
DataOwner10的分配到的支付 ： 0.1695
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC6', 'DataOwner3': 'CPC9', 'DataOwner6': 'CPC10', 'DataOwner2': 'CPC1', 'DataOwner5': 'CPC7', 'DataOwner4': 'CPC3', 'DataOwner7': 'CPC8', 'DataOwner9': 'CPC5', 'DataOwner8': 'CPC2', 'DataOwner10': 'CPC4'}
DONE
**** log-parameter_analysis 运行时间： 2025-02-05 19:30:57 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.09199598291105834
DataOwner2: noise random: 0.09553585304027457
DataOwner3: noise random: 0.06112649080727242
DataOwner4: noise random: 0.0945106397438262
DataOwner5: noise random: 0.01558944828150425
DataOwner6: noise random: 0.09233753697879332
DataOwner7: noise random: 0.029115341891865598
DataOwner8: noise random: 0.037759621005728564
DataOwner9: noise random: 0.08931209312388855
DataOwner10: noise random: 0.03585803767312751
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9965752589607318, 0.9963006064097019, 0.9984866912995254, 0.9963944568657316, 0.9999016789167123, 0.9965549244871741, 0.9996579092247024, 0.9994230212752218, 0.9967649932915135, 0.9994798641630456]
归一化后的数据质量列表avg_f_list: [0.9076269653136753, 0.9, 0.9607064946781192, 0.9026061806822, 1.0, 0.9070622870541242, 0.9932306363858174, 0.98670791436277, 0.9128957937088895, 0.9882864132047999]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3231
DataOwner1的最优x_1 = 0.0790
DataOwner2的最优x_2 = 0.0698
DataOwner3的最优x_3 = 0.1352
DataOwner4的最优x_4 = 0.0730
DataOwner5的最优x_5 = 0.1690
DataOwner6的最优x_6 = 0.0784
DataOwner7的最优x_7 = 0.1636
DataOwner8的最优x_8 = 0.1582
DataOwner9的最优x_9 = 0.0852
DataOwner10的最优x_10 = 0.1595
每个DataOwner应该贡献数据比例 xn_list = [0.07902905772190859, 0.06978773641842699, 0.13519651660720924, 0.07298190196774862, 0.16895837735583688, 0.07835585305068973, 0.16355438423400534, 0.15819148447310177, 0.08522757406916184, 0.1595035883758655]
ModelOwner的最大效用 U(Eta) = 0.5605
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.32314277625462
DataOwner1的分配到的支付 ： 0.0844
DataOwner2的分配到的支付 ： 0.0739
DataOwner3的分配到的支付 ： 0.1529
DataOwner4的分配到的支付 ： 0.0775
DataOwner5的分配到的支付 ： 0.1988
DataOwner6的分配到的支付 ： 0.0836
DataOwner7的分配到的支付 ： 0.1912
DataOwner8的分配到的支付 ： 0.1837
DataOwner9的分配到的支付 ： 0.0916
DataOwner10的分配到的支付 ： 0.1855
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC7', 'DataOwner2': 'CPC10', 'DataOwner4': 'CPC9', 'DataOwner3': 'CPC5', 'DataOwner6': 'CPC8', 'DataOwner5': 'CPC1', 'DataOwner9': 'CPC6', 'DataOwner7': 'CPC2', 'DataOwner8': 'CPC4', 'DataOwner10': 'CPC3'}
DONE
**** log-parameter_analysis 运行时间： 2025-02-05 19:33:00 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.08703891165962711
DataOwner2: noise random: 0.04151256471172711
DataOwner3: noise random: 0.03382708260383031
DataOwner4: noise random: 0.029792140398764846
DataOwner5: noise random: 0.04381299048007606
DataOwner6: noise random: 0.05232927129003339
DataOwner7: noise random: 0.02401726418813588
DataOwner8: noise random: 0.022879747291576016
DataOwner9: noise random: 0.09798645240730185
DataOwner10: noise random: 0.02644855691984528
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.996932460109732, 0.9993003626174406, 0.999537366910625, 0.9996411462263514, 0.999222807905318, 0.9988908963735039, 0.9997661121594207, 0.9997895706959351, 0.9961189556744955, 0.9997172577846266]
归一化后的数据质量列表avg_f_list: [0.922162619356292, 0.9866723130691424, 0.9931291136815763, 0.9959564141508491, 0.9845594597279563, 0.9755170641109969, 0.9993609099189809, 1.0, 0.9, 0.9980299510875937]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3596
DataOwner1的最优x_1 = 0.0635
DataOwner2的最优x_2 = 0.1344
DataOwner3的最优x_3 = 0.1405
DataOwner4的最优x_4 = 0.1431
DataOwner5的最优x_5 = 0.1324
DataOwner6的最优x_6 = 0.1235
DataOwner7的最优x_7 = 0.1462
DataOwner8的最优x_8 = 0.1468
DataOwner9的最优x_9 = 0.0340
DataOwner10的最优x_10 = 0.1450
每个DataOwner应该贡献数据比例 xn_list = [0.0634555250018704, 0.13443305452021126, 0.14049575185115828, 0.14309750168683524, 0.1324119661371745, 0.12354930977360386, 0.14618841032663613, 0.14676357333979706, 0.03399760479691945, 0.14498546662833103]
ModelOwner的最大效用 U(Eta) = 0.6027
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.359621022869791
DataOwner1的分配到的支付 ： 0.0667
DataOwner2的分配到的支付 ： 0.1513
DataOwner3的分配到的支付 ： 0.1591
DataOwner4的分配到的支付 ： 0.1625
DataOwner5的分配到的支付 ： 0.1487
DataOwner6的分配到的支付 ： 0.1374
DataOwner7的分配到的支付 ： 0.1666
DataOwner8的分配到的支付 ： 0.1674
DataOwner9的分配到的支付 ： 0.0349
DataOwner10的分配到的支付 ： 0.1650
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC9', 'DataOwner9': 'CPC10', 'DataOwner2': 'CPC6', 'DataOwner5': 'CPC7', 'DataOwner6': 'CPC8', 'DataOwner3': 'CPC5', 'DataOwner4': 'CPC4', 'DataOwner7': 'CPC2', 'DataOwner8': 'CPC1', 'DataOwner10': 'CPC3'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC9
DataOwner9 把数据交给 CPC10
DataOwner2 把数据交给 CPC6
DataOwner5 把数据交给 CPC7
DataOwner6 把数据交给 CPC8
DataOwner3 把数据交给 CPC5
DataOwner4 把数据交给 CPC4
DataOwner7 把数据交给 CPC2
DataOwner8 把数据交给 CPC1
DataOwner10 把数据交给 CPC3
DONE
最终Um的列表：
[('DataOwner1', 'CPC9', 9, 1010, -0.5076442000149632), ('DataOwner9', 'CPC10', 10, 249, -0.305978443172275), ('DataOwner2', 'CPC6', 6, 164, -0.6721652726010563), ('DataOwner5', 'CPC7', 7, 810, -0.794471796823047), ('DataOwner6', 'CPC8', 8, 1664, -0.864845168415227), ('DataOwner3', 'CPC5', 5, 171, -0.5619830074046331), ('DataOwner4', 'CPC4', 4, 876, -0.42929250506050576), ('DataOwner7', 'CPC2', 2, 536, -0.14618841032663613), ('DataOwner8', 'CPC1', 1, 179, 0.0), ('DataOwner10', 'CPC3', 3, 532, -0.2899709332566621)]
('DataOwner1', 'CPC9', 9, 1010, -0.5076442000149632)
('DataOwner9', 'CPC10', 10, 249, -0.305978443172275)
('DataOwner2', 'CPC6', 6, 164, -0.6721652726010563)
('DataOwner5', 'CPC7', 7, 810, -0.794471796823047)
('DataOwner6', 'CPC8', 8, 1664, -0.864845168415227)
('DataOwner3', 'CPC5', 5, 171, -0.5619830074046331)
('DataOwner4', 'CPC4', 4, 876, -0.42929250506050576)
('DataOwner7', 'CPC2', 2, 536, -0.14618841032663613)
('DataOwner8', 'CPC1', 1, 179, 0.0)
('DataOwner10', 'CPC3', 3, 532, -0.2899709332566621)
**** log-parameter_analysis 运行时间： 2025-02-05 19:55:41 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.013386038419303026
DataOwner2: noise random: 0.018880377729822984
DataOwner3: noise random: 0.08025783143102545
DataOwner4: noise random: 0.09164752773372514
DataOwner5: noise random: 0.043277583488363115
DataOwner6: noise random: 0.0775549103322844
DataOwner7: noise random: 0.042745401749642545
DataOwner8: noise random: 0.08740255178682774
DataOwner9: noise random: 0.004103140766997005
DataOwner10: noise random: 0.09216137816139497
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.999927253966148, 0.9998558081877454, 0.9973897991726589, 0.9965941130070786, 0.9992419775816979, 0.9975701541349249, 0.9992618617341107, 0.9969123125630577, 0.9999931793793566, 0.9965560758016908]
归一化后的数据质量列表avg_f_list: [0.9980819486023935, 0.9960032862406649, 0.9242565681286304, 0.9011066645077245, 0.9781443363377224, 0.9295038630730692, 0.9787228511238939, 0.9103644464973848, 1.0, 0.9]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3303
DataOwner1的最优x_1 = 0.1634
DataOwner2的最优x_2 = 0.1617
DataOwner3的最优x_3 = 0.0923
DataOwner4的最优x_4 = 0.0647
DataOwner5的最优x_5 = 0.1465
DataOwner6的最优x_6 = 0.0982
DataOwner7的最优x_7 = 0.1470
DataOwner8的最优x_8 = 0.0761
DataOwner9的最优x_9 = 0.1650
DataOwner10的最优x_10 = 0.0633
每个DataOwner应该贡献数据比例 xn_list = [0.16343390198784444, 0.16173363834921484, 0.09229528346736646, 0.06467300010145907, 0.14646395513789756, 0.09816367989866359, 0.14697774394141902, 0.07606905391846619, 0.16498902358948286, 0.06327825663294503]
ModelOwner的最大效用 U(Eta) = 0.5686
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3303209097341322
DataOwner1的分配到的支付 ： 0.1908
DataOwner2的分配到的支付 ： 0.1884
DataOwner3的分配到的支付 ： 0.0998
DataOwner4的分配到的支付 ： 0.0682
DataOwner5的分配到的支付 ： 0.1676
DataOwner6的分配到的支付 ： 0.1067
DataOwner7的分配到的支付 ： 0.1683
DataOwner8的分配到的支付 ： 0.0810
DataOwner9的分配到的支付 ： 0.1930
DataOwner10的分配到的支付 ： 0.0666
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC8', 'DataOwner2': 'CPC10', 'DataOwner9': 'CPC9', 'DataOwner3': 'CPC4', 'DataOwner5': 'CPC6', 'DataOwner7': 'CPC7', 'DataOwner4': 'CPC2', 'DataOwner6': 'CPC5', 'DataOwner8': 'CPC3', 'DataOwner10': 'CPC1'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC8
DataOwner2 把数据交给 CPC10
DataOwner9 把数据交给 CPC9
DataOwner3 把数据交给 CPC4
DataOwner5 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner4 把数据交给 CPC2
DataOwner6 把数据交给 CPC5
DataOwner8 把数据交给 CPC3
DataOwner10 把数据交给 CPC1
DONE
最终Um的列表：
[('DataOwner1', 'CPC8', 0.8, 213, 0.032686780397568865), ('DataOwner2', 'CPC10', 1.0, 843, 0.0), ('DataOwner9', 'CPC9', 0.9, 645, 0.01649890235894827), ('DataOwner3', 'CPC4', 0.4, 240, 0.05537717008041986), ('DataOwner5', 'CPC6', 0.6, 381, 0.05858558205515903), ('DataOwner7', 'CPC7', 0.7, 766, 0.04409332318242572), ('DataOwner4', 'CPC2', 0.2, 84, 0.05173840008116726), ('DataOwner6', 'CPC5', 0.5, 1280, 0.049081839949331796), ('DataOwner8', 'CPC3', 0.3, 396, 0.05324833774292634), ('DataOwner10', 'CPC1', 0.1, 1238, 0.05695043096965053)]
('DataOwner1', 'CPC8', 0.8, 213, 0.032686780397568865)
('DataOwner2', 'CPC10', 1.0, 843, 0.0)
('DataOwner9', 'CPC9', 0.9, 645, 0.01649890235894827)
('DataOwner3', 'CPC4', 0.4, 240, 0.05537717008041986)
('DataOwner5', 'CPC6', 0.6, 381, 0.05858558205515903)
('DataOwner7', 'CPC7', 0.7, 766, 0.04409332318242572)
('DataOwner4', 'CPC2', 0.2, 84, 0.05173840008116726)
('DataOwner6', 'CPC5', 0.5, 1280, 0.049081839949331796)
('DataOwner8', 'CPC3', 0.3, 396, 0.05324833774292634)
('DataOwner10', 'CPC1', 0.1, 1238, 0.05695043096965053)
**** log-parameter_analysis 运行时间： 2025-02-07 10:07:05 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.06139401652918894
DataOwner2: noise random: 0.09842719488987223
DataOwner3: noise random: 0.01665895160415648
DataOwner4: noise random: 0.002075713145254976
DataOwner5: noise random: 0.03152074184966661
DataOwner6: noise random: 0.051052779124224935
DataOwner7: noise random: 0.02983845977925972
DataOwner8: noise random: 0.022552346169717498
DataOwner9: noise random: 0.05798595486638909
DataOwner10: noise random: 0.005912235075640981
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9984736077927249, 0.9960853455636126, 0.9998876619073942, 0.9999982576538438, 0.9995973476892432, 0.9989449940551031, 0.9996396290994892, 0.9997931805047011, 0.9986422383550851, 0.9999858341253902]
归一化后的数据质量列表avg_f_list: [0.9610354174599199, 0.9, 0.9971735693545053, 1.0, 0.9897541791035527, 0.9730823597757221, 0.9908347403140011, 0.9947589635439389, 0.9653450098676122, 0.9996824991677028]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3619
DataOwner1的最优x_1 = 0.1068
DataOwner2的最优x_2 = 0.0316
DataOwner3的最优x_3 = 0.1427
DataOwner4的最优x_4 = 0.1452
DataOwner5的最优x_5 = 0.1357
DataOwner6的最优x_6 = 0.1194
DataOwner7的最优x_7 = 0.1368
DataOwner8的最优x_8 = 0.1404
DataOwner9的最优x_9 = 0.1113
DataOwner10的最优x_10 = 0.1449
每个DataOwner应该贡献数据比例 xn_list = [0.10676376554345278, 0.03157126996372916, 0.14265291500243954, 0.14522967816387808, 0.13573706754563797, 0.11936015489208461, 0.13675816830629187, 0.14042651751346663, 0.11134625771887859, 0.14494179002017904]
ModelOwner的最大效用 U(Eta) = 0.6054
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.048248772659247846
DataOwner2: noise random: 0.04397976746465074
DataOwner3: noise random: 0.06281684910452016
DataOwner4: noise random: 0.009754603511769933
DataOwner5: noise random: 0.08527745150854123
DataOwner6: noise random: 0.06141185781812524
DataOwner7: noise random: 0.04099037034881819
DataOwner8: noise random: 0.004211998992033528
DataOwner9: noise random: 0.06642111170107161
DataOwner10: noise random: 0.026261583174677763
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9990588470144457, 0.9992167376399737, 0.9984050285124779, 0.9999614286264014, 0.9970420441022492, 0.9984738026637845, 0.9993173640199422, 0.9999928170137883, 0.9982175151621872, 0.9997211891334427]
归一化后的数据质量列表avg_f_list: [0.9683482928933548, 0.9736991155510564, 0.946190759204097, 0.9989362655708213, 0.9, 0.9485214757101903, 0.9771092858008593, 1.0, 0.9398360394099207, 0.9907946870705159]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3467
DataOwner1的最优x_1 = 0.1261
DataOwner2的最优x_2 = 0.1313
DataOwner3的最优x_3 = 0.1032
DataOwner4的最优x_4 = 0.1543
DataOwner5的最优x_5 = 0.0475
DataOwner6的最优x_6 = 0.1057
DataOwner7的最优x_7 = 0.1346
DataOwner8的最优x_8 = 0.1552
DataOwner9的最优x_9 = 0.0962
DataOwner10的最优x_10 = 0.1472
每个DataOwner应该贡献数据比例 xn_list = [0.1261143658912543, 0.131322239168666, 0.1031907501231153, 0.154302348003194, 0.047466923581320535, 0.10570920947863627, 0.13457794368648401, 0.15521635211116755, 0.09619019875109824, 0.14716439732680997]
ModelOwner的最大效用 U(Eta) = 0.5875
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.09908384181544173
DataOwner2: noise random: 0.08899981515621722
DataOwner3: noise random: 0.006940917232991062
DataOwner4: noise random: 0.09936611997519335
DataOwner5: noise random: 0.07552912017947083
DataOwner6: noise random: 0.011323349889588032
DataOwner7: noise random: 0.018396166526036085
DataOwner8: noise random: 0.09455345458946329
DataOwner9: noise random: 0.0074784092169478235
DataOwner10: noise random: 0.05858252301426875
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9960445240919189, 0.9967895531295908, 0.9999804963570097, 0.996025733596317, 0.99769227130252, 0.9999481612634176, 0.9998627929885403, 0.9963827290386889, 0.9999773704624345, 0.9986088051669993]
归一化后的数据质量列表avg_f_list: [0.9004751358485698, 0.9193139153849035, 1.0, 0.9, 0.9421400171653065, 0.9991823758958814, 0.9970237565287287, 0.9090269749154145, 0.9999209587334479, 0.9653154620640233]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3321
DataOwner1的最优x_1 = 0.0622
DataOwner2的最优x_2 = 0.0851
DataOwner3的最优x_3 = 0.1639
DataOwner4的最优x_4 = 0.0616
DataOwner5的最优x_5 = 0.1104
DataOwner6的最优x_6 = 0.1633
DataOwner7的最优x_7 = 0.1615
DataOwner8的最优x_8 = 0.0728
DataOwner9的最优x_9 = 0.1639
DataOwner10的最优x_10 = 0.1335
每个DataOwner应该贡献数据比例 xn_list = [0.06218617502652582, 0.08509010560285088, 0.16394932917813013, 0.06158274758782015, 0.11035031300292718, 0.16328358744433485, 0.16151439010628496, 0.07282665453858517, 0.16388507527632273, 0.13348474063954396]
ModelOwner的最大效用 U(Eta) = 0.5707
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.05757393732244096
DataOwner2: noise random: 0.09450264494893199
**** log-parameter_analysis 运行时间： 2025-02-07 10:08:47 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.07766915706343724
DataOwner2: noise random: 0.019361920727292505
DataOwner3: noise random: 0.022669548156163233
DataOwner4: noise random: 0.009713720086100287
DataOwner5: noise random: 0.09381684741627776
DataOwner6: noise random: 0.00661989651537075
DataOwner7: noise random: 0.010095124827901703
DataOwner8: noise random: 0.09236905006446387
DataOwner9: noise random: 0.05506567666904666
DataOwner10: noise random: 0.050662587443155854
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9975492480668584, 0.9998480491221177, 0.9997920990797013, 0.9999618311364726, 0.9964421134605959, 0.9999823080875031, 0.9999588425338195, 0.9965260854244317, 0.9987734207542386, 0.9989620322065507]
归一化后的数据质量列表avg_f_list: [0.9312732694933712, 0.9962075823638339, 0.9946271595816775, 0.9994215868564166, 0.9, 1.0, 0.9993371676939666, 0.9023719589651251, 0.9658525177097221, 0.9711802319229053]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3479
DataOwner1的最优x_1 = 0.0854
DataOwner2的最优x_2 = 0.1511
DataOwner3的最优x_3 = 0.1498
DataOwner4的最优x_4 = 0.1539
DataOwner5的最优x_5 = 0.0462
DataOwner6的最优x_6 = 0.1544
DataOwner7的最优x_7 = 0.1539
DataOwner8的最优x_8 = 0.0494
DataOwner9的最优x_9 = 0.1227
DataOwner10的最优x_10 = 0.1280
每个DataOwner应该贡献数据比例 xn_list = [0.08535132381616292, 0.15114103783246638, 0.14975258197635047, 0.15393529392793162, 0.046212842729960166, 0.15443402177956633, 0.15386239958744244, 0.049378435395499146, 0.12271890115924348, 0.1279851281468642]
ModelOwner的最大效用 U(Eta) = 0.5889
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.0016132511917953509
DataOwner2: noise random: 0.08453083941629629
DataOwner3: noise random: 0.03636096694097042
DataOwner4: noise random: 0.020803733166616503
DataOwner5: noise random: 0.04682639298089977
DataOwner6: noise random: 0.0056010357748385925
DataOwner7: noise random: 0.013544598621060989
DataOwner8: noise random: 0.048354892690888954
DataOwner9: noise random: 0.05705574705410485
DataOwner10: noise random: 0.09082271002399088
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9975474822681747, 0.9959374726778525, 0.9989972469865551, 0.9997089254812392, 0.9949908629966653, 0.9999623393933852, 0.9998465928406317, 0.9949821268148555, 0.9967366593913205, 0.9940540910189961]
归一化后的数据质量列表avg_f_list: [0.9591273593764548, 0.9318771578226198, 0.9836653379195511, 0.9957108453117096, 0.915855324934034, 1.0, 0.9980409328549014, 0.9157074607743685, 0.9454037847148187, 0.9]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3343
DataOwner1的最优x_1 = 0.1260
DataOwner2的最优x_2 = 0.0976
DataOwner3的最优x_3 = 0.1488
DataOwner4的最优x_4 = 0.1592
DataOwner5的最优x_5 = 0.0792
DataOwner6的最优x_6 = 0.1627
DataOwner7的最优x_7 = 0.1611
DataOwner8的最优x_8 = 0.0790
DataOwner9的最优x_9 = 0.1121
DataOwner10的最优x_10 = 0.0596
每个DataOwner应该贡献数据比例 xn_list = [0.12601801171536853, 0.09757785700554805, 0.14883640237854173, 0.15916364334430752, 0.07915830630874805, 0.16271065354781494, 0.16109886172777899, 0.07898203159379862, 0.11212825769873995, 0.059566675329123284]
ModelOwner的最大效用 U(Eta) = 0.5731
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.05739703297475787
DataOwner2: noise random: 0.03968362528666752
DataOwner3: noise random: 0.00852261237439429
DataOwner4: noise random: 0.05489742624608879
DataOwner5: noise random: 0.004636790222709253
DataOwner6: noise random: 0.057586565586064946
DataOwner7: noise random: 0.013864793471893533
DataOwner8: noise random: 0.02737654337875095
DataOwner9: noise random: 0.05223533769427435
DataOwner10: noise random: 0.03723100061219298
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9954090353319818, 0.9948107557598924, 0.9989418157262652, 0.9978769770332421, 0.9949734470616213, 0.9981480251774413, 0.9997117669430844, 0.9944055695215777, 0.9947747526137031, 0.9930294984556545]
归一化后的数据质量列表avg_f_list: [0.9356097166823434, 0.9266564761291571, 0.9884776970834444, 0.9725424096129367, 0.9290911478582997, 0.9765986390911308, 1.0, 0.9205928730417183, 0.9261176898433761, 0.9]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3261
DataOwner1的最优x_1 = 0.1081
DataOwner2的最优x_2 = 0.0984
DataOwner3的最优x_3 = 0.1580
DataOwner4的最优x_4 = 0.1441
DataOwner5的最优x_5 = 0.1011
DataOwner6的最优x_6 = 0.1478
DataOwner7的最优x_7 = 0.1674
DataOwner8的最优x_8 = 0.0916
DataOwner9的最优x_9 = 0.0978
DataOwner10的最优x_10 = 0.0672
每个DataOwner应该贡献数据比例 xn_list = [0.108092441351829, 0.09841324371888643, 0.15795999594774054, 0.14414730796473815, 0.10108419903467586, 0.14775657905460496, 0.16735660310357386, 0.09163180585456508, 0.09781818310236882, 0.0671544967685262]
ModelOwner的最大效用 U(Eta) = 0.5638
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.04641876873408926
DataOwner2: noise random: 0.031189959877776608
DataOwner3: noise random: 0.06744390025263983
DataOwner4: noise random: 0.05704965008475465
DataOwner5: noise random: 0.07250434217488722
DataOwner6: noise random: 0.02738657490504236
DataOwner7: noise random: 0.04430749437898164
DataOwner8: noise random: 0.059327811061300355
DataOwner9: noise random: 0.07737319821076132
DataOwner10: noise random: 0.09820193381262253
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9938067985430972, 0.9940543087117861, 0.9959436018526284, 0.9956356100329178, 0.9912034778274562, 0.9976114029719944, 0.9984548667667175, 0.9917250717559839, 0.9904061134678787, 0.9860886689954061]
归一化后的数据质量列表avg_f_list: [0.9624131175194086, 0.9644146233441268, 0.9796925056470066, 0.9772019113236232, 0.9413612084056755, 0.9931792794331668, 1.0, 0.9455791089938235, 0.9349132736861825, 0.9]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3414
DataOwner1的最优x_1 = 0.1241
DataOwner2的最优x_2 = 0.1261
DataOwner3的最优x_3 = 0.1406
DataOwner4的最优x_4 = 0.1383
DataOwner5的最优x_5 = 0.1022
DataOwner6的最优x_6 = 0.1527
DataOwner7的最优x_7 = 0.1585
DataOwner8的最优x_8 = 0.1068
DataOwner9的最优x_9 = 0.0951
DataOwner10的最优x_10 = 0.0527
每个DataOwner应该贡献数据比例 xn_list = [0.12412987614413608, 0.12610661780860077, 0.140633217543114, 0.1383313584338446, 0.10223964491848073, 0.1526694352056095, 0.15849069544107136, 0.10679118031795636, 0.09511397227321873, 0.05273362386356789]
ModelOwner的最大效用 U(Eta) = 0.5813
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.023389041081695008
DataOwner2: noise random: 0.013189437195281352
**** log-parameter_analysis 运行时间： 2025-02-07 10:10:13 ****
========================= 客户端数量: 10 =========================
---------------------------------- 准备工作 ----------------------------------
**** log-parameter_analysis 运行时间： 2025-02-07 10:10:28 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.025452817771751502
DataOwner2: noise random: 0.07041373800030042
DataOwner3: noise random: 0.0677523350844504
DataOwner4: noise random: 0.05116932181271227
DataOwner5: noise random: 0.06818277906940078
DataOwner6: noise random: 0.04334630003993622
DataOwner7: noise random: 0.0882204494972606
DataOwner8: noise random: 0.04005425134337914
DataOwner9: noise random: 0.09623346339564849
DataOwner10: noise random: 0.04761676143181385
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9997377959355928, 0.9979892762095383, 0.9981421551638965, 0.9989427916935721, 0.9981230717587873, 0.9992380785887391, 0.9968489653313748, 0.9993500787481553, 0.9962541157514575, 0.9990823060145508]
归一化后的数据质量列表avg_f_list: [1.0, 0.9498082592650929, 0.9541966917926953, 0.9771791840812174, 0.9536488973884888, 0.985655475806036, 0.9170753211682934, 0.9888704712561407, 0.9, 0.9811839811235534]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3423
DataOwner1的最优x_1 = 0.1579
DataOwner2的最优x_2 = 0.1106
DataOwner3的最优x_3 = 0.1151
DataOwner4的最优x_4 = 0.1377
DataOwner5的最优x_5 = 0.1146
DataOwner6的最优x_6 = 0.1455
DataOwner7的最优x_7 = 0.0735
DataOwner8的最优x_8 = 0.1483
DataOwner9的最优x_9 = 0.0518
DataOwner10的最优x_10 = 0.1414
每个DataOwner应该贡献数据比例 xn_list = [0.15794033134167132, 0.11056607361491277, 0.11513977432320645, 0.137693628794857, 0.1145737091162736, 0.14545153513120468, 0.07347379702710721, 0.1483192946622813, 0.05184630432685866, 0.14139505540902342]
ModelOwner的最大效用 U(Eta) = 0.5823
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.08994898012976835
DataOwner2: noise random: 0.0036091131845601646
DataOwner3: noise random: 0.0014436727520418136
DataOwner4: noise random: 0.07904927216431772
DataOwner5: noise random: 0.08758551620783556
DataOwner6: noise random: 0.06637109621226132
DataOwner7: noise random: 0.06982717964680811
DataOwner8: noise random: 0.03810669022677435
DataOwner9: noise random: 0.0013503613986506236
DataOwner10: noise random: 0.0008276498592266424
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9952152486796804, 0.9979802270003506, 0.998140736476493, 0.9951901251170703, 0.9934138193129124, 0.9965741982976845, 0.9937301893654865, 0.9984313178466061, 0.9962529576144107, 0.9990818315838593]
归一化后的数据质量列表avg_f_list: [0.9317823829705121, 0.9805645342520637, 0.9833963819699166, 0.9313391312376451, 0.9, 0.9557581535412603, 0.9055816755054638, 0.988523071119877, 0.9500905461346852, 1.0]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3323
DataOwner1的最优x_1 = 0.0991
DataOwner2的最优x_2 = 0.1474
DataOwner3的最优x_3 = 0.1499
DataOwner4的最优x_4 = 0.0986
DataOwner5的最优x_5 = 0.0615
DataOwner6的最优x_6 = 0.1241
DataOwner7的最优x_7 = 0.0685
DataOwner8的最优x_8 = 0.1543
DataOwner9的最优x_9 = 0.1185
DataOwner10的最优x_10 = 0.1639
每个DataOwner应该贡献数据比例 xn_list = [0.0990964097159188, 0.14736699098057046, 0.14986044339590737, 0.09860772396499497, 0.061451656114549884, 0.12413276734386401, 0.06846194241772632, 0.1542958625860159, 0.11845461144170295, 0.16386887778095177]
ModelOwner的最大效用 U(Eta) = 0.5708
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.0645817757808992
DataOwner2: noise random: 0.06870631476614021
DataOwner3: noise random: 0.0585885743885491
DataOwner4: noise random: 0.013510554242138696
DataOwner5: noise random: 0.04145965783918222
DataOwner6: noise random: 0.0856639578786223
DataOwner7: noise random: 0.031232270591016367
DataOwner8: noise random: 0.04883897256840114
DataOwner9: noise random: 0.033162609815867605
DataOwner10: noise random: 0.07120406655324622
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9922996970434027, 0.9949640510545394, 0.9959394367732886, 0.9950497373278732, 0.9921326915681655, 0.9916256150086068, 0.9929834612853923, 0.9967693451342219, 0.995504892329478, 0.9960020486742265]
归一化后的数据质量列表avg_f_list: [0.9131049261593075, 0.9649030171569011, 0.9838656317367722, 0.9665688563677689, 0.9098581486037449, 0.9, 0.9263980855065391, 1.0, 0.9754175904671389, 0.9850828787425225]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3319
DataOwner1的最优x_1 = 0.0779
DataOwner2的最优x_2 = 0.1332
DataOwner3的最优x_3 = 0.1505
DataOwner4的最优x_4 = 0.1348
DataOwner5的最优x_5 = 0.0740
DataOwner6的最优x_6 = 0.0618
DataOwner7的最优x_7 = 0.0934
DataOwner8的最优x_8 = 0.1641
DataOwner9的最优x_9 = 0.1430
DataOwner10的最优x_10 = 0.1515
每个DataOwner应该贡献数据比例 xn_list = [0.07793170989366713, 0.13322997241550605, 0.15048205004275603, 0.13480437999913694, 0.07401663085769077, 0.06176881880495743, 0.09337383765686161, 0.16406349974027074, 0.14297492974604545, 0.15154077690563478]
ModelOwner的最大效用 U(Eta) = 0.5704
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.004207587735846408
DataOwner2: noise random: 0.09342573271498526
DataOwner3: noise random: 0.03340399857196193
DataOwner4: noise random: 0.062250482329410155
DataOwner5: noise random: 0.05592321221557772
DataOwner6: noise random: 0.02527747161085936
DataOwner7: noise random: 0.07541510470244336
DataOwner8: noise random: 0.033299311363697695
DataOwner9: noise random: 0.03845105626358358
DataOwner10: noise random: 0.00992553758015804
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9922844166877723, 0.9889318775939567, 0.9950977518374718, 0.9921987444106056, 0.9897069056201069, 0.9911222790070593, 0.98873107942978, 0.9959180892250648, 0.9943957696314896, 0.9959255616702124]
归一化后的数据质量列表avg_f_list: [0.9493897564722987, 0.9027910022912874, 0.9884938233902597, 0.9481989511536721, 0.9135635360226871, 0.9332365762728685, 0.9, 0.9998961364432085, 0.9787365930222813, 1.0]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3304
DataOwner1的最优x_1 = 0.1191
DataOwner2的最优x_2 = 0.0667
DataOwner3的最优x_3 = 0.1554
DataOwner4的最优x_4 = 0.1179
DataOwner5的最优x_5 = 0.0798
DataOwner6的最优x_6 = 0.1022
DataOwner7的最优x_7 = 0.0632
DataOwner8的最优x_8 = 0.1649
DataOwner9的最优x_9 = 0.1469
DataOwner10的最优x_10 = 0.1649
每个DataOwner应该贡献数据比例 xn_list = [0.11912334964897765, 0.06670298514839719, 0.15540701639735827, 0.11791616914835809, 0.0798212841090822, 0.10218569111140748, 0.0631978983239558, 0.1648555924800139, 0.14693477260361842, 0.16493949324240498]
ModelOwner的最大效用 U(Eta) = 0.5687
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.09846381053452362
DataOwner2: noise random: 0.013223612195737278
DataOwner3: noise random: 0.04080589691067589
DataOwner4: noise random: 0.025818899399827944
DataOwner5: noise random: 0.04182136870681115
DataOwner6: noise random: 0.04192885723768545
DataOwner7: noise random: 0.043290795550120845
DataOwner8: noise random: 0.07356426911393556
DataOwner9: noise random: 0.09520768833916278
DataOwner10: noise random: 0.08896886900028966
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
**** log-parameter_analysis 运行时间： 2025-02-07 10:14:09 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.06486535554630535
DataOwner2: noise random: 0.06153589706613391
DataOwner3: noise random: 0.021628753049011365
DataOwner4: noise random: 0.07282019833050148
DataOwner5: noise random: 0.04109605407674093
DataOwner6: noise random: 0.045899800259702556
DataOwner7: noise random: 0.013902740564934303
DataOwner8: noise random: 0.05233374812096934
DataOwner9: noise random: 0.08344574225633077
DataOwner10: noise random: 0.08895982849472261
DONE
----- 计算 DataOwner 的数据质量 -----
**** log-parameter_analysis 运行时间： 2025-02-07 10:14:34 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.06293376706085718
DataOwner2: noise random: 0.03530120734659064
DataOwner3: noise random: 0.025111129633268992
DataOwner4: noise random: 0.06565698751436555
DataOwner5: noise random: 0.0938335282636651
DataOwner6: noise random: 0.08996378069280891
DataOwner7: noise random: 0.09423071981659285
DataOwner8: noise random: 0.06298840154987688
DataOwner9: noise random: 0.006103394334217605
DataOwner10: noise random: 0.008348870498657813
DONE
----- 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9983954490113295, 0.9994952192981158, 0.9997452057195679, 0.9982556963304691, 0.9964378183630068, 0.9967231553971216, 0.9964030202945539, 0.9983968503036004, 0.9999849132012437, 0.9999718108560034]
归一化后的数据质量列表avg_f_list: [0.9556250219836107, 0.9863286280219791, 0.9933077987555662, 0.9517233787882097, 0.9009714994099343, 0.9089375955928184, 0.9, 0.9556641435404911, 1.0, 0.9996342061144304]
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3349
DataOwner1的最优x_1 = 0.1221
DataOwner2的最优x_2 = 0.1508
DataOwner3的最优x_3 = 0.1568
DataOwner4的最优x_4 = 0.1182
DataOwner5的最优x_5 = 0.0602
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0590
DataOwner8的最优x_8 = 0.1221
DataOwner9的最优x_9 = 0.1623
DataOwner10的最优x_10 = 0.1620
每个DataOwner应该贡献数据比例 xn_list = [0.12209710994428709, 0.15077520525156585, 0.1567685544214527, 0.11816482469181519, 0.0602167258953068, 0.07021365282464347, 0.05897257800011191, 0.12213619363213847, 0.16234506383006925, 0.16204447523215595]
ModelOwner的最大效用 U(Eta) = 0.5738
DONE
最终的列表 alpha_U_Eta_list：
[0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517, 0.5738314884709517]
最终的列表 alpha_U_qn_list：
[0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778, 0.015118349514911778]
**** log-parameter_analysis 运行时间： 2025-02-07 10:15:32 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.03558802668769846
DataOwner2: noise random: 0.08966210160078758
DataOwner3: noise random: 0.03865966135396193
DataOwner4: noise random: 0.05168574954822025
DataOwner5: noise random: 0.013273803696555175
DataOwner6: noise random: 0.03446116794881079
DataOwner7: noise random: 0.037382576297295154
DataOwner8: noise random: 0.0027819134385148273
DataOwner9: noise random: 0.05086307760792579
DataOwner10: noise random: 0.014561798535016879
DONE
----- 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9994880488799637, 0.9967495659937026, 0.9993941793796209, 0.9989193556586988, 0.9999286140255857, 0.9995182034218544, 0.9994336151143803, 0.9999968648115549, 0.9989536779681529, 0.9999139275967934]
归一化后的数据质量列表avg_f_list: [0.984331102244303, 0.9, 0.9814404073742565, 0.9668182938098466, 0.9978982289651339, 0.9852597060957541, 0.9826548239392673, 1.0, 0.9678752433355697, 0.9974459629552558]
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.1648
DataOwner1的最优x_1 = 0.0159
DataOwner2的最优x_2 = 0.0039
DataOwner3的最优x_3 = 0.0155
DataOwner4的最优x_4 = 0.0138
DataOwner5的最优x_5 = 0.0174
DataOwner6的最优x_6 = 0.0160
DataOwner7的最优x_7 = 0.0157
DataOwner8的最优x_8 = 0.0177
DataOwner9的最优x_9 = 0.0139
DataOwner10的最优x_10 = 0.0174
每个DataOwner应该贡献数据比例 xn_list = [0.01588240067731319, 0.003932708090391628, 0.01554183073335573, 0.013751879509049969, 0.01742497406524528, 0.015990900435101048, 0.015685429691392833, 0.017655940081268123, 0.01388512758413988, 0.017374998959442976]
ModelOwner的最大效用 U(Eta) = -0.1648
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.1648
DataOwner1的最优x_1 = 0.0159
DataOwner2的最优x_2 = 0.0039
DataOwner3的最优x_3 = 0.0155
DataOwner4的最优x_4 = 0.0138
DataOwner5的最优x_5 = 0.0174
DataOwner6的最优x_6 = 0.0160
DataOwner7的最优x_7 = 0.0157
DataOwner8的最优x_8 = 0.0177
DataOwner9的最优x_9 = 0.0139
DataOwner10的最优x_10 = 0.0174
每个DataOwner应该贡献数据比例 xn_list = [0.01588240067731319, 0.003932708090391628, 0.01554183073335573, 0.013751879509049969, 0.01742497406524528, 0.015990900435101048, 0.015685429691392833, 0.017655940081268123, 0.01388512758413988, 0.017374998959442976]
ModelOwner的最大效用 U(Eta) = -0.1581
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.1648
DataOwner1的最优x_1 = 0.0159
DataOwner2的最优x_2 = 0.0039
DataOwner3的最优x_3 = 0.0155
DataOwner4的最优x_4 = 0.0138
DataOwner5的最优x_5 = 0.0174
DataOwner6的最优x_6 = 0.0160
DataOwner7的最优x_7 = 0.0157
DataOwner8的最优x_8 = 0.0177
DataOwner9的最优x_9 = 0.0139
DataOwner10的最优x_10 = 0.0174
每个DataOwner应该贡献数据比例 xn_list = [0.01588240067731319, 0.003932708090391628, 0.01554183073335573, 0.013751879509049969, 0.01742497406524528, 0.015990900435101048, 0.015685429691392833, 0.017655940081268123, 0.01388512758413988, 0.017374998959442976]
ModelOwner的最大效用 U(Eta) = -0.1513
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.1648
DataOwner1的最优x_1 = 0.0159
DataOwner2的最优x_2 = 0.0039
DataOwner3的最优x_3 = 0.0155
DataOwner4的最优x_4 = 0.0138
DataOwner5的最优x_5 = 0.0174
DataOwner6的最优x_6 = 0.0160
DataOwner7的最优x_7 = 0.0157
DataOwner8的最优x_8 = 0.0177
DataOwner9的最优x_9 = 0.0139
DataOwner10的最优x_10 = 0.0174
每个DataOwner应该贡献数据比例 xn_list = [0.01588240067731319, 0.003932708090391628, 0.01554183073335573, 0.013751879509049969, 0.01742497406524528, 0.015990900435101048, 0.015685429691392833, 0.017655940081268123, 0.01388512758413988, 0.017374998959442976]
ModelOwner的最大效用 U(Eta) = -0.1445
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.1648
DataOwner1的最优x_1 = 0.0159
DataOwner2的最优x_2 = 0.0039
DataOwner3的最优x_3 = 0.0155
DataOwner4的最优x_4 = 0.0138
DataOwner5的最优x_5 = 0.0174
DataOwner6的最优x_6 = 0.0160
DataOwner7的最优x_7 = 0.0157
DataOwner8的最优x_8 = 0.0177
DataOwner9的最优x_9 = 0.0139
DataOwner10的最优x_10 = 0.0174
每个DataOwner应该贡献数据比例 xn_list = [0.01588240067731319, 0.003932708090391628, 0.01554183073335573, 0.013751879509049969, 0.01742497406524528, 0.015990900435101048, 0.015685429691392833, 0.017655940081268123, 0.01388512758413988, 0.017374998959442976]
ModelOwner的最大效用 U(Eta) = -0.1378
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.1648
DataOwner1的最优x_1 = 0.0159
DataOwner2的最优x_2 = 0.0039
DataOwner3的最优x_3 = 0.0155
DataOwner4的最优x_4 = 0.0138
DataOwner5的最优x_5 = 0.0174
DataOwner6的最优x_6 = 0.0160
DataOwner7的最优x_7 = 0.0157
DataOwner8的最优x_8 = 0.0177
DataOwner9的最优x_9 = 0.0139
DataOwner10的最优x_10 = 0.0174
每个DataOwner应该贡献数据比例 xn_list = [0.01588240067731319, 0.003932708090391628, 0.01554183073335573, 0.013751879509049969, 0.01742497406524528, 0.015990900435101048, 0.015685429691392833, 0.017655940081268123, 0.01388512758413988, 0.017374998959442976]
ModelOwner的最大效用 U(Eta) = -0.1310
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.1648
DataOwner1的最优x_1 = 0.0159
DataOwner2的最优x_2 = 0.0039
DataOwner3的最优x_3 = 0.0155
DataOwner4的最优x_4 = 0.0138
DataOwner5的最优x_5 = 0.0174
DataOwner6的最优x_6 = 0.0160
DataOwner7的最优x_7 = 0.0157
DataOwner8的最优x_8 = 0.0177
DataOwner9的最优x_9 = 0.0139
DataOwner10的最优x_10 = 0.0174
每个DataOwner应该贡献数据比例 xn_list = [0.01588240067731319, 0.003932708090391628, 0.01554183073335573, 0.013751879509049969, 0.01742497406524528, 0.015990900435101048, 0.015685429691392833, 0.017655940081268123, 0.01388512758413988, 0.017374998959442976]
ModelOwner的最大效用 U(Eta) = -0.1243
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.1648
DataOwner1的最优x_1 = 0.0159
DataOwner2的最优x_2 = 0.0039
DataOwner3的最优x_3 = 0.0155
DataOwner4的最优x_4 = 0.0138
DataOwner5的最优x_5 = 0.0174
DataOwner6的最优x_6 = 0.0160
DataOwner7的最优x_7 = 0.0157
DataOwner8的最优x_8 = 0.0177
DataOwner9的最优x_9 = 0.0139
DataOwner10的最优x_10 = 0.0174
每个DataOwner应该贡献数据比例 xn_list = [0.01588240067731319, 0.003932708090391628, 0.01554183073335573, 0.013751879509049969, 0.01742497406524528, 0.015990900435101048, 0.015685429691392833, 0.017655940081268123, 0.01388512758413988, 0.017374998959442976]
ModelOwner的最大效用 U(Eta) = -0.1175
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.1648
DataOwner1的最优x_1 = 0.0159
DataOwner2的最优x_2 = 0.0039
DataOwner3的最优x_3 = 0.0155
DataOwner4的最优x_4 = 0.0138
DataOwner5的最优x_5 = 0.0174
DataOwner6的最优x_6 = 0.0160
DataOwner7的最优x_7 = 0.0157
DataOwner8的最优x_8 = 0.0177
DataOwner9的最优x_9 = 0.0139
DataOwner10的最优x_10 = 0.0174
每个DataOwner应该贡献数据比例 xn_list = [0.01588240067731319, 0.003932708090391628, 0.01554183073335573, 0.013751879509049969, 0.01742497406524528, 0.015990900435101048, 0.015685429691392833, 0.017655940081268123, 0.01388512758413988, 0.017374998959442976]
ModelOwner的最大效用 U(Eta) = -0.1108
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.1648
DataOwner1的最优x_1 = 0.0159
DataOwner2的最优x_2 = 0.0039
DataOwner3的最优x_3 = 0.0155
DataOwner4的最优x_4 = 0.0138
DataOwner5的最优x_5 = 0.0174
DataOwner6的最优x_6 = 0.0160
DataOwner7的最优x_7 = 0.0157
DataOwner8的最优x_8 = 0.0177
DataOwner9的最优x_9 = 0.0139
DataOwner10的最优x_10 = 0.0174
每个DataOwner应该贡献数据比例 xn_list = [0.01588240067731319, 0.003932708090391628, 0.01554183073335573, 0.013751879509049969, 0.01742497406524528, 0.015990900435101048, 0.015685429691392833, 0.017655940081268123, 0.01388512758413988, 0.017374998959442976]
ModelOwner的最大效用 U(Eta) = -0.1040
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.1648
DataOwner1的最优x_1 = 0.0159
DataOwner2的最优x_2 = 0.0039
DataOwner3的最优x_3 = 0.0155
DataOwner4的最优x_4 = 0.0138
DataOwner5的最优x_5 = 0.0174
DataOwner6的最优x_6 = 0.0160
DataOwner7的最优x_7 = 0.0157
DataOwner8的最优x_8 = 0.0177
DataOwner9的最优x_9 = 0.0139
DataOwner10的最优x_10 = 0.0174
每个DataOwner应该贡献数据比例 xn_list = [0.01588240067731319, 0.003932708090391628, 0.01554183073335573, 0.013751879509049969, 0.01742497406524528, 0.015990900435101048, 0.015685429691392833, 0.017655940081268123, 0.01388512758413988, 0.017374998959442976]
ModelOwner的最大效用 U(Eta) = -0.0972
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.1648
DataOwner1的最优x_1 = 0.0159
DataOwner2的最优x_2 = 0.0039
DataOwner3的最优x_3 = 0.0155
DataOwner4的最优x_4 = 0.0138
DataOwner5的最优x_5 = 0.0174
DataOwner6的最优x_6 = 0.0160
DataOwner7的最优x_7 = 0.0157
DataOwner8的最优x_8 = 0.0177
DataOwner9的最优x_9 = 0.0139
DataOwner10的最优x_10 = 0.0174
每个DataOwner应该贡献数据比例 xn_list = [0.01588240067731319, 0.003932708090391628, 0.01554183073335573, 0.013751879509049969, 0.01742497406524528, 0.015990900435101048, 0.015685429691392833, 0.017655940081268123, 0.01388512758413988, 0.017374998959442976]
ModelOwner的最大效用 U(Eta) = -0.0905
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.1648
DataOwner1的最优x_1 = 0.0159
DataOwner2的最优x_2 = 0.0039
DataOwner3的最优x_3 = 0.0155
DataOwner4的最优x_4 = 0.0138
DataOwner5的最优x_5 = 0.0174
DataOwner6的最优x_6 = 0.0160
DataOwner7的最优x_7 = 0.0157
DataOwner8的最优x_8 = 0.0177
DataOwner9的最优x_9 = 0.0139
DataOwner10的最优x_10 = 0.0174
每个DataOwner应该贡献数据比例 xn_list = [0.01588240067731319, 0.003932708090391628, 0.01554183073335573, 0.013751879509049969, 0.01742497406524528, 0.015990900435101048, 0.015685429691392833, 0.017655940081268123, 0.01388512758413988, 0.017374998959442976]
ModelOwner的最大效用 U(Eta) = -0.0837
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.1648
DataOwner1的最优x_1 = 0.0159
DataOwner2的最优x_2 = 0.0039
DataOwner3的最优x_3 = 0.0155
DataOwner4的最优x_4 = 0.0138
DataOwner5的最优x_5 = 0.0174
DataOwner6的最优x_6 = 0.0160
DataOwner7的最优x_7 = 0.0157
DataOwner8的最优x_8 = 0.0177
DataOwner9的最优x_9 = 0.0139
DataOwner10的最优x_10 = 0.0174
每个DataOwner应该贡献数据比例 xn_list = [0.01588240067731319, 0.003932708090391628, 0.01554183073335573, 0.013751879509049969, 0.01742497406524528, 0.015990900435101048, 0.015685429691392833, 0.017655940081268123, 0.01388512758413988, 0.017374998959442976]
ModelOwner的最大效用 U(Eta) = -0.0770
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.1648
DataOwner1的最优x_1 = 0.0159
DataOwner2的最优x_2 = 0.0039
DataOwner3的最优x_3 = 0.0155
DataOwner4的最优x_4 = 0.0138
DataOwner5的最优x_5 = 0.0174
DataOwner6的最优x_6 = 0.0160
DataOwner7的最优x_7 = 0.0157
DataOwner8的最优x_8 = 0.0177
DataOwner9的最优x_9 = 0.0139
DataOwner10的最优x_10 = 0.0174
每个DataOwner应该贡献数据比例 xn_list = [0.01588240067731319, 0.003932708090391628, 0.01554183073335573, 0.013751879509049969, 0.01742497406524528, 0.015990900435101048, 0.015685429691392833, 0.017655940081268123, 0.01388512758413988, 0.017374998959442976]
ModelOwner的最大效用 U(Eta) = -0.0702
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.1648
DataOwner1的最优x_1 = 0.0159
DataOwner2的最优x_2 = 0.0039
DataOwner3的最优x_3 = 0.0155
DataOwner4的最优x_4 = 0.0138
DataOwner5的最优x_5 = 0.0174
DataOwner6的最优x_6 = 0.0160
DataOwner7的最优x_7 = 0.0157
DataOwner8的最优x_8 = 0.0177
DataOwner9的最优x_9 = 0.0139
DataOwner10的最优x_10 = 0.0174
每个DataOwner应该贡献数据比例 xn_list = [0.01588240067731319, 0.003932708090391628, 0.01554183073335573, 0.013751879509049969, 0.01742497406524528, 0.015990900435101048, 0.015685429691392833, 0.017655940081268123, 0.01388512758413988, 0.017374998959442976]
ModelOwner的最大效用 U(Eta) = -0.0635
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.1648
DataOwner1的最优x_1 = 0.0159
DataOwner2的最优x_2 = 0.0039
DataOwner3的最优x_3 = 0.0155
DataOwner4的最优x_4 = 0.0138
DataOwner5的最优x_5 = 0.0174
DataOwner6的最优x_6 = 0.0160
DataOwner7的最优x_7 = 0.0157
DataOwner8的最优x_8 = 0.0177
DataOwner9的最优x_9 = 0.0139
DataOwner10的最优x_10 = 0.0174
每个DataOwner应该贡献数据比例 xn_list = [0.01588240067731319, 0.003932708090391628, 0.01554183073335573, 0.013751879509049969, 0.01742497406524528, 0.015990900435101048, 0.015685429691392833, 0.017655940081268123, 0.01388512758413988, 0.017374998959442976]
ModelOwner的最大效用 U(Eta) = -0.0567
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.1648
DataOwner1的最优x_1 = 0.0159
DataOwner2的最优x_2 = 0.0039
DataOwner3的最优x_3 = 0.0155
DataOwner4的最优x_4 = 0.0138
DataOwner5的最优x_5 = 0.0174
DataOwner6的最优x_6 = 0.0160
DataOwner7的最优x_7 = 0.0157
DataOwner8的最优x_8 = 0.0177
DataOwner9的最优x_9 = 0.0139
DataOwner10的最优x_10 = 0.0174
每个DataOwner应该贡献数据比例 xn_list = [0.01588240067731319, 0.003932708090391628, 0.01554183073335573, 0.013751879509049969, 0.01742497406524528, 0.015990900435101048, 0.015685429691392833, 0.017655940081268123, 0.01388512758413988, 0.017374998959442976]
ModelOwner的最大效用 U(Eta) = -0.0499
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.1648
DataOwner1的最优x_1 = 0.0159
DataOwner2的最优x_2 = 0.0039
DataOwner3的最优x_3 = 0.0155
DataOwner4的最优x_4 = 0.0138
DataOwner5的最优x_5 = 0.0174
DataOwner6的最优x_6 = 0.0160
DataOwner7的最优x_7 = 0.0157
DataOwner8的最优x_8 = 0.0177
DataOwner9的最优x_9 = 0.0139
DataOwner10的最优x_10 = 0.0174
每个DataOwner应该贡献数据比例 xn_list = [0.01588240067731319, 0.003932708090391628, 0.01554183073335573, 0.013751879509049969, 0.01742497406524528, 0.015990900435101048, 0.015685429691392833, 0.017655940081268123, 0.01388512758413988, 0.017374998959442976]
ModelOwner的最大效用 U(Eta) = -0.0432
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.1648
DataOwner1的最优x_1 = 0.0159
DataOwner2的最优x_2 = 0.0039
DataOwner3的最优x_3 = 0.0155
DataOwner4的最优x_4 = 0.0138
DataOwner5的最优x_5 = 0.0174
DataOwner6的最优x_6 = 0.0160
DataOwner7的最优x_7 = 0.0157
DataOwner8的最优x_8 = 0.0177
DataOwner9的最优x_9 = 0.0139
DataOwner10的最优x_10 = 0.0174
每个DataOwner应该贡献数据比例 xn_list = [0.01588240067731319, 0.003932708090391628, 0.01554183073335573, 0.013751879509049969, 0.01742497406524528, 0.015990900435101048, 0.015685429691392833, 0.017655940081268123, 0.01388512758413988, 0.017374998959442976]
ModelOwner的最大效用 U(Eta) = -0.0364
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.1648
DataOwner1的最优x_1 = 0.0159
DataOwner2的最优x_2 = 0.0039
DataOwner3的最优x_3 = 0.0155
DataOwner4的最优x_4 = 0.0138
DataOwner5的最优x_5 = 0.0174
DataOwner6的最优x_6 = 0.0160
DataOwner7的最优x_7 = 0.0157
DataOwner8的最优x_8 = 0.0177
DataOwner9的最优x_9 = 0.0139
DataOwner10的最优x_10 = 0.0174
每个DataOwner应该贡献数据比例 xn_list = [0.01588240067731319, 0.003932708090391628, 0.01554183073335573, 0.013751879509049969, 0.01742497406524528, 0.015990900435101048, 0.015685429691392833, 0.017655940081268123, 0.01388512758413988, 0.017374998959442976]
ModelOwner的最大效用 U(Eta) = -0.0297
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.1648
DataOwner1的最优x_1 = 0.0159
DataOwner2的最优x_2 = 0.0039
DataOwner3的最优x_3 = 0.0155
DataOwner4的最优x_4 = 0.0138
DataOwner5的最优x_5 = 0.0174
DataOwner6的最优x_6 = 0.0160
DataOwner7的最优x_7 = 0.0157
DataOwner8的最优x_8 = 0.0177
DataOwner9的最优x_9 = 0.0139
DataOwner10的最优x_10 = 0.0174
每个DataOwner应该贡献数据比例 xn_list = [0.01588240067731319, 0.003932708090391628, 0.01554183073335573, 0.013751879509049969, 0.01742497406524528, 0.015990900435101048, 0.015685429691392833, 0.017655940081268123, 0.01388512758413988, 0.017374998959442976]
ModelOwner的最大效用 U(Eta) = -0.0229
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.1648
DataOwner1的最优x_1 = 0.0159
DataOwner2的最优x_2 = 0.0039
DataOwner3的最优x_3 = 0.0155
DataOwner4的最优x_4 = 0.0138
DataOwner5的最优x_5 = 0.0174
DataOwner6的最优x_6 = 0.0160
DataOwner7的最优x_7 = 0.0157
DataOwner8的最优x_8 = 0.0177
DataOwner9的最优x_9 = 0.0139
DataOwner10的最优x_10 = 0.0174
每个DataOwner应该贡献数据比例 xn_list = [0.01588240067731319, 0.003932708090391628, 0.01554183073335573, 0.013751879509049969, 0.01742497406524528, 0.015990900435101048, 0.015685429691392833, 0.017655940081268123, 0.01388512758413988, 0.017374998959442976]
ModelOwner的最大效用 U(Eta) = -0.0162
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.1648
DataOwner1的最优x_1 = 0.0159
DataOwner2的最优x_2 = 0.0039
DataOwner3的最优x_3 = 0.0155
DataOwner4的最优x_4 = 0.0138
DataOwner5的最优x_5 = 0.0174
DataOwner6的最优x_6 = 0.0160
DataOwner7的最优x_7 = 0.0157
DataOwner8的最优x_8 = 0.0177
DataOwner9的最优x_9 = 0.0139
DataOwner10的最优x_10 = 0.0174
每个DataOwner应该贡献数据比例 xn_list = [0.01588240067731319, 0.003932708090391628, 0.01554183073335573, 0.013751879509049969, 0.01742497406524528, 0.015990900435101048, 0.015685429691392833, 0.017655940081268123, 0.01388512758413988, 0.017374998959442976]
ModelOwner的最大效用 U(Eta) = -0.0094
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.1648
DataOwner1的最优x_1 = 0.0159
DataOwner2的最优x_2 = 0.0039
DataOwner3的最优x_3 = 0.0155
DataOwner4的最优x_4 = 0.0138
DataOwner5的最优x_5 = 0.0174
DataOwner6的最优x_6 = 0.0160
DataOwner7的最优x_7 = 0.0157
DataOwner8的最优x_8 = 0.0177
DataOwner9的最优x_9 = 0.0139
DataOwner10的最优x_10 = 0.0174
每个DataOwner应该贡献数据比例 xn_list = [0.01588240067731319, 0.003932708090391628, 0.01554183073335573, 0.013751879509049969, 0.01742497406524528, 0.015990900435101048, 0.015685429691392833, 0.017655940081268123, 0.01388512758413988, 0.017374998959442976]
ModelOwner的最大效用 U(Eta) = -0.0026
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.1648
DataOwner1的最优x_1 = 0.0159
DataOwner2的最优x_2 = 0.0039
DataOwner3的最优x_3 = 0.0155
DataOwner4的最优x_4 = 0.0138
DataOwner5的最优x_5 = 0.0174
DataOwner6的最优x_6 = 0.0160
DataOwner7的最优x_7 = 0.0157
DataOwner8的最优x_8 = 0.0177
DataOwner9的最优x_9 = 0.0139
DataOwner10的最优x_10 = 0.0174
每个DataOwner应该贡献数据比例 xn_list = [0.015882478436432215, 0.0039327273447580064, 0.015541906825105553, 0.013751946837407991, 0.01742505937671328, 0.015990978725493477, 0.015685506486003385, 0.017656026523590985, 0.013885195564750579, 0.017375084026255448]
ModelOwner的最大效用 U(Eta) = 0.0041
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.1648
DataOwner1的最优x_1 = 0.0159
DataOwner2的最优x_2 = 0.0039
DataOwner3的最优x_3 = 0.0155
DataOwner4的最优x_4 = 0.0138
DataOwner5的最优x_5 = 0.0174
DataOwner6的最优x_6 = 0.0160
DataOwner7的最优x_7 = 0.0157
DataOwner8的最优x_8 = 0.0177
DataOwner9的最优x_9 = 0.0139
DataOwner10的最优x_10 = 0.0174
每个DataOwner应该贡献数据比例 xn_list = [0.01588171220623813, 0.003932537615306921, 0.015541157025330786, 0.013751283391977022, 0.017424218726689568, 0.015990207260805148, 0.015684749758630667, 0.01765517473076257, 0.013884525691007623, 0.017374245787187822]
ModelOwner的最大效用 U(Eta) = 0.0109
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.2110
DataOwner1的最优x_1 = 0.0203
DataOwner2的最优x_2 = 0.0050
DataOwner3的最优x_3 = 0.0199
DataOwner4的最优x_4 = 0.0176
DataOwner5的最优x_5 = 0.0223
DataOwner6的最优x_6 = 0.0205
DataOwner7的最优x_7 = 0.0201
DataOwner8的最优x_8 = 0.0226
DataOwner9的最优x_9 = 0.0178
DataOwner10的最优x_10 = 0.0222
每个DataOwner应该贡献数据比例 xn_list = [0.020336089110727585, 0.005035504630753098, 0.01990001770905197, 0.017608134488119744, 0.02231122564500044, 0.020475013999120037, 0.020083884195309275, 0.022606958359849365, 0.017778747532545058, 0.022247236691249873]
ModelOwner的最大效用 U(Eta) = 0.0184
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.2612
DataOwner1的最优x_1 = 0.0252
DataOwner2的最优x_2 = 0.0062
DataOwner3的最优x_3 = 0.0246
DataOwner4的最优x_4 = 0.0218
DataOwner5的最优x_5 = 0.0276
DataOwner6的最优x_6 = 0.0253
DataOwner7的最优x_7 = 0.0249
DataOwner8的最优x_8 = 0.0280
DataOwner9的最优x_9 = 0.0220
DataOwner10的最优x_10 = 0.0275
每个DataOwner应该贡献数据比例 xn_list = [0.025168856236204677, 0.006232166444087205, 0.024629154705599676, 0.02179261721014015, 0.027613373826718274, 0.025340795910758055, 0.02485671611731258, 0.02797938590369055, 0.02200377556762802, 0.027534178226249116]
ModelOwner的最大效用 U(Eta) = 0.0279
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.3110
DataOwner1的最优x_1 = 0.0300
DataOwner2的最优x_2 = 0.0074
DataOwner3的最优x_3 = 0.0293
DataOwner4的最优x_4 = 0.0260
DataOwner5的最优x_5 = 0.0329
DataOwner6的最优x_6 = 0.0302
DataOwner7的最优x_7 = 0.0296
DataOwner8的最优x_8 = 0.0333
DataOwner9的最优x_9 = 0.0262
DataOwner10的最优x_10 = 0.0328
每个DataOwner应该贡献数据比例 xn_list = [0.02997259097846888, 0.007421639425355243, 0.029329881867013373, 0.025951962054896898, 0.03288366986093368, 0.030177347106842792, 0.02960087571229085, 0.03331953910184266, 0.026203422153806753, 0.03278935896731893]
ModelOwner的最大效用 U(Eta) = 0.0391
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.3610
DataOwner1的最优x_1 = 0.0348
DataOwner2的最优x_2 = 0.0086
DataOwner3的最优x_3 = 0.0340
DataOwner4的最优x_4 = 0.0301
DataOwner5的最优x_5 = 0.0382
DataOwner6的最优x_6 = 0.0350
DataOwner7的最优x_7 = 0.0344
DataOwner8的最优x_8 = 0.0387
DataOwner9的最优x_9 = 0.0304
DataOwner10的最优x_10 = 0.0381
每个DataOwner应该贡献数据比例 xn_list = [0.03479078630295245, 0.008614693052344063, 0.03404475952891324, 0.030123827687386823, 0.038169830956757494, 0.03502845767044632, 0.03435931655110958, 0.03867576765165301, 0.030415710847843884, 0.03806035926777222]
ModelOwner的最大效用 U(Eta) = 0.0520
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.4126
DataOwner1的最优x_1 = 0.0398
DataOwner2的最优x_2 = 0.0098
DataOwner3的最优x_3 = 0.0389
DataOwner4的最优x_4 = 0.0344
DataOwner5的最优x_5 = 0.0436
DataOwner6的最优x_6 = 0.0400
DataOwner7的最优x_7 = 0.0393
DataOwner8的最优x_8 = 0.0442
DataOwner9的最优x_9 = 0.0348
DataOwner10的最优x_10 = 0.0435
每个DataOwner应该贡献数据比例 xn_list = [0.03976340542062811, 0.009845984204895153, 0.03891074963951616, 0.034429402161967104, 0.04362541421037973, 0.04003504696562021, 0.03927026604402419, 0.044203664030348346, 0.03476300394788913, 0.0435002958208974]
ModelOwner的最大效用 U(Eta) = 0.0666
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.4610
DataOwner1的最优x_1 = 0.0444
DataOwner2的最优x_2 = 0.0110
DataOwner3的最优x_3 = 0.0435
DataOwner4的最优x_4 = 0.0385
DataOwner5的最优x_5 = 0.0487
DataOwner6的最优x_6 = 0.0447
DataOwner7的最优x_7 = 0.0439
DataOwner8的最优x_8 = 0.0494
DataOwner9的最优x_9 = 0.0388
DataOwner10的最优x_10 = 0.0486
每个DataOwner应该贡献数据比例 xn_list = [0.04442741710528193, 0.01100085977184302, 0.04347474985675141, 0.038467766891561755, 0.04874241662687117, 0.04473092059285093, 0.04387643540379703, 0.04938849172208006, 0.038840498188935216, 0.04860262259217357]
ModelOwner的最大效用 U(Eta) = 0.0828
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.5110
DataOwner1的最优x_1 = 0.0492
DataOwner2的最优x_2 = 0.0122
DataOwner3的最优x_3 = 0.0482
DataOwner4的最优x_4 = 0.0426
DataOwner5的最优x_5 = 0.0540
DataOwner6的最优x_6 = 0.0496
DataOwner7的最优x_7 = 0.0486
DataOwner8的最优x_8 = 0.0547
DataOwner9的最优x_9 = 0.0431
DataOwner10的最优x_10 = 0.0539
每个DataOwner应该贡献数据比例 xn_list = [0.0492456928516493, 0.012193933312661144, 0.04818970621759375, 0.04263970215893399, 0.0540286659565506, 0.04958211212913741, 0.04863495566770648, 0.05474480967502022, 0.043052857191907594, 0.05387371087369551]
ModelOwner的最大效用 U(Eta) = 0.1005
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.6142
DataOwner1的最优x_1 = 0.0592
DataOwner2的最优x_2 = 0.0147
DataOwner3的最优x_3 = 0.0579
DataOwner4的最优x_4 = 0.0512
DataOwner5的最优x_5 = 0.0649
DataOwner6的最优x_6 = 0.0596
DataOwner7的最优x_7 = 0.0585
DataOwner8的最优x_8 = 0.0658
DataOwner9的最优x_9 = 0.0517
DataOwner10的最优x_10 = 0.0648
每个DataOwner应该贡献数据比例 xn_list = [0.059188355153371605, 0.014655877791338676, 0.057919165740381896, 0.05124862072044828, 0.06493700634106712, 0.05959269718759501, 0.05845431066580018, 0.06579773884981338, 0.05174519139319839, 0.06475076596268889]
ModelOwner的最大效用 U(Eta) = 0.1190
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.6140
DataOwner1的最优x_1 = 0.0592
DataOwner2的最优x_2 = 0.0147
DataOwner3的最优x_3 = 0.0579
DataOwner4的最优x_4 = 0.0512
DataOwner5的最优x_5 = 0.0649
DataOwner6的最优x_6 = 0.0596
DataOwner7的最优x_7 = 0.0584
DataOwner8的最优x_8 = 0.0658
DataOwner9的最优x_9 = 0.0517
DataOwner10的最优x_10 = 0.0647
每个DataOwner应该贡献数据比例 xn_list = [0.05917289380699461, 0.014652049331348407, 0.057904035944232324, 0.051235233409469116, 0.06492004332553888, 0.05957713020940102, 0.058439041078511465, 0.06578055098674046, 0.05173167437499291, 0.06473385158863541]
ModelOwner的最大效用 U(Eta) = 0.1406
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.6638
DataOwner1的最优x_1 = 0.0640
DataOwner2的最优x_2 = 0.0158
DataOwner3的最优x_3 = 0.0626
DataOwner4的最优x_4 = 0.0554
DataOwner5的最优x_5 = 0.0702
DataOwner6的最优x_6 = 0.0644
DataOwner7的最优x_7 = 0.0632
DataOwner8的最优x_8 = 0.0711
DataOwner9的最优x_9 = 0.0559
DataOwner10的最优x_10 = 0.0700
每个DataOwner应该贡献数据比例 xn_list = [0.06396953841442493, 0.01583976669362301, 0.06259782499411728, 0.055388439216091406, 0.07018256058773181, 0.06440654284044942, 0.06317619845635584, 0.07111282231201808, 0.05592512243080166, 0.06998127587603131]
ModelOwner的最大效用 U(Eta) = 0.1628
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.7110
DataOwner1的最优x_1 = 0.0685
DataOwner2的最优x_2 = 0.0170
DataOwner3的最优x_3 = 0.0670
DataOwner4的最优x_4 = 0.0593
DataOwner5的最优x_5 = 0.0752
DataOwner6的最优x_6 = 0.0690
DataOwner7的最优x_7 = 0.0677
DataOwner8的最优x_8 = 0.0762
DataOwner9的最优x_9 = 0.0599
DataOwner10的最优x_10 = 0.0750
每个DataOwner应该贡献数据比例 xn_list = [0.06851874544156158, 0.016966214996435625, 0.06704948233903466, 0.05932739958979555, 0.07517360798058062, 0.06898682753049777, 0.06766898694990918, 0.07617002546035938, 0.05990224914322176, 0.07495800886405046]
ModelOwner的最大效用 U(Eta) = 0.1863
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.7610
DataOwner1的最优x_1 = 0.0733
DataOwner2的最优x_2 = 0.0182
DataOwner3的最优x_3 = 0.0718
DataOwner4的最优x_4 = 0.0635
DataOwner5的最优x_5 = 0.0805
DataOwner6的最优x_6 = 0.0738
DataOwner7的最优x_7 = 0.0724
DataOwner8的最优x_8 = 0.0815
DataOwner9的最优x_9 = 0.0641
DataOwner10的最优x_10 = 0.0802
每个DataOwner应该贡献数据比例 xn_list = [0.0733374340929196, 0.018159390694844436, 0.071764842853008, 0.06349969232681087, 0.08046031034168027, 0.07383843479338797, 0.0724279149219187, 0.08152680230874884, 0.06411496908353785, 0.08022954874269302]
ModelOwner的最大效用 U(Eta) = 0.2113
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.8110
DataOwner1的最优x_1 = 0.0782
DataOwner2的最优x_2 = 0.0194
DataOwner3的最优x_3 = 0.0765
DataOwner4的最优x_4 = 0.0677
DataOwner5的最优x_5 = 0.0857
DataOwner6的最优x_6 = 0.0787
DataOwner7的最优x_7 = 0.0772
DataOwner8的最优x_8 = 0.0869
DataOwner9的最优x_9 = 0.0683
DataOwner10的最优x_10 = 0.0855
每个DataOwner应该贡献数据比例 xn_list = [0.0781554162320553, 0.019352391607961174, 0.07647951179909784, 0.06767137342611591, 0.08574623751457275, 0.07868933071371469, 0.07718614529573391, 0.0868827939748277, 0.06832707145155177, 0.08550031592678903]
ModelOwner的最大效用 U(Eta) = 0.2375
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.8610
DataOwner1的最优x_1 = 0.0830
DataOwner2的最优x_2 = 0.0205
DataOwner3的最优x_3 = 0.0812
DataOwner4的最优x_4 = 0.0718
DataOwner5的最优x_5 = 0.0910
DataOwner6的最优x_6 = 0.0835
DataOwner7的最优x_7 = 0.0819
DataOwner8的最优x_8 = 0.0922
DataOwner9的最优x_9 = 0.0725
DataOwner10的最优x_10 = 0.0908
每个DataOwner应该贡献数据比例 xn_list = [0.08297361492732479, 0.020545446081790417, 0.08119439276884073, 0.07184324198308958, 0.09103240232699965, 0.0835404446730154, 0.08194458948810628, 0.0922390262799236, 0.0725393630666671, 0.09077131992823523]
ModelOwner的最大效用 U(Eta) = 0.2650
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.9110
DataOwner1的最优x_1 = 0.0878
DataOwner2的最优x_2 = 0.0217
DataOwner3的最优x_3 = 0.0859
DataOwner4的最优x_4 = 0.0760
DataOwner5的最优x_5 = 0.0963
DataOwner6的最优x_6 = 0.0884
DataOwner7的最优x_7 = 0.0867
DataOwner8的最优x_8 = 0.0976
DataOwner9的最优x_9 = 0.0768
DataOwner10的最优x_10 = 0.0960
每个DataOwner应该贡献数据比例 xn_list = [0.08779191282693052, 0.021738525105447538, 0.08590937079569863, 0.07601519641744069, 0.09631867595081724, 0.08839165851234419, 0.08670313160446459, 0.09759536886117474, 0.07675174145425348, 0.09604243243932595]
ModelOwner的最大效用 U(Eta) = 0.2938
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.9610
DataOwner1的最优x_1 = 0.0926
DataOwner2的最优x_2 = 0.0229
DataOwner3的最优x_3 = 0.0906
DataOwner4的最优x_4 = 0.0802
DataOwner5的最优x_5 = 0.1016
DataOwner6的最优x_6 = 0.0932
DataOwner7的最优x_7 = 0.0915
DataOwner8的最优x_8 = 0.1030
DataOwner9的最优x_9 = 0.0810
DataOwner10的最优x_10 = 0.1013
每个DataOwner应该贡献数据比例 xn_list = [0.09261019867778028, 0.022931601148558713, 0.09062433704838778, 0.08018714044161226, 0.10160493637017778, 0.09324286022339821, 0.09146166185124036, 0.10295169805323864, 0.08096410929152656, 0.10131353177493566]
ModelOwner的最大效用 U(Eta) = 0.3238
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.0110
DataOwner1的最优x_1 = 0.0974
DataOwner2的最优x_2 = 0.0241
DataOwner3的最优x_3 = 0.0953
DataOwner4的最优x_4 = 0.0844
DataOwner5的最优x_5 = 0.1069
DataOwner6的最优x_6 = 0.0981
DataOwner7的最优x_7 = 0.0962
DataOwner8的最优x_8 = 0.1083
DataOwner9的最优x_9 = 0.0852
DataOwner10的最优x_10 = 0.1066
每个DataOwner应该贡献数据比例 xn_list = [0.09742856940846863, 0.02412469820817988, 0.09533938635002871, 0.08435915794307383, 0.10689128990345849, 0.09809414739005196, 0.0962202759166725, 0.10830812158889667, 0.08517655133306062, 0.10658472396512568]
ModelOwner的最大效用 U(Eta) = 0.3550
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.0610
DataOwner1的最优x_1 = 0.1022
DataOwner2的最优x_2 = 0.0253
DataOwner3的最优x_3 = 0.1001
DataOwner4的最优x_4 = 0.0885
DataOwner5的最优x_5 = 0.1122
DataOwner6的最优x_6 = 0.1029
DataOwner7的最优x_7 = 0.1010
DataOwner8的最优x_8 = 0.1137
DataOwner9的最优x_9 = 0.0894
DataOwner10的最优x_10 = 0.1119
每个DataOwner应该贡献数据比例 xn_list = [0.10224683176497884, 0.025317768433290557, 0.10005432960671569, 0.08853108161599504, 0.11217752454198819, 0.10294532544421843, 0.10097878295619296, 0.11366442465620873, 0.08938889862942993, 0.111855797597718]
ModelOwner的最大效用 U(Eta) = 0.3873
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.1110
DataOwner1的最优x_1 = 0.1071
DataOwner2的最优x_2 = 0.0265
DataOwner3的最优x_3 = 0.1048
DataOwner4的最优x_4 = 0.0927
DataOwner5的最优x_5 = 0.1175
DataOwner6的最优x_6 = 0.1078
DataOwner7的最优x_7 = 0.1057
DataOwner8的最优x_8 = 0.1190
DataOwner9的最优x_9 = 0.0936
DataOwner10的最优x_10 = 0.1171
每个DataOwner应该贡献数据比例 xn_list = [0.10706505496536903, 0.0265108289628155, 0.10476923454693794, 0.09270297138545848, 0.11746371622128582, 0.10779646407479156, 0.1057372513252336, 0.11902068419485211, 0.09360121169381617, 0.11712682839427233]
ModelOwner的最大效用 U(Eta) = 0.4208
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.1610
DataOwner1的最优x_1 = 0.1119
DataOwner2的最优x_2 = 0.0277
DataOwner3的最优x_3 = 0.1095
DataOwner4的最优x_4 = 0.0969
DataOwner5的最优x_5 = 0.1227
DataOwner6的最优x_6 = 0.1126
DataOwner7的最优x_7 = 0.1105
DataOwner8的最优x_8 = 0.1244
DataOwner9的最优x_9 = 0.0978
DataOwner10的最优x_10 = 0.1224
每个DataOwner应该贡献数据比例 xn_list = [0.1118833183193557, 0.027703899434940372, 0.10948417877972094, 0.09687489592210291, 0.12274995195408321, 0.1126476431332215, 0.11049575934985688, 0.1243769883709163, 0.09781355986227423, 0.12239790311797814]
ModelOwner的最大效用 U(Eta) = 0.4554
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.2110
DataOwner1的最优x_1 = 0.1167
DataOwner2的最优x_2 = 0.0289
DataOwner3的最优x_3 = 0.1142
DataOwner4的最优x_4 = 0.1010
DataOwner5的最优x_5 = 0.1280
DataOwner6的最优x_6 = 0.1175
DataOwner7的最优x_7 = 0.1153
DataOwner8的最优x_8 = 0.1297
DataOwner9的最优x_9 = 0.1020
DataOwner10的最优x_10 = 0.1277
每个DataOwner应该贡献数据比例 xn_list = [0.11670160302950994, 0.028896975195130056, 0.11419914391050877, 0.10104683895018167, 0.1280362111172552, 0.11749884369376182, 0.11525428846592589, 0.12973331628786547, 0.10202592670121124, 0.1276690012047402]
ModelOwner的最大效用 U(Eta) = 0.4911
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.2610
DataOwner1的最优x_1 = 0.1215
DataOwner2的最优x_2 = 0.0301
DataOwner3的最优x_3 = 0.1189
DataOwner4的最优x_4 = 0.1052
DataOwner5的最优x_5 = 0.1333
DataOwner6的最优x_6 = 0.1224
DataOwner7的最优x_7 = 0.1200
DataOwner8的最优x_8 = 0.1351
DataOwner9的最优x_9 = 0.1062
DataOwner10的最优x_10 = 0.1329
每个DataOwner应该贡献数据比例 xn_list = [0.1215200372306289, 0.03009008797138811, 0.1189142553266069, 0.10521891141821284, 0.13332263429310307, 0.12235019476740107, 0.120012965221397, 0.13508981039016815, 0.10623842423365744, 0.13294026283348964]
ModelOwner的最大效用 U(Eta) = 0.5278
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3110
DataOwner1的最优x_1 = 0.1263
DataOwner2的最优x_2 = 0.0313
DataOwner3的最优x_3 = 0.1236
DataOwner4的最优x_4 = 0.1094
DataOwner5的最优x_5 = 0.1386
DataOwner6的最优x_6 = 0.1272
DataOwner7的最优x_7 = 0.1248
DataOwner8的最优x_8 = 0.1404
DataOwner9的最优x_9 = 0.1105
DataOwner10的最优x_10 = 0.1382
每个DataOwner应该贡献数据比例 xn_list = [0.12633803717002176, 0.03128309321727474, 0.12362894179160477, 0.1093906078710193, 0.13860858099792245, 0.12720110859558678, 0.12477121309401204, 0.14044582174015927, 0.11045054210140955, 0.13821104938668688]
ModelOwner的最大效用 U(Eta) = 0.5656
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3610
DataOwner1的最优x_1 = 0.1312
DataOwner2的最优x_2 = 0.0325
DataOwner3的最优x_3 = 0.1283
DataOwner4的最优x_4 = 0.1136
DataOwner5的最优x_5 = 0.1439
DataOwner6的最优x_6 = 0.1321
DataOwner7的最优x_7 = 0.1295
DataOwner8的最优x_8 = 0.1458
DataOwner9的最优x_9 = 0.1147
DataOwner10的最优x_10 = 0.1435
每个DataOwner应该贡献数据比例 xn_list = [0.13115662413624268, 0.032476243863991165, 0.12834420245790784, 0.11356281247199806, 0.14389517153848017, 0.13205261344554123, 0.12953004082432995, 0.14580248572331356, 0.11466317307089394, 0.14348247788332724]
ModelOwner的最大效用 U(Eta) = 0.6044
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.4110
DataOwner1的最优x_1 = 0.1360
DataOwner2的最优x_2 = 0.0337
DataOwner3的最优x_3 = 0.1331
DataOwner4的最优x_4 = 0.1177
DataOwner5的最优x_5 = 0.1492
DataOwner6的最优x_6 = 0.1369
DataOwner7的最优x_7 = 0.1343
DataOwner8的最优x_8 = 0.1512
DataOwner9的最优x_9 = 0.1189
DataOwner10的最优x_10 = 0.1488
每个DataOwner应该贡献数据比例 xn_list = [0.13597481511470838, 0.033669296372322674, 0.13305907610880668, 0.11773467448164984, 0.14918132812841856, 0.13690371968040357, 0.1342884772654684, 0.15115870937937126, 0.11887545809099956, 0.14875347367992853]
ModelOwner的最大效用 U(Eta) = 0.6442
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.4610
DataOwner1的最优x_1 = 0.1408
DataOwner2的最优x_2 = 0.0349
DataOwner3的最优x_3 = 0.1378
DataOwner4的最优x_4 = 0.1219
DataOwner5的最优x_5 = 0.1545
DataOwner6的最优x_6 = 0.1418
DataOwner7的最优x_7 = 0.1390
DataOwner8的最优x_8 = 0.1565
DataOwner9的最优x_9 = 0.1231
DataOwner10的最优x_10 = 0.1540
每个DataOwner应该贡献数据比例 xn_list = [0.1407930621229611, 0.03486236279692746, 0.1377740043466514, 0.1219065848647412, 0.1544675459275765, 0.14175488228170605, 0.13904696914669087, 0.1565149953841672, 0.12308779196870316, 0.15402453052112222]
ModelOwner的最大效用 U(Eta) = 0.6850
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.5110
DataOwner1的最优x_1 = 0.1456
DataOwner2的最优x_2 = 0.0361
DataOwner3的最优x_3 = 0.1425
DataOwner4的最优x_4 = 0.1261
DataOwner5的最优x_5 = 0.1598
DataOwner6的最优x_6 = 0.1466
DataOwner7的最优x_7 = 0.1438
DataOwner8的最优x_8 = 0.1619
DataOwner9的最优x_9 = 0.1273
DataOwner10的最优x_10 = 0.1593
每个DataOwner应该贡献数据比例 xn_list = [0.14561130740764314, 0.036055428796260496, 0.1424889308969439, 0.1260784937560532, 0.159753761836804, 0.14660604314729087, 0.14380545932675648, 0.16187127947420957, 0.127300124340328, 0.1592955854789938]
ModelOwner的最大效用 U(Eta) = 0.7267
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.5610
DataOwner1的最优x_1 = 0.1504
DataOwner2的最优x_2 = 0.0372
DataOwner3的最优x_3 = 0.1472
DataOwner4的最优x_4 = 0.1303
DataOwner5的最优x_5 = 0.1650
DataOwner6的最优x_6 = 0.1515
DataOwner7的最优x_7 = 0.1486
DataOwner8的最优x_8 = 0.1672
DataOwner9的最优x_9 = 0.1315
DataOwner10的最优x_10 = 0.1646
每个DataOwner应该贡献数据比例 xn_list = [0.1504294885561293, 0.03724847891187789, 0.14720379468735229, 0.13025034711500258, 0.16503990737957416, 0.1514571394390214, 0.14856388616572988, 0.16722749226479225, 0.1315124006419577, 0.164566570271193]
ModelOwner的最大效用 U(Eta) = 0.7694
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.6110
DataOwner1的最优x_1 = 0.1552
DataOwner2的最优x_2 = 0.0384
DataOwner3的最优x_3 = 0.1519
DataOwner4的最优x_4 = 0.1344
DataOwner5的最优x_5 = 0.1703
DataOwner6的最优x_6 = 0.1563
DataOwner7的最优x_7 = 0.1533
DataOwner8的最优x_8 = 0.1726
DataOwner9的最优x_9 = 0.1357
DataOwner10的最优x_10 = 0.1698
每个DataOwner应该贡献数据比例 xn_list = [0.15524779832755117, 0.03844156087759459, 0.1519187843422262, 0.13442231184252182, 0.170326194037957, 0.15630836523191569, 0.1533224400319569, 0.1725838480412597, 0.1357247893903055, 0.16983769577449542]
ModelOwner的最大效用 U(Eta) = 0.8130
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.6610
DataOwner1的最优x_1 = 0.1601
DataOwner2的最优x_2 = 0.0396
DataOwner3的最优x_3 = 0.1566
DataOwner4的最优x_4 = 0.1386
DataOwner5的最优x_5 = 0.1756
DataOwner6的最优x_6 = 0.1612
DataOwner7的最优x_7 = 0.1581
DataOwner8的最优x_8 = 0.1779
DataOwner9的最优x_9 = 0.1399
DataOwner10的最优x_10 = 0.1751
每个DataOwner应该贡献数据比例 xn_list = [0.16006608769916267, 0.03963463779220602, 0.1566337540350489, 0.13859425891025634, 0.17561245831727496, 0.16115957048567975, 0.15808097375171265, 0.17794018114315127, 0.13993716030781167, 0.1751087989626628]
ModelOwner的最大效用 U(Eta) = 0.8576
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.7110
DataOwner1的最优x_1 = 0.1649
DataOwner2的最优x_2 = 0.0408
DataOwner3的最优x_3 = 0.1613
DataOwner4的最优x_4 = 0.1428
DataOwner5的最优x_5 = 0.1809
DataOwner6的最优x_6 = 0.1660
DataOwner7的最优x_7 = 0.1628
DataOwner8的最优x_8 = 0.1833
DataOwner9的最优x_9 = 0.1441
DataOwner10的最优x_10 = 0.1804
每个DataOwner应该贡献数据比例 xn_list = [0.16488437318270419, 0.040827713743767494, 0.16134871992255034, 0.14276620260461095, 0.1808987183271118, 0.16601077182490198, 0.16283950363131766, 0.1832965099171645, 0.14414952781989057, 0.18037989789396186]
ModelOwner的最大效用 U(Eta) = 0.9030
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.7610
DataOwner1的最优x_1 = 0.1697
DataOwner2的最优x_2 = 0.0420
DataOwner3的最优x_3 = 0.1661
DataOwner4的最优x_4 = 0.1469
DataOwner5的最优x_5 = 0.1862
DataOwner6的最优x_6 = 0.1709
DataOwner7的最优x_7 = 0.1676
DataOwner8的最优x_8 = 0.1887
DataOwner9的最优x_9 = 0.1484
DataOwner10的最优x_10 = 0.1857
每个DataOwner应该贡献数据比例 xn_list = [0.16970259922577344, 0.04202077492981379, 0.16606362750099363, 0.14693809449440945, 0.18618491281403962, 0.170861913146921, 0.16759797454141626, 0.18865277228766383, 0.14836184298148616, 0.18565093135495195]
ModelOwner的最大效用 U(Eta) = 0.9493
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.8110
DataOwner1的最优x_1 = 0.1745
DataOwner2的最优x_2 = 0.0432
DataOwner3的最优x_3 = 0.1708
DataOwner4的最优x_4 = 0.1511
DataOwner5的最优x_5 = 0.1915
DataOwner6的最优x_6 = 0.1757
DataOwner7的最优x_7 = 0.1724
DataOwner8的最优x_8 = 0.1940
DataOwner9的最优x_9 = 0.1526
DataOwner10的最优x_10 = 0.1909
每个DataOwner应该贡献数据比例 xn_list = [0.17452103303181188, 0.043213887655807456, 0.17077873867464305, 0.15111016695964244, 0.19147133586624185, 0.17571326399353035, 0.17235665117144378, 0.1940092662780591, 0.15257434055508085, 0.1909221929953623]
ModelOwner的最大效用 U(Eta) = 0.9964
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.8610
DataOwner1的最优x_1 = 0.1793
DataOwner2的最优x_2 = 0.0444
DataOwner3的最优x_3 = 0.1755
DataOwner4的最优x_4 = 0.1553
DataOwner5的最优x_5 = 0.1968
DataOwner6的最优x_6 = 0.1806
DataOwner7的最优x_7 = 0.1771
DataOwner8的最优x_8 = 0.1994
DataOwner9的最优x_9 = 0.1568
DataOwner10的最优x_10 = 0.1962
每个DataOwner应该贡献数据比例 xn_list = [0.17933924231441872, 0.04440694473994116, 0.17549362999924448, 0.15528204468240842, 0.19675751227926785, 0.1805643886139744, 0.17711510580153358, 0.19936551035264474, 0.15678664145950147, 0.19619320857301484]
ModelOwner的最大效用 U(Eta) = 1.0444
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.9110
DataOwner1的最优x_1 = 0.1842
DataOwner2的最优x_2 = 0.0456
DataOwner3的最优x_3 = 0.1802
DataOwner4的最优x_4 = 0.1595
DataOwner5的最优x_5 = 0.2020
DataOwner6的最优x_6 = 0.1854
DataOwner7的最优x_7 = 0.1819
DataOwner8的最优x_8 = 0.2047
DataOwner9的最优x_9 = 0.1610
DataOwner10的最优x_10 = 0.2015
每个DataOwner应该贡献数据比例 xn_list = [0.18415730303833092, 0.04559996502458471, 0.18020837576056156, 0.15945379390266304, 0.20204352574228251, 0.18541536356815236, 0.18187341363283466, 0.2047215894019405, 0.16099881255636417, 0.20146406155577173]
ModelOwner的最大效用 U(Eta) = 1.0933
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.9610
DataOwner1的最优x_1 = 0.1890
DataOwner2的最优x_2 = 0.0468
DataOwner3的最优x_3 = 0.1849
DataOwner4的最优x_4 = 0.1636
DataOwner5的最优x_5 = 0.2073
DataOwner6的最优x_6 = 0.1903
DataOwner7的最优x_7 = 0.1866
DataOwner8的最优x_8 = 0.2101
DataOwner9的最优x_9 = 0.1652
DataOwner10的最优x_10 = 0.2067
每个DataOwner应该贡献数据比例 xn_list = [0.18897562617860458, 0.04679305031302135, 0.18492337867966244, 0.1636257700689652, 0.20732982702315916, 0.19026660290721212, 0.18663198077185006, 0.21007795990186307, 0.16521121290802573, 0.20673520174515078]
ModelOwner的最大效用 U(Eta) = 1.1429
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 2.0110
DataOwner1的最优x_1 = 0.1938
DataOwner2的最优x_2 = 0.0480
DataOwner3的最优x_3 = 0.1896
DataOwner4的最优x_4 = 0.1678
DataOwner5的最优x_5 = 0.2126
DataOwner6的最优x_6 = 0.1951
DataOwner7的最优x_7 = 0.1914
DataOwner8的最优x_8 = 0.2154
DataOwner9的最优x_9 = 0.1694
DataOwner10的最优x_10 = 0.2120
每个DataOwner应该贡献数据比例 xn_list = [0.1937940590607245, 0.04798616276258772, 0.1896384888044534, 0.16779784139132306, 0.21261624874884766, 0.19511795265154697, 0.1913906562214887, 0.21543445253534377, 0.1694237092855974, 0.21200646192838749]
ModelOwner的最大效用 U(Eta) = 1.1934
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 2.0610
DataOwner1的最优x_1 = 0.1986
DataOwner2的最优x_2 = 0.0492
DataOwner3的最优x_3 = 0.1944
DataOwner4的最优x_4 = 0.1720
DataOwner5的最优x_5 = 0.2179
DataOwner6的最优x_6 = 0.2000
DataOwner7的最优x_7 = 0.1961
DataOwner8的最优x_8 = 0.2208
DataOwner9的最优x_9 = 0.1736
DataOwner10的最优x_10 = 0.2173
每个DataOwner应该贡献数据比例 xn_list = [0.19861234400182534, 0.04917923857334213, 0.19435345416145308, 0.17196978466277713, 0.2179025082137945, 0.19996915344960894, 0.19614918557876757, 0.2207907807461726, 0.17363607636312198, 0.21727756028484396]
ModelOwner的最大效用 U(Eta) = 1.2447
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 2.1110
DataOwner1的最优x_1 = 0.2034
DataOwner2的最优x_2 = 0.0504
DataOwner3的最优x_3 = 0.1991
DataOwner4的最优x_4 = 0.1761
DataOwner5的最优x_5 = 0.2232
DataOwner6的最优x_6 = 0.2048
DataOwner7的最优x_7 = 0.2009
DataOwner8的最优x_8 = 0.2261
DataOwner9的最优x_9 = 0.1778
DataOwner10的最优x_10 = 0.2225
每个DataOwner应该贡献数据比例 xn_list = [0.20343062778926682, 0.05037231411369188, 0.19906841839065806, 0.176141726832156, 0.22318876630413406, 0.20482035307758772, 0.20090771376885594, 0.22614710758341672, 0.17784844234325436, 0.22254865733424373]
ModelOwner的最大效用 U(Eta) = 1.2967
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 2.1610
DataOwner1的最优x_1 = 0.2082
DataOwner2的最优x_2 = 0.0516
DataOwner3的最优x_3 = 0.2038
DataOwner4的最优x_4 = 0.1803
DataOwner5的最优x_5 = 0.2285
DataOwner6的最优x_6 = 0.2097
DataOwner7的最优x_7 = 0.2057
DataOwner8的最优x_8 = 0.2315
DataOwner9的最优x_9 = 0.1821
DataOwner10的最优x_10 = 0.2278
每个DataOwner应该贡献数据比例 xn_list = [0.20824890266133092, 0.05156538743517063, 0.2037833739077069, 0.18031366136045915, 0.22847501469216627, 0.20967154375197358, 0.20566623318086183, 0.2315034245885165, 0.18206080059997629, 0.22781974467548516]
ModelOwner的最大效用 U(Eta) = 1.3495
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 2.2110
DataOwner1的最优x_1 = 0.2131
DataOwner2的最优x_2 = 0.0528
DataOwner3的最优x_3 = 0.2085
DataOwner4的最优x_4 = 0.1845
DataOwner5的最优x_5 = 0.2338
DataOwner6的最优x_6 = 0.2145
DataOwner7的最优x_7 = 0.2104
DataOwner8的最优x_8 = 0.2369
DataOwner9的最优x_9 = 0.1863
DataOwner10的最优x_10 = 0.2331
每个DataOwner应该贡献数据比例 xn_list = [0.2130671768645063, 0.052758460591573374, 0.2084983287347616, 0.1844855952775101, 0.2337612623193507, 0.21452273369672054, 0.21042475188278864, 0.23685974080135075, 0.18627315824767945, 0.2330908312469838]
ModelOwner的最大效用 U(Eta) = 1.4031
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 2.2610
DataOwner1的最优x_1 = 0.2179
DataOwner2的最优x_2 = 0.0540
DataOwner3的最优x_3 = 0.2132
DataOwner4的最优x_4 = 0.1887
DataOwner5的最优x_5 = 0.2390
DataOwner6的最优x_6 = 0.2194
DataOwner7的最优x_7 = 0.2152
DataOwner8的最优x_8 = 0.2422
DataOwner9的最优x_9 = 0.1905
DataOwner10的最优x_10 = 0.2384
每个DataOwner应该贡献数据比例 xn_list = [0.21788545177323035, 0.05395153392779334, 0.21321328428381742, 0.18865752983143538, 0.23904751073182648, 0.21937392440590456, 0.2151832713456898, 0.24221605784016662, 0.19048551652095233, 0.23836191862699846]
ModelOwner的最大效用 U(Eta) = 1.4574
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 2.3110
DataOwner1的最优x_1 = 0.2227
DataOwner2的最优x_2 = 0.0551
DataOwner3的最优x_3 = 0.2179
DataOwner4的最优x_4 = 0.1928
DataOwner5的最优x_5 = 0.2443
DataOwner6的最优x_6 = 0.2242
DataOwner7的最优x_7 = 0.2199
DataOwner8的最优x_8 = 0.2476
DataOwner9的最优x_9 = 0.1947
DataOwner10的最优x_10 = 0.2436
每个DataOwner应该贡献数据比例 xn_list = [0.22270357319149175, 0.05514456925467371, 0.2179280896235411, 0.19282933147211387, 0.24433359074375643, 0.22422496055905122, 0.21994163919619855, 0.24757220423139445, 0.19469774060379721, 0.2436328380767751]
ModelOwner的最大效用 U(Eta) = 1.5124
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 2.3610
DataOwner1的最优x_1 = 0.2275
DataOwner2的最优x_2 = 0.0563
DataOwner3的最优x_3 = 0.2226
DataOwner4的最优x_4 = 0.1970
DataOwner5的最优x_5 = 0.2496
DataOwner6的最优x_6 = 0.2291
DataOwner7的最优x_7 = 0.2247
DataOwner8的最优x_8 = 0.2529
DataOwner9的最优x_9 = 0.1989
DataOwner10的最优x_10 = 0.2489
每个DataOwner应该贡献数据比例 xn_list = [0.22752209568100015, 0.056337703892745845, 0.22264328743497194, 0.19700148038242754, 0.24962011077938143, 0.22907640052478195, 0.224700403138974, 0.2529287964792293, 0.1989103153197539, 0.2489041962899574]
ModelOwner的最大效用 U(Eta) = 1.5682
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 2.4110
DataOwner1的最优x_1 = 0.2323
DataOwner2的最优x_2 = 0.0575
DataOwner3的最优x_3 = 0.2274
DataOwner4的最优x_4 = 0.2012
DataOwner5的最优x_5 = 0.2549
DataOwner6的最优x_6 = 0.2339
DataOwner7的最优x_7 = 0.2295
DataOwner8的最优x_8 = 0.2583
DataOwner9的最优x_9 = 0.2031
DataOwner10的最优x_10 = 0.2542
每个DataOwner应该贡献数据比例 xn_list = [0.23234033217777805, 0.05753076771443822, 0.2273582053871831, 0.20117338166717308, 0.25490631704953187, 0.23392755254201583, 0.2294588846427206, 0.2582850707991681, 0.20312264000923666, 0.2541752416333165]
ModelOwner的最大效用 U(Eta) = 1.6247
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 2.4610
DataOwner1的最优x_1 = 0.2372
DataOwner2的最优x_2 = 0.0587
DataOwner3的最优x_3 = 0.2321
DataOwner4的最优x_4 = 0.2053
DataOwner5的最优x_5 = 0.2602
DataOwner6的最优x_6 = 0.2388
DataOwner7的最优x_7 = 0.2342
DataOwner8的最优x_8 = 0.2636
DataOwner9的最优x_9 = 0.2073
DataOwner10的最优x_10 = 0.2594
每个DataOwner应该贡献数据比例 xn_list = [0.2371588697167284, 0.05872390607334759, 0.23207341792485336, 0.20534554365784727, 0.2601928536292112, 0.23877900775689248, 0.23421766343703523, 0.263641679779626, 0.207335227896312, 0.25944661632556076]
ModelOwner的最大效用 U(Eta) = 1.6819
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 2.5110
DataOwner1的最优x_1 = 0.2420
DataOwner2的最优x_2 = 0.0599
DataOwner3的最优x_3 = 0.2368
DataOwner4的最优x_4 = 0.2095
DataOwner5的最优x_5 = 0.2655
DataOwner6的最优x_6 = 0.2436
DataOwner7的最优x_7 = 0.2390
DataOwner8的最优x_8 = 0.2690
DataOwner9的最优x_9 = 0.2115
DataOwner10的最优x_10 = 0.2647
每个DataOwner应该贡献数据比例 xn_list = [0.24197685179635056, 0.0599169069029324, 0.23678808691436107, 0.209517224599894, 0.26547878073407866, 0.2436299035193973, 0.23897589368947494, 0.2689976712701438, 0.21154733014682178, 0.2647173833220796]
ModelOwner的最大效用 U(Eta) = 1.7398
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 2.5610
DataOwner1的最优x_1 = 0.2468
DataOwner2的最优x_2 = 0.0611
DataOwner3的最优x_3 = 0.2415
DataOwner4的最优x_4 = 0.2137
DataOwner5的最优x_5 = 0.2708
DataOwner6的最优x_6 = 0.2485
DataOwner7的最优x_7 = 0.2437
DataOwner8的最优x_8 = 0.2744
DataOwner9的最优x_9 = 0.2158
DataOwner10的最优x_10 = 0.2700
每个DataOwner应该贡献数据比例 xn_list = [0.2467951250727239, 0.061109979832094305, 0.24150304085786278, 0.21368915772797792, 0.27076502735381636, 0.24848109256892528, 0.2437344115134889, 0.2743539864763212, 0.21575968698964368, 0.2699884689005548]
ModelOwner的最大效用 U(Eta) = 1.7983
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 2.6110
DataOwner1的最优x_1 = 0.2516
DataOwner2的最优x_2 = 0.0623
DataOwner3的最优x_3 = 0.2462
DataOwner4的最优x_4 = 0.2179
DataOwner5的最优x_5 = 0.2761
DataOwner6的最优x_6 = 0.2533
DataOwner7的最优x_7 = 0.2485
DataOwner8的最优x_8 = 0.2797
DataOwner9的最优x_9 = 0.2200
DataOwner10的最优x_10 = 0.2753
每个DataOwner应该贡献数据比例 xn_list = [0.25161346953547553, 0.06230307038816255, 0.24621806446154032, 0.21786115249313026, 0.2760513520747374, 0.2533323532913355, 0.24849299964258442, 0.2797103808187509, 0.21997210606874154, 0.2752596323567297]
ModelOwner的最大效用 U(Eta) = 1.8576
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 2.6610
DataOwner1的最优x_1 = 0.2564
DataOwner2的最优x_2 = 0.0635
DataOwner3的最优x_3 = 0.2509
DataOwner4的最优x_4 = 0.2220
DataOwner5的最优x_5 = 0.2813
DataOwner6的最优x_6 = 0.2582
DataOwner7的最优x_7 = 0.2533
DataOwner8的最优x_8 = 0.2851
DataOwner9的最优x_9 = 0.2242
DataOwner10的最优x_10 = 0.2805
每个DataOwner应该贡献数据比例 xn_list = [0.25643152903896027, 0.0634960903837782, 0.25093280921401423, 0.2220329005249583, 0.2813373641565762, 0.2581833271054904, 0.2532513063433697, 0.2850664583810684, 0.22418427602089205, 0.2805304840720167]
ModelOwner的最大效用 U(Eta) = 1.9175
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 2.7110
DataOwner1的最优x_1 = 0.2612
DataOwner2的最优x_2 = 0.0647
DataOwner3的最优x_3 = 0.2556
DataOwner4的最优x_4 = 0.2262
DataOwner5的最优x_5 = 0.2866
DataOwner6的最优x_6 = 0.2630
DataOwner7的最优x_7 = 0.2580
DataOwner8的最优x_8 = 0.2904
DataOwner9的最优x_9 = 0.2284
DataOwner10的最优x_10 = 0.2858
每个DataOwner应该贡献数据比例 xn_list = [0.26124987146714, 0.06468918043578088, 0.255647830825385, 0.22620489352850748, 0.2866236866435179, 0.26303458577827665, 0.2580098924615144, 0.29042285046127, 0.22839669331958903, 0.2858016453011133]
ModelOwner的最大效用 U(Eta) = 1.9781
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 2.7610
DataOwner1的最优x_1 = 0.2661
DataOwner2的最优x_2 = 0.0659
DataOwner3的最优x_3 = 0.2604
DataOwner4的最优x_4 = 0.2304
DataOwner5的最优x_5 = 0.2919
DataOwner6的最优x_6 = 0.2679
DataOwner7的最优x_7 = 0.2628
DataOwner8的最优x_8 = 0.2958
DataOwner9的最优x_9 = 0.2326
DataOwner10的最优x_10 = 0.2911
每个DataOwner应该贡献数据比例 xn_list = [0.26606818782164254, 0.06588226403147755, 0.2603628269217746, 0.23037686395466456, 0.2919099805228816, 0.2678858181988434, 0.26276845282905625, 0.295779213555376, 0.23260908782246703, 0.2910727780054229]
ModelOwner的最大效用 U(Eta) = 2.0393
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 2.8110
DataOwner1的最优x_1 = 0.2709
DataOwner2的最优x_2 = 0.0671
DataOwner3的最优x_3 = 0.2651
DataOwner4的最优x_4 = 0.2345
DataOwner5的最优x_5 = 0.2972
DataOwner6的最优x_6 = 0.2727
DataOwner7的最优x_7 = 0.2675
DataOwner8的最优x_8 = 0.3011
DataOwner9的最优x_9 = 0.2368
DataOwner10的最优x_10 = 0.2963
每个DataOwner应该贡献数据比例 xn_list = [0.27088648824509615, 0.06707534368245838, 0.2650778074288399, 0.23454882058826304, 0.2971962569259206, 0.2727370345795135, 0.2675269974628013, 0.3011355589402701, 0.2368214683991829, 0.29634389328202876]
ModelOwner的最大效用 U(Eta) = 2.1012
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 2.8610
DataOwner1的最优x_1 = 0.2757
DataOwner2的最优x_2 = 0.0683
DataOwner3的最优x_3 = 0.2698
DataOwner4的最优x_4 = 0.2387
DataOwner5的最优x_5 = 0.3025
DataOwner6的最优x_6 = 0.2776
DataOwner7的最优x_7 = 0.2723
DataOwner8的最优x_8 = 0.3065
DataOwner9的最优x_9 = 0.2410
DataOwner10的最优x_10 = 0.3016
每个DataOwner应该贡献数据比例 xn_list = [0.27570477914180735, 0.06826842097589486, 0.2697927786197652, 0.23872076897892378, 0.302482522882491, 0.27758824137449006, 0.27228553269736677, 0.3064918937403858, 0.24103384065259054, 0.30161499814637116]
ModelOwner的最大效用 U(Eta) = 2.1636
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 2.9110
DataOwner1的最优x_1 = 0.2805
DataOwner2的最优x_2 = 0.0695
DataOwner3的最优x_3 = 0.2745
DataOwner4的最优x_4 = 0.2429
DataOwner5的最优x_5 = 0.3078
DataOwner6的最优x_6 = 0.2824
DataOwner7的最优x_7 = 0.2770
DataOwner8的最优x_8 = 0.3118
DataOwner9的最优x_9 = 0.2452
DataOwner10的最优x_10 = 0.3069
每个DataOwner应该贡献数据比例 xn_list = [0.2805230644683022, 0.06946149688758449, 0.2745077443487534, 0.2428927125359026, 0.3077687827171132, 0.28243944255101605, 0.2770440624143119, 0.31184822233787374, 0.24524620802541747, 0.3068860968991511]
ModelOwner的最大效用 U(Eta) = 2.2268
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 2.9610
DataOwner1的最优x_1 = 0.2853
DataOwner2的最优x_2 = 0.0707
DataOwner3的最优x_3 = 0.2792
DataOwner4的最优x_4 = 0.2471
DataOwner5的最优x_5 = 0.3131
DataOwner6的最优x_6 = 0.2873
DataOwner7的最优x_7 = 0.2818
DataOwner8的最优x_8 = 0.3172
DataOwner9的最优x_9 = 0.2495
DataOwner10的最优x_10 = 0.3122
每个DataOwner应该贡献数据比例 xn_list = [0.28534133833650516, 0.0706545699631497, 0.27922269887020085, 0.24706464617646998, 0.31305502998526, 0.2872906321953467, 0.281802580822632, 0.31720453820223576, 0.2494585653858717, 0.312157183125131]
ModelOwner的最大效用 U(Eta) = 2.2905
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 3.0110
DataOwner1的最优x_1 = 0.2902
DataOwner2的最优x_2 = 0.0718
DataOwner3的最优x_3 = 0.2839
DataOwner4的最优x_4 = 0.2512
DataOwner5的最优x_5 = 0.3183
DataOwner6的最优x_6 = 0.2921
DataOwner7的最优x_7 = 0.2866
DataOwner8的最优x_8 = 0.3226
DataOwner9的最优x_9 = 0.2537
DataOwner10的最优x_10 = 0.3174
每个DataOwner应该贡献数据比例 xn_list = [0.2901597534573444, 0.0718476780152021, 0.2839377916153777, 0.25123670213774263, 0.31834143221248035, 0.29214196404208115, 0.2865612387303059, 0.3225610110754452, 0.25367104625441167, 0.3174284238792644]
ModelOwner的最大效用 U(Eta) = 2.3549
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 3.0610
DataOwner1的最优x_1 = 0.2950
DataOwner2的最优x_2 = 0.0730
DataOwner3的最优x_3 = 0.2887
DataOwner4的最优x_4 = 0.2554
DataOwner5的最优x_5 = 0.3236
DataOwner6的最优x_6 = 0.2970
DataOwner7的最优x_7 = 0.2913
DataOwner8的最优x_8 = 0.3279
DataOwner9的最优x_9 = 0.2579
DataOwner10的最优x_10 = 0.3227
每个DataOwner应该贡献数据比例 xn_list = [0.2949779453378558, 0.07304073078807728, 0.2886526659056169, 0.25540856477475804, 0.3236275895428982, 0.2969930711542817, 0.2913196761700453, 0.327917235814295, 0.2578833319198036, 0.3226994204157092]
ModelOwner的最大效用 U(Eta) = 2.4198
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 3.1110
DataOwner1的最优x_1 = 0.2998
DataOwner2的最优x_2 = 0.0742
DataOwner3的最优x_3 = 0.2934
DataOwner4的最优x_4 = 0.2596
DataOwner5的最优x_5 = 0.3289
DataOwner6的最优x_6 = 0.3018
DataOwner7的最优x_7 = 0.2961
DataOwner8的最优x_8 = 0.3333
DataOwner9的最优x_9 = 0.2621
DataOwner10的最优x_10 = 0.3280
每个DataOwner应该贡献数据比例 xn_list = [0.29979625952588235, 0.07423381384852804, 0.29336765988417707, 0.2595805333237761, 0.32891388104651065, 0.30184430139365115, 0.29607823439770864, 0.3332735965002567, 0.2620957245287899, 0.3279705507466079]
ModelOwner的最大效用 U(Eta) = 2.4853
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 3.1610
DataOwner1的最优x_1 = 0.3046
DataOwner2的最优x_2 = 0.0754
DataOwner3的最优x_3 = 0.2981
DataOwner4的最优x_4 = 0.2638
DataOwner5的最优x_5 = 0.3342
DataOwner6的最优x_6 = 0.3067
DataOwner7的最优x_7 = 0.3008
DataOwner8的最优x_8 = 0.3386
DataOwner9的最优x_9 = 0.2663
DataOwner10的最优x_10 = 0.3332
每个DataOwner应该贡献数据比例 xn_list = [0.3046144470686621, 0.07542686554872867, 0.2980825299314019, 0.2637523922192765, 0.33420003360528394, 0.30669540412341184, 0.3008366675513274, 0.3386298163997029, 0.2663080064197755, 0.3332415425347767]
ModelOwner的最大效用 U(Eta) = 2.5515
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 3.2110
DataOwner1的最优x_1 = 0.3094
DataOwner2的最优x_2 = 0.0766
DataOwner3的最优x_3 = 0.3028
DataOwner4的最优x_4 = 0.2679
DataOwner5的最优x_5 = 0.3395
DataOwner6的最优x_6 = 0.3115
DataOwner7的最优x_7 = 0.3056
DataOwner8的最优x_8 = 0.3440
DataOwner9的最优x_9 = 0.2705
DataOwner10的最优x_10 = 0.3385
每个DataOwner应该贡献数据比例 xn_list = [0.3094327817369644, 0.07661995366068675, 0.3027975438035351, 0.2679243785499106, 0.3394863475900363, 0.3115466550905767, 0.30559524603333377, 0.3439861998440157, 0.2705204168542803, 0.33851269531718486]
ModelOwner的最大效用 U(Eta) = 2.6182
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 3.2610
DataOwner1的最优x_1 = 0.3143
DataOwner2的最优x_2 = 0.0778
DataOwner3的最优x_3 = 0.3075
DataOwner4的最优x_4 = 0.2721
DataOwner5的最优x_5 = 0.3448
DataOwner6的最优x_6 = 0.3164
DataOwner7的最优x_7 = 0.3104
DataOwner8的最优x_8 = 0.3493
DataOwner9的最优x_9 = 0.2747
DataOwner10的最优x_10 = 0.3438
每个DataOwner应该贡献数据比例 xn_list = [0.3142507100799974, 0.07781294119669825, 0.30751216033674705, 0.2720960129225321, 0.34477221566545685, 0.316397496768822, 0.31035342322052384, 0.3493421316226108, 0.27473247204755563, 0.34378340338403107]
ModelOwner的最大效用 U(Eta) = 2.6855
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 3.3110
DataOwner1的最优x_1 = 0.3191
DataOwner2的最优x_2 = 0.0790
DataOwner3的最优x_3 = 0.3122
DataOwner4的最优x_4 = 0.2763
DataOwner5的最优x_5 = 0.3501
DataOwner6的最优x_6 = 0.3212
DataOwner7的最优x_7 = 0.3151
DataOwner8的最优x_8 = 0.3547
DataOwner9的最优x_9 = 0.2789
DataOwner10的最优x_10 = 0.3491
每个DataOwner应该贡献数据比例 xn_list = [0.31906932811025035, 0.07900609949201082, 0.3122274516550211, 0.276268244600945, 0.3500588406110763, 0.3212490328941043, 0.3151122814721143, 0.3546988300682885, 0.27894513046024166, 0.3490548662290885]
ModelOwner的最大效用 U(Eta) = 2.7533
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 3.3610
DataOwner1的最优x_1 = 0.3239
DataOwner2的最优x_2 = 0.0802
DataOwner3的最优x_3 = 0.3169
DataOwner4的最优x_4 = 0.2804
DataOwner5的最优x_5 = 0.3553
DataOwner6的最优x_6 = 0.3261
DataOwner7的最优x_7 = 0.3199
DataOwner8的最优x_8 = 0.3601
DataOwner9的最优x_9 = 0.2832
DataOwner10的最优x_10 = 0.3543
每个DataOwner应该贡献数据比例 xn_list = [0.32388761908276575, 0.08019917680378848, 0.31694242291754576, 0.2804401930548775, 0.355345106653359, 0.3261002397654472, 0.3198708167773378, 0.36005516494556583, 0.28315750277772694, 0.3543259711733906]
ModelOwner的最大效用 U(Eta) = 2.8218
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 3.4110
DataOwner1的最优x_1 = 0.3287
DataOwner2的最优x_2 = 0.0814
DataOwner3的最优x_3 = 0.3217
DataOwner4的最优x_4 = 0.2846
DataOwner5的最优x_5 = 0.3606
DataOwner6的最优x_6 = 0.3310
DataOwner7的最优x_7 = 0.3246
DataOwner8的最优x_8 = 0.3654
DataOwner9的最优x_9 = 0.2874
DataOwner10的最优x_10 = 0.3596
每个DataOwner应该贡献数据比例 xn_list = [0.3287059073209394, 0.08139225343646042, 0.3216573914923908, 0.2846121391389865, 0.3606313696855112, 0.3309514438735403, 0.3246293493697734, 0.36541149677965096, 0.2873698726973219, 0.35959707311675126]
ModelOwner的最大效用 U(Eta) = 2.8907
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 3.4610
DataOwner1的最优x_1 = 0.3335
DataOwner2的最优x_2 = 0.0826
DataOwner3的最优x_3 = 0.3264
DataOwner4的最优x_4 = 0.2888
DataOwner5的最优x_5 = 0.3659
DataOwner6的最优x_6 = 0.3358
DataOwner7的最优x_7 = 0.3294
DataOwner8的最优x_8 = 0.3708
DataOwner9的最优x_9 = 0.2916
DataOwner10的最优x_10 = 0.3649
每个DataOwner应该贡献数据比例 xn_list = [0.3335241859147773, 0.08258532766766195, 0.3263723506128811, 0.28878407680797274, 0.36591762205228445, 0.33580263816029776, 0.3293878723897473, 0.3707678178156636, 0.29158223411174033, 0.3648681644555248]
ModelOwner的最大效用 U(Eta) = 2.9603
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 3.5110
DataOwner1的最优x_1 = 0.3383
DataOwner2的最优x_2 = 0.0838
DataOwner3的最优x_3 = 0.3311
DataOwner4的最优x_4 = 0.2930
DataOwner5的最优x_5 = 0.3712
DataOwner6的最优x_6 = 0.3407
DataOwner7的最优x_7 = 0.3341
DataOwner8的最优x_8 = 0.3761
DataOwner9的最优x_9 = 0.2958
DataOwner10的最优x_10 = 0.3701
每个DataOwner应该贡献数据比例 xn_list = [0.33834377842730856, 0.08377872727398575, 0.3310885955367108, 0.29295715226715807, 0.3712053161255787, 0.34065515557766574, 0.334147693152198, 0.3761255996600575, 0.2957957443733026, 0.3701406933096992]
ModelOwner的最大效用 U(Eta) = 3.0303
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 3.5610
DataOwner1的最优x_1 = 0.3432
DataOwner2的最优x_2 = 0.0850
DataOwner3的最优x_3 = 0.3358
DataOwner4的最优x_4 = 0.2971
DataOwner5的最优x_5 = 0.3765
DataOwner6的最优x_6 = 0.3455
DataOwner7的最优x_7 = 0.3389
DataOwner8的最优x_8 = 0.3815
DataOwner9的最优x_9 = 0.3000
DataOwner10的最优x_10 = 0.3754
每个DataOwner应该贡献数据比例 xn_list = [0.3431607232988056, 0.08497147127107339, 0.33580224955555676, 0.297127935184046, 0.3764901053175838, 0.3455050071446381, 0.33890489904882315, 0.3814804381225031, 0.3000069398653112, 0.37541032564140103]
ModelOwner的最大效用 U(Eta) = 3.1009
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 3.6110
DataOwner1的最优x_1 = 0.3480
DataOwner2的最优x_2 = 0.0862
DataOwner3的最优x_3 = 0.3405
DataOwner4的最优x_4 = 0.3013
DataOwner5的最优x_5 = 0.3818
DataOwner6的最优x_6 = 0.3504
DataOwner7的最优x_7 = 0.3437
DataOwner8的最优x_8 = 0.3868
DataOwner9的最优x_9 = 0.3042
DataOwner10的最优x_10 = 0.3807
每个DataOwner应该贡献数据比例 xn_list = [0.3479789927828224, 0.08616454326121044, 0.3405171997912815, 0.30129986502997086, 0.38177634777782016, 0.3503561923762959, 0.34366341312256127, 0.3868367491121673, 0.30421929339001125, 0.38068140707248066]
ModelOwner的最大效用 U(Eta) = 3.1721
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 3.6610
DataOwner1的最优x_1 = 0.3528
DataOwner2的最优x_2 = 0.0874
DataOwner3的最优x_3 = 0.3452
DataOwner4的最优x_4 = 0.3055
DataOwner5的最优x_5 = 0.3871
DataOwner6的最优x_6 = 0.3552
DataOwner7的最优x_7 = 0.3484
DataOwner8的最优x_8 = 0.3922
DataOwner9的最优x_9 = 0.3084
DataOwner10的最优x_10 = 0.3860
每个DataOwner应该贡献数据比例 xn_list = [0.35279726346063756, 0.08735761554769546, 0.34523215119026923, 0.3054717959080958, 0.38706259154683376, 0.3552073788093446, 0.3484219283820946, 0.39219306143337096, 0.3084316479608637, 0.38595248980994457]
ModelOwner的最大效用 U(Eta) = 3.2437
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 3.7110
DataOwner1的最优x_1 = 0.3576
DataOwner2的最优x_2 = 0.0886
DataOwner3的最优x_3 = 0.3499
DataOwner4的最优x_4 = 0.3096
DataOwner5的最优x_5 = 0.3923
DataOwner6的最优x_6 = 0.3601
DataOwner7的最优x_7 = 0.3532
DataOwner8的最优x_8 = 0.3975
DataOwner9的最优x_9 = 0.3126
DataOwner10的最优x_10 = 0.3912
每个DataOwner应该贡献数据比例 xn_list = [0.3576155357515567, 0.08855068823212053, 0.3499471041658848, 0.30964372817975017, 0.3923488370832606, 0.36005856686455834, 0.353180445230708, 0.39754937554002084, 0.31264400394116004, 0.39122357430815263]
ModelOwner的最大效用 U(Eta) = 3.3159
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 3.7610
DataOwner1的最优x_1 = 0.3624
DataOwner2的最优x_2 = 0.0897
DataOwner3的最优x_3 = 0.3547
DataOwner4的最优x_4 = 0.3138
DataOwner5的最优x_5 = 0.3976
DataOwner6的最优x_6 = 0.3649
DataOwner7的最优x_7 = 0.3579
DataOwner8的最优x_8 = 0.4029
DataOwner9的最优x_9 = 0.3169
DataOwner10的最优x_10 = 0.3965
每个DataOwner应该贡献数据比例 xn_list = [0.3624336600395535, 0.08974372426874629, 0.35466191231260474, 0.31381553231055975, 0.39763492024101676, 0.364909605908504, 0.35793881591323934, 0.40290552512273065, 0.3168562305353252, 0.3964944968969787]
ModelOwner的最大效用 U(Eta) = 3.3886
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 3.8110
DataOwner1的最优x_1 = 0.3673
DataOwner2的最优x_2 = 0.0909
DataOwner3的最优x_3 = 0.3594
DataOwner4的最优x_4 = 0.3180
DataOwner5的最优x_5 = 0.4029
DataOwner6的最优x_6 = 0.3698
DataOwner7的最优x_7 = 0.3627
DataOwner8的最优x_8 = 0.4083
DataOwner9的最优x_9 = 0.3211
DataOwner10的最优x_10 = 0.4018
每个DataOwner应该贡献数据比例 xn_list = [0.3672520119084948, 0.09093681665936529, 0.35937694316744917, 0.31798753349161996, 0.4029212530888111, 0.3697608740884741, 0.3626974113639957, 0.4082619277032197, 0.32106865608806223, 0.4017656684557582]
ModelOwner的最大效用 U(Eta) = 3.4618
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 3.8610
DataOwner1的最优x_1 = 0.3721
DataOwner2的最优x_2 = 0.0921
DataOwner3的最优x_3 = 0.3641
DataOwner4的最优x_4 = 0.3222
DataOwner5的最优x_5 = 0.4082
DataOwner6的最优x_6 = 0.3746
DataOwner7的最优x_7 = 0.3675
DataOwner8的最优x_8 = 0.4136
DataOwner9的最优x_9 = 0.3253
DataOwner10的最优x_10 = 0.4070
每个DataOwner应该贡献数据比例 xn_list = [0.37207031199322244, 0.09212989622596339, 0.3640919233426565, 0.3221594898276067, 0.40820752911790076, 0.37461209012641494, 0.3674559556572929, 0.4136182727065526, 0.32528103636743017, 0.4070367833603374]
ModelOwner的最大效用 U(Eta) = 3.5355
DONE
最终的列表 alpha_U_Eta_list：
[-0.1648140522618546, -0.15805672752365196, -0.15129940278544934, -0.1445420780472467, -0.13778475330904408, -0.13102742857084143, -0.12427010383263881, -0.11751277909443616, -0.11075545435623355, -0.10399812961803091, -0.09724080487982828, -0.09048348014162565, -0.083726155403423, -0.07696883066522038, -0.07021150592701773, -0.06345418118881511, -0.05669685645061248, -0.049939531712409835, -0.043182206974207216, -0.03642488223600457, -0.02966755749780195, -0.022910232759599303, -0.016152908021396684, -0.009395583283194037, -0.00263825854499139, 0.004119032905762904, 0.010876411647744777, 0.01844332422501016, 0.027857028020618624, 0.03905684220641581, 0.0519811280222921, 0.0665715631493104, 0.08277683361523308, 0.10054403235857035, 0.11901187346571906, 0.1405774011361658, 0.1627599831684875, 0.1863333615434244, 0.2112560934391473, 0.23749476671082337, 0.26501563178920085, 0.2937866271084467, 0.323777255562926, 0.3549584727117632, 0.3873025853942553, 0.42078315956063683, 0.45537493630554904, 0.4910537552024823, 0.527796484165701, 0.5655809551610931, 0.6043859051875364, 0.6441909218434805, 0.6849763935114208, 0.7267234629160657, 0.7694139845298003, 0.8130304849988192, 0.8575561264709539, 0.9029746725444276, 0.9492704566029804, 0.9964283523440984, 1.0444337462919306, 1.0932725121556859, 1.1429309868714126, 1.1933959481685585, 1.2446545935923683, 1.2966945208016076, 1.3495037091330246, 1.40307050220773, 1.457383591650054, 1.5124320017146782, 1.568205074830233, 1.6246924579575612, 1.681884089722523, 1.7397701882589764, 1.7983412397259868, 1.8575879874348606, 1.9175014215647326, 1.9780727694167801, 2.039293486172788, 2.1011552461313485, 2.163649934385355, 2.226769638916421, 2.2905066430796928, 2.3548534184516057, 2.419802618045006, 2.4853470697896567, 2.5514797703991787, 2.6181938794747044, 2.6854827138224624, 2.7533397422267725, 2.8217585802081233, 2.8907329851768044, 2.9602568517538215, 3.030324207272694, 3.10092920760384, 3.1720661328410538, 3.2437293835875898, 3.3159134770866974, 3.3886130436331925, 3.461822823117016, 3.5355376616945]
最终的列表 alpha_U_qn_list：
[0.0017687862435153939, 0.0017687862435153939, 0.0017687862435153939, 0.0017687862435153939, 0.0017687862435153939, 0.0017687862435153939, 0.0017687862435153939, 0.0017687862435153939, 0.0017687862435153939, 0.0017687862435153939, 0.0017687862435153939, 0.0017687862435153939, 0.0017687862435153939, 0.0017687862435153939, 0.0017687862435153939, 0.0017687862435153939, 0.0017687862435153939, 0.0017687862435153939, 0.0017687862435153939, 0.0017687862435153939, 0.0017687862435153939, 0.0017687862435153939, 0.0017687862435153939, 0.0017687862435153939, 0.0017687862435153939, 0.0017687949033678263, 0.0017687095700842537, 0.002264783227345346, 0.002802997328925133, 0.0033379781610296698, 0.0038745694346860094, 0.004428358529188265, 0.0049477787273703834, 0.0054843789575267695, 0.006591670271161099, 0.006589948376590438, 0.007124139596268497, 0.007630774265324413, 0.00816742047534993, 0.008703988011215769, 0.009240579661061187, 0.009777182357711777, 0.010313783713509318, 0.010850394521377926, 0.011386993260240707, 0.01192358763839545, 0.01246018648836178, 0.012996787716710978, 0.013533405593650284, 0.014069975106963905, 0.01460660998420693, 0.015143200787192779, 0.015679797816753705, 0.016216394654439404, 0.016752984349363897, 0.017289588368718477, 0.017826190116393414, 0.018362791430700232, 0.018899386096147698, 0.01943600395831342, 0.01997259678699337, 0.0205091730699549, 0.021045778578167963, 0.021582396308016127, 0.022118997564215294, 0.02265559868691285, 0.023192198821173494, 0.02372879887739976, 0.024265399015482636, 0.02480198205843238, 0.025338609767682117, 0.025875205626742125, 0.02641183501417581, 0.02694840253697737, 0.027485002492006538, 0.028021610374980056, 0.028558186522513206, 0.029094794178785445, 0.029631398931212295, 0.030168001909508524, 0.03070460382750273, 0.031241205123942527, 0.031777805144847314, 0.032314420896838184, 0.032851011786905374, 0.03338761629808969, 0.03392420670515901, 0.034460813495854746, 0.034997375033047895, 0.035534013386819344, 0.03607061531302582, 0.036607216933875544, 0.03714381747415239, 0.03768056435676752, 0.038217016370303634, 0.03875361590293909, 0.03929021556862442, 0.03982681541364217, 0.040363398776307055, 0.04090000748434375, 0.04143661042464153]
**** log-parameter_analysis 运行时间： 2025-02-09 18:45:34 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.07582464139441149
DataOwner2: noise random: 0.01898566654403592
DataOwner3: noise random: 0.026384210782512064
DataOwner4: noise random: 0.03352976651003027
DataOwner5: noise random: 0.06614583533137484
DataOwner6: noise random: 0.07919196873041745
DataOwner7: noise random: 0.00859559962927241
DataOwner8: noise random: 0.00623362242541834
DataOwner9: noise random: 0.09370762025701061
DataOwner10: noise random: 0.030026115725405013
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9976695665143198, 0.9998541064124328, 0.9997171538356531, 0.9995467065063478, 0.9982203360011641, 0.9974689187963202, 0.9999701840864765, 0.999984256016873, 0.9964504925802413, 0.9996365840138529]
归一化后的数据质量列表avg_f_list: [0.934497893136856, 0.9963169689546536, 0.992441424390464, 0.9876180305113385, 0.9500838115697344, 0.9288198753069214, 0.9996017862924662, 1.0, 0.9, 0.9901614239533958]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3506
DataOwner1的最优x_1 = 0.0868
DataOwner2的最优x_2 = 0.1495
DataOwner3的最优x_3 = 0.1461
DataOwner4的最优x_4 = 0.1417
DataOwner5的最优x_5 = 0.1042
DataOwner6的最优x_6 = 0.0801
DataOwner7的最优x_7 = 0.1524
DataOwner8的最优x_8 = 0.1527
DataOwner9的最优x_9 = 0.0435
DataOwner10的最优x_10 = 0.1440
每个DataOwner应该贡献数据比例 xn_list = [0.08675631646145804, 0.14950380335134295, 0.14605007460512562, 0.14166932387229, 0.10422269683927916, 0.08008641528706674, 0.15238579315439144, 0.15273238035004377, 0.04348865922379096, 0.14399079833530207]
ModelOwner的最大效用 U(Eta) = 0.5920
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.350615632477057
DataOwner1的分配到的支付 ： 0.0932
DataOwner2的分配到的支付 ： 0.1712
DataOwner3的分配到的支付 ： 0.1666
DataOwner4的分配到的支付 ： 0.1608
DataOwner5的分配到的支付 ： 0.1138
DataOwner6的分配到的支付 ： 0.0855
DataOwner7的分配到的支付 ： 0.1751
DataOwner8的分配到的支付 ： 0.1755
DataOwner9的分配到的支付 ： 0.0450
DataOwner10的分配到的支付 ： 0.1639
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC10', 'DataOwner2': 'CPC7', 'DataOwner7': 'CPC8', 'DataOwner8': 'CPC9', 'DataOwner3': 'CPC6', 'DataOwner4': 'CPC4', 'DataOwner5': 'CPC3', 'DataOwner6': 'CPC2', 'DataOwner10': 'CPC5', 'DataOwner9': 'CPC1'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC10
DataOwner2 把数据交给 CPC7
DataOwner7 把数据交给 CPC8
DataOwner8 把数据交给 CPC9
DataOwner3 把数据交给 CPC6
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC3
DataOwner6 把数据交给 CPC2
DataOwner10 把数据交给 CPC5
DataOwner9 把数据交给 CPC1
DONE
最终Um的列表：
[('DataOwner1', 'CPC10', 1.0, 726, 0.0), ('DataOwner2', 'CPC7', 0.7, 1460, 0.044851141005402884), ('DataOwner7', 'CPC8', 0.8, 637, 0.030477158630878284), ('DataOwner8', 'CPC9', 0.9, 2344, 0.015273238035004383), ('DataOwner3', 'CPC6', 0.6, 611, 0.05842002984205025), ('DataOwner4', 'CPC4', 0.4, 197, 0.08500159432337401), ('DataOwner5', 'CPC3', 0.3, 145, 0.07295588778749541), ('DataOwner6', 'CPC2', 0.2, 335, 0.0640691322296534), ('DataOwner10', 'CPC5', 0.5, 1205, 0.07199539916765103), ('DataOwner9', 'CPC1', 0.1, 121, 0.03913979330141186)]
('DataOwner1', 'CPC10', 1.0, 726, 0.0)
('DataOwner2', 'CPC7', 0.7, 1460, 0.044851141005402884)
('DataOwner7', 'CPC8', 0.8, 637, 0.030477158630878284)
('DataOwner8', 'CPC9', 0.9, 2344, 0.015273238035004383)
('DataOwner3', 'CPC6', 0.6, 611, 0.05842002984205025)
('DataOwner4', 'CPC4', 0.4, 197, 0.08500159432337401)
('DataOwner5', 'CPC3', 0.3, 145, 0.07295588778749541)
('DataOwner6', 'CPC2', 0.2, 335, 0.0640691322296534)
('DataOwner10', 'CPC5', 0.5, 1205, 0.07199539916765103)
('DataOwner9', 'CPC1', 0.1, 121, 0.03913979330141186)
**** log-parameter_analysis 运行时间： 2025-02-09 18:52:42 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.08459381113618909
DataOwner2: noise random: 0.03029653319883796
DataOwner3: noise random: 0.009667929394638952
DataOwner4: noise random: 0.09267253222647881
DataOwner5: noise random: 0.025376404336064952
DataOwner6: noise random: 0.011878797254417872
DataOwner7: noise random: 0.033509464865269734
DataOwner8: noise random: 0.02472542831419422
DataOwner9: noise random: 0.0005646034694596014
DataOwner10: noise random: 0.011858302197075666
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9971079540935821, 0.999628694480997, 0.9999621813221932, 0.996523815134361, 0.9997395938975664, 0.9999429643564608, 0.9995450312539188, 0.9997524862293916, 0.9999998709528964, 0.9999431303605022]
归一化后的数据质量列表avg_f_list: [0.9168046484209584, 0.989321907032672, 0.9989157357456028, 0.9, 0.9925122878078607, 0.998362897507796, 0.9869150634304467, 0.992883177474145, 1.0, 0.9983676731515161]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3617
DataOwner1的最优x_1 = 0.0545
DataOwner2的最优x_2 = 0.1355
DataOwner3的最优x_3 = 0.1444
DataOwner4的最优x_4 = 0.0318
DataOwner5的最优x_5 = 0.1385
DataOwner6的最优x_6 = 0.1439
DataOwner7的最优x_7 = 0.1332
DataOwner8的最优x_8 = 0.1388
DataOwner9的最优x_9 = 0.1454
DataOwner10的最优x_10 = 0.1439
每个DataOwner应该贡献数据比例 xn_list = [0.054542891515138255, 0.13546747695299388, 0.14437887692277518, 0.03178187655139729, 0.13847209100337027, 0.1438753456674008, 0.13317311947875424, 0.13881869809878955, 0.14536294983560838, 0.14387970053189683]
ModelOwner的最大效用 U(Eta) = 0.6051
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.36166746720408
DataOwner1的分配到的支付 ： 0.0569
DataOwner2的分配到的支付 ： 0.1526
DataOwner3的分配到的支付 ： 0.1642
DataOwner4的分配到的支付 ： 0.0326
DataOwner5的分配到的支付 ： 0.1564
DataOwner6的分配到的支付 ： 0.1635
DataOwner7的分配到的支付 ： 0.1496
DataOwner8的分配到的支付 ： 0.1569
DataOwner9的分配到的支付 ： 0.1655
DataOwner10的分配到的支付 ： 0.1635
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC10', 'DataOwner2': 'CPC3', 'DataOwner3': 'CPC8', 'DataOwner9': 'CPC9', 'DataOwner5': 'CPC4', 'DataOwner6': 'CPC6', 'DataOwner4': 'CPC1', 'DataOwner8': 'CPC5', 'DataOwner10': 'CPC7', 'DataOwner7': 'CPC2'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC10
DataOwner2 把数据交给 CPC3
DataOwner3 把数据交给 CPC8
DataOwner9 把数据交给 CPC9
DataOwner5 把数据交给 CPC4
DataOwner6 把数据交给 CPC6
DataOwner4 把数据交给 CPC1
DataOwner8 把数据交给 CPC5
DataOwner10 把数据交给 CPC7
DataOwner7 把数据交给 CPC2
DONE
最终Um的列表：
[('DataOwner1', 'CPC10', 1.0, 303, 0.0), ('DataOwner2', 'CPC3', 0.3, 1354, 0.09482723386709571), ('DataOwner3', 'CPC8', 0.8, 320, 0.028875775384555033), ('DataOwner9', 'CPC9', 0.9, 1130, 0.014536294983560821), ('DataOwner5', 'CPC4', 0.4, 153, 0.08308325460202215), ('DataOwner6', 'CPC6', 0.6, 799, 0.057550138266960316), ('DataOwner4', 'CPC1', 0.1, 35, 0.02860368889625756), ('DataOwner8', 'CPC5', 0.5, 1079, 0.06940934904939478), ('DataOwner10', 'CPC7', 0.7, 639, 0.04316391015956905), ('DataOwner7', 'CPC2', 0.2, 1923, 0.10653849558300339)]
('DataOwner1', 'CPC10', 1.0, 303, 0.0)
('DataOwner2', 'CPC3', 0.3, 1354, 0.09482723386709571)
('DataOwner3', 'CPC8', 0.8, 320, 0.028875775384555033)
('DataOwner9', 'CPC9', 0.9, 1130, 0.014536294983560821)
('DataOwner5', 'CPC4', 0.4, 153, 0.08308325460202215)
('DataOwner6', 'CPC6', 0.6, 799, 0.057550138266960316)
('DataOwner4', 'CPC1', 0.1, 35, 0.02860368889625756)
('DataOwner8', 'CPC5', 0.5, 1079, 0.06940934904939478)
('DataOwner10', 'CPC7', 0.7, 639, 0.04316391015956905)
('DataOwner7', 'CPC2', 0.2, 1923, 0.10653849558300339)
**** log-parameter_analysis 运行时间： 2025-02-09 18:54:06 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.09693840934990718
DataOwner2: noise random: 0.061797451261797934
DataOwner3: noise random: 0.06434022900718432
DataOwner4: noise random: 0.03379468954578576
DataOwner5: noise random: 0.05306175262939566
DataOwner6: noise random: 0.005588479253931133
DataOwner7: noise random: 0.055702875311627244
DataOwner8: noise random: 0.06410988764833624
DataOwner9: noise random: 0.07751803898308485
DataOwner10: noise random: 0.0653354312274928
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.996205729677715, 0.9984547058239273, 0.9983232023197786, 0.9995385957479197, 0.9988593211584249, 0.9999873322377667, 0.9987400393327273, 0.9983372697004529, 0.9975603337445985, 0.9982743939023105]
归一化后的数据质量列表avg_f_list: [0.9, 0.9594715100410102, 0.9559940556533417, 0.9881336951009239, 0.9701710832529586, 1.0, 0.9670168166740822, 0.9563660508710025, 0.9358208998796789, 0.9547033748720343]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3402
DataOwner1的最优x_1 = 0.0539
DataOwner2的最优x_2 = 0.1221
DataOwner3的最优x_3 = 0.1185
DataOwner4的最优x_4 = 0.1490
DataOwner5的最优x_5 = 0.1325
DataOwner6的最优x_6 = 0.1592
DataOwner7的最优x_7 = 0.1295
DataOwner8的最优x_8 = 0.1189
DataOwner9的最优x_9 = 0.0971
DataOwner10的最优x_10 = 0.1172
每个DataOwner应该贡献数据比例 xn_list = [0.05386742781169893, 0.1220508137752193, 0.1185433125942405, 0.14899546817256995, 0.13251006509463095, 0.15919316373469952, 0.12947794861155296, 0.11892111237392823, 0.09708980863188578, 0.1172276433265595]
ModelOwner的最大效用 U(Eta) = 0.5799
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3402323479804024
DataOwner1的分配到的支付 ： 0.0562
DataOwner2的分配到的支付 ： 0.1358
DataOwner3的分配到的支付 ： 0.1314
DataOwner4的分配到的支付 ： 0.1707
DataOwner5的分配到的支付 ： 0.1491
DataOwner6的分配到的支付 ： 0.1846
DataOwner7的分配到的支付 ： 0.1452
DataOwner8的分配到的支付 ： 0.1319
DataOwner9的分配到的支付 ： 0.1054
DataOwner10的分配到的支付 ： 0.1298
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC1', 'DataOwner10': 'CPC10', 'DataOwner2': 'CPC5', 'DataOwner4': 'CPC8', 'DataOwner6': 'CPC9', 'DataOwner3': 'CPC3', 'DataOwner5': 'CPC7', 'DataOwner7': 'CPC6', 'DataOwner8': 'CPC4', 'DataOwner9': 'CPC2'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner10 把数据交给 CPC10
DataOwner2 把数据交给 CPC5
DataOwner4 把数据交给 CPC8
DataOwner6 把数据交给 CPC9
DataOwner3 把数据交给 CPC3
DataOwner5 把数据交给 CPC7
DataOwner7 把数据交给 CPC6
DataOwner8 把数据交给 CPC4
DataOwner9 把数据交给 CPC2
DONE
最终Um的列表：
[('DataOwner1', 'CPC1', 0.1, 323, 0.04848068503052904), ('DataOwner10', 'CPC10', 1.0, 140, 1.3877787807814457e-17), ('DataOwner2', 'CPC5', 0.5, 585, 0.06102540688760965), ('DataOwner4', 'CPC8', 0.8, 1251, 0.029799093634513982), ('DataOwner6', 'CPC9', 0.9, 191, 0.01591931637346994), ('DataOwner3', 'CPC3', 0.3, 1849, 0.08298031881596835), ('DataOwner5', 'CPC7', 0.7, 795, 0.03975301952838929), ('DataOwner7', 'CPC6', 0.6, 155, 0.05179117944462118), ('DataOwner8', 'CPC4', 0.4, 1712, 0.07135266742435693), ('DataOwner9', 'CPC2', 0.2, 116, 0.0776718469055086)]
('DataOwner1', 'CPC1', 0.1, 323, 0.04848068503052904)
('DataOwner10', 'CPC10', 1.0, 140, 1.3877787807814457e-17)
('DataOwner2', 'CPC5', 0.5, 585, 0.06102540688760965)
('DataOwner4', 'CPC8', 0.8, 1251, 0.029799093634513982)
('DataOwner6', 'CPC9', 0.9, 191, 0.01591931637346994)
('DataOwner3', 'CPC3', 0.3, 1849, 0.08298031881596835)
('DataOwner5', 'CPC7', 0.7, 795, 0.03975301952838929)
('DataOwner7', 'CPC6', 0.6, 155, 0.05179117944462118)
('DataOwner8', 'CPC4', 0.4, 1712, 0.07135266742435693)
('DataOwner9', 'CPC2', 0.2, 116, 0.0776718469055086)
**** log-parameter_analysis 运行时间： 2025-02-09 19:08:10 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.07254214541641157
DataOwner2: noise random: 0.09587367479995847
DataOwner3: noise random: 0.028054705260018787
DataOwner4: noise random: 0.034138451439093165
DataOwner5: noise random: 0.026219799128688038
DataOwner6: noise random: 0.018665658949800025
DataOwner7: noise random: 0.05232411965273307
DataOwner8: noise random: 0.07265021604354154
DataOwner9: noise random: 0.04951834719668641
DataOwner10: noise random: 0.03631763555925575
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9978668050719289, 0.996283857944324, 0.9996819364204118, 0.9995280364916357, 0.9997223138689979, 0.9998586017998342, 0.9988918049349278, 0.9978506176818929, 0.9990111117245248, 0.9994667347381982]
归一化后的数据质量列表avg_f_list: [0.9442814140421547, 0.9, 0.9950579569736142, 0.9907527553984329, 0.996187477023669, 1.0, 0.9729547932947377, 0.9438285874707877, 0.9762922852779223, 0.9890378981690696]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3543
DataOwner1的最优x_1 = 0.0947
DataOwner2的最优x_2 = 0.0396
DataOwner3的最优x_3 = 0.1459
DataOwner4的最优x_4 = 0.1420
DataOwner5的最优x_5 = 0.1469
DataOwner6的最优x_6 = 0.1503
DataOwner7的最优x_7 = 0.1250
DataOwner8的最优x_8 = 0.0942
DataOwner9的最优x_9 = 0.1283
DataOwner10的最优x_10 = 0.1404
每个DataOwner应该贡献数据比例 xn_list = [0.09468776957218666, 0.03960826319870795, 0.14589398568251094, 0.1419798268130368, 0.14690883438029725, 0.15029775970835646, 0.12499590329139829, 0.09417772674153646, 0.1282821190433201, 0.14040019463334993]
ModelOwner的最大效用 U(Eta) = 0.5964
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.354340514785708
DataOwner1的分配到的支付 ： 0.1024
DataOwner2的分配到的支付 ： 0.0408
DataOwner3的分配到的支付 ： 0.1663
DataOwner4的分配到的支付 ： 0.1612
DataOwner5的分配到的支付 ： 0.1677
DataOwner6的分配到的支付 ： 0.1722
DataOwner7的分配到的支付 ： 0.1393
DataOwner8的分配到的支付 ： 0.1018
DataOwner9的分配到的支付 ： 0.1435
DataOwner10的分配到的支付 ： 0.1591
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC10', 'DataOwner2': 'CPC1', 'DataOwner3': 'CPC7', 'DataOwner5': 'CPC8', 'DataOwner6': 'CPC9', 'DataOwner4': 'CPC6', 'DataOwner7': 'CPC3', 'DataOwner9': 'CPC4', 'DataOwner10': 'CPC5', 'DataOwner8': 'CPC2'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC10
DataOwner2 把数据交给 CPC1
DataOwner3 把数据交给 CPC7
DataOwner5 把数据交给 CPC8
DataOwner6 把数据交给 CPC9
DataOwner4 把数据交给 CPC6
DataOwner7 把数据交给 CPC3
DataOwner9 把数据交给 CPC4
DataOwner10 把数据交给 CPC5
DataOwner8 把数据交给 CPC2
DONE
最终Um的列表：
[('DataOwner1', 'CPC10', 1.0, 299, 0.0), ('DataOwner2', 'CPC1', 0.1, 125, 0.03564743687883715), ('DataOwner3', 'CPC7', 0.7, 690, 0.04376819570475329), ('DataOwner5', 'CPC8', 0.8, 1391, 0.02938176687605945), ('DataOwner6', 'CPC9', 0.9, 711, 0.01502977597083563), ('DataOwner4', 'CPC6', 0.6, 896, 0.056791930725214726), ('DataOwner7', 'CPC3', 0.3, 1381, 0.08749713230397879), ('DataOwner9', 'CPC4', 0.4, 404, 0.07696927142599205), ('DataOwner10', 'CPC5', 0.5, 1773, 0.07020009731667497), ('DataOwner8', 'CPC2', 0.2, 148, 0.07534218139322915)]
('DataOwner1', 'CPC10', 1.0, 299, 0.0)
('DataOwner2', 'CPC1', 0.1, 125, 0.03564743687883715)
('DataOwner3', 'CPC7', 0.7, 690, 0.04376819570475329)
('DataOwner5', 'CPC8', 0.8, 1391, 0.02938176687605945)
('DataOwner6', 'CPC9', 0.9, 711, 0.01502977597083563)
('DataOwner4', 'CPC6', 0.6, 896, 0.056791930725214726)
('DataOwner7', 'CPC3', 0.3, 1381, 0.08749713230397879)
('DataOwner9', 'CPC4', 0.4, 404, 0.07696927142599205)
('DataOwner10', 'CPC5', 0.5, 1773, 0.07020009731667497)
('DataOwner8', 'CPC2', 0.2, 148, 0.07534218139322915)
**** log-parameter_analysis 运行时间： 2025-02-09 19:11:06 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.06361309128306514
DataOwner2: noise random: 0.08028901986376166
DataOwner3: noise random: 0.044050821543942256
DataOwner4: noise random: 0.06447982858375294
DataOwner5: noise random: 0.02364714180656412
DataOwner6: noise random: 0.05307096212579018
DataOwner7: noise random: 0.07626190785254044
DataOwner8: noise random: 0.07078673836608583
DataOwner9: noise random: 0.02203755954750174
DataOwner10: noise random: 0.010928843292961066
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9983605807577925, 0.9973917869333465, 0.9992138891405009, 0.998317047069139, 0.9997740212196918, 0.99886001487521, 0.9976510365362236, 0.9979654409560642, 0.9998032810763391, 0.9999516503219479]
归一化后的数据质量列表avg_f_list: [0.9378455283496732, 0.9, 0.9711796658863869, 0.9361449028847614, 0.9930609929011447, 0.9573557146995143, 0.9101274780533772, 0.9224095561220993, 0.9942040170475716, 1.0]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3316
DataOwner1的最优x_1 = 0.1062
DataOwner2的最优x_2 = 0.0621
DataOwner3的最优x_3 = 0.1393
DataOwner4的最优x_4 = 0.1044
DataOwner5的最优x_5 = 0.1585
DataOwner6的最优x_6 = 0.1262
DataOwner7的最优x_7 = 0.0746
DataOwner8的最优x_8 = 0.0891
DataOwner9的最优x_9 = 0.1595
DataOwner10的最优x_10 = 0.1643
每个DataOwner应该贡献数据比例 xn_list = [0.10620868707567994, 0.06207879215550284, 0.1393220491161625, 0.10438329109431295, 0.15853782019673618, 0.1261846621598327, 0.07463926027215295, 0.08911664455256792, 0.1594914274704027, 0.16425363728107478]
ModelOwner的最大效用 U(Eta) = 0.5701
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3316102166868402
DataOwner1的分配到的支付 ： 0.1164
DataOwner2的分配到的支付 ： 0.0653
DataOwner3的分配到的支付 ： 0.1581
DataOwner4的分配到的支付 ： 0.1142
DataOwner5的分配到的支付 ： 0.1839
DataOwner6的分配到的支付 ： 0.1411
DataOwner7的分配到的支付 ： 0.0794
DataOwner8的分配到的支付 ： 0.0960
DataOwner9的分配到的支付 ： 0.1853
DataOwner10的分配到的支付 ： 0.1919
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC4', 'DataOwner2': 'CPC10', 'DataOwner3': 'CPC6', 'DataOwner5': 'CPC7', 'DataOwner9': 'CPC8', 'DataOwner10': 'CPC9', 'DataOwner4': 'CPC3', 'DataOwner6': 'CPC5', 'DataOwner7': 'CPC1', 'DataOwner8': 'CPC2'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC4
DataOwner2 把数据交给 CPC10
DataOwner3 把数据交给 CPC6
DataOwner5 把数据交给 CPC7
DataOwner9 把数据交给 CPC8
DataOwner10 把数据交给 CPC9
DataOwner4 把数据交给 CPC3
DataOwner6 把数据交给 CPC5
DataOwner7 把数据交给 CPC1
DataOwner8 把数据交给 CPC2
DONE
最终Um的列表：
[('DataOwner1', 'CPC4', 0.4, 1509, 0.06372521224540796), ('DataOwner2', 'CPC10', 1.0, 195, 6.938893903907228e-18), ('DataOwner3', 'CPC6', 0.6, 1869, 0.055728819646465005), ('DataOwner5', 'CPC7', 0.7, 250, 0.04756134605902086), ('DataOwner9', 'CPC8', 0.8, 881, 0.031898285494080525), ('DataOwner10', 'CPC9', 0.9, 777, 0.01642536372810746), ('DataOwner4', 'CPC3', 0.3, 412, 0.07306830376601907), ('DataOwner6', 'CPC5', 0.5, 1295, 0.06309233107991635), ('DataOwner7', 'CPC1', 0.1, 117, 0.06717533424493766), ('DataOwner8', 'CPC2', 0.2, 140, 0.07129331564205434)]
('DataOwner1', 'CPC4', 0.4, 1509, 0.06372521224540796)
('DataOwner2', 'CPC10', 1.0, 195, 6.938893903907228e-18)
('DataOwner3', 'CPC6', 0.6, 1869, 0.055728819646465005)
('DataOwner5', 'CPC7', 0.7, 250, 0.04756134605902086)
('DataOwner9', 'CPC8', 0.8, 881, 0.031898285494080525)
('DataOwner10', 'CPC9', 0.9, 777, 0.01642536372810746)
('DataOwner4', 'CPC3', 0.3, 412, 0.07306830376601907)
('DataOwner6', 'CPC5', 0.5, 1295, 0.06309233107991635)
('DataOwner7', 'CPC1', 0.1, 117, 0.06717533424493766)
('DataOwner8', 'CPC2', 0.2, 140, 0.07129331564205434)
**** log-parameter_analysis 运行时间： 2025-02-09 19:11:53 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.04837039608800501
DataOwner2: noise random: 0.06905595931787017
DataOwner3: noise random: 0.08843425640198209
DataOwner4: noise random: 0.00199338192365357
DataOwner5: noise random: 0.041272945523212035
DataOwner6: noise random: 0.023328085130141854
DataOwner7: noise random: 0.004316824949601539
DataOwner8: noise random: 0.05554301614927326
DataOwner9: noise random: 0.03748859697276017
DataOwner10: noise random: 0.0854133800779101
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9990545568063846, 0.9980726052639104, 0.9968318981180471, 0.9999983798851042, 0.99931185879599, 0.9997797444108035, 0.999992471681385, 0.9987525651978033, 0.9994313557445308, 0.997047125999377]
归一化后的数据质量列表avg_f_list: [0.9701933202793452, 0.9391825135003514, 0.9, 1.0, 0.9783191207270967, 0.9930953187043338, 0.9998134142510904, 0.9606561862991958, 0.9820929289259609, 0.9067970668130488]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3447
DataOwner1的最优x_1 = 0.1294
DataOwner2的最优x_2 = 0.0971
DataOwner3的最优x_3 = 0.0495
DataOwner4的最优x_4 = 0.1565
DataOwner5的最优x_5 = 0.1371
DataOwner6的最优x_6 = 0.1505
DataOwner7的最优x_7 = 0.1563
DataOwner8的最优x_8 = 0.1199
DataOwner9的最优x_9 = 0.1406
DataOwner10的最优x_10 = 0.0584
每个DataOwner应该贡献数据比例 xn_list = [0.12937795142263378, 0.09714330899650198, 0.04948377315922044, 0.1564723971642024, 0.13711917891914016, 0.1505072348577322, 0.15631355532364516, 0.11993139004269093, 0.14062140309273619, 0.05836569704999775]
ModelOwner的最大效用 U(Eta) = 0.5851
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3446807730405532
DataOwner1的分配到的支付 ： 0.1450
DataOwner2的分配到的支付 ： 0.1054
DataOwner3的分配到的支付 ： 0.0515
DataOwner4的分配到的支付 ： 0.1808
DataOwner5的分配到的支付 ： 0.1550
DataOwner6的分配到的支付 ： 0.1727
DataOwner7的分配到的支付 ： 0.1806
DataOwner8的分配到的支付 ： 0.1331
DataOwner9的分配到的支付 ： 0.1596
DataOwner10的分配到的支付 ： 0.0611
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC7', 'DataOwner3': 'CPC10', 'DataOwner2': 'CPC6', 'DataOwner6': 'CPC9', 'DataOwner9': 'CPC8', 'DataOwner4': 'CPC3', 'DataOwner5': 'CPC1', 'DataOwner7': 'CPC4', 'DataOwner8': 'CPC5', 'DataOwner10': 'CPC2'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC7
DataOwner3 把数据交给 CPC10
DataOwner2 把数据交给 CPC6
DataOwner6 把数据交给 CPC9
DataOwner9 把数据交给 CPC8
DataOwner4 把数据交给 CPC3
DataOwner5 把数据交给 CPC1
DataOwner7 把数据交给 CPC4
DataOwner8 把数据交给 CPC5
DataOwner10 把数据交给 CPC2
DONE
最终Um的列表：
[('DataOwner1', 'CPC7', 0.7, 849, 0.03881338542679014), ('DataOwner3', 'CPC10', 1.0, 231, 0.0), ('DataOwner2', 'CPC6', 0.6, 637, 0.03885732359860079), ('DataOwner6', 'CPC9', 0.9, 2398, 0.015050723485773188), ('DataOwner9', 'CPC8', 0.8, 1450, 0.02812428061854723), ('DataOwner4', 'CPC3', 0.3, 293, 0.10953067801494168), ('DataOwner5', 'CPC1', 0.1, 257, 0.12340726102722614), ('DataOwner7', 'CPC4', 0.4, 439, 0.09378813319418709), ('DataOwner8', 'CPC5', 0.5, 562, 0.059965695021345466), ('DataOwner10', 'CPC2', 0.2, 273, 0.0466925576399982)]
('DataOwner1', 'CPC7', 0.7, 849, 0.03881338542679014)
('DataOwner3', 'CPC10', 1.0, 231, 0.0)
('DataOwner2', 'CPC6', 0.6, 637, 0.03885732359860079)
('DataOwner6', 'CPC9', 0.9, 2398, 0.015050723485773188)
('DataOwner9', 'CPC8', 0.8, 1450, 0.02812428061854723)
('DataOwner4', 'CPC3', 0.3, 293, 0.10953067801494168)
('DataOwner5', 'CPC1', 0.1, 257, 0.12340726102722614)
('DataOwner7', 'CPC4', 0.4, 439, 0.09378813319418709)
('DataOwner8', 'CPC5', 0.5, 562, 0.059965695021345466)
('DataOwner10', 'CPC2', 0.2, 273, 0.0466925576399982)
**** log-parameter_analysis 运行时间： 2025-02-09 19:23:30 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.08149154138214318
DataOwner2: noise random: 0.049719910637905464
DataOwner3: noise random: 0.04309304165936381
DataOwner4: noise random: 0.055793541150545245
DataOwner5: noise random: 0.0872935759391693
DataOwner6: noise random: 0.07438749443666968
DataOwner7: noise random: 0.06779431891550654
DataOwner8: noise random: 0.014887316756001768
DataOwner9: noise random: 0.06702529789347654
DataOwner10: noise random: 0.046895674148278124
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9973152801526662, 0.9989994711649065, 0.9992510304255016, 0.9987448222013949, 0.9969217825269789, 0.9977611023507389, 0.9981407948864526, 0.9999103462597965, 0.9981823483636357, 0.9991104694131704]
归一化后的数据质量列表avg_f_list: [0.9131667804626781, 0.9695213093538007, 0.9779387059056184, 0.9610005285949611, 0.9, 0.9280843876455889, 0.9407892375219447, 1.0, 0.9421796538188051, 0.9732354094429173]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3300
DataOwner1的最优x_1 = 0.0797
DataOwner2的最优x_2 = 0.1389
DataOwner3的最优x_3 = 0.1465
DataOwner4的最优x_4 = 0.1308
DataOwner5的最优x_5 = 0.0636
DataOwner6的最优x_6 = 0.0969
DataOwner7的最优x_7 = 0.1106
DataOwner8的最优x_8 = 0.1652
DataOwner9的最优x_9 = 0.1120
DataOwner10的最优x_10 = 0.1423
每个DataOwner应该贡献数据比例 xn_list = [0.07970892567762398, 0.13886312220314434, 0.14648787270283223, 0.13084854152218436, 0.06358026328216503, 0.0968543038025053, 0.11057319307638276, 0.16517361113704085, 0.11202755803181354, 0.14226263137155226]
ModelOwner的最大效用 U(Eta) = 0.5682
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.329990465350851
DataOwner1的分配到的支付 ： 0.0852
DataOwner2的分配到的支付 ： 0.1575
DataOwner3的分配到的支付 ： 0.1676
DataOwner4的分配到的支付 ： 0.1471
DataOwner5的分配到的支付 ： 0.0670
DataOwner6的分配到的支付 ： 0.1052
DataOwner7的分配到的支付 ： 0.1217
DataOwner8的分配到的支付 ： 0.1933
DataOwner9的分配到的支付 ： 0.1235
DataOwner10的分配到的支付 ： 0.1620
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC7', 'DataOwner2': 'CPC5', 'DataOwner3': 'CPC3', 'DataOwner4': 'CPC1', 'DataOwner5': 'CPC10', 'DataOwner8': 'CPC9', 'DataOwner6': 'CPC6', 'DataOwner9': 'CPC8', 'DataOwner7': 'CPC4', 'DataOwner10': 'CPC2'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC7
DataOwner2 把数据交给 CPC5
DataOwner3 把数据交给 CPC3
DataOwner4 把数据交给 CPC1
DataOwner5 把数据交给 CPC10
DataOwner8 把数据交给 CPC9
DataOwner6 把数据交给 CPC6
DataOwner9 把数据交给 CPC8
DataOwner7 把数据交给 CPC4
DataOwner10 把数据交给 CPC2
DONE
最终Um的列表：
[('DataOwner1', 'CPC7', 0.7, 597, 0.023912677703287186), ('DataOwner2', 'CPC5', 0.5, 416, 0.06943156110157217), ('DataOwner3', 'CPC3', 0.3, 219, 0.10254151089198256), ('DataOwner4', 'CPC1', 0.1, 196, 0.11776368736996592), ('DataOwner5', 'CPC10', 1.1, 95, -0.006358026328216512), ('DataOwner8', 'CPC9', 0.9, 2725, 0.016517361113704082), ('DataOwner6', 'CPC6', 0.6, 581, 0.03874172152100212), ('DataOwner9', 'CPC8', 0.8, 2016, 0.0224055116063627), ('DataOwner7', 'CPC4', 0.4, 331, 0.06634391584582963), ('DataOwner10', 'CPC2', 0.2, 213, 0.1138101050972418)]
('DataOwner1', 'CPC7', 0.7, 597, 0.023912677703287186)
('DataOwner2', 'CPC5', 0.5, 416, 0.06943156110157217)
('DataOwner3', 'CPC3', 0.3, 219, 0.10254151089198256)
('DataOwner4', 'CPC1', 0.1, 196, 0.11776368736996592)
('DataOwner5', 'CPC10', 1.1, 95, -0.006358026328216512)
('DataOwner8', 'CPC9', 0.9, 2725, 0.016517361113704082)
('DataOwner6', 'CPC6', 0.6, 581, 0.03874172152100212)
('DataOwner9', 'CPC8', 0.8, 2016, 0.0224055116063627)
('DataOwner7', 'CPC4', 0.4, 331, 0.06634391584582963)
('DataOwner10', 'CPC2', 0.2, 213, 0.1138101050972418)
**** log-parameter_analysis 运行时间： 2025-02-10 10:54:59 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.05736727667600719
DataOwner2: noise random: 0.07603682367942513
DataOwner3: noise random: 0.052111071227514176
DataOwner4: noise random: 0.09704354042381803
DataOwner5: noise random: 0.05854855276575114
DataOwner6: noise random: 0.09819639995742085
DataOwner7: noise random: 0.0606572981863209
DataOwner8: noise random: 0.009449441221370548
DataOwner9: noise random: 0.06162843437700436
DataOwner10: noise random: 0.05723731978762148
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9986673684708823, 0.9976600090855816, 0.9988976784706123, 0.9962035492294075, 0.9986135524727339, 0.996092070438446, 0.9985116043505574, 0.9999638221242702, 0.9984655659402987, 0.9986719708384243]
归一化后的数据质量列表avg_f_list: [0.9665150619515551, 0.9404968803365246, 0.9724635322672833, 0.9028792856569178, 0.9651250968268416, 0.9, 0.9624919702616819, 1.0, 0.9613028854754001, 0.9666339323728895]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3339
DataOwner1的最优x_1 = 0.1334
DataOwner2的最优x_2 = 0.1072
DataOwner3的最优x_3 = 0.1390
DataOwner4的最优x_4 = 0.0636
DataOwner5的最优x_5 = 0.1321
DataOwner6的最优x_6 = 0.0599
DataOwner7的最优x_7 = 0.1296
DataOwner8的最优x_8 = 0.1629
DataOwner9的最优x_9 = 0.1284
DataOwner10的最优x_10 = 0.1335
每个DataOwner应该贡献数据比例 xn_list = [0.1334108670445974, 0.10724648658791597, 0.13897912910932533, 0.06358222291507984, 0.13208842166881124, 0.05992343388373061, 0.1295607062004777, 0.1629300602840907, 0.12840949116697653, 0.13352358474750314]
ModelOwner的最大效用 U(Eta) = 0.5727
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3339114556468146
DataOwner1的分配到的支付 ： 0.1504
DataOwner2的分配到的支付 ： 0.1176
DataOwner3的分配到的支付 ： 0.1576
DataOwner4的分配到的支付 ： 0.0669
DataOwner5的分配到的支付 ： 0.1487
DataOwner6的分配到的支付 ： 0.0629
DataOwner7的分配到的支付 ： 0.1454
DataOwner8的分配到的支付 ： 0.1900
DataOwner9的分配到的支付 ： 0.1439
DataOwner10的分配到的支付 ： 0.1505
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC7', 'DataOwner6': 'CPC1', 'DataOwner8': 'CPC10', 'DataOwner2': 'CPC3', 'DataOwner3': 'CPC9', 'DataOwner5': 'CPC6', 'DataOwner10': 'CPC8', 'DataOwner4': 'CPC2', 'DataOwner7': 'CPC5', 'DataOwner9': 'CPC4'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC7
DataOwner6 把数据交给 CPC1
DataOwner8 把数据交给 CPC10
DataOwner2 把数据交给 CPC3
DataOwner3 把数据交给 CPC9
DataOwner5 把数据交给 CPC6
DataOwner10 把数据交给 CPC8
DataOwner4 把数据交给 CPC2
DataOwner7 把数据交给 CPC5
DataOwner9 把数据交给 CPC4
DONE
最终Um的列表：
[('DataOwner1', 'CPC7', 0.7, 213, 0.04002326011337923), ('DataOwner6', 'CPC1', 0.1, 767, 0.053931090495357555), ('DataOwner8', 'CPC10', 1.0, 130, 2.7755575615628914e-17), ('DataOwner2', 'CPC3', 0.3, 1115, 0.07507254061154117), ('DataOwner3', 'CPC9', 0.9, 1111, 0.013897912910932536), ('DataOwner5', 'CPC6', 0.6, 845, 0.0528353686675245), ('DataOwner10', 'CPC8', 0.8, 106, 0.02670471694950062), ('DataOwner4', 'CPC2', 0.2, 203, 0.05086577833206387), ('DataOwner7', 'CPC5', 0.5, 414, 0.06478035310023884), ('DataOwner9', 'CPC4', 0.4, 1643, 0.07704569470018591)]
('DataOwner1', 'CPC7', 0.7, 213, 0.04002326011337923)
('DataOwner6', 'CPC1', 0.1, 767, 0.053931090495357555)
('DataOwner8', 'CPC10', 1.0, 130, 2.7755575615628914e-17)
('DataOwner2', 'CPC3', 0.3, 1115, 0.07507254061154117)
('DataOwner3', 'CPC9', 0.9, 1111, 0.013897912910932536)
('DataOwner5', 'CPC6', 0.6, 845, 0.0528353686675245)
('DataOwner10', 'CPC8', 0.8, 106, 0.02670471694950062)
('DataOwner4', 'CPC2', 0.2, 203, 0.05086577833206387)
('DataOwner7', 'CPC5', 0.5, 414, 0.06478035310023884)
('DataOwner9', 'CPC4', 0.4, 1643, 0.07704569470018591)
**** log-parameter_analysis 运行时间： 2025-02-10 11:07:31 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.08915329884884811
DataOwner2: noise random: 0.09659294842158606
DataOwner3: noise random: 0.015115562780283365
DataOwner4: noise random: 0.014492724794365787
DataOwner5: noise random: 0.09053073312702975
DataOwner6: noise random: 0.06058888242519816
DataOwner7: noise random: 0.08544909024839643
DataOwner8: noise random: 0.05930317473643082
DataOwner9: noise random: 0.04815786781117503
DataOwner10: noise random: 0.05050156044195209
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9967808792011941, 0.996199635272744, 0.9999077108130531, 0.9999149615269207, 0.9966909478646387, 0.9985131364705931, 0.997042103393413, 0.9985749771842926, 0.9990612511554295, 0.998963903250175]
归一化后的数据质量列表avg_f_list: [0.9156444922648886, 0.9, 0.9998048431450807, 1.0, 0.9132239420789062, 0.9622691263048119, 0.922675481587167, 0.9639336023014997, 0.9770219271986826, 0.9744017560859823]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3324
DataOwner1的最优x_1 = 0.0806
DataOwner2的最优x_2 = 0.0614
DataOwner3的最优x_3 = 0.1637
DataOwner4的最优x_4 = 0.1638
DataOwner5的最优x_5 = 0.0777
DataOwner6的最优x_6 = 0.1304
DataOwner7的最优x_7 = 0.0888
DataOwner8的最优x_8 = 0.1320
DataOwner9的最优x_9 = 0.1441
DataOwner10的最优x_10 = 0.1418
每个DataOwner应该贡献数据比例 xn_list = [0.08058501339634581, 0.061371773520400526, 0.16366102911186584, 0.1638198471724491, 0.07770086118257949, 0.13042182126257112, 0.08878558523998452, 0.1320158971877835, 0.14414825360405878, 0.1417755289803353]
ModelOwner的最大效用 U(Eta) = 0.5709
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.332368164303352
DataOwner1的分配到的支付 ： 0.0862
DataOwner2的分配到的支付 ： 0.0645
DataOwner3的分配到的支付 ： 0.1911
DataOwner4的分配到的支付 ： 0.1913
DataOwner5的分配到的支付 ： 0.0829
DataOwner6的分配到的支付 ： 0.1465
DataOwner7的分配到的支付 ： 0.0957
DataOwner8的分配到的支付 ： 0.1486
DataOwner9的分配到的支付 ： 0.1644
DataOwner10的分配到的支付 ： 0.1613
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
**** log-parameter_analysis 运行时间： 2025-02-10 11:07:59 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.09532030453403903
DataOwner2: noise random: 0.01927661443999773
DataOwner3: noise random: 0.09397666182325758
DataOwner4: noise random: 0.06537215766414482
DataOwner5: noise random: 0.008253109000728542
DataOwner6: noise random: 0.019780440181464787
DataOwner7: noise random: 0.0037430304753602096
DataOwner8: noise random: 0.06530733139490552
DataOwner9: noise random: 0.02883544830344098
DataOwner10: noise random: 0.05472151999796565
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9963170632789152, 0.9998497867234597, 0.9964266254681073, 0.9982689267405938, 0.9999725052977567, 0.9998419232578749, 0.9999943233518006, 0.9982724262186632, 0.9996609882929717, 0.9987843309844273]
归一化后的数据质量列表avg_f_list: [0.9, 0.9960694477552279, 0.902979451739081, 0.9530792879206661, 0.9994066763402251, 0.9958556074113596, 1.0, 0.9531744532883604, 0.9909352329663379, 0.9670952735626375]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3479
DataOwner1的最优x_1 = 0.0462
DataOwner2的最优x_2 = 0.1510
DataOwner3的最优x_3 = 0.0502
DataOwner4的最优x_4 = 0.1096
DataOwner5的最优x_5 = 0.1539
DataOwner6的最优x_6 = 0.1508
DataOwner7的最优x_7 = 0.1544
DataOwner8的最优x_8 = 0.1097
DataOwner9的最优x_9 = 0.1465
DataOwner10的最优x_10 = 0.1240
每个DataOwner应该贡献数据比例 xn_list = [0.04623595595940053, 0.15103477382527006, 0.050206542082858775, 0.10959581715296095, 0.1539368897951045, 0.15084737007222435, 0.15444844799091223, 0.10969648313765488, 0.14648661035292063, 0.12397531571687595]
ModelOwner的最大效用 U(Eta) = 0.5889
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.34791133584547
DataOwner1的分配到的支付 ： 0.0479
DataOwner2的分配到的支付 ： 0.1733
DataOwner3的分配到的支付 ： 0.0522
DataOwner4的分配到的支付 ： 0.1203
DataOwner5的分配到的支付 ： 0.1772
DataOwner6的分配到的支付 ： 0.1731
DataOwner7的分配到的支付 ： 0.1779
DataOwner8的分配到的支付 ： 0.1205
DataOwner9的分配到的支付 ： 0.1672
DataOwner10的分配到的支付 ： 0.1381
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
**** log-parameter_analysis 运行时间： 2025-02-10 11:10:03 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.05504396766163242
DataOwner2: noise random: 0.06447346413921742
DataOwner3: noise random: 0.055953643975457684
DataOwner4: noise random: 0.05755330873592125
DataOwner5: noise random: 0.011380375243354314
DataOwner6: noise random: 0.07941182828518734
DataOwner7: noise random: 0.05551655233401656
DataOwner8: noise random: 0.047487134866466045
DataOwner9: noise random: 0.07063594299528879
DataOwner10: noise random: 0.030827050412399828
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9987724246938195, 0.9983162529944302, 0.9987329673604194, 0.9986571026069109, 0.9999475709219869, 0.9974515875476531, 0.998751503335376, 0.9990869247436779, 0.9979802518843812, 0.999615717055589]
归一化后的数据质量列表avg_f_list: [0.9529185073806428, 0.9346422758928778, 0.9513376741985831, 0.9482982006873157, 1.0, 0.9, 0.9520803063469863, 0.9655187535638637, 0.9211806032910475, 0.986704484099921]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3310
DataOwner1的最优x_1 = 0.1223
DataOwner2的最优x_2 = 0.1033
DataOwner3的最优x_3 = 0.1207
DataOwner4的最优x_4 = 0.1176
DataOwner5的最优x_5 = 0.1646
DataOwner6的最优x_6 = 0.0627
DataOwner7的最优x_7 = 0.1214
DataOwner8的最优x_8 = 0.1345
DataOwner9的最优x_9 = 0.0882
DataOwner10的最优x_10 = 0.1535
每个DataOwner应该贡献数据比例 xn_list = [0.12225128754659155, 0.10325901182540864, 0.12066923722392311, 0.11759582618878041, 0.1646147986041839, 0.06266790368238384, 0.12141383320942888, 0.1344701049841462, 0.0882373830744831, 0.15353280455178006]
ModelOwner的最大效用 U(Eta) = 0.5693
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.330976347645323
DataOwner1的分配到的支付 ： 0.1362
DataOwner2的分配到的支付 ： 0.1128
DataOwner3的分配到的支付 ： 0.1342
DataOwner4的分配到的支付 ： 0.1304
DataOwner5的分配到的支付 ： 0.1924
DataOwner6的分配到的支付 ： 0.0659
DataOwner7的分配到的支付 ： 0.1351
DataOwner8的分配到的支付 ： 0.1518
DataOwner9的分配到的支付 ： 0.0950
DataOwner10的分配到的支付 ： 0.1771
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
**** log-parameter_analysis 运行时间： 2025-02-10 11:12:49 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.0993855892420345
DataOwner2: noise random: 0.027194318933547814
DataOwner3: noise random: 0.09633076744392473
DataOwner4: noise random: 0.033297616819639296
DataOwner5: noise random: 0.010054687007896869
DataOwner6: noise random: 0.05507035974296137
DataOwner7: noise random: 0.06874222598035166
DataOwner8: noise random: 0.027539106611354704
DataOwner9: noise random: 0.06324375036270928
DataOwner10: noise random: 0.04194770249700868
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9960110655831464, 0.9997011013433553, 0.9962423774987679, 0.9995516950201955, 0.9999590984071994, 0.998769129011495, 0.9980855184715278, 0.9996923517744144, 0.998382362145373, 0.9992904350829059]
归一化后的数据质量列表avg_f_list: [0.9, 0.9934651742945934, 0.9058589157165102, 0.9896808510678561, 1.0, 0.9698591818068301, 0.9525439625461821, 0.9932435558498931, 0.9600627367579035, 0.9830633798123528]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3468
DataOwner1的最优x_1 = 0.0473
DataOwner2的最优x_2 = 0.1494
DataOwner3的最优x_3 = 0.0551
DataOwner4的最优x_4 = 0.1461
DataOwner5的最优x_5 = 0.1551
DataOwner6的最优x_6 = 0.1275
DataOwner7的最优x_7 = 0.1099
DataOwner8的最优x_8 = 0.1493
DataOwner9的最优x_9 = 0.1177
DataOwner10的最优x_10 = 0.1401
每个DataOwner应该贡献数据比例 xn_list = [0.04733469553272112, 0.14944856381914226, 0.055063339288548126, 0.1460811997720442, 0.15513391161836737, 0.12750178873950996, 0.10989119046686571, 0.1492528951842584, 0.11770386496733096, 0.14005717236619247]
ModelOwner的最大效用 U(Eta) = 0.5876
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3468245034854684
DataOwner1的分配到的支付 ： 0.0491
DataOwner2的分配到的支付 ： 0.1712
DataOwner3的分配到的支付 ： 0.0575
DataOwner4的分配到的支付 ： 0.1667
DataOwner5的分配到的支付 ： 0.1789
DataOwner6的分配到的支付 ： 0.1426
DataOwner7的分配到的支付 ： 0.1207
DataOwner8的分配到的支付 ： 0.1710
DataOwner9的分配到的支付 ： 0.1303
DataOwner10的分配到的支付 ： 0.1588
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
**** log-parameter_analysis 运行时间： 2025-02-10 11:13:35 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.04607560738754362
DataOwner2: noise random: 0.05748386601981309
DataOwner3: noise random: 0.03720331726199146
DataOwner4: noise random: 0.032269405188326473
DataOwner5: noise random: 0.016272080961597168
DataOwner6: noise random: 0.08357951742774118
DataOwner7: noise random: 0.050119291626156165
DataOwner8: noise random: 0.018975996821668218
DataOwner9: noise random: 0.05492675916402776
DataOwner10: noise random: 0.024470248505163696
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9991445424454455, 0.998659376204508, 0.9994393528137417, 0.9995786178162235, 0.9998930165052845, 0.9971709248951083, 0.9989823250306228, 0.9998542901317342, 0.9987797711383648, 0.999758313135079]
归一化后的数据质量列表avg_f_list: [0.9725037152665642, 0.9546804267657735, 0.9833340035343855, 0.988450106238685, 1.0, 0.9, 0.9665444222649524, 0.9985773302630405, 0.9591033100150648, 0.9950514755013418]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3556
DataOwner1的最优x_1 = 0.1236
DataOwner2的最优x_2 = 0.1051
DataOwner3的最优x_3 = 0.1341
DataOwner4的最优x_4 = 0.1389
DataOwner5的最优x_5 = 0.1494
DataOwner6的最优x_6 = 0.0382
DataOwner7的最优x_7 = 0.1175
DataOwner8的最优x_8 = 0.1482
DataOwner9的最优x_9 = 0.1098
DataOwner10的最优x_10 = 0.1450
每个DataOwner应该贡献数据比例 xn_list = [0.12356031414338751, 0.10505045066324381, 0.13412220816541215, 0.13894064121960914, 0.14943222373189416, 0.03823150950139865, 0.11753181246415241, 0.14816815107957948, 0.10977972616042339, 0.14500150519870625]
ModelOwner的最大效用 U(Eta) = 0.5980
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.355649626796348
DataOwner1的分配到的支付 ： 0.1375
DataOwner2的分配到的支付 ： 0.1148
DataOwner3的分配到的支付 ： 0.1509
DataOwner4的分配到的支付 ： 0.1572
DataOwner5的分配到的支付 ： 0.1710
DataOwner6的分配到的支付 ： 0.0394
DataOwner7的分配到的支付 ： 0.1300
DataOwner8的分配到的支付 ： 0.1693
DataOwner9的分配到的支付 ： 0.1205
DataOwner10的分配到的支付 ： 0.1651
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
**** log-parameter_analysis 运行时间： 2025-02-10 11:17:58 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.06288437680403976
DataOwner2: noise random: 0.0955145208998478
DataOwner3: noise random: 0.07689174378963791
DataOwner4: noise random: 0.04950708189233574
DataOwner5: noise random: 0.0895382880444584
DataOwner6: noise random: 0.02633482471233768
DataOwner7: noise random: 0.07650695584167028
DataOwner8: noise random: 0.043149411611477166
DataOwner9: noise random: 0.03925488110346731
**** log-parameter_analysis 运行时间： 2025-02-10 11:18:41 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.09581930524683158
DataOwner2: noise random: 0.07341284293562537
DataOwner3: noise random: 0.09306687246857692
DataOwner4: noise random: 0.04471089658127503
DataOwner5: noise random: 0.07608004098685489
DataOwner6: noise random: 0.03637258509254922
DataOwner7: noise random: 0.02406316558332018
DataOwner8: noise random: 0.07914629451809643
DataOwner9: noise random: 0.06447434184167354
DataOwner10: noise random: 0.0017648336896337047
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9962850249423667, 0.9978134473896265, 0.9964933809446939, 0.9991909859883074, 0.9976630256974677, 0.9994641796393842, 0.9997660561776599, 0.9974635898406214, 0.9983181290557245, 0.9999987397507107]
归一化后的数据质量列表avg_f_list: [0.9, 0.9411561610446157, 0.9056104470343017, 0.9782494401404085, 0.9371057236814431, 0.9856057845334403, 0.9937344792193509, 0.9317354713293202, 0.9547458331692504, 1.0]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3324
DataOwner1的最优x_1 = 0.0614
DataOwner2的最优x_2 = 0.1091
DataOwner3的最优x_3 = 0.0684
DataOwner4的最优x_4 = 0.1453
DataOwner5的最优x_5 = 0.1048
DataOwner6的最优x_6 = 0.1517
DataOwner7的最优x_7 = 0.1587
DataOwner8的最优x_8 = 0.0990
DataOwner9的最优x_9 = 0.1231
DataOwner10的最优x_10 = 0.1638
每个DataOwner应该贡献数据比例 xn_list = [0.06138329001203416, 0.1091499630440493, 0.06843101418464274, 0.14525830323672792, 0.10483253756695726, 0.15173907832336084, 0.15865949298342108, 0.09898585136744112, 0.12307637714457367, 0.16382691616716838]
ModelOwner的最大效用 U(Eta) = 0.5709
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3323558433518585
DataOwner1的分配到的支付 ： 0.0645
DataOwner2的分配到的支付 ： 0.1199
DataOwner3的分配到的支付 ： 0.0724
DataOwner4的分配到的支付 ： 0.1659
DataOwner5的分配到的支付 ： 0.1147
DataOwner6的分配到的支付 ： 0.1746
DataOwner7的分配到的支付 ： 0.1841
DataOwner8的分配到的支付 ： 0.1077
DataOwner9的分配到的支付 ： 0.1372
DataOwner10的分配到的支付 ： 0.1913
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
**** log-parameter_analysis 运行时间： 2025-02-10 11:19:06 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.027204297730638118
DataOwner2: noise random: 0.0454170447839984
DataOwner3: noise random: 0.05238931437666357
DataOwner4: noise random: 0.04304996936928473
DataOwner5: noise random: 0.09151630838811824
DataOwner6: noise random: 0.03164899540047933
DataOwner7: noise random: 0.0985781527712628
DataOwner8: noise random: 0.01878638910980024
DataOwner9: noise random: 0.08668095817878836
DataOwner10: noise random: 0.03407567215923864
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9997007750130927, 0.9991705352571164, 0.9988903098651996, 0.9992491603030526, 0.9966076778070542, 0.9995964657963438, 0.9960686249231127, 0.9998573695462649, 0.9969599814730097, 0.9995305736893336]
归一化后的数据质量列表avg_f_list: [0.9958668490820082, 0.9818717185383412, 0.9744754588325681, 0.9839469453946377, 0.9142277439510563, 0.9931137150726173, 0.9, 1.0, 0.9235264352326659, 0.991374560984282]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3479
DataOwner1的最优x_1 = 0.1508
DataOwner2的最优x_2 = 0.1382
DataOwner3的最优x_3 = 0.1312
DataOwner4的最优x_4 = 0.1401
DataOwner5的最优x_5 = 0.0647
DataOwner6的最优x_6 = 0.1484
DataOwner7的最优x_7 = 0.0462
DataOwner8的最优x_8 = 0.1544
DataOwner9的最优x_9 = 0.0762
DataOwner10的最优x_10 = 0.1469
每个DataOwner应该贡献数据比例 xn_list = [0.15083615840162864, 0.13818167092159872, 0.13117406479635854, 0.14010737405653415, 0.0646931001680684, 0.14840753109599303, 0.04620288804428107, 0.1544278078471497, 0.07615312765351409, 0.1468582528408445]
ModelOwner的最大效用 U(Eta) = 0.5889
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3479439684830685
DataOwner1的分配到的支付 ： 0.1731
DataOwner2的分配到的支付 ： 0.1563
DataOwner3的分配到的支付 ： 0.1473
DataOwner4的分配到的支付 ： 0.1588
DataOwner5的分配到的支付 ： 0.0681
DataOwner6的分配到的支付 ： 0.1698
DataOwner7的分配到的支付 ： 0.0479
DataOwner8的分配到的支付 ： 0.1779
DataOwner9的分配到的支付 ： 0.0810
DataOwner10的分配到的支付 ： 0.1677
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC8', 'DataOwner2': 'CPC10', 'DataOwner8': 'CPC9', 'DataOwner3': 'CPC4', 'DataOwner4': 'CPC5', 'DataOwner6': 'CPC7', 'DataOwner5': 'CPC2', 'DataOwner10': 'CPC6', 'DataOwner7': 'CPC1', 'DataOwner9': 'CPC3'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC8
DataOwner2 把数据交给 CPC10
DataOwner8 把数据交给 CPC9
DataOwner3 把数据交给 CPC4
DataOwner4 把数据交给 CPC5
DataOwner6 把数据交给 CPC7
DataOwner5 把数据交给 CPC2
DataOwner10 把数据交给 CPC6
DataOwner7 把数据交给 CPC1
DataOwner9 把数据交给 CPC3
DONE
最终Um的列表：
[('DataOwner1', 'CPC8', 0.8, 1006, 0.0301672316803257), ('DataOwner2', 'CPC10', 1.0, 102, 0.0), ('DataOwner8', 'CPC9', 0.9, 228, 0.015442780784714977), ('DataOwner3', 'CPC4', 0.4, 1165, 0.07870443887781511), ('DataOwner4', 'CPC5', 0.5, 103, 0.07005368702826707), ('DataOwner6', 'CPC7', 0.7, 109, 0.044522259328797914), ('DataOwner5', 'CPC2', 0.2, 239, 0.05175448013445472), ('DataOwner10', 'CPC6', 0.6, 870, 0.0587433011363378), ('DataOwner7', 'CPC1', 0.1, 616, 0.04158259923985296), ('DataOwner9', 'CPC3', 0.3, 1353, 0.053307189357459865)]
('DataOwner1', 'CPC8', 0.8, 1006, 0.0301672316803257)
('DataOwner2', 'CPC10', 1.0, 102, 0.0)
('DataOwner8', 'CPC9', 0.9, 228, 0.015442780784714977)
('DataOwner3', 'CPC4', 0.4, 1165, 0.07870443887781511)
('DataOwner4', 'CPC5', 0.5, 103, 0.07005368702826707)
('DataOwner6', 'CPC7', 0.7, 109, 0.044522259328797914)
('DataOwner5', 'CPC2', 0.2, 239, 0.05175448013445472)
('DataOwner10', 'CPC6', 0.6, 870, 0.0587433011363378)
('DataOwner7', 'CPC1', 0.1, 616, 0.04158259923985296)
('DataOwner9', 'CPC3', 0.3, 1353, 0.053307189357459865)
**** log-parameter_analysis 运行时间： 2025-02-10 11:22:01 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.09362798886258818
DataOwner2: noise random: 0.08517352809039694
DataOwner3: noise random: 0.01890989746679387
DataOwner4: noise random: 0.0641768939225468
DataOwner5: noise random: 0.0005921414873404474
DataOwner6: noise random: 0.07750455626295644
DataOwner7: noise random: 0.05350222685538628
DataOwner8: noise random: 0.060035839653950755
DataOwner9: noise random: 0.013101934437144925
DataOwner10: noise random: 0.00987510578400066
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9964433352409325, 0.9970818319835804, 0.9998550747073931, 0.9983359863165197, 0.9999998578331616, 0.9975704563230982, 0.9988409160501295, 0.9985482071539257, 0.9999304957917488, 0.9999604584093597]
归一化后的数据质量列表avg_f_list: [0.9, 0.9179528380908624, 0.9959290817922868, 0.9532163377711288, 1.0, 0.9316916609676081, 0.9674136251639628, 0.9591834259001264, 0.9980497230197725, 0.9988921925060165]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3437
DataOwner1的最优x_1 = 0.0504
DataOwner2的最优x_2 = 0.0732
DataOwner3的最优x_3 = 0.1536
DataOwner4的最优x_4 = 0.1130
DataOwner5的最优x_5 = 0.1571
DataOwner6的最优x_6 = 0.0895
DataOwner7的最优x_7 = 0.1273
DataOwner8的最优x_8 = 0.1191
DataOwner9的最优x_9 = 0.1554
DataOwner10的最优x_10 = 0.1561
每个DataOwner应该贡献数据比例 xn_list = [0.05041360126899585, 0.0732213016981826, 0.15356731285141206, 0.11300711624154848, 0.1570505784710484, 0.089471478241573, 0.12734429428618707, 0.11914288519130044, 0.15538956743521132, 0.1561088191681118]
ModelOwner的最大效用 U(Eta) = 0.5840
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3437458023937947
DataOwner1的分配到的支付 ： 0.0525
DataOwner2的分配到的支付 ： 0.0777
DataOwner3的分配到的支付 ： 0.1768
DataOwner4的分配到的支付 ： 0.1246
DataOwner5的分配到的支付 ： 0.1816
DataOwner6的分配到的支付 ： 0.0964
DataOwner7的分配到的支付 ： 0.1424
DataOwner8的分配到的支付 ： 0.1321
DataOwner9的分配到的支付 ： 0.1793
DataOwner10的分配到的支付 ： 0.1803
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
**** log-parameter_analysis 运行时间： 2025-02-10 11:22:23 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.04758625649201118
DataOwner2: noise random: 0.03302550835300766
DataOwner3: noise random: 0.0013278054732163036
DataOwner4: noise random: 0.05473900270515391
DataOwner5: noise random: 0.027653617032400712
DataOwner6: noise random: 0.011500422600300065
DataOwner7: noise random: 0.06326032700841303
DataOwner8: noise random: 0.02932712834706426
DataOwner9: noise random: 0.09860865310286963
DataOwner10: noise random: 0.00902558250971749
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9990841716970109, 0.9995584538138071, 0.999999287188955, 0.9987876564057468, 0.9996903399055016, 0.9999466297268929, 0.9983782755388847, 0.9996527774994656, 0.9960665868787841, 0.9999671070564622]
归一化后的数据质量列表avg_f_list: [0.976730606968006, 0.9887905677936408, 1.0, 0.9691908691828204, 0.9921441437412757, 0.9986610354741247, 0.9587812057308813, 0.9911890135998082, 0.9, 0.9991817293473999]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3622
DataOwner1的最优x_1 = 0.1228
DataOwner2的最优x_2 = 0.1346
DataOwner3的最优x_3 = 0.1450
DataOwner4的最优x_4 = 0.1151
DataOwner5的最优x_5 = 0.1377
DataOwner6的最优x_6 = 0.1438
DataOwner7的最优x_7 = 0.1040
DataOwner8的最优x_8 = 0.1368
DataOwner9的最优x_9 = 0.0312
DataOwner10的最优x_10 = 0.1442
每个DataOwner应该贡献数据比例 xn_list = [0.12277017038278566, 0.13456180177699736, 0.1449825979572853, 0.11507769625800206, 0.13773260609937632, 0.14376424123151985, 0.10403239073067852, 0.1368342173747165, 0.031180915842399338, 0.14423887106927022]
ModelOwner的最大效用 U(Eta) = 0.6058
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.362218825754886
DataOwner1的分配到的支付 ： 0.1364
DataOwner2的分配到的支付 ： 0.1514
DataOwner3的分配到的支付 ： 0.1650
DataOwner4的分配到的支付 ： 0.1269
DataOwner5的分配到的支付 ： 0.1555
DataOwner6的分配到的支付 ： 0.1634
DataOwner7的分配到的支付 ： 0.1135
DataOwner8的分配到的支付 ： 0.1543
DataOwner9的分配到的支付 ： 0.0319
DataOwner10的分配到的支付 ： 0.1640
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC10', 'DataOwner2': 'CPC4', 'DataOwner3': 'CPC9', 'DataOwner5': 'CPC6', 'DataOwner6': 'CPC7', 'DataOwner10': 'CPC8', 'DataOwner4': 'CPC3', 'DataOwner8': 'CPC5', 'DataOwner7': 'CPC2', 'DataOwner9': 'CPC1'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC10
DataOwner2 把数据交给 CPC4
DataOwner3 把数据交给 CPC9
DataOwner5 把数据交给 CPC6
DataOwner6 把数据交给 CPC7
DataOwner10 把数据交给 CPC8
DataOwner4 把数据交给 CPC3
DataOwner8 把数据交给 CPC5
DataOwner7 把数据交给 CPC2
DataOwner9 把数据交给 CPC1
DONE
最终Um的列表：
[('DataOwner1', 'CPC10', 1.0, 1307, 0.0), ('DataOwner2', 'CPC4', 0.4, 781, 0.08073708106619842), ('DataOwner3', 'CPC9', 0.9, 420, 0.014498259795728524), ('DataOwner5', 'CPC6', 0.6, 533, 0.055093042439750536), ('DataOwner6', 'CPC7', 0.7, 417, 0.04312927236945596), ('DataOwner10', 'CPC8', 0.8, 1256, 0.02884777421385404), ('DataOwner4', 'CPC3', 0.3, 1225, 0.08055438738060144), ('DataOwner8', 'CPC5', 0.5, 1059, 0.06841710868735824), ('DataOwner7', 'CPC2', 0.2, 201, 0.0832259125845428), ('DataOwner9', 'CPC1', 0.1, 150, 0.028062824258159404)]
('DataOwner1', 'CPC10', 1.0, 1307, 0.0)
('DataOwner2', 'CPC4', 0.4, 781, 0.08073708106619842)
('DataOwner3', 'CPC9', 0.9, 420, 0.014498259795728524)
('DataOwner5', 'CPC6', 0.6, 533, 0.055093042439750536)
('DataOwner6', 'CPC7', 0.7, 417, 0.04312927236945596)
('DataOwner10', 'CPC8', 0.8, 1256, 0.02884777421385404)
('DataOwner4', 'CPC3', 0.3, 1225, 0.08055438738060144)
('DataOwner8', 'CPC5', 0.5, 1059, 0.06841710868735824)
('DataOwner7', 'CPC2', 0.2, 201, 0.0832259125845428)
('DataOwner9', 'CPC1', 0.1, 150, 0.028062824258159404)
**** log-parameter_analysis 运行时间： 2025-02-10 11:24:22 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.09625237043884109
DataOwner2: noise random: 0.06678762398316884
DataOwner3: noise random: 0.06008479658874186
DataOwner4: noise random: 0.041814880959007386
DataOwner5: noise random: 0.0010346457120490539
DataOwner6: noise random: 0.002535574667356577
DataOwner7: noise random: 0.07416278750368015
DataOwner8: noise random: 0.04427845816035378
DataOwner9: noise random: 0.07565500077915191
DataOwner10: noise random: 0.050643398774160846
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9962535864032549, 0.9981955330435908, 0.9985424325752874, 0.9992934546417163, 0.9999995654117235, 0.9999974027417209, 0.9977696419391278, 0.9992082951752982, 0.9976857918653584, 0.9989621144997466]
归一化后的数据质量列表avg_f_list: [0.9, 0.9518408308200783, 0.9611014147932506, 0.9811501674619388, 1.0, 0.9999422668947758, 0.9404715438192655, 0.9788768107179328, 0.9382331416931521, 0.972304945926511]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3444
DataOwner1的最优x_1 = 0.0498
DataOwner2的最优x_2 = 0.1111
DataOwner3的最优x_3 = 0.1206
DataOwner4的最优x_4 = 0.1400
DataOwner5的最优x_5 = 0.1567
DataOwner6的最优x_6 = 0.1566
DataOwner7的最优x_7 = 0.0988
DataOwner8的最优x_8 = 0.1379
DataOwner9的最优x_9 = 0.0963
DataOwner10的最优x_10 = 0.1316
每个DataOwner应该贡献数据比例 xn_list = [0.04979154763498078, 0.11108043780169366, 0.12061291412744764, 0.139962116517534, 0.15666383874861423, 0.1566147594678352, 0.09882882895894063, 0.13785294466492198, 0.09634281858393831, 0.13163579271866865]
ModelOwner的最大效用 U(Eta) = 0.5847
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3443717972485723
DataOwner1的分配到的支付 ： 0.0518
DataOwner2的分配到的支付 ： 0.1222
DataOwner3的分配到的支付 ： 0.1340
DataOwner4的分配到的支付 ： 0.1587
DataOwner5的分配到的支付 ： 0.1810
DataOwner6的分配到的支付 ： 0.1810
DataOwner7的分配到的支付 ： 0.1074
DataOwner8的分配到的支付 ： 0.1559
DataOwner9的分配到的支付 ： 0.1045
DataOwner10的分配到的支付 ： 0.1479
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC1', 'DataOwner2': 'CPC10', 'DataOwner3': 'CPC4', 'DataOwner4': 'CPC7', 'DataOwner5': 'CPC9', 'DataOwner6': 'CPC8', 'DataOwner8': 'CPC6', 'DataOwner7': 'CPC3', 'DataOwner10': 'CPC5', 'DataOwner9': 'CPC2'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC10
DataOwner3 把数据交给 CPC4
DataOwner4 把数据交给 CPC7
DataOwner5 把数据交给 CPC9
DataOwner6 把数据交给 CPC8
DataOwner8 把数据交给 CPC6
DataOwner7 把数据交给 CPC3
DataOwner10 把数据交给 CPC5
DataOwner9 把数据交给 CPC2
DONE
最终Um的列表：
[('DataOwner1', 'CPC1', 0.1, 632, 0.0448123928714827), ('DataOwner2', 'CPC10', 1.0, 384, 0.0), ('DataOwner3', 'CPC4', 0.4, 1252, 0.07236774847646858), ('DataOwner4', 'CPC7', 0.7, 807, 0.041988634955260215), ('DataOwner5', 'CPC9', 0.9, 723, 0.01566638387486141), ('DataOwner6', 'CPC8', 0.8, 542, 0.03132295189356704), ('DataOwner8', 'CPC6', 0.6, 318, 0.055141177865968796), ('DataOwner7', 'CPC3', 0.3, 456, 0.06918018027125844), ('DataOwner10', 'CPC5', 0.5, 1063, 0.06581789635933433), ('DataOwner9', 'CPC2', 0.2, 444, 0.07707425486715065)]
('DataOwner1', 'CPC1', 0.1, 632, 0.0448123928714827)
('DataOwner2', 'CPC10', 1.0, 384, 0.0)
('DataOwner3', 'CPC4', 0.4, 1252, 0.07236774847646858)
('DataOwner4', 'CPC7', 0.7, 807, 0.041988634955260215)
('DataOwner5', 'CPC9', 0.9, 723, 0.01566638387486141)
('DataOwner6', 'CPC8', 0.8, 542, 0.03132295189356704)
('DataOwner8', 'CPC6', 0.6, 318, 0.055141177865968796)
('DataOwner7', 'CPC3', 0.3, 456, 0.06918018027125844)
('DataOwner10', 'CPC5', 0.5, 1063, 0.06581789635933433)
('DataOwner9', 'CPC2', 0.2, 444, 0.07707425486715065)
**** log-parameter_analysis 运行时间： 2025-02-10 11:25:17 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.051013758540757216
DataOwner2: noise random: 0.04461001669972077
DataOwner3: noise random: 0.020378772201501173
DataOwner4: noise random: 0.0035990531251232864
DataOwner5: noise random: 0.08418252068834348
DataOwner6: noise random: 0.07564756666639963
DataOwner7: noise random: 0.025932721268358428
DataOwner8: noise random: 0.004572413189302638
DataOwner9: noise random: 0.08992103198708076
DataOwner10: noise random: 0.09780049265089598
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9989491482792489, 0.9991950680873203, 0.9998317049929575, 0.9999947630715202, 0.9971298619846966, 0.997682292016315, 0.9997277309170437, 0.9999915420716409, 0.9967271695474843, 0.9961236306309813]
归一化后的数据质量列表avg_f_list: [0.9729894337553118, 0.9793420918430622, 0.9957878455189185, 1.0, 0.9259932040345062, 0.9402637060156143, 0.993101962834437, 0.9999167943766109, 0.9155907586674812, 0.9]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3437
DataOwner1的最优x_1 = 0.1328
DataOwner2的最优x_2 = 0.1388
DataOwner3的最优x_3 = 0.1535
DataOwner4的最优x_4 = 0.1571
DataOwner5的最优x_5 = 0.0829
DataOwner6的最优x_6 = 0.0992
DataOwner7的最优x_7 = 0.1512
DataOwner8的最优x_8 = 0.1570
DataOwner9的最优x_9 = 0.0704
DataOwner10的最优x_10 = 0.0505
每个DataOwner应该贡献数据比例 xn_list = [0.13278002875389044, 0.13875972472840364, 0.1534867666055861, 0.15709113041112727, 0.08291062282099408, 0.09916980674402844, 0.15115351966420246, 0.15702057167235858, 0.07038577196433798, 0.05047886040025654]
ModelOwner的最大效用 U(Eta) = 0.5839
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3436799459923037
DataOwner1的分配到的支付 ： 0.1494
DataOwner2的分配到的支付 ： 0.1571
DataOwner3的分配到的支付 ： 0.1767
DataOwner4的分配到的支付 ： 0.1816
DataOwner5的分配到的支付 ： 0.0888
DataOwner6的分配到的支付 ： 0.1078
DataOwner7的分配到的支付 ： 0.1736
DataOwner8的分配到的支付 ： 0.1816
DataOwner9的分配到的支付 ： 0.0745
DataOwner10的分配到的支付 ： 0.0525
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC5', 'DataOwner8': 'CPC10', 'DataOwner2': 'CPC6', 'DataOwner3': 'CPC8', 'DataOwner4': 'CPC9', 'DataOwner7': 'CPC7', 'DataOwner5': 'CPC3', 'DataOwner6': 'CPC4', 'DataOwner9': 'CPC2', 'DataOwner10': 'CPC1'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC5
DataOwner8 把数据交给 CPC10
DataOwner2 把数据交给 CPC6
DataOwner3 把数据交给 CPC8
DataOwner4 把数据交给 CPC9
DataOwner7 把数据交给 CPC7
DataOwner5 把数据交给 CPC3
DataOwner6 把数据交给 CPC4
DataOwner9 把数据交给 CPC2
DataOwner10 把数据交给 CPC1
DONE
最终Um的列表：
[('DataOwner1', 'CPC5', 0.5, 820, 0.06639001437694522), ('DataOwner8', 'CPC10', 1.0, 1662, 2.7755575615628914e-17), ('DataOwner2', 'CPC6', 0.6, 979, 0.05550388989136146), ('DataOwner3', 'CPC8', 0.8, 1083, 0.030697353321117185), ('DataOwner4', 'CPC9', 0.9, 138, 0.01570911304111272), ('DataOwner7', 'CPC7', 0.7, 133, 0.045346055899260745), ('DataOwner5', 'CPC3', 0.3, 1024, 0.05803743597469586), ('DataOwner6', 'CPC4', 0.4, 262, 0.059501884046417056), ('DataOwner9', 'CPC2', 0.2, 621, 0.05630861757147039), ('DataOwner10', 'CPC1', 0.1, 178, 0.045430974360230886)]
('DataOwner1', 'CPC5', 0.5, 820, 0.06639001437694522)
('DataOwner8', 'CPC10', 1.0, 1662, 2.7755575615628914e-17)
('DataOwner2', 'CPC6', 0.6, 979, 0.05550388989136146)
('DataOwner3', 'CPC8', 0.8, 1083, 0.030697353321117185)
('DataOwner4', 'CPC9', 0.9, 138, 0.01570911304111272)
('DataOwner7', 'CPC7', 0.7, 133, 0.045346055899260745)
('DataOwner5', 'CPC3', 0.3, 1024, 0.05803743597469586)
('DataOwner6', 'CPC4', 0.4, 262, 0.059501884046417056)
('DataOwner9', 'CPC2', 0.2, 621, 0.05630861757147039)
('DataOwner10', 'CPC1', 0.1, 178, 0.045430974360230886)
**** log-parameter_analysis 运行时间： 2025-02-10 11:27:57 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.013787923773903199
DataOwner2: noise random: 0.06821917306217261
DataOwner3: noise random: 0.018241803709709983
DataOwner4: noise random: 0.0786577926151072
DataOwner5: noise random: 0.06746402496001792
DataOwner6: noise random: 0.030206764447858303
DataOwner7: noise random: 0.05324025945289154
DataOwner8: noise random: 0.061679087609697106
DataOwner9: noise random: 0.029018008951846898
DataOwner10: noise random: 0.05594862136379358
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9999227771647594, 0.9981178425191447, 0.999865451706485, 0.9974981854542777, 0.9981544207754263, 0.9996309626057859, 0.9988549610861457, 0.9984590670219522, 0.999659579746487, 0.9987324243259456]
归一化后的数据质量列表avg_f_list: [1.0, 0.9255571716338123, 0.9976356655008515, 0.9, 0.9270658073403286, 0.987964383540866, 0.9559589322194961, 0.9396306546591131, 0.9891446705383597, 0.9509050190319522]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3380
DataOwner1的最优x_1 = 0.1605
DataOwner2的最优x_2 = 0.0873
DataOwner3的最优x_3 = 0.1585
DataOwner4的最优x_4 = 0.0560
DataOwner5的最优x_5 = 0.0890
DataOwner6的最优x_6 = 0.1502
DataOwner7的最优x_7 = 0.1201
DataOwner8的最优x_8 = 0.1031
DataOwner9的最优x_9 = 0.1513
DataOwner10的最优x_10 = 0.1150
每个DataOwner应该贡献数据比例 xn_list = [0.16051020355602652, 0.08730148007344725, 0.15853639054842542, 0.05599702872609582, 0.0890389049800962, 0.15024571257908026, 0.12014539115806434, 0.10306119856180064, 0.15127649455560263, 0.1149897266199135]
ModelOwner的最大效用 U(Eta) = 0.5774
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.338033392317491
DataOwner1的分配到的支付 ： 0.1865
DataOwner2的分配到的支付 ： 0.0939
DataOwner3的分配到的支付 ： 0.1838
DataOwner4的分配到的支付 ： 0.0586
DataOwner5的分配到的支付 ： 0.0959
DataOwner6的分配到的支付 ： 0.1725
DataOwner7的分配到的支付 ： 0.1335
DataOwner8的分配到的支付 ： 0.1125
DataOwner9的分配到的支付 ： 0.1739
DataOwner10的分配到的支付 ： 0.1271
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC10', 'DataOwner2': 'CPC2', 'DataOwner3': 'CPC9', 'DataOwner5': 'CPC3', 'DataOwner6': 'CPC7', 'DataOwner9': 'CPC8', 'DataOwner4': 'CPC1', 'DataOwner7': 'CPC6', 'DataOwner8': 'CPC4', 'DataOwner10': 'CPC5'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC10
DataOwner2 把数据交给 CPC2
DataOwner3 把数据交给 CPC9
DataOwner5 把数据交给 CPC3
DataOwner6 把数据交给 CPC7
DataOwner9 把数据交给 CPC8
DataOwner4 把数据交给 CPC1
DataOwner7 把数据交给 CPC6
DataOwner8 把数据交给 CPC4
DataOwner10 把数据交给 CPC5
DONE
最终Um的列表：
[('DataOwner1', 'CPC10', 1.0, 230, 0.0), ('DataOwner2', 'CPC2', 0.2, 997, 0.0698411840587578), ('DataOwner3', 'CPC9', 0.9, 452, 0.01585363905484255), ('DataOwner5', 'CPC3', 0.3, 254, 0.06232723348606734), ('DataOwner6', 'CPC7', 0.7, 214, 0.04507371377372406), ('DataOwner9', 'CPC8', 0.8, 1080, 0.0302552989111205), ('DataOwner4', 'CPC1', 0.1, 239, 0.05039732585348624), ('DataOwner7', 'CPC6', 0.6, 1716, 0.04805815646322574), ('DataOwner8', 'CPC4', 0.4, 441, 0.06183671913708038), ('DataOwner10', 'CPC5', 0.5, 1149, 0.05749486330995675)]
('DataOwner1', 'CPC10', 1.0, 230, 0.0)
('DataOwner2', 'CPC2', 0.2, 997, 0.0698411840587578)
('DataOwner3', 'CPC9', 0.9, 452, 0.01585363905484255)
('DataOwner5', 'CPC3', 0.3, 254, 0.06232723348606734)
('DataOwner6', 'CPC7', 0.7, 214, 0.04507371377372406)
('DataOwner9', 'CPC8', 0.8, 1080, 0.0302552989111205)
('DataOwner4', 'CPC1', 0.1, 239, 0.05039732585348624)
('DataOwner7', 'CPC6', 0.6, 1716, 0.04805815646322574)
('DataOwner8', 'CPC4', 0.4, 441, 0.06183671913708038)
('DataOwner10', 'CPC5', 0.5, 1149, 0.05749486330995675)
**** log-parameter_analysis 运行时间： 2025-02-10 11:29:40 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.05927928514789459
DataOwner2: noise random: 0.05626400361548496
DataOwner3: noise random: 0.021346700460324997
DataOwner4: noise random: 0.028553057799537398
DataOwner5: noise random: 0.07555642126024531
DataOwner6: noise random: 0.07815983994527714
DataOwner7: noise random: 0.07041940596094474
DataOwner8: noise random: 0.06486888788127514
DataOwner9: noise random: 0.012850151391797782
DataOwner10: noise random: 0.07902463946982696
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9985766691191101, 0.998716896561934, 0.9998156066326395, 0.999670228519019, 0.9976935790029077, 0.9975231229304451, 0.9979956677357533, 0.9982973629215627, 0.9999332077304961, 0.9974620869455427]
归一化后的数据质量列表avg_f_list: [0.9451043178607089, 0.9507789673427443, 0.9952409813970767, 0.9893578973120866, 0.9093678973028959, 0.9024699717340414, 0.9215926632748818, 0.9338015033949754, 1.0, 0.9]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3222
DataOwner1的最优x_1 = 0.1207
DataOwner2的最优x_2 = 0.1263
DataOwner3的最优x_3 = 0.1657
DataOwner4的最优x_4 = 0.1609
DataOwner5的最优x_5 = 0.0819
DataOwner6的最优x_6 = 0.0736
DataOwner7的最优x_7 = 0.0959
DataOwner8的最优x_8 = 0.1091
DataOwner9的最优x_9 = 0.1695
DataOwner10的最优x_10 = 0.0706
每个DataOwner应该贡献数据比例 xn_list = [0.12073335181787695, 0.12634348146508179, 0.16569843985936938, 0.16092771802667916, 0.08189617680888914, 0.07364514303078139, 0.09589589258827404, 0.10912246523178988, 0.16946753761084926, 0.07062672867062174]
ModelOwner的最大效用 U(Eta) = 0.5594
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3221978799800895
DataOwner1的分配到的支付 ： 0.1344
DataOwner2的分配到的支付 ： 0.1415
DataOwner3的分配到的支付 ： 0.1942
DataOwner4的分配到的支付 ： 0.1875
DataOwner5的分配到的支付 ： 0.0877
DataOwner6的分配到的支付 ： 0.0783
DataOwner7的分配到的支付 ： 0.1041
DataOwner8的分配到的支付 ： 0.1200
DataOwner9的分配到的支付 ： 0.1996
DataOwner10的分配到的支付 ： 0.0749
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC6', 'DataOwner2': 'CPC10', 'DataOwner3': 'CPC8', 'DataOwner9': 'CPC9', 'DataOwner4': 'CPC7', 'DataOwner5': 'CPC3', 'DataOwner7': 'CPC4', 'DataOwner8': 'CPC5', 'DataOwner6': 'CPC2', 'DataOwner10': 'CPC1'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC6
DataOwner2 把数据交给 CPC10
DataOwner3 把数据交给 CPC8
DataOwner9 把数据交给 CPC9
DataOwner4 把数据交给 CPC7
DataOwner5 把数据交给 CPC3
DataOwner7 把数据交给 CPC4
DataOwner8 把数据交给 CPC5
DataOwner6 把数据交给 CPC2
DataOwner10 把数据交给 CPC1
DONE
最终Um的列表：
[('DataOwner1', 'CPC6', 0.6, 701, 0.048293340727150774), ('DataOwner2', 'CPC10', 1.0, 1467, 0.0), ('DataOwner3', 'CPC8', 0.8, 641, 0.03313968797187386), ('DataOwner9', 'CPC9', 0.9, 1639, 0.016946753761084926), ('DataOwner4', 'CPC7', 0.7, 1245, 0.04827831540800376), ('DataOwner5', 'CPC3', 0.3, 316, 0.0573273237662224), ('DataOwner7', 'CPC4', 0.4, 742, 0.05753753555296442), ('DataOwner8', 'CPC5', 0.5, 633, 0.054561232615894927), ('DataOwner6', 'CPC2', 0.2, 142, 0.05891611442462511), ('DataOwner10', 'CPC1', 0.1, 136, 0.06356405580355956)]
('DataOwner1', 'CPC6', 0.6, 701, 0.048293340727150774)
('DataOwner2', 'CPC10', 1.0, 1467, 0.0)
('DataOwner3', 'CPC8', 0.8, 641, 0.03313968797187386)
('DataOwner9', 'CPC9', 0.9, 1639, 0.016946753761084926)
('DataOwner4', 'CPC7', 0.7, 1245, 0.04827831540800376)
('DataOwner5', 'CPC3', 0.3, 316, 0.0573273237662224)
('DataOwner7', 'CPC4', 0.4, 742, 0.05753753555296442)
('DataOwner8', 'CPC5', 0.5, 633, 0.054561232615894927)
('DataOwner6', 'CPC2', 0.2, 142, 0.05891611442462511)
('DataOwner10', 'CPC1', 0.1, 136, 0.06356405580355956)
**** log-parameter_analysis 运行时间： 2025-02-10 11:30:11 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 5.177706362913259e-05
DataOwner2: noise random: 0.05927808102956283
DataOwner3: noise random: 0.08826846719079186
DataOwner4: noise random: 0.006941629635059166
DataOwner5: noise random: 0.07823974217320867
DataOwner6: noise random: 0.08111393316458776
DataOwner7: noise random: 0.011898356856204018
DataOwner8: noise random: 0.0762363243633246
DataOwner9: noise random: 0.07586217546585952
DataOwner10: noise random: 0.05537155283814199
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.999999998915425, 0.9985780531926877, 0.9968490971683137, 0.9999804938152359, 0.997531147571946, 0.9973351261871571, 0.9999427471609458, 0.9976407015978646, 0.9976745539170321, 0.998759515993284]
归一化后的数据质量列表avg_f_list: [1.0, 0.9548717847504776, 0.9, 0.9993809676799034, 0.9216461971325394, 0.915425076941514, 0.998183004134238, 0.9251231073858959, 0.9261974766263383, 0.9606308599346769]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3289
DataOwner1的最优x_1 = 0.1658
DataOwner2的最优x_2 = 0.1257
DataOwner3的最优x_3 = 0.0646
DataOwner4的最优x_4 = 0.1653
DataOwner5的最优x_5 = 0.0905
DataOwner6的最优x_6 = 0.0833
DataOwner7的最优x_7 = 0.1643
DataOwner8的最优x_8 = 0.0944
DataOwner9的最优x_9 = 0.0956
DataOwner10的最优x_10 = 0.1312
每个DataOwner应该贡献数据比例 xn_list = [0.1657832544032794, 0.12565917670411614, 0.0645767998059658, 0.1652853716374896, 0.0904963346673361, 0.08330634782078752, 0.1643179870254875, 0.09442735982060273, 0.09562960426540214, 0.13123881867460915]
ModelOwner的最大效用 U(Eta) = 0.5670
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3289077384259107
DataOwner1的分配到的支付 ： 0.1941
DataOwner2的分配到的支付 ： 0.1405
DataOwner3的分配到的支付 ： 0.0681
DataOwner4的分配到的支付 ： 0.1934
DataOwner5的分配到的支付 ： 0.0977
DataOwner6的分配到的支付 ： 0.0893
DataOwner7的分配到的支付 ： 0.1921
DataOwner8的分配到的支付 ： 0.1023
DataOwner9的分配到的支付 ： 0.1037
DataOwner10的分配到的支付 ： 0.1476
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC10', 'DataOwner2': 'CPC6', 'DataOwner4': 'CPC9', 'DataOwner3': 'CPC1', 'DataOwner7': 'CPC8', 'DataOwner5': 'CPC3', 'DataOwner10': 'CPC7', 'DataOwner6': 'CPC2', 'DataOwner8': 'CPC4', 'DataOwner9': 'CPC5'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC10
DataOwner2 把数据交给 CPC6
DataOwner4 把数据交给 CPC9
DataOwner3 把数据交给 CPC1
DataOwner7 把数据交给 CPC8
DataOwner5 把数据交给 CPC3
DataOwner10 把数据交给 CPC7
DataOwner6 把数据交给 CPC2
DataOwner8 把数据交给 CPC4
DataOwner9 把数据交给 CPC5
DONE
最终Um的列表：
[('DataOwner1', 'CPC10', 1.0, 1015, 0.0), ('DataOwner2', 'CPC6', 0.6, 2769, 0.05026367068164646), ('DataOwner4', 'CPC9', 0.9, 202, 0.01652853716374897), ('DataOwner3', 'CPC1', 0.1, 237, 0.058119119825369214), ('DataOwner7', 'CPC8', 0.8, 603, 0.032863597405097505), ('DataOwner5', 'CPC3', 0.3, 221, 0.06334743426713527), ('DataOwner10', 'CPC7', 0.7, 964, 0.03937164560238275), ('DataOwner6', 'CPC2', 0.2, 510, 0.06664507825663002), ('DataOwner8', 'CPC4', 0.4, 115, 0.05665641589236163), ('DataOwner9', 'CPC5', 0.5, 585, 0.04781480213270107)]
('DataOwner1', 'CPC10', 1.0, 1015, 0.0)
('DataOwner2', 'CPC6', 0.6, 2769, 0.05026367068164646)
('DataOwner4', 'CPC9', 0.9, 202, 0.01652853716374897)
('DataOwner3', 'CPC1', 0.1, 237, 0.058119119825369214)
('DataOwner7', 'CPC8', 0.8, 603, 0.032863597405097505)
('DataOwner5', 'CPC3', 0.3, 221, 0.06334743426713527)
('DataOwner10', 'CPC7', 0.7, 964, 0.03937164560238275)
('DataOwner6', 'CPC2', 0.2, 510, 0.06664507825663002)
('DataOwner8', 'CPC4', 0.4, 115, 0.05665641589236163)
('DataOwner9', 'CPC5', 0.5, 585, 0.04781480213270107)
**** log-parameter_analysis 运行时间： 2025-02-10 11:32:36 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.02008157247623208
DataOwner2: noise random: 0.04044144806285972
DataOwner3: noise random: 0.09713214983021147
DataOwner4: noise random: 0.06408971043850249
DataOwner5: noise random: 0.09443490067342454
DataOwner6: noise random: 0.06437283179072291
DataOwner7: noise random: 0.04439038532626617
DataOwner8: noise random: 0.046016097672927206
DataOwner9: noise random: 0.07121048107400738
DataOwner10: noise random: 0.017434213337183948
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9998372207506565, 0.9993396792765539, 0.9961794736624825, 0.9983440470033002, 0.9963907417479911, 0.9983150386977088, 0.9992029688303226, 0.9991446292743479, 0.9979484013319335, 0.9998771530253605]
归一化后的数据质量列表avg_f_list: [0.9989200720023241, 0.985464565851694, 0.9, 0.958538697609868, 0.9057135317796784, 0.9577541973126653, 0.9817673700481911, 0.980189635738387, 0.9478388604271577, 1.0]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3431
DataOwner1的最优x_1 = 0.1565
DataOwner2的最优x_2 = 0.1448
DataOwner3的最优x_3 = 0.0511
DataOwner4的最优x_4 = 0.1190
DataOwner5的最优x_5 = 0.0585
DataOwner6的最优x_6 = 0.1182
DataOwner7的最优x_7 = 0.1414
DataOwner8的最优x_8 = 0.1399
DataOwner9的最优x_9 = 0.1079
DataOwner10的最优x_10 = 0.1575
每个DataOwner应该贡献数据比例 xn_list = [0.15654203734485178, 0.1447609964042629, 0.05106808157535563, 0.11898641158409533, 0.05851169527187524, 0.11818855169915017, 0.14140027189977927, 0.13994942038130678, 0.10786144692535056, 0.15745782898104663]
ModelOwner的最大效用 U(Eta) = 0.5833
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3430919164487776
DataOwner1的分配到的支付 ： 0.1809
DataOwner2的分配到的支付 ： 0.1650
DataOwner3的分配到的支付 ： 0.0532
DataOwner4的分配到的支付 ： 0.1319
DataOwner5的分配到的支付 ： 0.0613
DataOwner6的分配到的支付 ： 0.1310
DataOwner7的分配到的支付 ： 0.1606
DataOwner8的分配到的支付 ： 0.1587
DataOwner9的分配到的支付 ： 0.1183
DataOwner10的分配到的支付 ： 0.1822
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC10', 'DataOwner2': 'CPC8', 'DataOwner10': 'CPC9', 'DataOwner3': 'CPC1', 'DataOwner4': 'CPC5', 'DataOwner7': 'CPC7', 'DataOwner5': 'CPC2', 'DataOwner6': 'CPC4', 'DataOwner8': 'CPC6', 'DataOwner9': 'CPC3'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC10
DataOwner2 把数据交给 CPC8
DataOwner10 把数据交给 CPC9
DataOwner3 把数据交给 CPC1
DataOwner4 把数据交给 CPC5
DataOwner7 把数据交给 CPC7
DataOwner5 把数据交给 CPC2
DataOwner6 把数据交给 CPC4
DataOwner8 把数据交给 CPC6
DataOwner9 把数据交给 CPC3
DONE
最终Um的列表：
[('DataOwner1', 'CPC10', 1.0, 427, 2.7755575615628914e-17), ('DataOwner2', 'CPC8', 0.8, 197, 0.028952199280852575), ('DataOwner10', 'CPC9', 0.9, 214, 0.01574578289810466), ('DataOwner3', 'CPC1', 0.1, 69, 0.045961273417820074), ('DataOwner4', 'CPC5', 0.5, 162, 0.059493205792047664), ('DataOwner7', 'CPC7', 0.7, 4434, 0.04242008156993379), ('DataOwner5', 'CPC2', 0.2, 797, 0.046809356217500195), ('DataOwner6', 'CPC4', 0.4, 161, 0.0709131310194901), ('DataOwner8', 'CPC6', 0.6, 190, 0.055979768152522716), ('DataOwner9', 'CPC3', 0.3, 441, 0.0755030128477454)]
('DataOwner1', 'CPC10', 1.0, 427, 2.7755575615628914e-17)
('DataOwner2', 'CPC8', 0.8, 197, 0.028952199280852575)
('DataOwner10', 'CPC9', 0.9, 214, 0.01574578289810466)
('DataOwner3', 'CPC1', 0.1, 69, 0.045961273417820074)
('DataOwner4', 'CPC5', 0.5, 162, 0.059493205792047664)
('DataOwner7', 'CPC7', 0.7, 4434, 0.04242008156993379)
('DataOwner5', 'CPC2', 0.2, 797, 0.046809356217500195)
('DataOwner6', 'CPC4', 0.4, 161, 0.0709131310194901)
('DataOwner8', 'CPC6', 0.6, 190, 0.055979768152522716)
('DataOwner9', 'CPC3', 0.3, 441, 0.0755030128477454)
**** log-parameter_analysis 运行时间： 2025-02-10 11:34:01 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.07983985437529338
DataOwner2: noise random: 0.05600381564155077
DataOwner3: noise random: 0.00247628278458536
DataOwner4: noise random: 0.08115930560506242
DataOwner5: noise random: 0.08746682326204239
DataOwner6: noise random: 0.0664018991842219
DataOwner7: noise random: 0.005947747292349948
DataOwner8: noise random: 0.07554583624213976
DataOwner9: noise random: 0.06345190229856194
DataOwner10: noise random: 0.05151835174252303
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9974177577507702, 0.9987348834806297, 0.9999975199490339, 0.9973331562172348, 0.9969068623522863, 0.9982182691949105, 0.9999856928461394, 0.9976878614898367, 0.9983704222663485, 0.9989294623671745]
归一化后的数据质量列表avg_f_list: [0.9165303137759927, 0.9591466725484917, 1.0, 0.9137929826130574, 0.9, 0.9424313208944359, 0.9996173272993121, 0.9252696752423255, 0.9473543208281112, 0.9654423840744014]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3253
DataOwner1的最优x_1 = 0.0877
DataOwner2的最优x_2 = 0.1323
DataOwner3的最优x_3 = 0.1678
DataOwner4的最优x_4 = 0.0845
DataOwner5的最优x_5 = 0.0679
DataOwner6的最优x_6 = 0.1158
DataOwner7的最优x_7 = 0.1675
DataOwner8的最优x_8 = 0.0975
DataOwner9的最优x_9 = 0.1208
DataOwner10的最优x_10 = 0.1382
每个DataOwner应该贡献数据比例 xn_list = [0.08767148832327765, 0.13231278410525935, 0.16781016022370507, 0.08449936206820469, 0.06789918666764048, 0.11581690354181813, 0.16750663609302235, 0.09753905304617785, 0.12080445988033912, 0.13821262523559927]
ModelOwner的最大效用 U(Eta) = 0.5628
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3252525350640472
DataOwner1的分配到的支付 ： 0.0944
DataOwner2的分配到的支付 ： 0.1491
DataOwner3的分配到的支付 ： 0.1971
DataOwner4的分配到的支付 ： 0.0907
DataOwner5的分配到的支付 ： 0.0718
DataOwner6的分配到的支付 ： 0.1282
DataOwner7的分配到的支付 ： 0.1967
DataOwner8的分配到的支付 ： 0.1060
DataOwner9的分配到的支付 ： 0.1344
DataOwner10的分配到的支付 ： 0.1568
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC10', 'DataOwner2': 'CPC6', 'DataOwner3': 'CPC9', 'DataOwner7': 'CPC8', 'DataOwner4': 'CPC2', 'DataOwner6': 'CPC4', 'DataOwner10': 'CPC7', 'DataOwner5': 'CPC1', 'DataOwner9': 'CPC5', 'DataOwner8': 'CPC3'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC10
DataOwner2 把数据交给 CPC6
DataOwner3 把数据交给 CPC9
DataOwner7 把数据交给 CPC8
DataOwner4 把数据交给 CPC2
DataOwner6 把数据交给 CPC4
DataOwner10 把数据交给 CPC7
DataOwner5 把数据交给 CPC1
DataOwner9 把数据交给 CPC5
DataOwner8 把数据交给 CPC3
DONE
最终Um的列表：
[('DataOwner1', 'CPC10', 1.0, 405, 0.0), ('DataOwner2', 'CPC6', 0.6, 457, 0.05292511364210374), ('DataOwner3', 'CPC9', 0.9, 387, 0.016781016022370504), ('DataOwner7', 'CPC8', 0.8, 966, 0.033501327218604465), ('DataOwner4', 'CPC2', 0.2, 292, 0.06759948965456375), ('DataOwner6', 'CPC4', 0.4, 2539, 0.06949014212509087), ('DataOwner10', 'CPC7', 0.7, 159, 0.04146378757067978), ('DataOwner5', 'CPC1', 0.1, 156, 0.06110926800087643), ('DataOwner9', 'CPC5', 0.5, 1533, 0.06040222994016956), ('DataOwner8', 'CPC3', 0.3, 225, 0.0682773371323245)]
('DataOwner1', 'CPC10', 1.0, 405, 0.0)
('DataOwner2', 'CPC6', 0.6, 457, 0.05292511364210374)
('DataOwner3', 'CPC9', 0.9, 387, 0.016781016022370504)
('DataOwner7', 'CPC8', 0.8, 966, 0.033501327218604465)
('DataOwner4', 'CPC2', 0.2, 292, 0.06759948965456375)
('DataOwner6', 'CPC4', 0.4, 2539, 0.06949014212509087)
('DataOwner10', 'CPC7', 0.7, 159, 0.04146378757067978)
('DataOwner5', 'CPC1', 0.1, 156, 0.06110926800087643)
('DataOwner9', 'CPC5', 0.5, 1533, 0.06040222994016956)
('DataOwner8', 'CPC3', 0.3, 225, 0.0682773371323245)
**** log-parameter_analysis 运行时间： 2025-02-10 11:34:41 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.01979792667577214
DataOwner2: noise random: 0.07585492152356337
DataOwner3: noise random: 0.05139838825694918
DataOwner4: noise random: 0.03482475951237708
DataOwner5: noise random: 0.04227150302316029
DataOwner6: noise random: 0.04550965949050348
DataOwner7: noise random: 0.031769609912959094
DataOwner8: noise random: 0.0876067255320695
DataOwner9: noise random: 0.03906617583760469
DataOwner10: noise random: 0.021262641957881002
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9998405885227638, 0.9976771754806883, 0.998929427784137, 0.9995089613930568, 0.9992765818025289, 0.9991609811358403, 0.9995928028758929, 0.9968976765905123, 0.9993824354190394, 0.9998168879504333]
归一化后的数据质量列表avg_f_list: [1.0, 0.9264873332305148, 0.9690388037561912, 0.9887313267491049, 0.9808350799066068, 0.9769069750448298, 0.9915802561349052, 0.9, 0.9844319804917192, 0.9991946557397542]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3553
DataOwner1的最优x_1 = 0.1497
DataOwner2的最优x_2 = 0.0729
DataOwner3的最优x_3 = 0.1203
DataOwner4的最优x_4 = 0.1394
DataOwner5的最优x_5 = 0.1320
DataOwner6的最优x_6 = 0.1282
DataOwner7的最优x_7 = 0.1421
DataOwner8的最优x_8 = 0.0386
DataOwner9的最优x_9 = 0.1354
DataOwner10的最优x_10 = 0.1489
每个DataOwner应该贡献数据比例 xn_list = [0.1496501765903456, 0.07293916683547783, 0.12032697399322963, 0.13943253355595897, 0.1319683089667472, 0.12815826529739305, 0.14206320021320487, 0.0385775323807097, 0.13540028417509903, 0.14893642344422817]
ModelOwner的最大效用 U(Eta) = 0.5976
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3553248847909607
DataOwner1的分配到的支付 ： 0.1713
DataOwner2的分配到的支付 ： 0.0774
DataOwner3的分配到的支付 ： 0.1335
DataOwner4的分配到的支付 ： 0.1578
DataOwner5的分配到的支付 ： 0.1482
DataOwner6的分配到的支付 ： 0.1433
DataOwner7的分配到的支付 ： 0.1612
DataOwner8的分配到的支付 ： 0.0397
DataOwner9的分配到的支付 ： 0.1526
DataOwner10的分配到的支付 ： 0.1703
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC10', 'DataOwner2': 'CPC2', 'DataOwner3': 'CPC3', 'DataOwner4': 'CPC7', 'DataOwner7': 'CPC8', 'DataOwner10': 'CPC9', 'DataOwner5': 'CPC5', 'DataOwner6': 'CPC4', 'DataOwner9': 'CPC6', 'DataOwner8': 'CPC1'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC10
DataOwner2 把数据交给 CPC2
DataOwner3 把数据交给 CPC3
DataOwner4 把数据交给 CPC7
DataOwner7 把数据交给 CPC8
DataOwner10 把数据交给 CPC9
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC4
DataOwner9 把数据交给 CPC6
DataOwner8 把数据交给 CPC1
DONE
最终Um的列表：
[('DataOwner1', 'CPC10', 1.0, 367, 0.0), ('DataOwner2', 'CPC2', 0.2, 267, 0.05835133346838226), ('DataOwner3', 'CPC3', 0.3, 294, 0.08422888179526074), ('DataOwner4', 'CPC7', 0.7, 2731, 0.041829760066787694), ('DataOwner7', 'CPC8', 0.8, 521, 0.028412640042640963), ('DataOwner10', 'CPC9', 0.9, 1094, 0.014893642344422808), ('DataOwner5', 'CPC5', 0.5, 161, 0.0659841544833736), ('DataOwner6', 'CPC4', 0.4, 941, 0.07689495917843583), ('DataOwner9', 'CPC6', 0.6, 994, 0.05416011367003962), ('DataOwner8', 'CPC1', 0.1, 188, 0.034719779142638726)]
('DataOwner1', 'CPC10', 1.0, 367, 0.0)
('DataOwner2', 'CPC2', 0.2, 267, 0.05835133346838226)
('DataOwner3', 'CPC3', 0.3, 294, 0.08422888179526074)
('DataOwner4', 'CPC7', 0.7, 2731, 0.041829760066787694)
('DataOwner7', 'CPC8', 0.8, 521, 0.028412640042640963)
('DataOwner10', 'CPC9', 0.9, 1094, 0.014893642344422808)
('DataOwner5', 'CPC5', 0.5, 161, 0.0659841544833736)
('DataOwner6', 'CPC4', 0.4, 941, 0.07689495917843583)
('DataOwner9', 'CPC6', 0.6, 994, 0.05416011367003962)
('DataOwner8', 'CPC1', 0.1, 188, 0.034719779142638726)
**** log-parameter_analysis 运行时间： 2025-02-10 11:46:20 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.0859349700230169
DataOwner2: noise random: 0.03543508574912054
DataOwner3: noise random: 0.01723000772571297
DataOwner4: noise random: 0.06285776136244815
DataOwner5: noise random: 0.09169995196554091
DataOwner6: noise random: 0.0439748185169328
DataOwner7: noise random: 0.0017967806068931891
DataOwner8: noise random: 0.09936380648283344
DataOwner9: noise random: 0.03326682997633205
DataOwner10: noise random: 0.04687710372233969
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9970149729166519, 0.999492733866704, 0.9998799856640866, 0.9983994974776548, 0.9966047362398143, 0.9992178456300606, 0.9999986924228778, 0.9960157398266375, 0.9995517268786099, 0.9991102508592965]
归一化后的数据质量列表avg_f_list: [0.925087747490583, 0.9872968973657545, 0.997019629133845, 0.9598490088299682, 0.9147879343011204, 0.9803952777757313, 1.0, 0.9, 0.9887780350514406, 0.9776938956185443]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3448
DataOwner1的最优x_1 = 0.0809
DataOwner2的最优x_2 = 0.1453
DataOwner3的最优x_3 = 0.1539
DataOwner4的最优x_4 = 0.1190
DataOwner5的最优x_5 = 0.0684
DataOwner6的最优x_6 = 0.1390
DataOwner7的最优x_7 = 0.1564
DataOwner8的最优x_8 = 0.0494
DataOwner9的最优x_9 = 0.1466
DataOwner10的最优x_10 = 0.1365
每个DataOwner应该贡献数据比例 xn_list = [0.08088012282080803, 0.14529988953079237, 0.15386551683724273, 0.11904854601500288, 0.06838430026583879, 0.1389943373832799, 0.1564190829967212, 0.049397976108504674, 0.14662841397916293, 0.13647377012448744]
ModelOwner的最大效用 U(Eta) = 0.5852
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.344767529803467
DataOwner1的分配到的支付 ： 0.0864
DataOwner2的分配到的支付 ： 0.1657
DataOwner3的分配到的支付 ： 0.1772
DataOwner4的分配到的支付 ： 0.1320
DataOwner5的分配到的支付 ： 0.0723
DataOwner6的分配到的支付 ： 0.1574
DataOwner7的分配到的支付 ： 0.1807
DataOwner8的分配到的支付 ： 0.0514
DataOwner9的分配到的支付 ： 0.1675
DataOwner10的分配到的支付 ： 0.1541
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC3', 'DataOwner4': 'CPC10', 'DataOwner2': 'CPC6', 'DataOwner3': 'CPC8', 'DataOwner7': 'CPC9', 'DataOwner6': 'CPC5', 'DataOwner9': 'CPC7', 'DataOwner5': 'CPC2', 'DataOwner10': 'CPC4', 'DataOwner8': 'CPC1'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC3
DataOwner4 把数据交给 CPC10
DataOwner2 把数据交给 CPC6
DataOwner3 把数据交给 CPC8
DataOwner7 把数据交给 CPC9
DataOwner6 把数据交给 CPC5
DataOwner9 把数据交给 CPC7
DataOwner5 把数据交给 CPC2
DataOwner10 把数据交给 CPC4
DataOwner8 把数据交给 CPC1
DONE
最终Um的列表：
[('DataOwner1', 'CPC3', 0.3, 404, 0.05661608597456562), ('DataOwner4', 'CPC10', 1.0, 119, 1.3877787807814457e-17), ('DataOwner2', 'CPC6', 0.6, 145, 0.05811995581231695), ('DataOwner3', 'CPC8', 0.8, 1384, 0.03077310336744854), ('DataOwner7', 'CPC9', 0.9, 469, 0.015641908299672103), ('DataOwner6', 'CPC5', 0.5, 1806, 0.06949716869163995), ('DataOwner9', 'CPC7', 0.7, 1759, 0.04398852419374888), ('DataOwner5', 'CPC2', 0.2, 683, 0.05470744021267103), ('DataOwner10', 'CPC4', 0.4, 682, 0.08188426207469246), ('DataOwner8', 'CPC1', 0.1, 49, 0.044458178497654204)]
('DataOwner1', 'CPC3', 0.3, 404, 0.05661608597456562)
('DataOwner4', 'CPC10', 1.0, 119, 1.3877787807814457e-17)
('DataOwner2', 'CPC6', 0.6, 145, 0.05811995581231695)
('DataOwner3', 'CPC8', 0.8, 1384, 0.03077310336744854)
('DataOwner7', 'CPC9', 0.9, 469, 0.015641908299672103)
('DataOwner6', 'CPC5', 0.5, 1806, 0.06949716869163995)
('DataOwner9', 'CPC7', 0.7, 1759, 0.04398852419374888)
('DataOwner5', 'CPC2', 0.2, 683, 0.05470744021267103)
('DataOwner10', 'CPC4', 0.4, 682, 0.08188426207469246)
('DataOwner8', 'CPC1', 0.1, 49, 0.044458178497654204)
**** log-parameter_analysis 运行时间： 2025-02-10 11:49:28 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.060591680879143066
DataOwner2: noise random: 0.052317518928964316
DataOwner3: noise random: 0.005010157756773959
DataOwner4: noise random: 0.05374046344297611
DataOwner5: noise random: 0.07443776039407222
DataOwner6: noise random: 0.02663143743914236
DataOwner7: noise random: 0.08885050076158005
DataOwner8: noise random: 0.04045446369096707
DataOwner9: noise random: 0.010091532257505188
DataOwner10: noise random: 0.024949502763331945
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9985168898251534, 0.9988905344551139, 0.999989842734916, 0.9988314366282445, 0.9977584673386783, 0.9997126250782208, 0.9968057040186149, 0.9993390218953616, 0.9999587736984932, 0.99974786177423]
归一化后的数据质量列表avg_f_list: [0.9537409315045905, 0.9654754903052991, 1.0, 0.9636194836380372, 0.9299221674980995, 0.9912937946052416, 0.9, 0.9795605374783921, 0.9990242561901042, 0.992400426544009]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3504
DataOwner1的最优x_1 = 0.1083
DataOwner2的最优x_2 = 0.1205
DataOwner3的最优x_3 = 0.1529
DataOwner4的最优x_4 = 0.1186
DataOwner5的最优x_5 = 0.0816
DataOwner6的最优x_6 = 0.1452
DataOwner7的最优x_7 = 0.0437
DataOwner8的最优x_8 = 0.1343
DataOwner9的最优x_9 = 0.1520
DataOwner10的最优x_10 = 0.1462
每个DataOwner应该贡献数据比例 xn_list = [0.10834509313074775, 0.12051409328538842, 0.1528885291322687, 0.11863101269650293, 0.08161356142251044, 0.1451792165953687, 0.043740065780291146, 0.1343150679144791, 0.15203898691852466, 0.14617544022750972]
ModelOwner的最大效用 U(Eta) = 0.5917
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3503576779016435
DataOwner1的分配到的支付 ： 0.1188
DataOwner2的分配到的支付 ： 0.1338
DataOwner3的分配到的支付 ： 0.1758
DataOwner4的分配到的支付 ： 0.1314
DataOwner5的分配到的支付 ： 0.0873
DataOwner6的分配到的支付 ： 0.1655
DataOwner7的分配到的支付 ： 0.0453
DataOwner8的分配到的支付 ： 0.1513
DataOwner9的分配到的支付 ： 0.1746
DataOwner10的分配到的支付 ： 0.1668
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC10', 'DataOwner2': 'CPC4', 'DataOwner3': 'CPC9', 'DataOwner6': 'CPC6', 'DataOwner9': 'CPC8', 'DataOwner4': 'CPC3', 'DataOwner8': 'CPC5', 'DataOwner10': 'CPC7', 'DataOwner5': 'CPC2', 'DataOwner7': 'CPC1'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC10
DataOwner2 把数据交给 CPC4
DataOwner3 把数据交给 CPC9
DataOwner6 把数据交给 CPC6
DataOwner9 把数据交给 CPC8
DataOwner4 把数据交给 CPC3
DataOwner8 把数据交给 CPC5
DataOwner10 把数据交给 CPC7
DataOwner5 把数据交给 CPC2
DataOwner7 把数据交给 CPC1
DONE
最终Um的列表：
[('DataOwner1', 'CPC10', 1.0, 133, 0.0), ('DataOwner2', 'CPC4', 0.4, 295, 0.07230845597123306), ('DataOwner3', 'CPC9', 0.9, 187, 0.015288852913226875), ('DataOwner6', 'CPC6', 0.6, 710, 0.05807168663814748), ('DataOwner9', 'CPC8', 0.8, 4467, 0.03040779738370493), ('DataOwner4', 'CPC3', 0.3, 145, 0.08304170888755205), ('DataOwner8', 'CPC5', 0.5, 822, 0.06715753395723956), ('DataOwner10', 'CPC7', 0.7, 536, 0.04385263206825292), ('DataOwner5', 'CPC2', 0.2, 299, 0.06529084913800835), ('DataOwner7', 'CPC1', 0.1, 267, 0.03936605920226203)]
('DataOwner1', 'CPC10', 1.0, 133, 0.0)
('DataOwner2', 'CPC4', 0.4, 295, 0.07230845597123306)
('DataOwner3', 'CPC9', 0.9, 187, 0.015288852913226875)
('DataOwner6', 'CPC6', 0.6, 710, 0.05807168663814748)
('DataOwner9', 'CPC8', 0.8, 4467, 0.03040779738370493)
('DataOwner4', 'CPC3', 0.3, 145, 0.08304170888755205)
('DataOwner8', 'CPC5', 0.5, 822, 0.06715753395723956)
('DataOwner10', 'CPC7', 0.7, 536, 0.04385263206825292)
('DataOwner5', 'CPC2', 0.2, 299, 0.06529084913800835)
('DataOwner7', 'CPC1', 0.1, 267, 0.03936605920226203)
**** log-parameter_analysis 运行时间： 2025-02-10 11:50:38 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.03796826078113857
DataOwner2: noise random: 0.0493639800427727
DataOwner3: noise random: 0.03980632217594611
DataOwner4: noise random: 0.07726622462491467
DataOwner5: noise random: 0.00349814109119887
DataOwner6: noise random: 0.07489526645193662
DataOwner7: noise random: 0.055864789535616105
DataOwner8: noise random: 0.07270557507902076
DataOwner9: noise random: 0.03740083686125406
DataOwner10: noise random: 0.09406445612756295
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9994166139874409, 0.9990155580172811, 0.9993581506197454, 0.9975812241413291, 0.9999950463298363, 0.9977343637913134, 0.998734775120638, 0.997858505490409, 0.9994336523532524, 0.9964193631969982]
归一化后的数据质量列表avg_f_list: [0.983823165506942, 0.9726069599523578, 0.9821881389812824, 0.9324933977974922, 1.0, 0.9367762059853297, 0.9647543934297681, 0.9402480376461247, 0.9842996720982307, 0.9]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3411
DataOwner1的最优x_1 = 0.1446
DataOwner2的最优x_2 = 0.1342
DataOwner3的最优x_3 = 0.1431
DataOwner4的最优x_4 = 0.0926
DataOwner5的最优x_5 = 0.1586
DataOwner6的最优x_6 = 0.0974
DataOwner7的最优x_7 = 0.1266
DataOwner8的最优x_8 = 0.1012
DataOwner9的最优x_9 = 0.1450
DataOwner10的最优x_10 = 0.0530
每个DataOwner应该贡献数据比例 xn_list = [0.14456523599305318, 0.13419693051311266, 0.14308550234660736, 0.0926033507848333, 0.15864705906980942, 0.09740658072593673, 0.1266266089652992, 0.10123336126259061, 0.14499448616516825, 0.05298587772934105]
ModelOwner的最大效用 U(Eta) = 0.5810
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.341135258318736
DataOwner1的分配到的支付 ： 0.1648
DataOwner2的分配到的支付 ： 0.1513
DataOwner3的分配到的支付 ： 0.1629
DataOwner4的分配到的支付 ： 0.1001
DataOwner5的分配到的支付 ： 0.1839
DataOwner6的分配到的支付 ： 0.1057
DataOwner7的分配到的支付 ： 0.1416
DataOwner8的分配到的支付 ： 0.1103
DataOwner9的分配到的支付 ： 0.1654
DataOwner10的分配到的支付 ： 0.0553
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC10', 'DataOwner2': 'CPC6', 'DataOwner3': 'CPC7', 'DataOwner5': 'CPC9', 'DataOwner9': 'CPC8', 'DataOwner4': 'CPC2', 'DataOwner6': 'CPC3', 'DataOwner7': 'CPC5', 'DataOwner8': 'CPC4', 'DataOwner10': 'CPC1'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC10
DataOwner2 把数据交给 CPC6
DataOwner3 把数据交给 CPC7
DataOwner5 把数据交给 CPC9
DataOwner9 把数据交给 CPC8
DataOwner4 把数据交给 CPC2
DataOwner6 把数据交给 CPC3
DataOwner7 把数据交给 CPC5
DataOwner8 把数据交给 CPC4
DataOwner10 把数据交给 CPC1
DONE
最终Um的列表：
[('DataOwner1', 'CPC10', 1.0, 501, 0.0), ('DataOwner2', 'CPC6', 0.6, 309, 0.05367877220524507), ('DataOwner3', 'CPC7', 0.7, 3136, 0.042925650703982215), ('DataOwner5', 'CPC9', 0.9, 1647, 0.015864705906980914), ('DataOwner9', 'CPC8', 0.8, 334, 0.028998897233033644), ('DataOwner4', 'CPC2', 0.2, 213, 0.07408268062786663), ('DataOwner6', 'CPC3', 0.3, 786, 0.06818460650815572), ('DataOwner7', 'CPC5', 0.5, 584, 0.0633133044826496), ('DataOwner8', 'CPC4', 0.4, 350, 0.06074001675755436), ('DataOwner10', 'CPC1', 0.1, 61, 0.04768728995640694)]
('DataOwner1', 'CPC10', 1.0, 501, 0.0)
('DataOwner2', 'CPC6', 0.6, 309, 0.05367877220524507)
('DataOwner3', 'CPC7', 0.7, 3136, 0.042925650703982215)
('DataOwner5', 'CPC9', 0.9, 1647, 0.015864705906980914)
('DataOwner9', 'CPC8', 0.8, 334, 0.028998897233033644)
('DataOwner4', 'CPC2', 0.2, 213, 0.07408268062786663)
('DataOwner6', 'CPC3', 0.3, 786, 0.06818460650815572)
('DataOwner7', 'CPC5', 0.5, 584, 0.0633133044826496)
('DataOwner8', 'CPC4', 0.4, 350, 0.06074001675755436)
('DataOwner10', 'CPC1', 0.1, 61, 0.04768728995640694)
**** log-parameter_analysis 运行时间： 2025-02-10 11:59:02 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.08268479146069521
DataOwner2: noise random: 0.03825111233681647
DataOwner3: noise random: 0.002007810300976609
DataOwner4: noise random: 0.0329344367606599
DataOwner5: noise random: 0.008267037072936545
DataOwner6: noise random: 0.06441108802865902
DataOwner7: noise random: 0.059877545278828465
DataOwner8: noise random: 0.09466129431945322
DataOwner9: noise random: 0.012975848127704881
DataOwner10: noise random: 0.07475566396406541
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9972339389650342, 0.9994081754579272, 0.9999983652608878, 0.9995616317522863, 0.9999723367208814, 0.9983210544779659, 0.9985395203918841, 0.9963789843775991, 0.9999320115319764, 0.997741821493616]
归一化后的数据质量列表avg_f_list: [0.9236215699591769, 0.9836936254571697, 1.0, 0.9879334747382353, 0.9992808565650932, 0.9536575221837995, 0.9596935244991915, 0.9, 0.9981667105217426, 0.9376538739625158]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3464
DataOwner1的最优x_1 = 0.0777
DataOwner2的最优x_2 = 0.1409
DataOwner3的最优x_3 = 0.1554
DataOwner4的最优x_4 = 0.1448
DataOwner5的最优x_5 = 0.1548
DataOwner6的最优x_6 = 0.1114
DataOwner7的最优x_7 = 0.1176
DataOwner8的最优x_8 = 0.0477
DataOwner9的最优x_9 = 0.1538
DataOwner10的最优x_10 = 0.0940
每个DataOwner应该贡献数据比例 xn_list = [0.07765348588572442, 0.14091687422807808, 0.15539095967869465, 0.1447801871486693, 0.15477412696385856, 0.11138602777554389, 0.1176385451214342, 0.04774569409549946, 0.15381465677695277, 0.09397515281408325]
ModelOwner的最大效用 U(Eta) = 0.5871
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3464256874643068
DataOwner1的分配到的支付 ： 0.0827
DataOwner2的分配到的支付 ： 0.1599
DataOwner3的分配到的支付 ： 0.1793
DataOwner4的分配到的支付 ： 0.1650
DataOwner5的分配到的支付 ： 0.1784
DataOwner6的分配到的支付 ： 0.1225
DataOwner7的分配到的支付 ： 0.1302
DataOwner8的分配到的支付 ： 0.0496
DataOwner9的分配到的支付 ： 0.1771
DataOwner10的分配到的支付 ： 0.1016
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
**** log-parameter_analysis 运行时间： 2025-02-10 11:59:30 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.056961568711033084
DataOwner2: noise random: 0.09432441068252388
DataOwner3: noise random: 0.08325774848718144
DataOwner4: noise random: 0.08414495490028306
DataOwner5: noise random: 0.0192223173297871
DataOwner6: noise random: 0.030687259991455385
DataOwner7: noise random: 0.009921325244641878
DataOwner8: noise random: 0.050382154653668354
DataOwner9: noise random: 0.07597859765349538
DataOwner10: noise random: 0.04397934982895794
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9986912258766465, 0.9964134612875967, 0.9971933700882227, 0.9971306342304056, 0.9998504016779651, 0.9996190911434719, 0.9999600671567094, 0.9989718253550426, 0.9976700866506034, 0.9992178914655558]
归一化后的数据质量列表avg_f_list: [0.9642237867163874, 0.9, 0.9219902867532664, 0.9202213882589766, 0.9969078752251745, 0.9903858498569855, 1.0, 0.9721355617698217, 0.9354317736275876, 0.9790736349472288]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3387
DataOwner1的最优x_1 = 0.1278
DataOwner2的最优x_2 = 0.0553
DataOwner3的最优x_3 = 0.0825
DataOwner4的最优x_4 = 0.0804
DataOwner5的最优x_5 = 0.1575
DataOwner6的最优x_6 = 0.1519
DataOwner7的最优x_7 = 0.1601
DataOwner8的最优x_8 = 0.1354
DataOwner9的最优x_9 = 0.0979
DataOwner10的最优x_10 = 0.1418
每个DataOwner应该贡献数据比例 xn_list = [0.12783263222333735, 0.05532209317768143, 0.08253686554061453, 0.08044634992019112, 0.15750098364206888, 0.1519167869157789, 0.16009313482352364, 0.13541158469842862, 0.09789056688039592, 0.1418431611829905]
ModelOwner的最大效用 U(Eta) = 0.5782
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3387330527435828
DataOwner1的分配到的支付 ： 0.1431
DataOwner2的分配到的支付 ： 0.0578
DataOwner3的分配到的支付 ： 0.0884
DataOwner4的分配到的支付 ： 0.0860
DataOwner5的分配到的支付 ： 0.1823
DataOwner6的分配到的支付 ： 0.1747
DataOwner7的分配到的支付 ： 0.1859
DataOwner8的分配到的支付 ： 0.1529
DataOwner9的分配到的支付 ： 0.1063
DataOwner10的分配到的支付 ： 0.1613
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC10', 'DataOwner2': 'CPC1', 'DataOwner3': 'CPC3', 'DataOwner5': 'CPC8', 'DataOwner7': 'CPC9', 'DataOwner4': 'CPC2', 'DataOwner6': 'CPC7', 'DataOwner8': 'CPC5', 'DataOwner10': 'CPC6', 'DataOwner9': 'CPC4'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC10
DataOwner2 把数据交给 CPC1
DataOwner3 把数据交给 CPC3
DataOwner5 把数据交给 CPC8
DataOwner7 把数据交给 CPC9
DataOwner4 把数据交给 CPC2
DataOwner6 把数据交给 CPC7
DataOwner8 把数据交给 CPC5
DataOwner10 把数据交给 CPC6
DataOwner9 把数据交给 CPC4
DONE
最终Um的列表：
[('DataOwner1', 'CPC10', 1.0, 627, 0.0), ('DataOwner2', 'CPC1', 0.1, 135, 0.049789883859913286), ('DataOwner3', 'CPC3', 0.3, 808, 0.05777580587843017), ('DataOwner5', 'CPC8', 0.8, 385, 0.03150019672841378), ('DataOwner7', 'CPC9', 0.9, 783, 0.01600931348235235), ('DataOwner4', 'CPC2', 0.2, 98, 0.06435707993615289), ('DataOwner6', 'CPC7', 0.7, 2046, 0.04557503607473368), ('DataOwner8', 'CPC5', 0.5, 663, 0.06770579234921431), ('DataOwner10', 'CPC6', 0.6, 1389, 0.0567372644731962), ('DataOwner9', 'CPC4', 0.4, 599, 0.058734340128237535)]
('DataOwner1', 'CPC10', 1.0, 627, 0.0)
('DataOwner2', 'CPC1', 0.1, 135, 0.049789883859913286)
('DataOwner3', 'CPC3', 0.3, 808, 0.05777580587843017)
('DataOwner5', 'CPC8', 0.8, 385, 0.03150019672841378)
('DataOwner7', 'CPC9', 0.9, 783, 0.01600931348235235)
('DataOwner4', 'CPC2', 0.2, 98, 0.06435707993615289)
('DataOwner6', 'CPC7', 0.7, 2046, 0.04557503607473368)
('DataOwner8', 'CPC5', 0.5, 663, 0.06770579234921431)
('DataOwner10', 'CPC6', 0.6, 1389, 0.0567372644731962)
('DataOwner9', 'CPC4', 0.4, 599, 0.058734340128237535)
**** log-parameter_analysis 运行时间： 2025-02-10 12:00:08 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.05462274849644521
DataOwner2: noise random: 0.009827584399234202
DataOwner3: noise random: 0.07617490401970084
DataOwner4: noise random: 0.026811301287364767
DataOwner5: noise random: 0.08588164256697811
DataOwner6: noise random: 0.036944755668357254
DataOwner7: noise random: 0.08515475515392479
DataOwner8: noise random: 0.07223635742231233
DataOwner9: noise random: 0.03898405397627226
DataOwner10: noise random: 0.04849882991088479
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9987916714105457, 0.9999608459957882, 0.9976549388590893, 0.999708591095438, 0.9970167285448814, 0.9994481995814541, 0.9970646875625596, 0.9978914359670389, 0.9993863452889531, 0.9990493939030604]
归一化后的数据质量列表avg_f_list: [0.9602877736796003, 1.0, 0.9216774746541209, 0.9914319009157566, 0.9, 0.9825874333180515, 0.9016289777319655, 0.9297103439907963, 0.9804864881780402, 0.9690415851973876]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3333
DataOwner1的最优x_1 = 0.1279
DataOwner2的最优x_2 = 0.1633
DataOwner3的最优x_3 = 0.0869
DataOwner4的最优x_4 = 0.1562
DataOwner5的最优x_5 = 0.0605
DataOwner6的最优x_6 = 0.1485
DataOwner7的最优x_7 = 0.0626
DataOwner8的最优x_8 = 0.0960
DataOwner9的最优x_9 = 0.1467
DataOwner10的最优x_10 = 0.1362
每个DataOwner应该贡献数据比例 xn_list = [0.12785961513065336, 0.16328848456132664, 0.0868556284347904, 0.15616156877077797, 0.06050655644351454, 0.1485168090274973, 0.06257805157150483, 0.09598156114528048, 0.14665652356805112, 0.13621310738178138]
ModelOwner的最大效用 U(Eta) = 0.5720
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3332915731319002
DataOwner1的分配到的支付 ： 0.1433
DataOwner2的分配到的支付 ： 0.1905
DataOwner3的分配到的支付 ： 0.0934
DataOwner4的分配到的支付 ： 0.1806
DataOwner5的分配到的支付 ： 0.0635
DataOwner6的分配到的支付 ： 0.1703
DataOwner7的分配到的支付 ： 0.0658
DataOwner8的分配到的支付 ： 0.1041
DataOwner9的分配到的支付 ： 0.1678
DataOwner10的分配到的支付 ： 0.1540
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC4', 'DataOwner8': 'CPC10', 'DataOwner2': 'CPC9', 'DataOwner3': 'CPC3', 'DataOwner4': 'CPC8', 'DataOwner6': 'CPC7', 'DataOwner5': 'CPC1', 'DataOwner9': 'CPC6', 'DataOwner7': 'CPC2', 'DataOwner10': 'CPC5'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC4
DataOwner8 把数据交给 CPC10
DataOwner2 把数据交给 CPC9
DataOwner3 把数据交给 CPC3
DataOwner4 把数据交给 CPC8
DataOwner6 把数据交给 CPC7
DataOwner5 把数据交给 CPC1
DataOwner9 把数据交给 CPC6
DataOwner7 把数据交给 CPC2
DataOwner10 把数据交给 CPC5
DONE
最终Um的列表：
[('DataOwner1', 'CPC4', 0.4, 213, 0.076715769078392), ('DataOwner8', 'CPC10', 1.0, 639, 1.3877787807814457e-17), ('DataOwner2', 'CPC9', 0.9, 544, 0.016328848456132666), ('DataOwner3', 'CPC3', 0.3, 868, 0.06079893990435328), ('DataOwner4', 'CPC8', 0.8, 1040, 0.031232313754155583), ('DataOwner6', 'CPC7', 0.7, 742, 0.0445550427082492), ('DataOwner5', 'CPC1', 0.1, 504, 0.054455900799163084), ('DataOwner9', 'CPC6', 0.6, 1710, 0.058662609427220444), ('DataOwner7', 'CPC2', 0.2, 104, 0.05006244125720386), ('DataOwner10', 'CPC5', 0.5, 681, 0.06810655369089069)]
('DataOwner1', 'CPC4', 0.4, 213, 0.076715769078392)
('DataOwner8', 'CPC10', 1.0, 639, 1.3877787807814457e-17)
('DataOwner2', 'CPC9', 0.9, 544, 0.016328848456132666)
('DataOwner3', 'CPC3', 0.3, 868, 0.06079893990435328)
('DataOwner4', 'CPC8', 0.8, 1040, 0.031232313754155583)
('DataOwner6', 'CPC7', 0.7, 742, 0.0445550427082492)
('DataOwner5', 'CPC1', 0.1, 504, 0.054455900799163084)
('DataOwner9', 'CPC6', 0.6, 1710, 0.058662609427220444)
('DataOwner7', 'CPC2', 0.2, 104, 0.05006244125720386)
('DataOwner10', 'CPC5', 0.5, 681, 0.06810655369089069)
**** log-parameter_analysis 运行时间： 2025-02-10 12:00:21 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.060893170149134625
DataOwner2: noise random: 0.007682529159279395
DataOwner3: noise random: 0.07721491995103674
DataOwner4: noise random: 0.006775611143845806
DataOwner5: noise random: 0.088869285633596
DataOwner6: noise random: 0.05954948954383781
DataOwner7: noise random: 0.07475542673822014
DataOwner8: noise random: 0.01889273637232397
DataOwner9: noise random: 0.013904245662476378
DataOwner10: noise random: 0.04193542917866632
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9985029312381631, 0.9999760938680745, 0.9975928815940628, 0.9999814308214092, 0.9968077490300982, 0.9985693576398714, 0.9977422934531436, 0.9998559892321537, 0.9999216468375975, 0.9992888394858362]
归一化后的数据质量列表avg_f_list: [0.9534137421308608, 0.9998318371630917, 0.9247388558649504, 1.0, 0.9, 0.9555067812594259, 0.9294466958093915, 0.9960474427650884, 0.9981162577806207, 0.9781770391263181]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3453
DataOwner1的最优x_1 = 0.1120
DataOwner2的最优x_2 = 0.1559
DataOwner3的最优x_3 = 0.0800
DataOwner4的最优x_4 = 0.1561
DataOwner5的最优x_5 = 0.0489
DataOwner6的最优x_6 = 0.1142
DataOwner7的最优x_7 = 0.0855
DataOwner8的最优x_8 = 0.1527
DataOwner9的最优x_9 = 0.1545
DataOwner10的最优x_10 = 0.1365
每个DataOwner应该贡献数据比例 xn_list = [0.11199371725458615, 0.15593578246101705, 0.07997875379734791, 0.1560792666646977, 0.048852061922016826, 0.11417458365371476, 0.08551998582518117, 0.15267873560225215, 0.15446593555534144, 0.13654827759272004]
ModelOwner的最大效用 U(Eta) = 0.5858
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3453133985878036
DataOwner1的分配到的支付 ： 0.1233
DataOwner2的分配到的支付 ： 0.1800
DataOwner3的分配到的支付 ： 0.0854
DataOwner4的分配到的支付 ： 0.1802
DataOwner5的分配到的支付 ： 0.0508
DataOwner6的分配到的支付 ： 0.1260
DataOwner7的分配到的支付 ： 0.0918
DataOwner8的分配到的支付 ： 0.1756
DataOwner9的分配到的支付 ： 0.1780
DataOwner10的分配到的支付 ： 0.1542
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC4', 'DataOwner6': 'CPC5', 'DataOwner8': 'CPC10', 'DataOwner2': 'CPC8', 'DataOwner4': 'CPC9', 'DataOwner3': 'CPC2', 'DataOwner9': 'CPC7', 'DataOwner5': 'CPC1', 'DataOwner7': 'CPC3', 'DataOwner10': 'CPC6'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC4
DataOwner6 把数据交给 CPC5
DataOwner8 把数据交给 CPC10
DataOwner2 把数据交给 CPC8
DataOwner4 把数据交给 CPC9
DataOwner3 把数据交给 CPC2
DataOwner9 把数据交给 CPC7
DataOwner5 把数据交给 CPC1
DataOwner7 把数据交给 CPC3
DataOwner10 把数据交给 CPC6
DONE
最终Um的列表：
[('DataOwner1', 'CPC4', 0.4, 103, 0.06719623035275168), ('DataOwner6', 'CPC5', 0.5, 316, 0.05708729182685739), ('DataOwner8', 'CPC10', 1.0, 422, 2.7755575615628914e-17), ('DataOwner2', 'CPC8', 0.8, 1151, 0.03118715649220341), ('DataOwner4', 'CPC9', 0.9, 288, 0.015607926666469762), ('DataOwner3', 'CPC2', 0.2, 221, 0.06398300303787832), ('DataOwner9', 'CPC7', 0.7, 2138, 0.046339780666602465), ('DataOwner5', 'CPC1', 0.1, 450, 0.043966855729815145), ('DataOwner7', 'CPC3', 0.3, 157, 0.059863990077626814), ('DataOwner10', 'CPC6', 0.6, 2268, 0.05461931103708802)]
('DataOwner1', 'CPC4', 0.4, 103, 0.06719623035275168)
('DataOwner6', 'CPC5', 0.5, 316, 0.05708729182685739)
('DataOwner8', 'CPC10', 1.0, 422, 2.7755575615628914e-17)
('DataOwner2', 'CPC8', 0.8, 1151, 0.03118715649220341)
('DataOwner4', 'CPC9', 0.9, 288, 0.015607926666469762)
('DataOwner3', 'CPC2', 0.2, 221, 0.06398300303787832)
('DataOwner9', 'CPC7', 0.7, 2138, 0.046339780666602465)
('DataOwner5', 'CPC1', 0.1, 450, 0.043966855729815145)
('DataOwner7', 'CPC3', 0.3, 157, 0.059863990077626814)
('DataOwner10', 'CPC6', 0.6, 2268, 0.05461931103708802)
**** log-parameter_analysis 运行时间： 2025-02-10 12:01:47 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.07383468853234948
DataOwner2: noise random: 0.07329127229378453
DataOwner3: noise random: 0.08594485726558611
DataOwner4: noise random: 0.08385491070376239
DataOwner5: noise random: 0.02776073113091906
DataOwner6: noise random: 0.011047673623449772
DataOwner7: noise random: 0.08146237986427303
DataOwner8: noise random: 0.05048624772931546
DataOwner9: noise random: 0.029186736149617823
DataOwner10: noise random: 0.00818601423364751
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9977943929049111, 0.9978232806375449, 0.9970039966709712, 0.9971550861531063, 0.9996884440128659, 0.9999507677190123, 0.9973087638880881, 0.9989686356192553, 0.9996546825350011, 0.9999728696377188]
归一化后的数据质量列表avg_f_list: [0.9266227704180223, 0.9275957905828227, 0.9, 0.9050891191313138, 0.9904197441911955, 0.9992555451528543, 0.9102654179053942, 0.9661745709664491, 0.9892825625656101, 1.0]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3302
DataOwner1的最优x_1 = 0.0950
DataOwner2的最优x_2 = 0.0961
DataOwner3的最优x_3 = 0.0634
DataOwner4的最优x_4 = 0.0697
DataOwner5的最优x_5 = 0.1571
DataOwner6的最优x_6 = 0.1644
DataOwner7的最优x_7 = 0.0760
DataOwner8的最优x_8 = 0.1356
DataOwner9的最优x_9 = 0.1562
DataOwner10的最优x_10 = 0.1650
每个DataOwner应该贡献数据比例 xn_list = [0.09504275965614022, 0.09612940830527501, 0.0633735776715106, 0.06972731614263085, 0.15714957914584163, 0.1644452739887388, 0.07604048614090805, 0.1356010421335812, 0.1561897636167802, 0.16504707389215373]
ModelOwner的最大效用 U(Eta) = 0.5685
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3302142608397067
DataOwner1的分配到的支付 ： 0.1030
DataOwner2的分配到的支付 ： 0.1043
DataOwner3的分配到的支付 ： 0.0667
DataOwner4的分配到的支付 ： 0.0738
DataOwner5的分配到的支付 ： 0.1821
DataOwner6的分配到的支付 ： 0.1922
DataOwner7的分配到的支付 ： 0.0810
DataOwner8的分配到的支付 ： 0.1533
DataOwner9的分配到的支付 ： 0.1808
DataOwner10的分配到的支付 ： 0.1931
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC4', 'DataOwner10': 'CPC10', 'DataOwner2': 'CPC5', 'DataOwner5': 'CPC8', 'DataOwner6': 'CPC9', 'DataOwner3': 'CPC1', 'DataOwner4': 'CPC2', 'DataOwner8': 'CPC6', 'DataOwner9': 'CPC7', 'DataOwner7': 'CPC3'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC4
DataOwner10 把数据交给 CPC10
DataOwner2 把数据交给 CPC5
DataOwner5 把数据交给 CPC8
DataOwner6 把数据交给 CPC9
DataOwner3 把数据交给 CPC1
DataOwner4 把数据交给 CPC2
DataOwner8 把数据交给 CPC6
DataOwner9 把数据交给 CPC7
DataOwner7 把数据交给 CPC3
DONE
最终Um的列表：
[('DataOwner1', 'CPC4', 0.4, 346, 0.05702565579368413), ('DataOwner10', 'CPC10', 1.0, 300, 2.7755575615628914e-17), ('DataOwner2', 'CPC5', 0.5, 436, 0.04806470415263751), ('DataOwner5', 'CPC8', 0.8, 714, 0.03142991582916832), ('DataOwner6', 'CPC9', 0.9, 448, 0.016444527398873865), ('DataOwner3', 'CPC1', 0.1, 518, 0.05703621990435954), ('DataOwner4', 'CPC2', 0.2, 570, 0.05578185291410468), ('DataOwner8', 'CPC6', 0.6, 2588, 0.05424041685343249), ('DataOwner9', 'CPC7', 0.7, 993, 0.04685692908503407), ('DataOwner7', 'CPC3', 0.3, 69, 0.05322834029863564)]
('DataOwner1', 'CPC4', 0.4, 346, 0.05702565579368413)
('DataOwner10', 'CPC10', 1.0, 300, 2.7755575615628914e-17)
('DataOwner2', 'CPC5', 0.5, 436, 0.04806470415263751)
('DataOwner5', 'CPC8', 0.8, 714, 0.03142991582916832)
('DataOwner6', 'CPC9', 0.9, 448, 0.016444527398873865)
('DataOwner3', 'CPC1', 0.1, 518, 0.05703621990435954)
('DataOwner4', 'CPC2', 0.2, 570, 0.05578185291410468)
('DataOwner8', 'CPC6', 0.6, 2588, 0.05424041685343249)
('DataOwner9', 'CPC7', 0.7, 993, 0.04685692908503407)
('DataOwner7', 'CPC3', 0.3, 69, 0.05322834029863564)
**** log-parameter_analysis 运行时间： 2025-02-10 12:53:29 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.010724079153763133
DataOwner2: noise random: 0.09770108048594996
DataOwner3: noise random: 0.05909750589692367
DataOwner4: noise random: 0.033597434254012404
DataOwner5: noise random: 0.08787619007382255
DataOwner6: noise random: 0.0625128339240354
DataOwner7: noise random: 0.004714756410213639
DataOwner8: noise random: 0.06735707269859183
DataOwner9: noise random: 0.009400528853021362
DataOwner10: noise random: 0.0393393020985596
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.999953445629287, 0.9961318742495388, 0.9985845117472064, 0.9995429605682701, 0.9968746525494298, 0.998417430868099, 0.9999910153340738, 0.998162774146721, 0.9999642931699787, 0.9993747844840323]
归一化后的数据质量列表avg_f_list: [0.9990264749600034, 0.9, 0.9635539733827376, 0.9883897801093826, 0.9192472439752873, 0.9592244898150859, 1.0, 0.9526256970837595, 0.9993075618768608, 0.9840319170369038]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3490
DataOwner1的最优x_1 = 0.1529
DataOwner2的最优x_2 = 0.0451
DataOwner3的最优x_3 = 0.1196
DataOwner4的最优x_4 = 0.1434
DataOwner5的最优x_5 = 0.0699
DataOwner6的最优x_6 = 0.1151
DataOwner7的最优x_7 = 0.1537
DataOwner8的最优x_8 = 0.1082
DataOwner9的最优x_9 = 0.1531
DataOwner10的最优x_10 = 0.1394
每个DataOwner应该贡献数据比例 xn_list = [0.15289062004468115, 0.045092286734586026, 0.11957307297849003, 0.14343721440258939, 0.06992100117701629, 0.11513974265677018, 0.15373414198748905, 0.10821760223075104, 0.1531345385022411, 0.1394355324805803]
ModelOwner的最大效用 U(Eta) = 0.5902
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3490361311301164
DataOwner1的分配到的支付 ： 0.1758
DataOwner2的分配到的支付 ： 0.0467
DataOwner3的分配到的支付 ： 0.1326
DataOwner4的分配到的支付 ： 0.1632
DataOwner5的分配到的支付 ： 0.0740
DataOwner6的分配到的支付 ： 0.1271
DataOwner7的分配到的支付 ： 0.1769
DataOwner8的分配到的支付 ： 0.1187
DataOwner9的分配到的支付 ： 0.1761
DataOwner10的分配到的支付 ： 0.1579
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC6', 'DataOwner2': 'CPC10', 'DataOwner3': 'CPC3', 'DataOwner5': 'CPC9', 'DataOwner7': 'CPC8', 'DataOwner4': 'CPC5', 'DataOwner9': 'CPC7', 'DataOwner6': 'CPC2', 'DataOwner10': 'CPC4', 'DataOwner8': 'CPC1'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC6
DataOwner2 把数据交给 CPC10
DataOwner3 把数据交给 CPC3
DataOwner5 把数据交给 CPC9
DataOwner7 把数据交给 CPC8
DataOwner4 把数据交给 CPC5
DataOwner9 把数据交给 CPC7
DataOwner6 把数据交给 CPC2
DataOwner10 把数据交给 CPC4
DataOwner8 把数据交给 CPC1
DONE
最终Um的列表：
[('DataOwner1', 'CPC6', 0.6, 1311, 0.061156248017872436), ('DataOwner2', 'CPC10', 1.3, 386, -0.01352768602037581), ('DataOwner3', 'CPC3', 0.3, 512, 0.08370115108494303), ('DataOwner5', 'CPC9', 1.2, 749, -0.013984200235403255), ('DataOwner7', 'CPC8', 0.8, 988, 0.030746828397497802), ('DataOwner4', 'CPC5', 0.5, 307, 0.07171860720129469), ('DataOwner9', 'CPC7', 0.7, 656, 0.04594036155067234), ('DataOwner6', 'CPC2', 0.2, 986, 0.09211179412541615), ('DataOwner10', 'CPC4', 0.4, 298, 0.08366131948834818), ('DataOwner8', 'CPC1', 0.1, 463, 0.09739584200767594)]
('DataOwner1', 'CPC6', 0.6, 1311, 0.061156248017872436)
('DataOwner2', 'CPC10', 1.3, 386, -0.01352768602037581)
('DataOwner3', 'CPC3', 0.3, 512, 0.08370115108494303)
('DataOwner5', 'CPC9', 1.2, 749, -0.013984200235403255)
('DataOwner7', 'CPC8', 0.8, 988, 0.030746828397497802)
('DataOwner4', 'CPC5', 0.5, 307, 0.07171860720129469)
('DataOwner9', 'CPC7', 0.7, 656, 0.04594036155067234)
('DataOwner6', 'CPC2', 0.2, 986, 0.09211179412541615)
('DataOwner10', 'CPC4', 0.4, 298, 0.08366131948834818)
('DataOwner8', 'CPC1', 0.1, 463, 0.09739584200767594)
**** log-parameter_analysis 运行时间： 2025-02-10 13:04:31 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.04115685188442515
DataOwner2: noise random: 0.017605055295827766
DataOwner3: noise random: 0.05228787508276464
DataOwner4: noise random: 0.07706052297732902
DataOwner5: noise random: 0.08473827803592227
DataOwner6: noise random: 0.036789974208745434
DataOwner7: noise random: 0.023579526810217646
DataOwner8: noise random: 0.07935647529822515
DataOwner9: noise random: 0.07272608341430646
DataOwner10: noise random: 0.0508044390517575
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.999313900158735, 0.9998747013825275, 0.9988995994182158, 0.9975887956052916, 0.9970895617945666, 0.999453100914762, 0.9997753798616312, 0.9974501286068013, 0.9978531857466089, 0.9989570094861102]
归一化后的数据质量列表avg_f_list: [0.9798645200327958, 1.0, 0.964989116935944, 0.917924911659111, 0.9, 0.9848625013414791, 0.9964338763728167, 0.9129460948310532, 0.9274177982081426, 0.9670504164177565]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3351
DataOwner1的最优x_1 = 0.1449
DataOwner2的最优x_2 = 0.1622
DataOwner3的最优x_3 = 0.1311
DataOwner4的最优x_4 = 0.0809
DataOwner5的最优x_5 = 0.0588
DataOwner6的最优x_6 = 0.1494
DataOwner7的最优x_7 = 0.1593
DataOwner8的最优x_8 = 0.0749
DataOwner9的最优x_9 = 0.0919
DataOwner10的最优x_10 = 0.1331
每个DataOwner应该贡献数据比例 xn_list = [0.14493267224445217, 0.16223268340018526, 0.1311267425326977, 0.08089874960153286, 0.05879003799703863, 0.14937116360116887, 0.15927944574338143, 0.07493702659015387, 0.09190079912086387, 0.13309458341106747]
ModelOwner的最大效用 U(Eta) = 0.5741
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3351104703401324
DataOwner1的分配到的支付 ： 0.1654
DataOwner2的分配到的支付 ： 0.1890
DataOwner3的分配到的支付 ： 0.1474
DataOwner4的分配到的支付 ： 0.0865
DataOwner5的分配到的支付 ： 0.0616
DataOwner6的分配到的支付 ： 0.1714
DataOwner7的分配到的支付 ： 0.1849
DataOwner8的分配到的支付 ： 0.0797
DataOwner9的分配到的支付 ： 0.0993
DataOwner10的分配到的支付 ： 0.1499
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC6', 'DataOwner8': 'CPC10', 'DataOwner2': 'CPC9', 'DataOwner3': 'CPC4', 'DataOwner6': 'CPC7', 'DataOwner7': 'CPC8', 'DataOwner4': 'CPC2', 'DataOwner5': 'CPC1', 'DataOwner10': 'CPC5', 'DataOwner9': 'CPC3'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC6
DataOwner8 把数据交给 CPC10
DataOwner2 把数据交给 CPC9
DataOwner3 把数据交给 CPC4
DataOwner6 把数据交给 CPC7
DataOwner7 把数据交给 CPC8
DataOwner4 把数据交给 CPC2
DataOwner5 把数据交给 CPC1
DataOwner10 把数据交给 CPC5
DataOwner9 把数据交给 CPC3
DONE
最终Um的列表：
[('DataOwner1', 'CPC6', 0.6, 1790, 0.057973068897780866), ('DataOwner8', 'CPC10', 1.0, 1520, 1.3877787807814457e-17), ('DataOwner2', 'CPC9', 0.9, 429, 0.016223268340018526), ('DataOwner3', 'CPC4', 0.4, 115, 0.07867604551961863), ('DataOwner6', 'CPC7', 0.7, 1581, 0.04481134908035067), ('DataOwner7', 'CPC8', 0.8, 562, 0.03185588914867629), ('DataOwner4', 'CPC2', 0.2, 356, 0.06471899968122628), ('DataOwner5', 'CPC1', 0.1, 103, 0.052911034197334755), ('DataOwner10', 'CPC5', 0.5, 117, 0.06654729170553374), ('DataOwner9', 'CPC3', 0.3, 243, 0.0643305593846047)]
('DataOwner1', 'CPC6', 0.6, 1790, 0.057973068897780866)
('DataOwner8', 'CPC10', 1.0, 1520, 1.3877787807814457e-17)
('DataOwner2', 'CPC9', 0.9, 429, 0.016223268340018526)
('DataOwner3', 'CPC4', 0.4, 115, 0.07867604551961863)
('DataOwner6', 'CPC7', 0.7, 1581, 0.04481134908035067)
('DataOwner7', 'CPC8', 0.8, 562, 0.03185588914867629)
('DataOwner4', 'CPC2', 0.2, 356, 0.06471899968122628)
('DataOwner5', 'CPC1', 0.1, 103, 0.052911034197334755)
('DataOwner10', 'CPC5', 0.5, 117, 0.06654729170553374)
('DataOwner9', 'CPC3', 0.3, 243, 0.0643305593846047)
**** log-parameter_analysis 运行时间： 2025-02-10 13:06:35 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.007179130210491592
DataOwner2: noise random: 0.04558448642822996
DataOwner3: noise random: 0.0696571255414005
DataOwner4: noise random: 0.08303011605261967
DataOwner5: noise random: 0.03318333097795397
DataOwner6: noise random: 0.027478452284503564
DataOwner7: noise random: 0.03114563383944362
DataOwner8: noise random: 0.017274513953281036
DataOwner9: noise random: 0.09700514207803163
DataOwner10: noise random: 0.06811873192009664
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9999791431470749, 0.9991636272357726, 0.9980352374199473, 0.9972093877002723, 0.999554293549805, 0.9996949766330555, 0.999608133791852, 0.9998791956025869, 0.996190971279504, 0.9981222362672393]
归一化后的数据质量列表avg_f_list: [1.0, 0.9784720456248656, 0.9486848592122057, 0.9268841134027378, 0.9887848383832073, 0.9924985844372044, 0.9902061108050825, 0.9973615889673944, 0.9, 0.950981451086423]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3501
DataOwner1的最优x_1 = 0.1531
DataOwner2的最优x_2 = 0.1335
DataOwner3的最优x_3 = 0.1031
DataOwner4的最优x_4 = 0.0782
DataOwner5的最优x_5 = 0.1431
DataOwner6的最优x_6 = 0.1464
DataOwner7的最优x_7 = 0.1444
DataOwner8的最优x_8 = 0.1507
DataOwner9的最优x_9 = 0.0440
DataOwner10的最优x_10 = 0.1056
每个DataOwner应该贡献数据比例 xn_list = [0.1530525974693376, 0.13346089271793543, 0.10311422460419971, 0.07822789135417127, 0.1430760381378035, 0.14643381120823154, 0.14436751006365492, 0.15074924695362513, 0.04400228507156669, 0.10559837807097337]
ModelOwner的最大效用 U(Eta) = 0.5914
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3501021187594935
DataOwner1的分配到的支付 ： 0.1760
DataOwner2的分配到的支付 ： 0.1502
DataOwner3的分配到的支付 ： 0.1125
DataOwner4的分配到的支付 ： 0.0834
DataOwner5的分配到的支付 ： 0.1627
DataOwner6的分配到的支付 ： 0.1671
DataOwner7的分配到的支付 ： 0.1644
DataOwner8的分配到的支付 ： 0.1729
DataOwner9的分配到的支付 ： 0.0455
DataOwner10的分配到的支付 ： 0.1155
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC9', 'DataOwner2': 'CPC10', 'DataOwner3': 'CPC3', 'DataOwner5': 'CPC5', 'DataOwner6': 'CPC7', 'DataOwner8': 'CPC8', 'DataOwner4': 'CPC2', 'DataOwner7': 'CPC6', 'DataOwner10': 'CPC4', 'DataOwner9': 'CPC1'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC9
DataOwner2 把数据交给 CPC10
DataOwner3 把数据交给 CPC3
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC7
DataOwner8 把数据交给 CPC8
DataOwner4 把数据交给 CPC2
DataOwner7 把数据交给 CPC6
DataOwner10 把数据交给 CPC4
DataOwner9 把数据交给 CPC1
DONE
最终Um的列表：
[('DataOwner1', 'CPC9', 0.9, 1060, 0.015305259746933725), ('DataOwner2', 'CPC10', 1.0, 307, 0.0), ('DataOwner3', 'CPC3', 0.3, 713, 0.0721799572229398), ('DataOwner5', 'CPC5', 0.5, 330, 0.07153801906890175), ('DataOwner6', 'CPC7', 0.7, 1689, 0.04393014336246946), ('DataOwner8', 'CPC8', 0.8, 347, 0.030149849390725014), ('DataOwner4', 'CPC2', 0.2, 180, 0.06258231308333702), ('DataOwner7', 'CPC6', 0.6, 1332, 0.05774700402546197), ('DataOwner10', 'CPC4', 0.4, 243, 0.06335902684258402), ('DataOwner9', 'CPC1', 0.1, 609, 0.03960205656441002)]
('DataOwner1', 'CPC9', 0.9, 1060, 0.015305259746933725)
('DataOwner2', 'CPC10', 1.0, 307, 0.0)
('DataOwner3', 'CPC3', 0.3, 713, 0.0721799572229398)
('DataOwner5', 'CPC5', 0.5, 330, 0.07153801906890175)
('DataOwner6', 'CPC7', 0.7, 1689, 0.04393014336246946)
('DataOwner8', 'CPC8', 0.8, 347, 0.030149849390725014)
('DataOwner4', 'CPC2', 0.2, 180, 0.06258231308333702)
('DataOwner7', 'CPC6', 0.6, 1332, 0.05774700402546197)
('DataOwner10', 'CPC4', 0.4, 243, 0.06335902684258402)
('DataOwner9', 'CPC1', 0.1, 609, 0.03960205656441002)
**** log-parameter_analysis 运行时间： 2025-02-10 13:37:03 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.0544184771473676
DataOwner2: noise random: 0.07458038887739861
DataOwner3: noise random: 0.01555695200444216
DataOwner4: noise random: 0.014139404623574426
DataOwner5: noise random: 0.07576365357027327
DataOwner6: noise random: 0.017071309411689363
DataOwner7: noise random: 0.05975428683833992
DataOwner8: noise random: 0.03374120372358929
DataOwner9: noise random: 0.043478127324017936
DataOwner10: noise random: 0.09082239249686885
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9988063746099072, 0.9977470140680788, 0.9999020377767407, 0.9999191015203867, 0.9976805660662099, 0.9998821879934902, 0.9985546011030764, 0.9995388204060187, 0.9992364674575286, 0.9966647914752322]
归一化后的数据质量列表avg_f_list: [0.9658075937744071, 0.9332550549219463, 0.9994756571006083, 1.0, 0.9312132088486819, 0.9988657034399209, 0.9580709766931399, 0.988314539515552, 0.9790236930905065, 0.9]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3477
DataOwner1的最优x_1 = 0.1228
DataOwner2的最优x_2 = 0.0878
DataOwner3的最优x_3 = 0.1541
DataOwner4的最优x_4 = 0.1546
DataOwner5的最优x_5 = 0.0855
DataOwner6的最优x_6 = 0.1536
DataOwner7的最优x_7 = 0.1150
DataOwner8的最优x_8 = 0.1443
DataOwner9的最优x_9 = 0.1357
DataOwner10的最优x_10 = 0.0464
每个DataOwner应该贡献数据比例 xn_list = [0.1228397851769342, 0.0878445082588531, 0.15412286051971189, 0.15457454920676802, 0.0854761647553498, 0.1535961225909803, 0.11497358912521626, 0.14425897141531863, 0.13567273656033044, 0.04643740145183441]
ModelOwner的最大效用 U(Eta) = 0.5886
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3477166290546354
DataOwner1的分配到的支付 ： 0.1367
DataOwner2的分配到的支付 ： 0.0945
DataOwner3的分配到的支付 ： 0.1775
DataOwner4的分配到的支付 ： 0.1781
DataOwner5的分配到的支付 ： 0.0917
DataOwner6的分配到的支付 ： 0.1768
DataOwner7的分配到的支付 ： 0.1269
DataOwner8的分配到的支付 ： 0.1643
DataOwner9的分配到的支付 ： 0.1531
DataOwner10的分配到的支付 ： 0.0482
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
**** log-parameter_analysis 运行时间： 2025-02-10 13:37:36 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.09454934912060564
DataOwner2: noise random: 0.011637544443978787
DataOwner3: noise random: 0.0573084305416339
DataOwner4: noise random: 0.07159893935192667
DataOwner5: noise random: 0.03187925498433845
DataOwner6: noise random: 0.09704106267030249
DataOwner7: noise random: 0.07998371061160087
DataOwner8: noise random: 0.03393788725306001
DataOwner9: noise random: 0.003999882924184839
DataOwner10: noise random: 0.06748260179566416
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9963830570946502, 0.9999452046615777, 0.9986727137022321, 0.9979226049416728, 0.99958917883893, 0.9961914341103643, 0.9973991218601549, 0.9995337420718166, 0.9999935250359266, 0.9981599656834246]
归一化后的数据质量列表avg_f_list: [0.9050399369199106, 0.9987291105001168, 0.9652609219623232, 0.9455320734091207, 0.9893651623563714, 0.9, 0.9317637787584472, 0.9879071023520544, 1.0, 0.9517749736026931]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3380
DataOwner1的最优x_1 = 0.0625
DataOwner2的最优x_2 = 0.1595
DataOwner3的最优x_3 = 0.1293
DataOwner4的最优x_4 = 0.1094
DataOwner5的最优x_5 = 0.1515
DataOwner6的最优x_6 = 0.0560
DataOwner7的最优x_7 = 0.0944
DataOwner8的最优x_8 = 0.1502
DataOwner9的最优x_9 = 0.1605
DataOwner10的最优x_10 = 0.1159
每个DataOwner应该贡献数据比例 xn_list = [0.06247159150664976, 0.1594588429730098, 0.1293456317382131, 0.10938903524863473, 0.15147587624971148, 0.05600839806420833, 0.09438431578898986, 0.15020301717477128, 0.1605172260285777, 0.11589439411858882]
ModelOwner的最大效用 U(Eta) = 0.5774
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3380215806699447
DataOwner1的分配到的支付 ： 0.0657
DataOwner2的分配到的支付 ： 0.1851
DataOwner3的分配到的支付 ： 0.1451
DataOwner4的分配到的支付 ： 0.1202
DataOwner5的分配到的支付 ： 0.1741
DataOwner6的分配到的支付 ： 0.0586
DataOwner7的分配到的支付 ： 0.1022
DataOwner8的分配到的支付 ： 0.1724
DataOwner9的分配到的支付 ： 0.1865
DataOwner10的分配到的支付 ： 0.1282
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC6', 'DataOwner2': 'CPC7', 'DataOwner6': 'CPC10', 'DataOwner7': 'CPC9', 'DataOwner3': 'CPC2', 'DataOwner8': 'CPC8', 'DataOwner4': 'CPC3', 'DataOwner5': 'CPC5', 'DataOwner9': 'CPC4', 'DataOwner10': 'CPC1'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC6
DataOwner2 把数据交给 CPC7
DataOwner6 把数据交给 CPC10
DataOwner7 把数据交给 CPC9
DataOwner3 把数据交给 CPC2
DataOwner8 把数据交给 CPC8
DataOwner4 把数据交给 CPC3
DataOwner5 把数据交给 CPC5
DataOwner9 把数据交给 CPC4
DataOwner10 把数据交给 CPC1
DONE
最终Um的列表：
[('DataOwner1', 'CPC6', 0.6, 340, 0.024988636602659903), ('DataOwner2', 'CPC7', 0.7, 289, 0.04783765289190295), ('DataOwner6', 'CPC10', 1.0, 101, 0.0), ('DataOwner7', 'CPC9', 0.9, 171, 0.009438431578898968), ('DataOwner3', 'CPC2', 0.2, 470, 0.10347650539057048), ('DataOwner8', 'CPC8', 0.8, 273, 0.030040603434954252), ('DataOwner4', 'CPC3', 0.3, 1193, 0.0765723246740443), ('DataOwner5', 'CPC5', 0.5, 2753, 0.07573793812485574), ('DataOwner9', 'CPC4', 0.4, 1751, 0.09631033561714662), ('DataOwner10', 'CPC1', 0.1, 421, 0.10430495470672993)]
('DataOwner1', 'CPC6', 0.6, 340, 0.024988636602659903)
('DataOwner2', 'CPC7', 0.7, 289, 0.04783765289190295)
('DataOwner6', 'CPC10', 1.0, 101, 0.0)
('DataOwner7', 'CPC9', 0.9, 171, 0.009438431578898968)
('DataOwner3', 'CPC2', 0.2, 470, 0.10347650539057048)
('DataOwner8', 'CPC8', 0.8, 273, 0.030040603434954252)
('DataOwner4', 'CPC3', 0.3, 1193, 0.0765723246740443)
('DataOwner5', 'CPC5', 0.5, 2753, 0.07573793812485574)
('DataOwner9', 'CPC4', 0.4, 1751, 0.09631033561714662)
('DataOwner10', 'CPC1', 0.1, 421, 0.10430495470672993)
**** log-parameter_analysis 运行时间： 2025-02-10 13:38:49 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.01666826659403693
DataOwner2: noise random: 0.03293832523963487
DataOwner3: noise random: 0.000822504365855703
DataOwner4: noise random: 0.014113557539755918
DataOwner5: noise random: 0.0330587399941094
DataOwner6: noise random: 0.023456247895236006
DataOwner7: noise random: 0.017842546027775574
DataOwner8: noise random: 0.050196946942017345
DataOwner9: noise random: 0.02547801587572599
DataOwner10: noise random: 0.0330411240850535
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9998874762584511, 0.9995593739641336, 0.99999972631582, 0.9999196596811407, 0.999557403613656, 0.9997775453568596, 0.9998716881414437, 0.9989841061917363, 0.9997374261753081, 0.9995591620802364]
归一化后的数据质量列表avg_f_list: [0.9889476335977301, 0.9566420218303914, 1.0, 0.9921164781220211, 0.9564480171596547, 0.9781236159375168, 0.9873931038446234, 0.9, 0.9741734006355465, 0.9566211593157384]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3524
DataOwner1的最优x_1 = 0.1416
DataOwner2的最优x_2 = 0.1098
DataOwner3的最优x_3 = 0.1515
DataOwner4的最优x_4 = 0.1445
DataOwner5的最优x_5 = 0.1096
DataOwner6的最优x_6 = 0.1315
DataOwner7的最优x_7 = 0.1402
DataOwner8的最优x_8 = 0.0416
DataOwner9的最优x_9 = 0.1276
DataOwner10的最优x_10 = 0.1097
每个DataOwner应该贡献数据比例 xn_list = [0.1416338670401935, 0.10975892215573863, 0.15154588262229396, 0.14452469790360606, 0.10955351765601636, 0.13145251240847933, 0.14020105448981585, 0.041596735045098386, 0.1276151945733969, 0.1097368421747798]
ModelOwner的最大效用 U(Eta) = 0.5942
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3524339743676097
DataOwner1的分配到的支付 ： 0.1607
DataOwner2的分配到的支付 ： 0.1205
DataOwner3的分配到的支付 ： 0.1739
DataOwner4的分配到的支付 ： 0.1645
DataOwner5的分配到的支付 ： 0.1202
DataOwner6的分配到的支付 ： 0.1476
DataOwner7的分配到的支付 ： 0.1589
DataOwner8的分配到的支付 ： 0.0430
DataOwner9的分配到的支付 ： 0.1427
DataOwner10的分配到的支付 ： 0.1205
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC3', 'DataOwner2': 'CPC9', 'DataOwner8': 'CPC10', 'DataOwner4': 'CPC6', 'DataOwner6': 'CPC8', 'DataOwner3': 'CPC5', 'DataOwner5': 'CPC2', 'DataOwner7': 'CPC7', 'DataOwner9': 'CPC4', 'DataOwner10': 'CPC1'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC3
DataOwner2 把数据交给 CPC9
DataOwner8 把数据交给 CPC10
DataOwner4 把数据交给 CPC6
DataOwner6 把数据交给 CPC8
DataOwner3 把数据交给 CPC5
DataOwner5 把数据交给 CPC2
DataOwner7 把数据交给 CPC7
DataOwner9 把数据交给 CPC4
DataOwner10 把数据交给 CPC1
DONE
最终Um的列表：
[('DataOwner1', 'CPC3', 0.3, 1085, 0.09914370692813546), ('DataOwner2', 'CPC9', 0.9, 140, 0.010975892215573865), ('DataOwner8', 'CPC10', 1.0, 53, 0.0), ('DataOwner4', 'CPC6', 0.6, 184, 0.05780987916144242), ('DataOwner6', 'CPC8', 0.8, 167, 0.026290502481695857), ('DataOwner3', 'CPC5', 0.5, 2901, 0.07577294131114698), ('DataOwner5', 'CPC2', 0.2, 699, 0.08764281412481309), ('DataOwner7', 'CPC7', 0.7, 178, 0.042060316346944765), ('DataOwner9', 'CPC4', 0.4, 2117, 0.07656911674403813), ('DataOwner10', 'CPC1', 0.1, 420, 0.09876315795730183)]
('DataOwner1', 'CPC3', 0.3, 1085, 0.09914370692813546)
('DataOwner2', 'CPC9', 0.9, 140, 0.010975892215573865)
('DataOwner8', 'CPC10', 1.0, 53, 0.0)
('DataOwner4', 'CPC6', 0.6, 184, 0.05780987916144242)
('DataOwner6', 'CPC8', 0.8, 167, 0.026290502481695857)
('DataOwner3', 'CPC5', 0.5, 2901, 0.07577294131114698)
('DataOwner5', 'CPC2', 0.2, 699, 0.08764281412481309)
('DataOwner7', 'CPC7', 0.7, 178, 0.042060316346944765)
('DataOwner9', 'CPC4', 0.4, 2117, 0.07656911674403813)
('DataOwner10', 'CPC1', 0.1, 420, 0.09876315795730183)
**** log-parameter_analysis 运行时间： 2025-02-10 13:40:53 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.08014682169943929
DataOwner2: noise random: 0.05583999609264189
DataOwner3: noise random: 0.09461720452742421
DataOwner4: noise random: 0.03208697256968858
DataOwner5: noise random: 0.030996216441428373
DataOwner6: noise random: 0.006979398925120473
DataOwner7: noise random: 0.02753319596288688
DataOwner8: noise random: 0.014148899931728964
DataOwner9: noise random: 0.02377690568037997
DataOwner10: noise random: 0.021723155933871877
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9973967234111546, 0.9987404232616373, 0.996378369570497, 0.9995813709255863, 0.9996109959443409, 0.9999803188361728, 0.9996930787772101, 0.9999189560174907, 0.9997706699709693, 0.9998093282731295]
归一化后的数据质量列表avg_f_list: [0.9282722982903137, 0.9655770949815727, 0.9, 0.9889241107755685, 0.9897465826253775, 1.0, 0.9920254274067699, 0.9982963996948313, 0.9941795719556273, 0.9952528325517329]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3593
DataOwner1的最优x_1 = 0.0713
DataOwner2的最优x_2 = 0.1136
DataOwner3的最优x_3 = 0.0343
DataOwner4的最优x_4 = 0.1368
DataOwner5的最优x_5 = 0.1375
DataOwner6的最优x_6 = 0.1470
DataOwner7的最优x_7 = 0.1397
DataOwner8的最优x_8 = 0.1454
DataOwner9的最优x_9 = 0.1417
DataOwner10的最优x_10 = 0.1427
每个DataOwner应该贡献数据比例 xn_list = [0.07134653833780427, 0.11362280247205643, 0.034303995414929116, 0.13677074655674967, 0.1375440894994574, 0.14695703537548785, 0.13967244732650586, 0.1454219008090494, 0.1416650884256935, 0.1426509592445051]
ModelOwner的最大效用 U(Eta) = 0.6024
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3593363356279746
DataOwner1的分配到的支付 ： 0.0755
DataOwner2的分配到的支付 ： 0.1251
DataOwner3的分配到的支付 ： 0.0352
DataOwner4的分配到的支付 ： 0.1543
DataOwner5的分配到的支付 ： 0.1553
DataOwner6的分配到的支付 ： 0.1676
DataOwner7的分配到的支付 ： 0.1580
DataOwner8的分配到的支付 ： 0.1656
DataOwner9的分配到的支付 ： 0.1607
DataOwner10的分配到的支付 ： 0.1619
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC10', 'DataOwner2': 'CPC2', 'DataOwner3': 'CPC8', 'DataOwner4': 'CPC9', 'DataOwner5': 'CPC7', 'DataOwner6': 'CPC1', 'DataOwner9': 'CPC6', 'DataOwner7': 'CPC5', 'DataOwner8': 'CPC4', 'DataOwner10': 'CPC3'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC10
DataOwner2 把数据交给 CPC2
DataOwner3 把数据交给 CPC8
DataOwner4 把数据交给 CPC9
DataOwner5 把数据交给 CPC7
DataOwner6 把数据交给 CPC1
DataOwner9 把数据交给 CPC6
DataOwner7 把数据交给 CPC5
DataOwner8 把数据交给 CPC4
DataOwner10 把数据交给 CPC3
DONE
最终Um的列表：
[('DataOwner1', 'CPC10', 1.0, 126, 0.0), ('DataOwner2', 'CPC2', 0.2, 601, 0.09089824197764515), ('DataOwner3', 'CPC8', 0.8, 242, 0.00686079908298582), ('DataOwner4', 'CPC9', 0.9, 241, 0.01367707465567497), ('DataOwner5', 'CPC7', 0.7, 242, 0.04126322684983723), ('DataOwner6', 'CPC1', 0.1, 518, 0.13226133183793906), ('DataOwner9', 'CPC6', 0.6, 499, 0.05666603537027741), ('DataOwner7', 'CPC5', 0.5, 2218, 0.06983622366325293), ('DataOwner8', 'CPC4', 0.4, 2052, 0.08725314048542965), ('DataOwner10', 'CPC3', 0.3, 755, 0.09985567147115357)]
('DataOwner1', 'CPC10', 1.0, 126, 0.0)
('DataOwner2', 'CPC2', 0.2, 601, 0.09089824197764515)
('DataOwner3', 'CPC8', 0.8, 242, 0.00686079908298582)
('DataOwner4', 'CPC9', 0.9, 241, 0.01367707465567497)
('DataOwner5', 'CPC7', 0.7, 242, 0.04126322684983723)
('DataOwner6', 'CPC1', 0.1, 518, 0.13226133183793906)
('DataOwner9', 'CPC6', 0.6, 499, 0.05666603537027741)
('DataOwner7', 'CPC5', 0.5, 2218, 0.06983622366325293)
('DataOwner8', 'CPC4', 0.4, 2052, 0.08725314048542965)
('DataOwner10', 'CPC3', 0.3, 755, 0.09985567147115357)
**** log-parameter_analysis 运行时间： 2025-02-10 13:42:44 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.036305545615350564
DataOwner2: noise random: 0.061565523710926245
DataOwner3: noise random: 0.0037877471289496593
DataOwner4: noise random: 0.0005538587211618373
DataOwner5: noise random: 0.04345694327309138
DataOwner6: noise random: 0.00560673858085784
DataOwner7: noise random: 0.00634709283213033
DataOwner8: noise random: 0.03480322414530864
DataOwner9: noise random: 0.04007447338363852
DataOwner10: noise random: 0.0991766513972705
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9994660431440292, 0.9984691944032982, 0.9999941924962789, 0.9999998751742002, 0.9992388852742036, 0.9999873131101573, 0.9999836908012432, 0.9995098497370019, 0.9993510071087667, 0.9960289032680366]
归一化后的数据质量列表avg_f_list: [0.9865566404702487, 0.9614532460296141, 0.9998568945322299, 1.0, 0.9808361802103046, 0.9996836526588522, 0.999592432952449, 0.9876598110291906, 0.9836597165437919, 0.9]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3651
DataOwner1的最优x_1 = 0.1303
DataOwner2的最优x_2 = 0.1045
DataOwner3的最优x_3 = 0.1429
DataOwner4的最优x_4 = 0.1430
DataOwner5的最优x_5 = 0.1247
DataOwner6的最优x_6 = 0.1427
DataOwner7的最优x_7 = 0.1426
DataOwner8的最优x_8 = 0.1314
DataOwner9的最优x_9 = 0.1275
DataOwner10的最优x_10 = 0.0280
每个DataOwner应该贡献数据比例 xn_list = [0.13030453819085167, 0.10453425728210013, 0.14286284324832138, 0.14299408222729368, 0.12467693302350326, 0.1427038584274985, 0.14262009795639988, 0.13137385228376697, 0.1274720764150421, 0.028044064470634438]
ModelOwner的最大效用 U(Eta) = 0.6092
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3650717207020644
DataOwner1的分配到的支付 ： 0.1459
DataOwner2的分配到的支付 ： 0.1141
DataOwner3的分配到的支付 ： 0.1621
DataOwner4的分配到的支付 ： 0.1623
DataOwner5的分配到的支付 ： 0.1388
DataOwner6的分配到的支付 ： 0.1619
DataOwner7的分配到的支付 ： 0.1618
DataOwner8的分配到的支付 ： 0.1473
DataOwner9的分配到的支付 ： 0.1423
DataOwner10的分配到的支付 ： 0.0286
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC7', 'DataOwner4': 'CPC9', 'DataOwner10': 'CPC10', 'DataOwner2': 'CPC4', 'DataOwner3': 'CPC3', 'DataOwner5': 'CPC8', 'DataOwner6': 'CPC6', 'DataOwner8': 'CPC2', 'DataOwner9': 'CPC1', 'DataOwner7': 'CPC5'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC7
DataOwner4 把数据交给 CPC9
DataOwner10 把数据交给 CPC10
DataOwner2 把数据交给 CPC4
DataOwner3 把数据交给 CPC3
DataOwner5 把数据交给 CPC8
DataOwner6 把数据交给 CPC6
DataOwner8 把数据交给 CPC2
DataOwner9 把数据交给 CPC1
DataOwner7 把数据交给 CPC5
DONE
最终Um的列表：
[('DataOwner1', 'CPC7', 0.7, 401, 0.0390913614572555), ('DataOwner4', 'CPC9', 0.9, 219, 0.014299408222729376), ('DataOwner10', 'CPC10', 1.0, 43, 0.0), ('DataOwner2', 'CPC4', 0.4, 804, 0.06272055436926008), ('DataOwner3', 'CPC3', 0.3, 659, 0.10000399027382498), ('DataOwner5', 'CPC8', 0.8, 383, 0.02493538660470064), ('DataOwner6', 'CPC6', 0.6, 438, 0.057081543370999396), ('DataOwner8', 'CPC2', 0.2, 606, 0.10509908182701358), ('DataOwner9', 'CPC1', 0.1, 588, 0.1147248687735379), ('DataOwner7', 'CPC5', 0.5, 3729, 0.07131004897819994)]
('DataOwner1', 'CPC7', 0.7, 401, 0.0390913614572555)
('DataOwner4', 'CPC9', 0.9, 219, 0.014299408222729376)
('DataOwner10', 'CPC10', 1.0, 43, 0.0)
('DataOwner2', 'CPC4', 0.4, 804, 0.06272055436926008)
('DataOwner3', 'CPC3', 0.3, 659, 0.10000399027382498)
('DataOwner5', 'CPC8', 0.8, 383, 0.02493538660470064)
('DataOwner6', 'CPC6', 0.6, 438, 0.057081543370999396)
('DataOwner8', 'CPC2', 0.2, 606, 0.10509908182701358)
('DataOwner9', 'CPC1', 0.1, 588, 0.1147248687735379)
('DataOwner7', 'CPC5', 0.5, 3729, 0.07131004897819994)
**** log-parameter_analysis 运行时间： 2025-02-10 13:44:52 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.047420384983334875
DataOwner2: noise random: 0.08691609253346774
DataOwner3: noise random: 0.08020910459538255
DataOwner4: noise random: 0.09879711205313496
DataOwner5: noise random: 0.04681797320257905
DataOwner6: noise random: 0.007893517843741848
DataOwner7: noise random: 0.059547217266325725
DataOwner8: noise random: 0.04623778659760844
DataOwner9: noise random: 0.014688222781475758
DataOwner10: noise random: 0.04634358336184752
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9990916927929311, 0.9969423077067123, 0.997396382357927, 0.9960519696308497, 0.9991133335844353, 0.9999747229562104, 0.9985675962845122, 0.9991348949575579, 0.999912773946124, 0.9991303609395881]
归一化后的数据质量列表avg_f_list: [0.977489531203237, 0.922696764288148, 0.9342721709872928, 0.9, 0.9780412047271452, 1.0, 0.9641291063957274, 0.9785908536939358, 0.9984207773227575, 0.9784752711529554]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3452
DataOwner1的最优x_1 = 0.1360
DataOwner2的最优x_2 = 0.0777
DataOwner3的最优x_3 = 0.0912
DataOwner4的最优x_4 = 0.0490
DataOwner5的最优x_5 = 0.1365
DataOwner6的最优x_6 = 0.1562
DataOwner7的最优x_7 = 0.1230
DataOwner8的最优x_8 = 0.1370
DataOwner9的最优x_9 = 0.1548
DataOwner10的最优x_10 = 0.1369
每个DataOwner应该贡献数据比例 xn_list = [0.13599628634625738, 0.07766062241234095, 0.09119552368930373, 0.04898745480955101, 0.1365146988107357, 0.15616354713749495, 0.12304924242973811, 0.13702996056409045, 0.1548125910086657, 0.13692171240577758]
ModelOwner的最大效用 U(Eta) = 0.5857
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3451779847885352
DataOwner1的分配到的支付 ： 0.1535
DataOwner2的分配到的支付 ： 0.0828
DataOwner3的分配到的支付 ： 0.0984
DataOwner4的分配到的支付 ： 0.0509
DataOwner5的分配到的支付 ： 0.1542
DataOwner6的分配到的支付 ： 0.1803
DataOwner7的分配到的支付 ： 0.1370
DataOwner8的分配到的支付 ： 0.1549
DataOwner9的分配到的支付 ： 0.1785
DataOwner10的分配到的支付 ： 0.1547
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC6', 'DataOwner3': 'CPC8', 'DataOwner4': 'CPC10', 'DataOwner2': 'CPC2', 'DataOwner9': 'CPC9', 'DataOwner7': 'CPC7', 'DataOwner6': 'CPC1', 'DataOwner5': 'CPC3', 'DataOwner8': 'CPC4', 'DataOwner10': 'CPC5'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC6
DataOwner3 把数据交给 CPC8
DataOwner4 把数据交给 CPC10
DataOwner2 把数据交给 CPC2
DataOwner9 把数据交给 CPC9
DataOwner7 把数据交给 CPC7
DataOwner6 把数据交给 CPC1
DataOwner5 把数据交给 CPC3
DataOwner8 把数据交给 CPC4
DataOwner10 把数据交给 CPC5
DONE
最终Um的列表：
[('DataOwner1', 'CPC6', 0.6, 380, 0.05439851453850296), ('DataOwner3', 'CPC8', 0.8, 254, 0.018239104737860737), ('DataOwner4', 'CPC10', 1.0, 136, 0.0), ('DataOwner2', 'CPC2', 0.2, 758, 0.06212849792987276), ('DataOwner9', 'CPC9', 0.9, 215, 0.015481259100866562), ('DataOwner7', 'CPC7', 0.7, 343, 0.036914772728921436), ('DataOwner6', 'CPC1', 0.1, 435, 0.14054719242374544), ('DataOwner5', 'CPC3', 0.3, 952, 0.095560289167515), ('DataOwner8', 'CPC4', 0.4, 955, 0.08221797633845426), ('DataOwner10', 'CPC5', 0.5, 2865, 0.06846085620288879)]
('DataOwner1', 'CPC6', 0.6, 380, 0.05439851453850296)
('DataOwner3', 'CPC8', 0.8, 254, 0.018239104737860737)
('DataOwner4', 'CPC10', 1.0, 136, 0.0)
('DataOwner2', 'CPC2', 0.2, 758, 0.06212849792987276)
('DataOwner9', 'CPC9', 0.9, 215, 0.015481259100866562)
('DataOwner7', 'CPC7', 0.7, 343, 0.036914772728921436)
('DataOwner6', 'CPC1', 0.1, 435, 0.14054719242374544)
('DataOwner5', 'CPC3', 0.3, 952, 0.095560289167515)
('DataOwner8', 'CPC4', 0.4, 955, 0.08221797633845426)
('DataOwner10', 'CPC5', 0.5, 2865, 0.06846085620288879)
**** log-parameter_analysis 运行时间： 2025-02-10 13:45:47 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.05051143243893511
DataOwner2: noise random: 0.06957799086484275
DataOwner3: noise random: 0.029154563693242763
DataOwner4: noise random: 0.08613355125020988
DataOwner5: noise random: 0.01043061329831453
DataOwner6: noise random: 0.0914184851240683
DataOwner7: noise random: 0.038373873341903064
DataOwner8: noise random: 0.035190488268275755
DataOwner9: noise random: 0.02026326695624341
DataOwner10: noise random: 0.09112336662275346
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9989677560108621, 0.9980391535510833, 0.9996561348439407, 0.9970040996288643, 0.9999559929326891, 0.9966261680681553, 0.999402081136854, 0.9994985724253822, 0.9998332169071836, 0.9966391871624465]
归一化后的数据质量列表avg_f_list: [0.9703216546806167, 0.9424342282375813, 0.9909947789764487, 0.9113498930449572, 1.0, 0.9, 0.9833651372558709, 0.9862629259520848, 0.9963128383473505, 0.9003909843556596]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3384
DataOwner1的最优x_1 = 0.1339
DataOwner2的最优x_2 = 0.1058
DataOwner3的最优x_3 = 0.1526
DataOwner4的最优x_4 = 0.0700
DataOwner5的最优x_5 = 0.1603
DataOwner6的最优x_6 = 0.0556
DataOwner7的最优x_7 = 0.1459
DataOwner8的最优x_8 = 0.1485
DataOwner9的最优x_9 = 0.1572
DataOwner10的最优x_10 = 0.0561
每个DataOwner应该贡献数据比例 xn_list = [0.13392217593619105, 0.10578740030713339, 0.15264783158995462, 0.07000407154851011, 0.16028695592317058, 0.05563568663872214, 0.14593475224122401, 0.1485110058026165, 0.15719560676799862, 0.05614306895165453]
ModelOwner的最大效用 U(Eta) = 0.5778
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3384082885929833
DataOwner1的分配到的支付 ： 0.1509
DataOwner2的分配到的支付 ： 0.1158
DataOwner3的分配到的支付 ： 0.1757
DataOwner4的分配到的支付 ： 0.0741
DataOwner5的分配到的支付 ： 0.1862
DataOwner6的分配到的支付 ： 0.0582
DataOwner7的分配到的支付 ： 0.1667
DataOwner8的分配到的支付 ： 0.1701
DataOwner9的分配到的支付 ： 0.1819
DataOwner10的分配到的支付 ： 0.0587
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC7', 'DataOwner4': 'CPC1', 'DataOwner6': 'CPC10', 'DataOwner2': 'CPC6', 'DataOwner7': 'CPC8', 'DataOwner10': 'CPC9', 'DataOwner3': 'CPC3', 'DataOwner5': 'CPC5', 'DataOwner8': 'CPC4', 'DataOwner9': 'CPC2'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC7
DataOwner4 把数据交给 CPC1
DataOwner6 把数据交给 CPC10
DataOwner2 把数据交给 CPC6
DataOwner7 把数据交给 CPC8
DataOwner10 把数据交给 CPC9
DataOwner3 把数据交给 CPC3
DataOwner5 把数据交给 CPC5
DataOwner8 把数据交给 CPC4
DataOwner9 把数据交给 CPC2
DONE
最终Um的列表：
[('DataOwner1', 'CPC7', 0.7, 731, 0.040176652780857317), ('DataOwner4', 'CPC1', 0.1, 152, 0.0630036643936591), ('DataOwner6', 'CPC10', 1.0, 121, 0.0), ('DataOwner2', 'CPC6', 0.6, 923, 0.04231496012285338), ('DataOwner7', 'CPC8', 0.8, 477, 0.02918695044824479), ('DataOwner10', 'CPC9', 0.9, 306, 0.005614306895165455), ('DataOwner3', 'CPC3', 0.3, 1998, 0.10685348211296825), ('DataOwner5', 'CPC5', 0.5, 1223, 0.08014347796158529), ('DataOwner8', 'CPC4', 0.4, 1620, 0.08910660348156987), ('DataOwner9', 'CPC2', 0.2, 171, 0.1257564854143989)]
('DataOwner1', 'CPC7', 0.7, 731, 0.040176652780857317)
('DataOwner4', 'CPC1', 0.1, 152, 0.0630036643936591)
('DataOwner6', 'CPC10', 1.0, 121, 0.0)
('DataOwner2', 'CPC6', 0.6, 923, 0.04231496012285338)
('DataOwner7', 'CPC8', 0.8, 477, 0.02918695044824479)
('DataOwner10', 'CPC9', 0.9, 306, 0.005614306895165455)
('DataOwner3', 'CPC3', 0.3, 1998, 0.10685348211296825)
('DataOwner5', 'CPC5', 0.5, 1223, 0.08014347796158529)
('DataOwner8', 'CPC4', 0.4, 1620, 0.08910660348156987)
('DataOwner9', 'CPC2', 0.2, 171, 0.1257564854143989)
**** log-parameter_analysis 运行时间： 2025-02-10 13:51:25 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.030279817388268928
DataOwner2: noise random: 0.04664739897311126
DataOwner3: noise random: 0.02638254800679787
DataOwner4: noise random: 0.0025012666652382554
DataOwner5: noise random: 0.04133282447155384
DataOwner6: noise random: 0.07250331308462825
DataOwner7: noise random: 0.03661500523885957
DataOwner8: noise random: 0.059983109252089085
DataOwner9: noise random: 0.048526142384976656
DataOwner10: noise random: 0.028755778115427046
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9996302224516331, 0.999119876549397, 0.9997183798047523, 0.9999974666530601, 0.9993093592537758, 0.9978725965947637, 0.9994578520925496, 0.9985410651557606, 0.9990456688642729, 0.9996658810811363]
归一化后的数据质量列表avg_f_list: [0.98271686308567, 0.95869911667132, 0.9868656981061915, 1.0, 0.9676164951076611, 0.9, 0.9746048207322769, 0.9314592677508396, 0.9552067767593155, 0.9843950188563695]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3466
DataOwner1的最优x_1 = 0.1399
DataOwner2的最优x_2 = 0.1165
DataOwner3的最优x_3 = 0.1437
DataOwner4的最优x_4 = 0.1553
DataOwner5的最优x_5 = 0.1255
DataOwner6的最优x_6 = 0.0476
DataOwner7的最优x_7 = 0.1323
DataOwner8的最优x_8 = 0.0868
DataOwner9的最优x_9 = 0.1129
DataOwner10的最优x_10 = 0.1415
每个DataOwner应该贡献数据比例 xn_list = [0.13991733934219672, 0.11650871061562211, 0.14371663912799376, 0.15529960343244129, 0.12549007285090194, 0.04760047093638469, 0.1322859926894116, 0.08676985250903221, 0.1128937911650878, 0.14146243462032285]
ModelOwner的最大效用 U(Eta) = 0.5873
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3465606835829076
DataOwner1的分配到的支付 ： 0.1586
DataOwner2的分配到的支付 ： 0.1288
DataOwner3的分配到的支付 ： 0.1636
DataOwner4的分配到的支付 ： 0.1791
DataOwner5的分配到的支付 ： 0.1401
DataOwner6的分配到的支付 ： 0.0494
DataOwner7的分配到的支付 ： 0.1487
DataOwner8的分配到的支付 ： 0.0932
DataOwner9的分配到的支付 ： 0.1244
DataOwner10的分配到的支付 ： 0.1606
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC1', 'DataOwner6': 'CPC10', 'DataOwner2': 'CPC7', 'DataOwner7': 'CPC8', 'DataOwner9': 'CPC9', 'DataOwner3': 'CPC3', 'DataOwner5': 'CPC6', 'DataOwner4': 'CPC2', 'DataOwner8': 'CPC4', 'DataOwner10': 'CPC5'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner6 把数据交给 CPC10
DataOwner2 把数据交给 CPC7
DataOwner7 把数据交给 CPC8
DataOwner9 把数据交给 CPC9
DataOwner3 把数据交给 CPC3
DataOwner5 把数据交给 CPC6
DataOwner4 把数据交给 CPC2
DataOwner8 把数据交给 CPC4
DataOwner10 把数据交给 CPC5
DONE
最终Um的列表：
[('DataOwner1', 'CPC1', 0.1, 1370.2084010844294, 494, 0.12592560540797704), ('DataOwner6', 'CPC10', 1.0, 167.9820619345016, 167, 0.0), ('DataOwner2', 'CPC7', 0.7, 568.7241749854072, 616, 0.034952613184686634), ('DataOwner7', 'CPC8', 0.8, 435.1434706310166, 466, 0.02645719853788231), ('DataOwner9', 'CPC9', 0.9, 301.562766280186, 199, 0.011289379116508777), ('DataOwner3', 'CPC3', 0.3, 1103.0469923863072, 1268, 0.10060164738959564), ('DataOwner5', 'CPC6', 0.6, 702.3048793346022, 664, 0.05019602914036078), ('DataOwner4', 'CPC2', 0.2, 1236.6276967385431, 1370, 0.12423968274595303), ('DataOwner8', 'CPC4', 0.4, 969.4662880382666, 1071, 0.05206191150541931), ('DataOwner10', 'CPC5', 0.5, 835.8855836856014, 998, 0.07073121731016142)]
('DataOwner1', 'CPC1', 0.1, 1370.2084010844294, 494, 0.12592560540797704)
('DataOwner6', 'CPC10', 1.0, 167.9820619345016, 167, 0.0)
('DataOwner2', 'CPC7', 0.7, 568.7241749854072, 616, 0.034952613184686634)
('DataOwner7', 'CPC8', 0.8, 435.1434706310166, 466, 0.02645719853788231)
('DataOwner9', 'CPC9', 0.9, 301.562766280186, 199, 0.011289379116508777)
('DataOwner3', 'CPC3', 0.3, 1103.0469923863072, 1268, 0.10060164738959564)
('DataOwner5', 'CPC6', 0.6, 702.3048793346022, 664, 0.05019602914036078)
('DataOwner4', 'CPC2', 0.2, 1236.6276967385431, 1370, 0.12423968274595303)
('DataOwner8', 'CPC4', 0.4, 969.4662880382666, 1071, 0.05206191150541931)
('DataOwner10', 'CPC5', 0.5, 835.8855836856014, 998, 0.07073121731016142)
**** log-parameter_analysis 运行时间： 2025-02-10 13:53:02 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.0322344936827267
DataOwner2: noise random: 0.029556430762524878
DataOwner3: noise random: 0.033585820203267426
DataOwner4: noise random: 0.019154867911107088
DataOwner5: noise random: 0.07520716602492017
DataOwner6: noise random: 0.0723778886277847
DataOwner7: noise random: 0.015513701586891193
DataOwner8: noise random: 0.037665435317252915
DataOwner9: noise random: 0.0393572130068672
DataOwner10: noise random: 0.041907228416244335
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.99957919204932, 0.9996454829705984, 0.9995428512821449, 0.9998505870190894, 0.9977122257818314, 0.9978794545221554, 0.999902495321279, 0.9994266088642495, 0.9993680598774394, 0.9992903561780326]
归一化后的数据质量列表avg_f_list: [0.9852391102493945, 0.9882657204489378, 0.9835799186969139, 0.9976300495781589, 0.9, 0.9076350758348318, 1.0, 0.9782726989323205, 0.9755995582181007, 0.9720518807287659]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3517
DataOwner1的最优x_1 = 0.1387
DataOwner2的最优x_2 = 0.1415
DataOwner3的最优x_3 = 0.1372
DataOwner4的最优x_4 = 0.1499
DataOwner5的最优x_5 = 0.0424
DataOwner6的最优x_6 = 0.0526
DataOwner7的最优x_7 = 0.1520
DataOwner8的最优x_8 = 0.1321
DataOwner9的最优x_9 = 0.1296
DataOwner10的最优x_10 = 0.1261
每个DataOwner应该贡献数据比例 xn_list = [0.13871834468287636, 0.14151775630961574, 0.1371679319079689, 0.14994881550357356, 0.04236683484950237, 0.05256666907721287, 0.15202861898088812, 0.1321324784931007, 0.12955159692694893, 0.12607935482519064]
ModelOwner的最大效用 U(Eta) = 0.5933
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3516905267375945
DataOwner1的分配到的支付 ： 0.1569
DataOwner2的分配到的支付 ： 0.1606
DataOwner3的分配到的支付 ： 0.1549
DataOwner4的分配到的支付 ： 0.1718
DataOwner5的分配到的支付 ： 0.0438
DataOwner6的分配到的支付 ： 0.0548
DataOwner7的分配到的支付 ： 0.1746
DataOwner8的分配到的支付 ： 0.1484
DataOwner9的分配到的支付 ： 0.1451
DataOwner10的分配到的支付 ： 0.1407
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC5', 'DataOwner2': 'CPC1', 'DataOwner4': 'CPC9', 'DataOwner9': 'CPC10', 'DataOwner3': 'CPC2', 'DataOwner5': 'CPC8', 'DataOwner8': 'CPC7', 'DataOwner6': 'CPC6', 'DataOwner7': 'CPC4', 'DataOwner10': 'CPC3'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC5
DataOwner2 把数据交给 CPC1
DataOwner4 把数据交给 CPC9
DataOwner9 把数据交给 CPC10
DataOwner3 把数据交给 CPC2
DataOwner5 把数据交给 CPC8
DataOwner8 把数据交给 CPC7
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC4
DataOwner10 把数据交给 CPC3
DONE
最终Um的列表：
[('DataOwner1', 'CPC5', 0.5, 797.8566182574489, 584, 0.06935917234143818), ('DataOwner2', 'CPC1', 0.1, 1327.1112888899568, 446, 0.12736598067865418), ('DataOwner4', 'CPC9', 0.9, 268.6019476265966, 157, 0.014994881550357358), ('DataOwner9', 'CPC10', 1.0, 136.28827996715026, 136, 0.0), ('DataOwner3', 'CPC2', 0.2, 1194.7976212312494, 577, 0.10973434552637511), ('DataOwner5', 'CPC8', 0.8, 400.91561528385046, 445, 0.008473366969900469), ('DataOwner8', 'CPC7', 0.7, 533.2292829415477, 556, 0.03963974354793022), ('DataOwner6', 'CPC6', 0.6, 665.5429505994833, 719, 0.02102666763088515), ('DataOwner7', 'CPC4', 0.4, 930.1702859154256, 1120, 0.09121717138853287), ('DataOwner10', 'CPC3', 0.3, 1062.4839535730093, 1327, 0.08825554837763344)]
('DataOwner1', 'CPC5', 0.5, 797.8566182574489, 584, 0.06935917234143818)
('DataOwner2', 'CPC1', 0.1, 1327.1112888899568, 446, 0.12736598067865418)
('DataOwner4', 'CPC9', 0.9, 268.6019476265966, 157, 0.014994881550357358)
('DataOwner9', 'CPC10', 1.0, 136.28827996715026, 136, 0.0)
('DataOwner3', 'CPC2', 0.2, 1194.7976212312494, 577, 0.10973434552637511)
('DataOwner5', 'CPC8', 0.8, 400.91561528385046, 445, 0.008473366969900469)
('DataOwner8', 'CPC7', 0.7, 533.2292829415477, 556, 0.03963974354793022)
('DataOwner6', 'CPC6', 0.6, 665.5429505994833, 719, 0.02102666763088515)
('DataOwner7', 'CPC4', 0.4, 930.1702859154256, 1120, 0.09121717138853287)
('DataOwner10', 'CPC3', 0.3, 1062.4839535730093, 1327, 0.08825554837763344)
**** log-parameter_analysis 运行时间： 2025-02-10 15:04:29 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.043418356359658385
DataOwner2: noise random: 0.051958926911503514
DataOwner3: noise random: 0.04921217460395547
DataOwner4: noise random: 0.03148434927293713
DataOwner5: noise random: 0.020597077996094084
DataOwner6: noise random: 0.07610836413806696
DataOwner7: noise random: 0.05868552266840669
DataOwner8: noise random: 0.08960528076098762
DataOwner9: noise random: 0.003451086477140564
DataOwner10: noise random: 0.01560104163200663
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9992374349146924, 0.9989032967713111, 0.9990188602774137, 0.9996010595254247, 0.999828569775997, 0.9976647513431781, 0.9986069402891022, 0.9967426372549761, 0.9999951803412302, 0.9999016086508701]
归一化后的数据质量列表avg_f_list: [0.9767029857424421, 0.966429850705631, 0.9699828706976193, 0.987882687320232, 0.9948775293419666, 0.9283505572024245, 0.9573183193792272, 0.9, 1.0, 0.9971231221884308]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3508
DataOwner1的最优x_1 = 0.1313
DataOwner2的最优x_2 = 0.1211
DataOwner3的最优x_3 = 0.1247
DataOwner4的最优x_4 = 0.1418
DataOwner5的最优x_5 = 0.1481
DataOwner6的最优x_6 = 0.0793
DataOwner7的最优x_7 = 0.1118
DataOwner8的最优x_8 = 0.0433
DataOwner9的最优x_9 = 0.1526
DataOwner10的最优x_10 = 0.1501
每个DataOwner应该贡献数据比例 xn_list = [0.13125865267449072, 0.12113505775205376, 0.12468811517462566, 0.1417700672882387, 0.14809047733647432, 0.07934055139869321, 0.11176550439228568, 0.043276479355896603, 0.15259836309643682, 0.15007902985175142]
ModelOwner的最大效用 U(Eta) = 0.5923
xn开始变化：
new_xn_list: [0.13125865267449072, 0.12113505775205376, 0.12468811517462566, 0.1417700672882387, 0.14809047733647432, 0.07934055139869321, 0.11176550439228568, 0.043276479355896603, 0.15259836309643682, 0.15007902985175142]
avg_f_list: [0.9767029857424421, 0.966429850705631, 0.9699828706976193, 0.987882687320232, 0.9948775293419666, 0.9283505572024245, 0.9573183193792272, 0.9, 1.0, 0.9971231221884308]
============= xn0: 0.0 =============
new_qn: 0.0
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.0472434302100053
============= xn0: 0.001 =============
new_qn: 0.0009767029857424422
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.0482201331957477
============= xn0: 0.002 =============
new_qn: 0.0019534059714848844
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.0491968361814903
============= xn0: 0.003 =============
new_qn: 0.0029301089572273263
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.0501735391672327
============= xn0: 0.004 =============
new_qn: 0.003906811942969769
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.051150242152975
============= xn0: 0.005 =============
new_qn: 0.004883514928712211
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.0521269451387174
============= xn0: 0.006 =============
new_qn: 0.005860217914454653
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.05310364812446
============= xn0: 0.007 =============
new_qn: 0.006836920900197095
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.0540803511102024
============= xn0: 0.008 =============
new_qn: 0.007813623885939537
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.0550570540959447
============= xn0: 0.009000000000000001 =============
new_qn: 0.00879032687168198
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.0560337570816873
============= xn0: 0.01 =============
new_qn: 0.009767029857424421
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.0570104600674297
============= xn0: 0.011 =============
new_qn: 0.010743732843166862
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.057987163053172
============= xn0: 0.012 =============
new_qn: 0.011720435828909305
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.0589638660389147
============= xn0: 0.013000000000000001 =============
new_qn: 0.012697138814651747
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.059940569024657
============= xn0: 0.014 =============
new_qn: 0.01367384180039419
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.0609172720103994
============= xn0: 0.015 =============
new_qn: 0.014650544786136631
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.061893974996142
============= xn0: 0.016 =============
new_qn: 0.015627247771879075
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.0628706779818844
============= xn0: 0.017 =============
new_qn: 0.016603950757621517
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.0638473809676268
============= xn0: 0.018000000000000002 =============
new_qn: 0.01758065374336396
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.0648240839533694
============= xn0: 0.019 =============
new_qn: 0.018557356729106397
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.0658007869391117
============= xn0: 0.02 =============
new_qn: 0.019534059714848843
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.066777489924854
============= xn0: 0.021 =============
new_qn: 0.020510762700591285
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.0677541929105965
============= xn0: 0.022 =============
new_qn: 0.021487465686333723
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.068730895896339
============= xn0: 0.023 =============
new_qn: 0.02246416867207617
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.0697075988820814
============= xn0: 0.024 =============
new_qn: 0.02344087165781861
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.0706843018678238
============= xn0: 0.025 =============
new_qn: 0.024417574643561053
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.0716610048535664
============= xn0: 0.026000000000000002 =============
new_qn: 0.025394277629303495
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.0726377078393088
============= xn0: 0.027 =============
new_qn: 0.026370980615045937
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.0736144108250512
============= xn0: 0.028 =============
new_qn: 0.02734768360078838
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.0745911138107938
============= xn0: 0.029 =============
new_qn: 0.02832438658653082
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.0755678167965361
============= xn0: 0.03 =============
new_qn: 0.029301089572273262
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.0765445197822785
============= xn0: 0.031 =============
new_qn: 0.030277792558015704
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.077521222768021
============= xn0: 0.032 =============
new_qn: 0.03125449554375815
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.0784979257537635
============= xn0: 0.033 =============
new_qn: 0.03223119852950059
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.0794746287395058
============= xn0: 0.034 =============
new_qn: 0.033207901515243034
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.0804513317252484
============= xn0: 0.035 =============
new_qn: 0.034184604500985476
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.0814280347109908
============= xn0: 0.036000000000000004 =============
new_qn: 0.03516130748672792
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.0824047376967332
============= xn0: 0.037 =============
new_qn: 0.03613801047247035
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.0833814406824755
============= xn0: 0.038 =============
new_qn: 0.037114713458212795
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.0843581436682181
============= xn0: 0.039 =============
new_qn: 0.038091416443955244
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.0853348466539605
============= xn0: 0.04 =============
new_qn: 0.039068119429697686
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.086311549639703
============= xn0: 0.041 =============
new_qn: 0.04004482241544013
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.0872882526254455
============= xn0: 0.042 =============
new_qn: 0.04102152540118257
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.0882649556111879
============= xn0: 0.043000000000000003 =============
new_qn: 0.04199822838692501
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.0892416585969302
============= xn0: 0.044 =============
new_qn: 0.042974931372667446
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.0902183615826728
============= xn0: 0.045 =============
new_qn: 0.04395163435840989
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.0911950645684152
============= xn0: 0.046 =============
new_qn: 0.04492833734415234
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.0921717675541576
============= xn0: 0.047 =============
new_qn: 0.04590504032989478
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.0931484705399002
============= xn0: 0.048 =============
new_qn: 0.04688174331563722
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.0941251735256425
============= xn0: 0.049 =============
new_qn: 0.04785844630137966
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.095101876511385
============= xn0: 0.05 =============
new_qn: 0.048835149287122105
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.0960785794971275
============= xn0: 0.051000000000000004 =============
new_qn: 0.04981185227286455
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.0970552824828699
============= xn0: 0.052000000000000005 =============
new_qn: 0.05078855525860699
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.0980319854686122
============= xn0: 0.053 =============
new_qn: 0.05176525824434943
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.0990086884543546
============= xn0: 0.054 =============
new_qn: 0.05274196123009187
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.0999853914400972
============= xn0: 0.055 =============
new_qn: 0.053718664215834315
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1009620944258396
============= xn0: 0.056 =============
new_qn: 0.05469536720157676
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.101938797411582
============= xn0: 0.057 =============
new_qn: 0.0556720701873192
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1029155003973246
============= xn0: 0.058 =============
new_qn: 0.05664877317306164
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.103892203383067
============= xn0: 0.059000000000000004 =============
new_qn: 0.05762547615880408
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1048689063688093
============= xn0: 0.06 =============
new_qn: 0.058602179144546525
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.105845609354552
============= xn0: 0.061 =============
new_qn: 0.05957888213028897
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1068223123402943
============= xn0: 0.062 =============
new_qn: 0.06055558511603141
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1077990153260366
============= xn0: 0.063 =============
new_qn: 0.06153228810177385
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1087757183117792
============= xn0: 0.064 =============
new_qn: 0.0625089910875163
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1097524212975216
============= xn0: 0.065 =============
new_qn: 0.06348569407325874
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.110729124283264
============= xn0: 0.066 =============
new_qn: 0.06446239705900118
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1117058272690066
============= xn0: 0.067 =============
new_qn: 0.06543910004474363
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.112682530254749
============= xn0: 0.068 =============
new_qn: 0.06641580303048607
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1136592332404913
============= xn0: 0.069 =============
new_qn: 0.06739250601622851
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1146359362262337
============= xn0: 0.07 =============
new_qn: 0.06836920900197095
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1156126392119763
============= xn0: 0.07100000000000001 =============
new_qn: 0.0693459119877134
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1165893421977187
============= xn0: 0.07200000000000001 =============
new_qn: 0.07032261497345584
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.117566045183461
============= xn0: 0.073 =============
new_qn: 0.07129931795919826
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1185427481692036
============= xn0: 0.074 =============
new_qn: 0.0722760209449407
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.119519451154946
============= xn0: 0.075 =============
new_qn: 0.07325272393068315
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1204961541406884
============= xn0: 0.076 =============
new_qn: 0.07422942691642559
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.121472857126431
============= xn0: 0.077 =============
new_qn: 0.07520612990216805
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1224495601121733
============= xn0: 0.078 =============
new_qn: 0.07618283288791049
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1234262630979157
============= xn0: 0.079 =============
new_qn: 0.07715953587365293
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1244029660836583
============= xn0: 0.08 =============
new_qn: 0.07813623885939537
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1253796690694007
============= xn0: 0.081 =============
new_qn: 0.07911294184513781
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.126356372055143
============= xn0: 0.082 =============
new_qn: 0.08008964483088025
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1273330750408856
============= xn0: 0.083 =============
new_qn: 0.0810663478166227
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.128309778026628
============= xn0: 0.084 =============
new_qn: 0.08204305080236514
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1292864810123704
============= xn0: 0.085 =============
new_qn: 0.08301975378810758
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1302631839981128
============= xn0: 0.08600000000000001 =============
new_qn: 0.08399645677385002
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1312398869838554
============= xn0: 0.08700000000000001 =============
new_qn: 0.08497315975959246
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1322165899695977
============= xn0: 0.088 =============
new_qn: 0.08594986274533489
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.13319329295534
============= xn0: 0.089 =============
new_qn: 0.08692656573107733
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1341699959410827
============= xn0: 0.09 =============
new_qn: 0.08790326871681978
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.135146698926825
============= xn0: 0.091 =============
new_qn: 0.08887997170256223
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1361234019125674
============= xn0: 0.092 =============
new_qn: 0.08985667468830467
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.13710010489831
============= xn0: 0.093 =============
new_qn: 0.09083337767404712
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1380768078840524
============= xn0: 0.094 =============
new_qn: 0.09181008065978956
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1390535108697948
============= xn0: 0.095 =============
new_qn: 0.092786783645532
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1400302138555374
============= xn0: 0.096 =============
new_qn: 0.09376348663127444
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1410069168412798
============= xn0: 0.097 =============
new_qn: 0.09474018961701688
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1419836198270221
============= xn0: 0.098 =============
new_qn: 0.09571689260275933
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1429603228127647
============= xn0: 0.099 =============
new_qn: 0.09669359558850177
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.143937025798507
============= xn0: 0.1 =============
new_qn: 0.09767029857424421
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1449137287842495
============= xn0: 0.101 =============
new_qn: 0.09864700155998665
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1458904317699918
============= xn0: 0.10200000000000001 =============
new_qn: 0.0996237045457291
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1468671347557344
============= xn0: 0.10300000000000001 =============
new_qn: 0.10060040753147154
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1478438377414768
============= xn0: 0.10400000000000001 =============
new_qn: 0.10157711051721398
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1488205407272192
============= xn0: 0.105 =============
new_qn: 0.10255381350295642
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1497972437129615
============= xn0: 0.106 =============
new_qn: 0.10353051648869886
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1507739466987041
============= xn0: 0.107 =============
new_qn: 0.1045072194744413
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1517506496844465
============= xn0: 0.108 =============
new_qn: 0.10548392246018375
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1527273526701889
============= xn0: 0.109 =============
new_qn: 0.10646062544592619
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1537040556559315
============= xn0: 0.11 =============
new_qn: 0.10743732843166863
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1546807586416739
============= xn0: 0.111 =============
new_qn: 0.10841403141741107
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1556574616274162
============= xn0: 0.112 =============
new_qn: 0.10939073440315351
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1566341646131588
============= xn0: 0.113 =============
new_qn: 0.11036743738889596
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1576108675989012
============= xn0: 0.114 =============
new_qn: 0.1113441403746384
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1585875705846436
============= xn0: 0.115 =============
new_qn: 0.11232084336038084
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1595642735703862
============= xn0: 0.116 =============
new_qn: 0.11329754634612328
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1605409765561285
============= xn0: 0.117 =============
new_qn: 0.11427424933186572
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.161517679541871
============= xn0: 0.11800000000000001 =============
new_qn: 0.11525095231760817
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1624943825276135
============= xn0: 0.11900000000000001 =============
new_qn: 0.11622765530335062
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1634710855133559
============= xn0: 0.12 =============
new_qn: 0.11720435828909305
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1644477884990982
============= xn0: 0.121 =============
new_qn: 0.11818106127483549
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1654244914848408
============= xn0: 0.122 =============
new_qn: 0.11915776426057793
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1664011944705832
============= xn0: 0.123 =============
new_qn: 0.12013446724632038
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1673778974563256
============= xn0: 0.124 =============
new_qn: 0.12111117023206282
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1683546004420682
============= xn0: 0.125 =============
new_qn: 0.12208787321780526
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1693313034278103
============= xn0: 0.126 =============
new_qn: 0.1230645762035477
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.170308006413553
============= xn0: 0.127 =============
new_qn: 0.12404127918929014
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1712847093992955
============= xn0: 0.128 =============
new_qn: 0.1250179821750326
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1722614123850377
============= xn0: 0.129 =============
new_qn: 0.12599468516077503
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1732381153707803
============= xn0: 0.13 =============
new_qn: 0.12697138814651748
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1742148183565229
============= xn0: 0.131 =============
new_qn: 0.1279480911322599
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.175191521342265
============= xn0: 0.132 =============
new_qn: 0.12892479411800237
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1761682243280076
============= xn0: 0.133 =============
new_qn: 0.1299014971037448
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.17714492731375
============= xn0: 0.134 =============
new_qn: 0.13087820008948725
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1781216302994924
============= xn0: 0.135 =============
new_qn: 0.13185490307522968
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.179098333285235
============= xn0: 0.136 =============
new_qn: 0.13283160606097213
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1800750362709773
============= xn0: 0.137 =============
new_qn: 0.13380830904671456
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1810517392567197
============= xn0: 0.138 =============
new_qn: 0.13478501203245702
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1820284422424623
============= xn0: 0.139 =============
new_qn: 0.13576171501819945
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1830051452282047
============= xn0: 0.14 =============
new_qn: 0.1367384180039419
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.183981848213947
============= xn0: 0.14100000000000001 =============
new_qn: 0.13771512098968436
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1849585511996896
============= xn0: 0.14200000000000002 =============
new_qn: 0.1386918239754268
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.185935254185432
============= xn0: 0.14300000000000002 =============
new_qn: 0.13966852696116924
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1869119571711744
============= xn0: 0.14400000000000002 =============
new_qn: 0.14064522994691167
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.187888660156917
============= xn0: 0.145 =============
new_qn: 0.1416219329326541
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1888653631426593
============= xn0: 0.146 =============
new_qn: 0.14259863591839653
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1898420661284017
============= xn0: 0.147 =============
new_qn: 0.14357533890413898
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1908187691141443
============= xn0: 0.148 =============
new_qn: 0.1445520418898814
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1917954720998867
============= xn0: 0.149 =============
new_qn: 0.14552874487562387
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.192772175085629
============= xn0: 0.15 =============
new_qn: 0.1465054478613663
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1937488780713716
============= xn0: 0.151 =============
new_qn: 0.14748215084710875
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.194725581057114
============= xn0: 0.152 =============
new_qn: 0.14845885383285118
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1957022840428564
============= xn0: 0.153 =============
new_qn: 0.14943555681859363
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.196678987028599
============= xn0: 0.154 =============
new_qn: 0.1504122598043361
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1976556900143414
============= xn0: 0.155 =============
new_qn: 0.15138896279007852
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1986323930000837
============= xn0: 0.156 =============
new_qn: 0.15236566577582097
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.1996090959858263
============= xn0: 0.157 =============
new_qn: 0.1533423687615634
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2005857989715685
============= xn0: 0.158 =============
new_qn: 0.15431907174730586
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.201562501957311
============= xn0: 0.159 =============
new_qn: 0.1552957747330483
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2025392049430537
============= xn0: 0.16 =============
new_qn: 0.15627247771879074
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2035159079287958
============= xn0: 0.161 =============
new_qn: 0.15724918070453317
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2044926109145384
============= xn0: 0.162 =============
new_qn: 0.15822588369027563
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.205469313900281
============= xn0: 0.163 =============
new_qn: 0.15920258667601805
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2064460168860232
============= xn0: 0.164 =============
new_qn: 0.1601792896617605
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2074227198717657
============= xn0: 0.165 =============
new_qn: 0.16115599264750294
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2083994228575081
============= xn0: 0.166 =============
new_qn: 0.1621326956332454
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2093761258432505
============= xn0: 0.167 =============
new_qn: 0.16310939861898785
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.210352828828993
============= xn0: 0.168 =============
new_qn: 0.16408610160473028
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2113295318147355
============= xn0: 0.169 =============
new_qn: 0.16506280459047273
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2123062348004778
============= xn0: 0.17 =============
new_qn: 0.16603950757621516
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2132829377862204
============= xn0: 0.171 =============
new_qn: 0.16701621056195762
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2142596407719628
============= xn0: 0.17200000000000001 =============
new_qn: 0.16799291354770005
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2152363437577052
============= xn0: 0.17300000000000001 =============
new_qn: 0.1689696165334425
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2162130467434478
============= xn0: 0.17400000000000002 =============
new_qn: 0.16994631951918493
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2171897497291901
============= xn0: 0.17500000000000002 =============
new_qn: 0.17092302250492739
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2181664527149325
============= xn0: 0.176 =============
new_qn: 0.17189972549066979
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.219143155700675
============= xn0: 0.177 =============
new_qn: 0.17287642847641224
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2201198586864175
============= xn0: 0.178 =============
new_qn: 0.17385313146215467
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2210965616721599
============= xn0: 0.179 =============
new_qn: 0.17482983444789713
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2220732646579024
============= xn0: 0.18 =============
new_qn: 0.17580653743363955
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2230499676436448
============= xn0: 0.181 =============
new_qn: 0.176783240419382
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2240266706293872
============= xn0: 0.182 =============
new_qn: 0.17775994340512447
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2250033736151298
============= xn0: 0.183 =============
new_qn: 0.1787366463908669
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2259800766008722
============= xn0: 0.184 =============
new_qn: 0.17971334937660935
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2269567795866145
============= xn0: 0.185 =============
new_qn: 0.18069005236235178
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2279334825723571
============= xn0: 0.186 =============
new_qn: 0.18166675534809423
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2289101855580995
============= xn0: 0.187 =============
new_qn: 0.18264345833383666
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2298868885438419
============= xn0: 0.188 =============
new_qn: 0.18362016131957912
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2308635915295845
============= xn0: 0.189 =============
new_qn: 0.18459686430532155
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2318402945153266
============= xn0: 0.19 =============
new_qn: 0.185573567291064
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2328169975010692
============= xn0: 0.191 =============
new_qn: 0.18655027027680643
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2337937004868118
============= xn0: 0.192 =============
new_qn: 0.18752697326254888
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.234770403472554
============= xn0: 0.193 =============
new_qn: 0.1885036762482913
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2357471064582966
============= xn0: 0.194 =============
new_qn: 0.18948037923403377
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2367238094440391
============= xn0: 0.195 =============
new_qn: 0.19045708221977622
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2377005124297813
============= xn0: 0.196 =============
new_qn: 0.19143378520551865
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.238677215415524
============= xn0: 0.197 =============
new_qn: 0.1924104881912611
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2396539184012665
============= xn0: 0.198 =============
new_qn: 0.19338719117700354
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2406306213870086
============= xn0: 0.199 =============
new_qn: 0.194363894162746
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2416073243727512
============= xn0: 0.2 =============
new_qn: 0.19534059714848842
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2425840273584936
============= xn0: 0.201 =============
new_qn: 0.19631730013423088
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.243560730344236
============= xn0: 0.202 =============
new_qn: 0.1972940031199733
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2445374333299786
============= xn0: 0.203 =============
new_qn: 0.19827070610571576
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.245514136315721
============= xn0: 0.20400000000000001 =============
new_qn: 0.1992474090914582
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2464908393014633
============= xn0: 0.20500000000000002 =============
new_qn: 0.20022411207720064
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.247467542287206
============= xn0: 0.20600000000000002 =============
new_qn: 0.20120081506294307
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2484442452729483
============= xn0: 0.20700000000000002 =============
new_qn: 0.20217751804868553
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2494209482586907
============= xn0: 0.20800000000000002 =============
new_qn: 0.20315422103442796
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2503976512444333
============= xn0: 0.209 =============
new_qn: 0.20413092402017038
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2513743542301756
============= xn0: 0.21 =============
new_qn: 0.20510762700591284
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.252351057215918
============= xn0: 0.211 =============
new_qn: 0.20608432999165527
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2533277602016606
============= xn0: 0.212 =============
new_qn: 0.20706103297739772
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.254304463187403
============= xn0: 0.213 =============
new_qn: 0.20803773596314015
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2552811661731453
============= xn0: 0.214 =============
new_qn: 0.2090144389488826
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.256257869158888
============= xn0: 0.215 =============
new_qn: 0.20999114193462504
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2572345721446303
============= xn0: 0.216 =============
new_qn: 0.2109678449203675
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2582112751303727
============= xn0: 0.217 =============
new_qn: 0.21194454790610992
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2591879781161153
============= xn0: 0.218 =============
new_qn: 0.21292125089185238
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2601646811018576
============= xn0: 0.219 =============
new_qn: 0.2138979538775948
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2611413840876
============= xn0: 0.22 =============
new_qn: 0.21487465686333726
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2621180870733426
============= xn0: 0.221 =============
new_qn: 0.2158513598490797
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2630947900590848
============= xn0: 0.222 =============
new_qn: 0.21682806283482214
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2640714930448274
============= xn0: 0.223 =============
new_qn: 0.2178047658205646
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.26504819603057
============= xn0: 0.224 =============
new_qn: 0.21878146880630703
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.266024899016312
============= xn0: 0.225 =============
new_qn: 0.21975817179204948
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2670016020020547
============= xn0: 0.226 =============
new_qn: 0.2207348747777919
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2679783049877973
============= xn0: 0.227 =============
new_qn: 0.22171157776353437
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2689550079735394
============= xn0: 0.228 =============
new_qn: 0.2226882807492768
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.269931710959282
============= xn0: 0.229 =============
new_qn: 0.22366498373501925
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2709084139450246
============= xn0: 0.23 =============
new_qn: 0.22464168672076168
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2718851169307668
============= xn0: 0.231 =============
new_qn: 0.22561838970650414
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2728618199165094
============= xn0: 0.232 =============
new_qn: 0.22659509269224656
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2738385229022517
============= xn0: 0.233 =============
new_qn: 0.22757179567798902
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2748152258879941
============= xn0: 0.234 =============
new_qn: 0.22854849866373145
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2757919288737367
============= xn0: 0.23500000000000001 =============
new_qn: 0.2295252016494739
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.276768631859479
============= xn0: 0.23600000000000002 =============
new_qn: 0.23050190463521633
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2777453348452215
============= xn0: 0.23700000000000002 =============
new_qn: 0.2314786076209588
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.278722037830964
============= xn0: 0.23800000000000002 =============
new_qn: 0.23245531060670124
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2796987408167064
============= xn0: 0.23900000000000002 =============
new_qn: 0.23343201359244367
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2806754438024488
============= xn0: 0.24 =============
new_qn: 0.2344087165781861
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2816521467881914
============= xn0: 0.241 =============
new_qn: 0.23538541956392853
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2826288497739338
============= xn0: 0.242 =============
new_qn: 0.23636212254967098
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2836055527596761
============= xn0: 0.243 =============
new_qn: 0.2373388255354134
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2845822557454187
============= xn0: 0.244 =============
new_qn: 0.23831552852115587
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.285558958731161
============= xn0: 0.245 =============
new_qn: 0.2392922315068983
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2865356617169035
============= xn0: 0.246 =============
new_qn: 0.24026893449264075
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.287512364702646
============= xn0: 0.247 =============
new_qn: 0.24124563747838318
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2884890676883884
============= xn0: 0.248 =============
new_qn: 0.24222234046412564
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2894657706741308
============= xn0: 0.249 =============
new_qn: 0.24319904344986806
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2904424736598734
============= xn0: 0.25 =============
new_qn: 0.24417574643561052
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2914191766456158
============= xn0: 0.251 =============
new_qn: 0.24515244942135297
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2923958796313582
============= xn0: 0.252 =============
new_qn: 0.2461291524070954
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2933725826171008
============= xn0: 0.253 =============
new_qn: 0.24710585539283786
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2943492856028431
============= xn0: 0.254 =============
new_qn: 0.2480825583785803
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2953259885885855
============= xn0: 0.255 =============
new_qn: 0.24905926136432274
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.296302691574328
============= xn0: 0.256 =============
new_qn: 0.2500359643500652
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2972793945600705
============= xn0: 0.257 =============
new_qn: 0.2510126673358076
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2982560975458128
============= xn0: 0.258 =============
new_qn: 0.25198937032155005
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.2992328005315554
============= xn0: 0.259 =============
new_qn: 0.2529660733072925
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3002095035172976
============= xn0: 0.26 =============
new_qn: 0.25394277629303497
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3011862065030402
============= xn0: 0.261 =============
new_qn: 0.2549194792787774
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3021629094887828
============= xn0: 0.262 =============
new_qn: 0.2558961822645198
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.303139612474525
============= xn0: 0.263 =============
new_qn: 0.25687288525026225
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3041163154602675
============= xn0: 0.264 =============
new_qn: 0.25784958823600473
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3050930184460101
============= xn0: 0.265 =============
new_qn: 0.25882629122174716
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3060697214317523
============= xn0: 0.266 =============
new_qn: 0.2598029942074896
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3070464244174949
============= xn0: 0.267 =============
new_qn: 0.2607796971932321
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3080231274032375
============= xn0: 0.268 =============
new_qn: 0.2617564001789745
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3089998303889796
============= xn0: 0.269 =============
new_qn: 0.26273310316471693
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3099765333747222
============= xn0: 0.27 =============
new_qn: 0.26370980615045936
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3109532363604648
============= xn0: 0.271 =============
new_qn: 0.26468650913620184
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.311929939346207
============= xn0: 0.272 =============
new_qn: 0.26566321212194427
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3129066423319495
============= xn0: 0.273 =============
new_qn: 0.2666399151076867
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3138833453176921
============= xn0: 0.274 =============
new_qn: 0.2676166180934291
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3148600483034343
============= xn0: 0.275 =============
new_qn: 0.2685933210791716
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3158367512891769
============= xn0: 0.276 =============
new_qn: 0.26957002406491404
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3168134542749195
============= xn0: 0.277 =============
new_qn: 0.27054672705065647
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3177901572606616
============= xn0: 0.278 =============
new_qn: 0.2715234300363989
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3187668602464042
============= xn0: 0.279 =============
new_qn: 0.2725001330221414
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3197435632321468
============= xn0: 0.28 =============
new_qn: 0.2734768360078838
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.320720266217889
============= xn0: 0.281 =============
new_qn: 0.27445353899362623
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3216969692036316
============= xn0: 0.28200000000000003 =============
new_qn: 0.2754302419793687
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3226736721893741
============= xn0: 0.28300000000000003 =============
new_qn: 0.27640694496511115
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3236503751751163
============= xn0: 0.28400000000000003 =============
new_qn: 0.2773836479508536
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.324627078160859
============= xn0: 0.28500000000000003 =============
new_qn: 0.278360350936596
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3256037811466015
============= xn0: 0.28600000000000003 =============
new_qn: 0.2793370539223385
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3265804841323436
============= xn0: 0.28700000000000003 =============
new_qn: 0.2803137569080809
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3275571871180862
============= xn0: 0.28800000000000003 =============
new_qn: 0.28129045989382334
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3285338901038288
============= xn0: 0.289 =============
new_qn: 0.2822671628795657
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.329510593089571
============= xn0: 0.29 =============
new_qn: 0.2832438658653082
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3304872960753136
============= xn0: 0.291 =============
new_qn: 0.2842205688510506
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3314639990610557
============= xn0: 0.292 =============
new_qn: 0.28519727183679305
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3324407020467983
============= xn0: 0.293 =============
new_qn: 0.28617397482253554
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.333417405032541
============= xn0: 0.294 =============
new_qn: 0.28715067780827797
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.334394108018283
============= xn0: 0.295 =============
new_qn: 0.2881273807940204
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3353708110040257
============= xn0: 0.296 =============
new_qn: 0.2891040837797628
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3363475139897683
============= xn0: 0.297 =============
new_qn: 0.2900807867655053
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3373242169755104
============= xn0: 0.298 =============
new_qn: 0.29105748975124773
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.338300919961253
============= xn0: 0.299 =============
new_qn: 0.29203419273699016
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3392776229469956
============= xn0: 0.3 =============
new_qn: 0.2930108957227326
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3402543259327377
============= xn0: 0.301 =============
new_qn: 0.2939875987084751
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3412310289184803
============= xn0: 0.302 =============
new_qn: 0.2949643016942175
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.342207731904223
============= xn0: 0.303 =============
new_qn: 0.29594100467995993
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3431844348899655
============= xn0: 0.304 =============
new_qn: 0.29691770766570236
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3441611378757077
============= xn0: 0.305 =============
new_qn: 0.29789441065144484
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3451378408614503
============= xn0: 0.306 =============
new_qn: 0.29887111363718727
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3461145438471929
============= xn0: 0.307 =============
new_qn: 0.2998478166229297
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.347091246832935
============= xn0: 0.308 =============
new_qn: 0.3008245196086722
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3480679498186776
============= xn0: 0.309 =============
new_qn: 0.3018012225944146
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3490446528044202
============= xn0: 0.31 =============
new_qn: 0.30277792558015704
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3500213557901624
============= xn0: 0.311 =============
new_qn: 0.30375462856589946
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.350998058775905
============= xn0: 0.312 =============
new_qn: 0.30473133155164195
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3519747617616475
============= xn0: 0.313 =============
new_qn: 0.3057080345373844
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3529514647473897
============= xn0: 0.314 =============
new_qn: 0.3066847375231268
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3539281677331323
============= xn0: 0.315 =============
new_qn: 0.30766144050886923
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3549048707188744
============= xn0: 0.316 =============
new_qn: 0.3086381434946117
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.355881573704617
============= xn0: 0.317 =============
new_qn: 0.30961484648035414
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3568582766903596
============= xn0: 0.318 =============
new_qn: 0.3105915494660966
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3578349796761018
============= xn0: 0.319 =============
new_qn: 0.311568252451839
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3588116826618444
============= xn0: 0.32 =============
new_qn: 0.3125449554375815
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.359788385647587
============= xn0: 0.321 =============
new_qn: 0.3135216584233239
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3607650886333291
============= xn0: 0.322 =============
new_qn: 0.31449836140906634
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3617417916190717
============= xn0: 0.323 =============
new_qn: 0.3154750643948088
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3627184946048143
============= xn0: 0.324 =============
new_qn: 0.31645176738055125
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3636951975905565
============= xn0: 0.325 =============
new_qn: 0.3174284703662937
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.364671900576299
============= xn0: 0.326 =============
new_qn: 0.3184051733520361
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3656486035620417
============= xn0: 0.327 =============
new_qn: 0.3193818763377786
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3666253065477838
============= xn0: 0.328 =============
new_qn: 0.320358579323521
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3676020095335264
============= xn0: 0.329 =============
new_qn: 0.32133528230926345
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.368578712519269
============= xn0: 0.33 =============
new_qn: 0.3223119852950059
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3695554155050111
============= xn0: 0.331 =============
new_qn: 0.32328868828074836
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3705321184907537
============= xn0: 0.332 =============
new_qn: 0.3242653912664908
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3715088214764963
============= xn0: 0.333 =============
new_qn: 0.3252420942522332
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3724855244622385
============= xn0: 0.334 =============
new_qn: 0.3262187972379757
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.373462227447981
============= xn0: 0.335 =============
new_qn: 0.3271955002237181
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3744389304337237
============= xn0: 0.336 =============
new_qn: 0.32817220320946056
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3754156334194658
============= xn0: 0.337 =============
new_qn: 0.329148906195203
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3763923364052084
============= xn0: 0.338 =============
new_qn: 0.33012560918094547
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.377369039390951
============= xn0: 0.339 =============
new_qn: 0.3311023121666879
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3783457423766932
============= xn0: 0.34 =============
new_qn: 0.3320790151524303
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3793224453624358
============= xn0: 0.341 =============
new_qn: 0.33305571813817275
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3802991483481784
============= xn0: 0.342 =============
new_qn: 0.33403242112391524
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3812758513339205
============= xn0: 0.343 =============
new_qn: 0.33500912410965766
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.382252554319663
============= xn0: 0.34400000000000003 =============
new_qn: 0.3359858270954001
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3832292573054057
============= xn0: 0.34500000000000003 =============
new_qn: 0.3369625300811425
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3842059602911478
============= xn0: 0.34600000000000003 =============
new_qn: 0.337939233066885
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3851826632768904
============= xn0: 0.34700000000000003 =============
new_qn: 0.33891593605262743
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.386159366262633
============= xn0: 0.34800000000000003 =============
new_qn: 0.33989263903836986
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3871360692483752
============= xn0: 0.34900000000000003 =============
new_qn: 0.34086934202411234
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3881127722341178
============= xn0: 0.35000000000000003 =============
new_qn: 0.34184604500985477
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3890894752198604
============= xn0: 0.35100000000000003 =============
new_qn: 0.3428227479955972
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3900661782056025
============= xn0: 0.352 =============
new_qn: 0.34379945098133957
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3910428811913451
============= xn0: 0.353 =============
new_qn: 0.34477615396708206
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3920195841770873
============= xn0: 0.354 =============
new_qn: 0.3457528569528245
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3929962871628299
============= xn0: 0.355 =============
new_qn: 0.3467295599385669
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3939729901485725
============= xn0: 0.356 =============
new_qn: 0.34770626292430934
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3949496931343146
============= xn0: 0.357 =============
new_qn: 0.3486829659100518
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3959263961200572
============= xn0: 0.358 =============
new_qn: 0.34965966889579425
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3969030991057998
============= xn0: 0.359 =============
new_qn: 0.3506363718815367
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.397879802091542
============= xn0: 0.36 =============
new_qn: 0.3516130748672791
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3988565050772845
============= xn0: 0.361 =============
new_qn: 0.3525897778530216
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.3998332080630271
============= xn0: 0.362 =============
new_qn: 0.353566480838764
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4008099110487693
============= xn0: 0.363 =============
new_qn: 0.35454318382450645
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4017866140345119
============= xn0: 0.364 =============
new_qn: 0.35551988681024893
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4027633170202545
============= xn0: 0.365 =============
new_qn: 0.35649658979599136
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4037400200059966
============= xn0: 0.366 =============
new_qn: 0.3574732927817338
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4047167229917392
============= xn0: 0.367 =============
new_qn: 0.3584499957674762
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4056934259774818
============= xn0: 0.368 =============
new_qn: 0.3594266987532187
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.406670128963224
============= xn0: 0.369 =============
new_qn: 0.3604034017389611
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4076468319489666
============= xn0: 0.37 =============
new_qn: 0.36138010472470355
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4086235349347092
============= xn0: 0.371 =============
new_qn: 0.362356807710446
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4096002379204513
============= xn0: 0.372 =============
new_qn: 0.36333351069618847
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.410576940906194
============= xn0: 0.373 =============
new_qn: 0.3643102136819309
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4115536438919365
============= xn0: 0.374 =============
new_qn: 0.3652869166676733
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4125303468776786
============= xn0: 0.375 =============
new_qn: 0.3662636196534158
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4135070498634212
============= xn0: 0.376 =============
new_qn: 0.36724032263915823
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4144837528491638
============= xn0: 0.377 =============
new_qn: 0.36821702562490066
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.415460455834906
============= xn0: 0.378 =============
new_qn: 0.3691937286106431
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4164371588206486
============= xn0: 0.379 =============
new_qn: 0.3701704315963856
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4174138618063912
============= xn0: 0.38 =============
new_qn: 0.371147134582128
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4183905647921333
============= xn0: 0.381 =============
new_qn: 0.37212383756787043
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.419367267777876
============= xn0: 0.382 =============
new_qn: 0.37310054055361286
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.420343970763618
============= xn0: 0.383 =============
new_qn: 0.37407724353935534
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4213206737493607
============= xn0: 0.384 =============
new_qn: 0.37505394652509777
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4222973767351033
============= xn0: 0.385 =============
new_qn: 0.3760306495108402
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4232740797208454
============= xn0: 0.386 =============
new_qn: 0.3770073524965826
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.424250782706588
============= xn0: 0.387 =============
new_qn: 0.3779840554823251
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4252274856923306
============= xn0: 0.388 =============
new_qn: 0.37896075846806754
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4262041886780727
============= xn0: 0.389 =============
new_qn: 0.37993746145380997
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4271808916638153
============= xn0: 0.39 =============
new_qn: 0.38091416443955245
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.428157594649558
============= xn0: 0.391 =============
new_qn: 0.3818908674252949
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4291342976353
============= xn0: 0.392 =============
new_qn: 0.3828675704110373
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4301110006210427
============= xn0: 0.393 =============
new_qn: 0.38384427339677973
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4310877036067853
============= xn0: 0.394 =============
new_qn: 0.3848209763825222
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4320644065925274
============= xn0: 0.395 =============
new_qn: 0.38579767936826465
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.43304110957827
============= xn0: 0.396 =============
new_qn: 0.3867743823540071
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4340178125640126
============= xn0: 0.397 =============
new_qn: 0.3877510853397495
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4349945155497548
============= xn0: 0.398 =============
new_qn: 0.388727788325492
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4359712185354974
============= xn0: 0.399 =============
new_qn: 0.3897044913112344
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.43694792152124
============= xn0: 0.4 =============
new_qn: 0.39068119429697684
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.437924624506982
============= xn0: 0.401 =============
new_qn: 0.39165789728271927
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4389013274927247
============= xn0: 0.402 =============
new_qn: 0.39263460026846175
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4398780304784673
============= xn0: 0.403 =============
new_qn: 0.3936113032542042
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4408547334642094
============= xn0: 0.404 =============
new_qn: 0.3945880062399466
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.441831436449952
============= xn0: 0.405 =============
new_qn: 0.3955647092256891
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4428081394356946
============= xn0: 0.406 =============
new_qn: 0.3965414122114315
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4437848424214368
============= xn0: 0.40700000000000003 =============
new_qn: 0.39751811519717395
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4447615454071794
============= xn0: 0.40800000000000003 =============
new_qn: 0.3984948181829164
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.445738248392922
============= xn0: 0.40900000000000003 =============
new_qn: 0.39947152116865886
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4467149513786641
============= xn0: 0.41000000000000003 =============
new_qn: 0.4004482241544013
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4476916543644067
============= xn0: 0.41100000000000003 =============
new_qn: 0.4014249271401437
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4486683573501493
============= xn0: 0.41200000000000003 =============
new_qn: 0.40240163012588614
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4496450603358915
============= xn0: 0.41300000000000003 =============
new_qn: 0.40337833311162863
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.450621763321634
============= xn0: 0.41400000000000003 =============
new_qn: 0.40435503609737106
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4515984663073767
============= xn0: 0.41500000000000004 =============
new_qn: 0.4053317390831135
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4525751692931188
============= xn0: 0.41600000000000004 =============
new_qn: 0.4063084420688559
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4535518722788614
============= xn0: 0.417 =============
new_qn: 0.40728514505459834
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4545285752646036
============= xn0: 0.418 =============
new_qn: 0.40826184804034077
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4555052782503461
============= xn0: 0.419 =============
new_qn: 0.4092385510260832
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4564819812360887
============= xn0: 0.42 =============
new_qn: 0.4102152540118257
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.457458684221831
============= xn0: 0.421 =============
new_qn: 0.4111919569975681
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4584353872075735
============= xn0: 0.422 =============
new_qn: 0.41216865998331054
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.459412090193316
============= xn0: 0.423 =============
new_qn: 0.41314536296905296
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4603887931790582
============= xn0: 0.424 =============
new_qn: 0.41412206595479545
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4613654961648008
============= xn0: 0.425 =============
new_qn: 0.4150987689405379
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4623421991505434
============= xn0: 0.426 =============
new_qn: 0.4160754719262803
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4633189021362856
============= xn0: 0.427 =============
new_qn: 0.41705217491202273
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4642956051220282
============= xn0: 0.428 =============
new_qn: 0.4180288778977652
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4652723081077708
============= xn0: 0.429 =============
new_qn: 0.41900558088350764
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.466249011093513
============= xn0: 0.43 =============
new_qn: 0.4199822838692501
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4672257140792555
============= xn0: 0.431 =============
new_qn: 0.42095898685499256
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.468202417064998
============= xn0: 0.432 =============
new_qn: 0.421935689840735
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4691791200507403
============= xn0: 0.433 =============
new_qn: 0.4229123928264774
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4701558230364828
============= xn0: 0.434 =============
new_qn: 0.42388909581221984
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4711325260222254
============= xn0: 0.435 =============
new_qn: 0.4248657987979623
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4721092290079676
============= xn0: 0.436 =============
new_qn: 0.42584250178370475
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4730859319937102
============= xn0: 0.437 =============
new_qn: 0.4268192047694472
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4740626349794528
============= xn0: 0.438 =============
new_qn: 0.4277959077551896
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.475039337965195
============= xn0: 0.439 =============
new_qn: 0.4287726107409321
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4760160409509375
============= xn0: 0.44 =============
new_qn: 0.4297493137266745
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4769927439366801
============= xn0: 0.441 =============
new_qn: 0.43072601671241695
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4779694469224223
============= xn0: 0.442 =============
new_qn: 0.4317027196981594
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4789461499081649
============= xn0: 0.443 =============
new_qn: 0.43267942268390186
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4799228528939075
============= xn0: 0.444 =============
new_qn: 0.4336561256696443
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4808995558796496
============= xn0: 0.445 =============
new_qn: 0.4346328286553867
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4818762588653922
============= xn0: 0.446 =============
new_qn: 0.4356095316411292
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4828529618511348
============= xn0: 0.447 =============
new_qn: 0.4365862346268716
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.483829664836877
============= xn0: 0.448 =============
new_qn: 0.43756293761261406
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4848063678226195
============= xn0: 0.449 =============
new_qn: 0.4385396405983565
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4857830708083617
============= xn0: 0.45 =============
new_qn: 0.43951634358409897
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4867597737941043
============= xn0: 0.451 =============
new_qn: 0.4404930465698414
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4877364767798469
============= xn0: 0.452 =============
new_qn: 0.4414697495555838
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.488713179765589
============= xn0: 0.453 =============
new_qn: 0.44244645254132625
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4896898827513316
============= xn0: 0.454 =============
new_qn: 0.44342315552706874
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4906665857370742
============= xn0: 0.455 =============
new_qn: 0.44439985851281116
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4916432887228164
============= xn0: 0.456 =============
new_qn: 0.4453765614985536
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.492619991708559
============= xn0: 0.457 =============
new_qn: 0.446353264484296
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4935966946943016
============= xn0: 0.458 =============
new_qn: 0.4473299674700385
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4945733976800437
============= xn0: 0.459 =============
new_qn: 0.44830667045578093
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4955501006657863
============= xn0: 0.46 =============
new_qn: 0.44928337344152336
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.496526803651529
============= xn0: 0.461 =============
new_qn: 0.45026007642726584
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.497503506637271
============= xn0: 0.462 =============
new_qn: 0.45123677941300827
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4984802096230136
============= xn0: 0.463 =============
new_qn: 0.4522134823987507
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.4994569126087562
============= xn0: 0.464 =============
new_qn: 0.4531901853844931
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5004336155944984
============= xn0: 0.465 =============
new_qn: 0.4541668883702356
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.501410318580241
============= xn0: 0.466 =============
new_qn: 0.45514359135597804
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5023870215659836
============= xn0: 0.467 =============
new_qn: 0.45612029434172047
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5033637245517257
============= xn0: 0.468 =============
new_qn: 0.4570969973274629
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5043404275374683
============= xn0: 0.46900000000000003 =============
new_qn: 0.4580737003132054
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.505317130523211
============= xn0: 0.47000000000000003 =============
new_qn: 0.4590504032989478
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.506293833508953
============= xn0: 0.47100000000000003 =============
new_qn: 0.46002710628469023
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5072705364946957
============= xn0: 0.47200000000000003 =============
new_qn: 0.46100380927043266
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5082472394804383
============= xn0: 0.47300000000000003 =============
new_qn: 0.46198051225617515
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5092239424661804
============= xn0: 0.47400000000000003 =============
new_qn: 0.4629572152419176
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.510200645451923
============= xn0: 0.47500000000000003 =============
new_qn: 0.46393391822766
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5111773484376656
============= xn0: 0.47600000000000003 =============
new_qn: 0.4649106212134025
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5121540514234078
============= xn0: 0.47700000000000004 =============
new_qn: 0.4658873241991449
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5131307544091503
============= xn0: 0.47800000000000004 =============
new_qn: 0.46686402718488734
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.514107457394893
============= xn0: 0.47900000000000004 =============
new_qn: 0.46784073017062977
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.515084160380635
============= xn0: 0.48 =============
new_qn: 0.4688174331563722
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5160608633663777
============= xn0: 0.481 =============
new_qn: 0.4697941361421146
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5170375663521198
============= xn0: 0.482 =============
new_qn: 0.47077083912785705
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5180142693378624
============= xn0: 0.483 =============
new_qn: 0.4717475421135995
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.518990972323605
============= xn0: 0.484 =============
new_qn: 0.47272424509934197
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5199676753093472
============= xn0: 0.485 =============
new_qn: 0.4737009480850844
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5209443782950898
============= xn0: 0.486 =============
new_qn: 0.4746776510708268
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5219210812808324
============= xn0: 0.487 =============
new_qn: 0.4756543540565693
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5228977842665745
============= xn0: 0.488 =============
new_qn: 0.47663105704231173
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5238744872523171
============= xn0: 0.489 =============
new_qn: 0.47760776002805416
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5248511902380597
============= xn0: 0.49 =============
new_qn: 0.4785844630137966
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5258278932238019
============= xn0: 0.491 =============
new_qn: 0.4795611659995391
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5268045962095445
============= xn0: 0.492 =============
new_qn: 0.4805378689852815
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.527781299195287
============= xn0: 0.493 =============
new_qn: 0.48151457197102393
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5287580021810292
============= xn0: 0.494 =============
new_qn: 0.48249127495676636
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5297347051667718
============= xn0: 0.495 =============
new_qn: 0.48346797794250884
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5307114081525144
============= xn0: 0.496 =============
new_qn: 0.48444468092825127
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5316881111382565
============= xn0: 0.497 =============
new_qn: 0.4854213839139937
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5326648141239991
============= xn0: 0.498 =============
new_qn: 0.4863980868997361
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5336415171097417
============= xn0: 0.499 =============
new_qn: 0.4873747898854786
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5346182200954839
============= xn0: 0.5 =============
new_qn: 0.48835149287122104
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5355949230812265
============= xn0: 0.501 =============
new_qn: 0.48932819585696347
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.536571626066969
============= xn0: 0.502 =============
new_qn: 0.49030489884270595
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5375483290527112
============= xn0: 0.503 =============
new_qn: 0.4912816018284484
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5385250320384538
============= xn0: 0.504 =============
new_qn: 0.4922583048141908
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5395017350241964
============= xn0: 0.505 =============
new_qn: 0.49323500779993323
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5404784380099386
============= xn0: 0.506 =============
new_qn: 0.4942117107856757
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5414551409956812
============= xn0: 0.507 =============
new_qn: 0.49518841377141815
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5424318439814237
============= xn0: 0.508 =============
new_qn: 0.4961651167571606
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.543408546967166
============= xn0: 0.509 =============
new_qn: 0.497141819742903
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5443852499529085
============= xn0: 0.51 =============
new_qn: 0.4981185227286455
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.545361952938651
============= xn0: 0.511 =============
new_qn: 0.4990952257143879
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5463386559243932
============= xn0: 0.512 =============
new_qn: 0.5000719287001304
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5473153589101358
============= xn0: 0.513 =============
new_qn: 0.5010486316858728
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.548292061895878
============= xn0: 0.514 =============
new_qn: 0.5020253346716153
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5492687648816206
============= xn0: 0.515 =============
new_qn: 0.5030020376573577
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5502454678673632
============= xn0: 0.516 =============
new_qn: 0.5039787406431001
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5512221708531053
============= xn0: 0.517 =============
new_qn: 0.5049554436288426
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.552198873838848
============= xn0: 0.518 =============
new_qn: 0.505932146614585
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5531755768245905
============= xn0: 0.519 =============
new_qn: 0.5069088496003274
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5541522798103327
============= xn0: 0.52 =============
new_qn: 0.5078855525860699
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5551289827960753
============= xn0: 0.521 =============
new_qn: 0.5088622555718123
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5561056857818178
============= xn0: 0.522 =============
new_qn: 0.5098389585575548
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.55708238876756
============= xn0: 0.523 =============
new_qn: 0.5108156615432973
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5580590917533026
============= xn0: 0.524 =============
new_qn: 0.5117923645290396
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5590357947390452
============= xn0: 0.525 =============
new_qn: 0.5127690675147821
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5600124977247873
============= xn0: 0.526 =============
new_qn: 0.5137457705005245
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.56098920071053
============= xn0: 0.527 =============
new_qn: 0.514722473486267
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5619659036962725
============= xn0: 0.528 =============
new_qn: 0.5156991764720095
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5629426066820147
============= xn0: 0.529 =============
new_qn: 0.5166758794577518
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5639193096677573
============= xn0: 0.53 =============
new_qn: 0.5176525824434943
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5648960126534999
============= xn0: 0.531 =============
new_qn: 0.5186292854292368
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.565872715639242
============= xn0: 0.532 =============
new_qn: 0.5196059884149792
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5668494186249846
============= xn0: 0.533 =============
new_qn: 0.5205826914007217
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5678261216107272
============= xn0: 0.534 =============
new_qn: 0.5215593943864641
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5688028245964694
============= xn0: 0.535 =============
new_qn: 0.5225360973722065
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.569779527582212
============= xn0: 0.536 =============
new_qn: 0.523512800357949
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5707562305679545
============= xn0: 0.537 =============
new_qn: 0.5244895033436914
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5717329335536967
============= xn0: 0.538 =============
new_qn: 0.5254662063294339
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5727096365394393
============= xn0: 0.539 =============
new_qn: 0.5264429093151763
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5736863395251819
============= xn0: 0.54 =============
new_qn: 0.5274196123009187
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.574663042510924
============= xn0: 0.541 =============
new_qn: 0.5283963152866612
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5756397454966666
============= xn0: 0.542 =============
new_qn: 0.5293730182724037
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5766164484824092
============= xn0: 0.543 =============
new_qn: 0.5303497212581461
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5775931514681514
============= xn0: 0.544 =============
new_qn: 0.5313264242438885
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.578569854453894
============= xn0: 0.545 =============
new_qn: 0.532303127229631
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5795465574396366
============= xn0: 0.546 =============
new_qn: 0.5332798302153734
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5805232604253787
============= xn0: 0.547 =============
new_qn: 0.5342565332011159
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5814999634111213
============= xn0: 0.548 =============
new_qn: 0.5352332361868583
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5824766663968635
============= xn0: 0.549 =============
new_qn: 0.5362099391726007
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.583453369382606
============= xn0: 0.55 =============
new_qn: 0.5371866421583432
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5844300723683487
============= xn0: 0.551 =============
new_qn: 0.5381633451440856
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5854067753540908
============= xn0: 0.552 =============
new_qn: 0.5391400481298281
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5863834783398334
============= xn0: 0.553 =============
new_qn: 0.5401167511155706
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.587360181325576
============= xn0: 0.554 =============
new_qn: 0.5410934541013129
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5883368843113181
============= xn0: 0.555 =============
new_qn: 0.5420701570870554
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5893135872970607
============= xn0: 0.556 =============
new_qn: 0.5430468600727978
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5902902902828033
============= xn0: 0.557 =============
new_qn: 0.5440235630585403
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5912669932685455
============= xn0: 0.558 =============
new_qn: 0.5450002660442828
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.592243696254288
============= xn0: 0.559 =============
new_qn: 0.5459769690300251
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5932203992400307
============= xn0: 0.56 =============
new_qn: 0.5469536720157676
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5941971022257728
============= xn0: 0.561 =============
new_qn: 0.5479303750015101
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5951738052115154
============= xn0: 0.562 =============
new_qn: 0.5489070779872525
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.596150508197258
============= xn0: 0.5630000000000001 =============
new_qn: 0.549883780972995
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5971272111830002
============= xn0: 0.5640000000000001 =============
new_qn: 0.5508604839587374
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5981039141687428
============= xn0: 0.5650000000000001 =============
new_qn: 0.5518371869444798
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.5990806171544854
============= xn0: 0.5660000000000001 =============
new_qn: 0.5528138899302223
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6000573201402275
============= xn0: 0.5670000000000001 =============
new_qn: 0.5537905929159647
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.60103402312597
============= xn0: 0.5680000000000001 =============
new_qn: 0.5547672959017071
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6020107261117127
============= xn0: 0.5690000000000001 =============
new_qn: 0.5557439988874496
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6029874290974548
============= xn0: 0.5700000000000001 =============
new_qn: 0.556720701873192
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6039641320831974
============= xn0: 0.5710000000000001 =============
new_qn: 0.5576974048589345
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.60494083506894
============= xn0: 0.5720000000000001 =============
new_qn: 0.558674107844677
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6059175380546822
============= xn0: 0.5730000000000001 =============
new_qn: 0.5596508108304193
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6068942410404248
============= xn0: 0.5740000000000001 =============
new_qn: 0.5606275138161618
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6078709440261674
============= xn0: 0.5750000000000001 =============
new_qn: 0.5616042168019043
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6088476470119095
============= xn0: 0.5760000000000001 =============
new_qn: 0.5625809197876467
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6098243499976521
============= xn0: 0.577 =============
new_qn: 0.563557622773389
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6108010529833943
============= xn0: 0.578 =============
new_qn: 0.5645343257591314
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6117777559691369
============= xn0: 0.579 =============
new_qn: 0.5655110287448739
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6127544589548795
============= xn0: 0.58 =============
new_qn: 0.5664877317306164
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6137311619406216
============= xn0: 0.581 =============
new_qn: 0.5674644347163588
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6147078649263642
============= xn0: 0.582 =============
new_qn: 0.5684411377021013
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6156845679121068
============= xn0: 0.583 =============
new_qn: 0.5694178406878437
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.616661270897849
============= xn0: 0.584 =============
new_qn: 0.5703945436735861
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6176379738835915
============= xn0: 0.585 =============
new_qn: 0.5713712466593286
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6186146768693341
============= xn0: 0.586 =============
new_qn: 0.5723479496450711
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6195913798550763
============= xn0: 0.587 =============
new_qn: 0.5733246526308134
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6205680828408189
============= xn0: 0.588 =============
new_qn: 0.5743013556165559
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6215447858265615
============= xn0: 0.589 =============
new_qn: 0.5752780586022983
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6225214888123036
============= xn0: 0.59 =============
new_qn: 0.5762547615880408
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6234981917980462
============= xn0: 0.591 =============
new_qn: 0.5772314645737833
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6244748947837888
============= xn0: 0.592 =============
new_qn: 0.5782081675595256
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.625451597769531
============= xn0: 0.593 =============
new_qn: 0.5791848705452681
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6264283007552736
============= xn0: 0.594 =============
new_qn: 0.5801615735310106
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6274050037410162
============= xn0: 0.595 =============
new_qn: 0.581138276516753
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6283817067267583
============= xn0: 0.596 =============
new_qn: 0.5821149795024955
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.629358409712501
============= xn0: 0.597 =============
new_qn: 0.583091682488238
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6303351126982435
============= xn0: 0.598 =============
new_qn: 0.5840683854739803
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6313118156839856
============= xn0: 0.599 =============
new_qn: 0.5850450884597228
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6322885186697282
============= xn0: 0.6 =============
new_qn: 0.5860217914454652
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6332652216554704
============= xn0: 0.601 =============
new_qn: 0.5869984944312077
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.634241924641213
============= xn0: 0.602 =============
new_qn: 0.5879751974169501
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6352186276269556
============= xn0: 0.603 =============
new_qn: 0.5889519004026925
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6361953306126977
============= xn0: 0.604 =============
new_qn: 0.589928603388435
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6371720335984403
============= xn0: 0.605 =============
new_qn: 0.5909053063741775
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.638148736584183
============= xn0: 0.606 =============
new_qn: 0.5918820093599199
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.639125439569925
============= xn0: 0.607 =============
new_qn: 0.5928587123456623
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6401021425556677
============= xn0: 0.608 =============
new_qn: 0.5938354153314047
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6410788455414103
============= xn0: 0.609 =============
new_qn: 0.5948121183171472
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6420555485271524
============= xn0: 0.61 =============
new_qn: 0.5957888213028897
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.643032251512895
============= xn0: 0.611 =============
new_qn: 0.596765524288632
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6440089544986376
============= xn0: 0.612 =============
new_qn: 0.5977422272743745
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6449856574843797
============= xn0: 0.613 =============
new_qn: 0.598718930260117
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6459623604701223
============= xn0: 0.614 =============
new_qn: 0.5996956332458594
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.646939063455865
============= xn0: 0.615 =============
new_qn: 0.6006723362316019
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.647915766441607
============= xn0: 0.616 =============
new_qn: 0.6016490392173444
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6488924694273497
============= xn0: 0.617 =============
new_qn: 0.6026257422030867
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6498691724130923
============= xn0: 0.618 =============
new_qn: 0.6036024451888292
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6508458753988344
============= xn0: 0.619 =============
new_qn: 0.6045791481745716
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.651822578384577
============= xn0: 0.62 =============
new_qn: 0.6055558511603141
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6527992813703196
============= xn0: 0.621 =============
new_qn: 0.6065325541460566
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6537759843560618
============= xn0: 0.622 =============
new_qn: 0.6075092571317989
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6547526873418044
============= xn0: 0.623 =============
new_qn: 0.6084859601175414
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.655729390327547
============= xn0: 0.624 =============
new_qn: 0.6094626631032839
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.656706093313289
============= xn0: 0.625 =============
new_qn: 0.6104393660890263
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6576827962990317
============= xn0: 0.626 =============
new_qn: 0.6114160690747688
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6586594992847743
============= xn0: 0.627 =============
new_qn: 0.6123927720605112
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6596362022705164
============= xn0: 0.628 =============
new_qn: 0.6133694750462536
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.660612905256259
============= xn0: 0.629 =============
new_qn: 0.6143461780319961
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6615896082420016
============= xn0: 0.63 =============
new_qn: 0.6153228810177385
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6625663112277438
============= xn0: 0.631 =============
new_qn: 0.616299584003481
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6635430142134864
============= xn0: 0.632 =============
new_qn: 0.6172762869892234
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.664519717199229
============= xn0: 0.633 =============
new_qn: 0.6182529899749658
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6654964201849711
============= xn0: 0.634 =============
new_qn: 0.6192296929607083
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6664731231707137
============= xn0: 0.635 =============
new_qn: 0.6202063959464508
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6674498261564563
============= xn0: 0.636 =============
new_qn: 0.6211830989321931
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6684265291421985
============= xn0: 0.637 =============
new_qn: 0.6221598019179356
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.669403232127941
============= xn0: 0.638 =============
new_qn: 0.623136504903678
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6703799351136837
============= xn0: 0.639 =============
new_qn: 0.6241132078894205
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6713566380994258
============= xn0: 0.64 =============
new_qn: 0.625089910875163
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6723333410851684
============= xn0: 0.641 =============
new_qn: 0.6260666138609053
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.673310044070911
============= xn0: 0.642 =============
new_qn: 0.6270433168466478
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6742867470566531
============= xn0: 0.643 =============
new_qn: 0.6280200198323903
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6752634500423957
============= xn0: 0.644 =============
new_qn: 0.6289967228181327
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6762401530281383
============= xn0: 0.645 =============
new_qn: 0.6299734258038752
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6772168560138805
============= xn0: 0.646 =============
new_qn: 0.6309501287896176
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.678193558999623
============= xn0: 0.647 =============
new_qn: 0.63192683177536
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6791702619853657
============= xn0: 0.648 =============
new_qn: 0.6329035347611025
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6801469649711078
============= xn0: 0.649 =============
new_qn: 0.6338802377468449
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6811236679568504
============= xn0: 0.65 =============
new_qn: 0.6348569407325874
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.682100370942593
============= xn0: 0.651 =============
new_qn: 0.6358336437183298
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6830770739283352
============= xn0: 0.652 =============
new_qn: 0.6368103467040722
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6840537769140778
============= xn0: 0.653 =============
new_qn: 0.6377870496898147
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6850304798998204
============= xn0: 0.654 =============
new_qn: 0.6387637526755572
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6860071828855625
============= xn0: 0.655 =============
new_qn: 0.6397404556612996
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.686983885871305
============= xn0: 0.656 =============
new_qn: 0.640717158647042
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6879605888570477
============= xn0: 0.657 =============
new_qn: 0.6416938616327845
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6889372918427898
============= xn0: 0.658 =============
new_qn: 0.6426705646185269
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6899139948285324
============= xn0: 0.659 =============
new_qn: 0.6436472676042694
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.690890697814275
============= xn0: 0.66 =============
new_qn: 0.6446239705900118
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6918674008000172
============= xn0: 0.661 =============
new_qn: 0.6456006735757542
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6928441037857598
============= xn0: 0.662 =============
new_qn: 0.6465773765614967
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6938208067715024
============= xn0: 0.663 =============
new_qn: 0.6475540795472391
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6947975097572445
============= xn0: 0.664 =============
new_qn: 0.6485307825329816
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6957742127429871
============= xn0: 0.665 =============
new_qn: 0.6495074855187241
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6967509157287297
============= xn0: 0.666 =============
new_qn: 0.6504841885044664
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6977276187144719
============= xn0: 0.667 =============
new_qn: 0.6514608914902089
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.6987043217002145
============= xn0: 0.668 =============
new_qn: 0.6524375944759514
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.699681024685957
============= xn0: 0.669 =============
new_qn: 0.6534142974616938
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7006577276716992
============= xn0: 0.67 =============
new_qn: 0.6543910004474363
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7016344306574418
============= xn0: 0.671 =============
new_qn: 0.6553677034331786
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.702611133643184
============= xn0: 0.672 =============
new_qn: 0.6563444064189211
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7035878366289265
============= xn0: 0.673 =============
new_qn: 0.6573211094046636
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7045645396146691
============= xn0: 0.674 =============
new_qn: 0.658297812390406
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7055412426004113
============= xn0: 0.675 =============
new_qn: 0.6592745153761485
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7065179455861539
============= xn0: 0.676 =============
new_qn: 0.6602512183618909
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7074946485718965
============= xn0: 0.677 =============
new_qn: 0.6612279213476333
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7084713515576386
============= xn0: 0.678 =============
new_qn: 0.6622046243333758
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7094480545433812
============= xn0: 0.679 =============
new_qn: 0.6631813273191182
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7104247575291238
============= xn0: 0.68 =============
new_qn: 0.6641580303048606
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.711401460514866
============= xn0: 0.681 =============
new_qn: 0.6651347332906031
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7123781635006086
============= xn0: 0.682 =============
new_qn: 0.6661114362763455
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7133548664863512
============= xn0: 0.683 =============
new_qn: 0.667088139262088
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7143315694720933
============= xn0: 0.684 =============
new_qn: 0.6680648422478305
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.715308272457836
============= xn0: 0.685 =============
new_qn: 0.6690415452335728
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7162849754435785
============= xn0: 0.686 =============
new_qn: 0.6700182482193153
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7172616784293206
============= xn0: 0.687 =============
new_qn: 0.6709949512050578
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7182383814150632
============= xn0: 0.6880000000000001 =============
new_qn: 0.6719716541908002
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7192150844008058
============= xn0: 0.6890000000000001 =============
new_qn: 0.6729483571765427
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.720191787386548
============= xn0: 0.6900000000000001 =============
new_qn: 0.673925060162285
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7211684903722906
============= xn0: 0.6910000000000001 =============
new_qn: 0.6749017631480275
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7221451933580332
============= xn0: 0.6920000000000001 =============
new_qn: 0.67587846613377
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7231218963437753
============= xn0: 0.6930000000000001 =============
new_qn: 0.6768551691195124
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.724098599329518
============= xn0: 0.6940000000000001 =============
new_qn: 0.6778318721052549
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7250753023152605
============= xn0: 0.6950000000000001 =============
new_qn: 0.6788085750909973
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7260520053010027
============= xn0: 0.6960000000000001 =============
new_qn: 0.6797852780767397
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7270287082867453
============= xn0: 0.6970000000000001 =============
new_qn: 0.6807619810624822
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7280054112724879
============= xn0: 0.6980000000000001 =============
new_qn: 0.6817386840482247
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.72898211425823
============= xn0: 0.6990000000000001 =============
new_qn: 0.6827153870339671
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7299588172439726
============= xn0: 0.7000000000000001 =============
new_qn: 0.6836920900197095
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7309355202297152
============= xn0: 0.7010000000000001 =============
new_qn: 0.6846687930054519
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7319122232154573
============= xn0: 0.7020000000000001 =============
new_qn: 0.6856454959911944
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7328889262012
============= xn0: 0.7030000000000001 =============
new_qn: 0.6866221989769369
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7338656291869425
============= xn0: 0.704 =============
new_qn: 0.6875989019626791
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7348423321726847
============= xn0: 0.705 =============
new_qn: 0.6885756049484216
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7358190351584273
============= xn0: 0.706 =============
new_qn: 0.6895523079341641
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7367957381441694
============= xn0: 0.707 =============
new_qn: 0.6905290109199065
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.737772441129912
============= xn0: 0.708 =============
new_qn: 0.691505713905649
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7387491441156546
============= xn0: 0.709 =============
new_qn: 0.6924824168913915
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7397258471013968
============= xn0: 0.71 =============
new_qn: 0.6934591198771338
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7407025500871394
============= xn0: 0.711 =============
new_qn: 0.6944358228628763
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.741679253072882
============= xn0: 0.712 =============
new_qn: 0.6954125258486187
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7426559560586241
============= xn0: 0.713 =============
new_qn: 0.6963892288343612
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7436326590443667
============= xn0: 0.714 =============
new_qn: 0.6973659318201036
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7446093620301093
============= xn0: 0.715 =============
new_qn: 0.698342634805846
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7455860650158515
============= xn0: 0.716 =============
new_qn: 0.6993193377915885
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.746562768001594
============= xn0: 0.717 =============
new_qn: 0.700296040777331
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7475394709873366
============= xn0: 0.718 =============
new_qn: 0.7012727437630734
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7485161739730788
============= xn0: 0.719 =============
new_qn: 0.7022494467488158
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7494928769588214
============= xn0: 0.72 =============
new_qn: 0.7032261497345582
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7504695799445635
============= xn0: 0.721 =============
new_qn: 0.7042028527203007
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7514462829303061
============= xn0: 0.722 =============
new_qn: 0.7051795557060432
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7524229859160487
============= xn0: 0.723 =============
new_qn: 0.7061562586917856
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7533996889017909
============= xn0: 0.724 =============
new_qn: 0.707132961677528
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7543763918875335
============= xn0: 0.725 =============
new_qn: 0.7081096646632705
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.755353094873276
============= xn0: 0.726 =============
new_qn: 0.7090863676490129
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7563297978590182
============= xn0: 0.727 =============
new_qn: 0.7100630706347554
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7573065008447608
============= xn0: 0.728 =============
new_qn: 0.7110397736204979
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7582832038305034
============= xn0: 0.729 =============
new_qn: 0.7120164766062402
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7592599068162456
============= xn0: 0.73 =============
new_qn: 0.7129931795919827
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7602366098019882
============= xn0: 0.731 =============
new_qn: 0.7139698825777251
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7612133127877307
============= xn0: 0.732 =============
new_qn: 0.7149465855634676
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.762190015773473
============= xn0: 0.733 =============
new_qn: 0.7159232885492101
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7631667187592155
============= xn0: 0.734 =============
new_qn: 0.7168999915349524
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.764143421744958
============= xn0: 0.735 =============
new_qn: 0.7178766945206949
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7651201247307002
============= xn0: 0.736 =============
new_qn: 0.7188533975064374
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7660968277164428
============= xn0: 0.737 =============
new_qn: 0.7198301004921798
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7670735307021854
============= xn0: 0.738 =============
new_qn: 0.7208068034779223
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7680502336879276
============= xn0: 0.739 =============
new_qn: 0.7217835064636647
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7690269366736702
============= xn0: 0.74 =============
new_qn: 0.7227602094494071
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7700036396594128
============= xn0: 0.741 =============
new_qn: 0.7237369124351496
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.770980342645155
============= xn0: 0.742 =============
new_qn: 0.724713615420892
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7719570456308975
============= xn0: 0.743 =============
new_qn: 0.7256903184066344
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.77293374861664
============= xn0: 0.744 =============
new_qn: 0.7266670213923769
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7739104516023823
============= xn0: 0.745 =============
new_qn: 0.7276437243781193
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7748871545881248
============= xn0: 0.746 =============
new_qn: 0.7286204273638618
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7758638575738674
============= xn0: 0.747 =============
new_qn: 0.7295971303496043
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7768405605596096
============= xn0: 0.748 =============
new_qn: 0.7305738333353466
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7778172635453522
============= xn0: 0.749 =============
new_qn: 0.7315505363210891
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7787939665310948
============= xn0: 0.75 =============
new_qn: 0.7325272393068316
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.779770669516837
============= xn0: 0.751 =============
new_qn: 0.733503942292574
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7807473725025795
============= xn0: 0.752 =============
new_qn: 0.7344806452783165
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7817240754883221
============= xn0: 0.753 =============
new_qn: 0.7354573482640588
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7827007784740643
============= xn0: 0.754 =============
new_qn: 0.7364340512498013
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7836774814598069
============= xn0: 0.755 =============
new_qn: 0.7374107542355438
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7846541844455495
============= xn0: 0.756 =============
new_qn: 0.7383874572212862
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7856308874312916
============= xn0: 0.757 =============
new_qn: 0.7393641602070287
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7866075904170342
============= xn0: 0.758 =============
new_qn: 0.7403408631927711
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7875842934027768
============= xn0: 0.759 =============
new_qn: 0.7413175661785135
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.788560996388519
============= xn0: 0.76 =============
new_qn: 0.742294269164256
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7895376993742615
============= xn0: 0.761 =============
new_qn: 0.7432709721499984
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7905144023600037
============= xn0: 0.762 =============
new_qn: 0.7442476751357409
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7914911053457463
============= xn0: 0.763 =============
new_qn: 0.7452243781214833
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7924678083314889
============= xn0: 0.764 =============
new_qn: 0.7462010811072257
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.793444511317231
============= xn0: 0.765 =============
new_qn: 0.7471777840929682
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7944212143029736
============= xn0: 0.766 =============
new_qn: 0.7481544870787107
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7953979172887162
============= xn0: 0.767 =============
new_qn: 0.7491311900644531
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7963746202744584
============= xn0: 0.768 =============
new_qn: 0.7501078930501955
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.797351323260201
============= xn0: 0.769 =============
new_qn: 0.751084596035938
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7983280262459436
============= xn0: 0.77 =============
new_qn: 0.7520612990216804
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.7993047292316857
============= xn0: 0.771 =============
new_qn: 0.7530380020074229
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8002814322174283
============= xn0: 0.772 =============
new_qn: 0.7540147049931653
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.801258135203171
============= xn0: 0.773 =============
new_qn: 0.7549914079789077
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.802234838188913
============= xn0: 0.774 =============
new_qn: 0.7559681109646502
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8032115411746557
============= xn0: 0.775 =============
new_qn: 0.7569448139503926
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8041882441603982
============= xn0: 0.776 =============
new_qn: 0.7579215169361351
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8051649471461404
============= xn0: 0.777 =============
new_qn: 0.7588982199218776
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.806141650131883
============= xn0: 0.778 =============
new_qn: 0.7598749229076199
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8071183531176256
============= xn0: 0.779 =============
new_qn: 0.7608516258933624
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8080950561033677
============= xn0: 0.78 =============
new_qn: 0.7618283288791049
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8090717590891103
============= xn0: 0.781 =============
new_qn: 0.7628050318648473
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.810048462074853
============= xn0: 0.782 =============
new_qn: 0.7637817348505898
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.811025165060595
============= xn0: 0.783 =============
new_qn: 0.7647584378363321
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8120018680463377
============= xn0: 0.784 =============
new_qn: 0.7657351408220746
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8129785710320803
============= xn0: 0.785 =============
new_qn: 0.7667118438078171
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8139552740178224
============= xn0: 0.786 =============
new_qn: 0.7676885467935595
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.814931977003565
============= xn0: 0.787 =============
new_qn: 0.768665249779302
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8159086799893076
============= xn0: 0.788 =============
new_qn: 0.7696419527650444
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8168853829750498
============= xn0: 0.789 =============
new_qn: 0.7706186557507868
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8178620859607924
============= xn0: 0.79 =============
new_qn: 0.7715953587365293
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.818838788946535
============= xn0: 0.791 =============
new_qn: 0.7725720617222717
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.819815491932277
============= xn0: 0.792 =============
new_qn: 0.7735487647080141
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8207921949180197
============= xn0: 0.793 =============
new_qn: 0.7745254676937566
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8217688979037623
============= xn0: 0.794 =============
new_qn: 0.775502170679499
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8227456008895044
============= xn0: 0.795 =============
new_qn: 0.7764788736652415
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.823722303875247
============= xn0: 0.796 =============
new_qn: 0.777455576650984
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8246990068609896
============= xn0: 0.797 =============
new_qn: 0.7784322796367263
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8256757098467318
============= xn0: 0.798 =============
new_qn: 0.7794089826224688
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8266524128324744
============= xn0: 0.799 =============
new_qn: 0.7803856856082113
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.827629115818217
============= xn0: 0.8 =============
new_qn: 0.7813623885939537
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8286058188039591
============= xn0: 0.801 =============
new_qn: 0.7823390915796962
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8295825217897017
============= xn0: 0.802 =============
new_qn: 0.7833157945654385
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8305592247754439
============= xn0: 0.803 =============
new_qn: 0.784292497551181
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8315359277611865
============= xn0: 0.804 =============
new_qn: 0.7852692005369235
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.832512630746929
============= xn0: 0.805 =============
new_qn: 0.7862459035226659
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8334893337326712
============= xn0: 0.806 =============
new_qn: 0.7872226065084084
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8344660367184138
============= xn0: 0.807 =============
new_qn: 0.7881993094941508
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8354427397041564
============= xn0: 0.808 =============
new_qn: 0.7891760124798932
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8364194426898985
============= xn0: 0.809 =============
new_qn: 0.7901527154656357
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8373961456756411
============= xn0: 0.81 =============
new_qn: 0.7911294184513782
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8383728486613837
============= xn0: 0.811 =============
new_qn: 0.7921061214371206
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8393495516471259
============= xn0: 0.812 =============
new_qn: 0.793082824422863
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8403262546328685
============= xn0: 0.8130000000000001 =============
new_qn: 0.7940595274086054
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.841302957618611
============= xn0: 0.8140000000000001 =============
new_qn: 0.7950362303943479
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8422796606043532
============= xn0: 0.8150000000000001 =============
new_qn: 0.7960129333800904
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8432563635900958
============= xn0: 0.8160000000000001 =============
new_qn: 0.7969896363658328
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8442330665758384
============= xn0: 0.8170000000000001 =============
new_qn: 0.7979663393515752
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8452097695615806
============= xn0: 0.8180000000000001 =============
new_qn: 0.7989430423373177
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8461864725473232
============= xn0: 0.8190000000000001 =============
new_qn: 0.7999197453230601
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8471631755330657
============= xn0: 0.8200000000000001 =============
new_qn: 0.8008964483088026
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.848139878518808
============= xn0: 0.8210000000000001 =============
new_qn: 0.8018731512945451
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8491165815045505
============= xn0: 0.8220000000000001 =============
new_qn: 0.8028498542802874
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.850093284490293
============= xn0: 0.8230000000000001 =============
new_qn: 0.8038265572660299
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8510699874760352
============= xn0: 0.8240000000000001 =============
new_qn: 0.8048032602517723
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8520466904617778
============= xn0: 0.8250000000000001 =============
new_qn: 0.8057799632375148
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8530233934475204
============= xn0: 0.8260000000000001 =============
new_qn: 0.8067566662232573
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8540000964332626
============= xn0: 0.8270000000000001 =============
new_qn: 0.8077333692089996
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8549767994190052
============= xn0: 0.8280000000000001 =============
new_qn: 0.8087100721947421
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8559535024047478
============= xn0: 0.8290000000000001 =============
new_qn: 0.8096867751804846
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.85693020539049
============= xn0: 0.8300000000000001 =============
new_qn: 0.810663478166227
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8579069083762325
============= xn0: 0.8310000000000001 =============
new_qn: 0.8116401811519695
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.858883611361975
============= xn0: 0.8320000000000001 =============
new_qn: 0.8126168841377118
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8598603143477173
============= xn0: 0.833 =============
new_qn: 0.8135935871234542
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8608370173334599
============= xn0: 0.834 =============
new_qn: 0.8145702901091967
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.861813720319202
============= xn0: 0.835 =============
new_qn: 0.815546993094939
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8627904233049446
============= xn0: 0.836 =============
new_qn: 0.8165236960806815
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8637671262906872
============= xn0: 0.837 =============
new_qn: 0.817500399066424
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8647438292764293
============= xn0: 0.838 =============
new_qn: 0.8184771020521664
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.865720532262172
============= xn0: 0.839 =============
new_qn: 0.8194538050379089
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8666972352479145
============= xn0: 0.84 =============
new_qn: 0.8204305080236514
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8676739382336567
============= xn0: 0.841 =============
new_qn: 0.8214072110093937
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8686506412193993
============= xn0: 0.842 =============
new_qn: 0.8223839139951362
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8696273442051419
============= xn0: 0.843 =============
new_qn: 0.8233606169808786
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.870604047190884
============= xn0: 0.844 =============
new_qn: 0.8243373199666211
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8715807501766266
============= xn0: 0.845 =============
new_qn: 0.8253140229523636
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8725574531623692
============= xn0: 0.846 =============
new_qn: 0.8262907259381059
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8735341561481114
============= xn0: 0.847 =============
new_qn: 0.8272674289238484
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.874510859133854
============= xn0: 0.848 =============
new_qn: 0.8282441319095909
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8754875621195966
============= xn0: 0.849 =============
new_qn: 0.8292208348953333
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8764642651053387
============= xn0: 0.85 =============
new_qn: 0.8301975378810758
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8774409680910813
============= xn0: 0.851 =============
new_qn: 0.8311742408668182
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.878417671076824
============= xn0: 0.852 =============
new_qn: 0.8321509438525606
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.879394374062566
============= xn0: 0.853 =============
new_qn: 0.8331276468383031
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8803710770483086
============= xn0: 0.854 =============
new_qn: 0.8341043498240455
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8813477800340508
============= xn0: 0.855 =============
new_qn: 0.835081052809788
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8823244830197934
============= xn0: 0.856 =============
new_qn: 0.8360577557955304
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.883301186005536
============= xn0: 0.857 =============
new_qn: 0.8370344587812728
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8842778889912781
============= xn0: 0.858 =============
new_qn: 0.8380111617670153
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8852545919770207
============= xn0: 0.859 =============
new_qn: 0.8389878647527578
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8862312949627633
============= xn0: 0.86 =============
new_qn: 0.8399645677385001
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8872079979485055
============= xn0: 0.861 =============
new_qn: 0.8409412707242426
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.888184700934248
============= xn0: 0.862 =============
new_qn: 0.8419179737099851
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8891614039199907
============= xn0: 0.863 =============
new_qn: 0.8428946766957275
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8901381069057328
============= xn0: 0.864 =============
new_qn: 0.84387137968147
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8911148098914754
============= xn0: 0.865 =============
new_qn: 0.8448480826672123
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.892091512877218
============= xn0: 0.866 =============
new_qn: 0.8458247856529548
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8930682158629601
============= xn0: 0.867 =============
new_qn: 0.8468014886386973
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8940449188487027
============= xn0: 0.868 =============
new_qn: 0.8477781916244397
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8950216218344453
============= xn0: 0.869 =============
new_qn: 0.8487548946101822
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8959983248201875
============= xn0: 0.87 =============
new_qn: 0.8497315975959246
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.89697502780593
============= xn0: 0.871 =============
new_qn: 0.850708300581667
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8979517307916727
============= xn0: 0.872 =============
new_qn: 0.8516850035674095
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8989284337774148
============= xn0: 0.873 =============
new_qn: 0.8526617065531519
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.8999051367631574
============= xn0: 0.874 =============
new_qn: 0.8536384095388944
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9008818397489
============= xn0: 0.875 =============
new_qn: 0.8546151125246368
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9018585427346422
============= xn0: 0.876 =============
new_qn: 0.8555918155103792
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9028352457203848
============= xn0: 0.877 =============
new_qn: 0.8565685184961217
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9038119487061274
============= xn0: 0.878 =============
new_qn: 0.8575452214818642
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9047886516918695
============= xn0: 0.879 =============
new_qn: 0.8585219244676066
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.905765354677612
============= xn0: 0.88 =============
new_qn: 0.859498627453349
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9067420576633547
============= xn0: 0.881 =============
new_qn: 0.8604753304390915
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9077187606490968
============= xn0: 0.882 =============
new_qn: 0.8614520334248339
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9086954636348394
============= xn0: 0.883 =============
new_qn: 0.8624287364105764
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.909672166620582
============= xn0: 0.884 =============
new_qn: 0.8634054393963188
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9106488696063242
============= xn0: 0.885 =============
new_qn: 0.8643821423820612
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9116255725920668
============= xn0: 0.886 =============
new_qn: 0.8653588453678037
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9126022755778094
============= xn0: 0.887 =============
new_qn: 0.8663355483535461
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9135789785635515
============= xn0: 0.888 =============
new_qn: 0.8673122513392886
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9145556815492941
============= xn0: 0.889 =============
new_qn: 0.8682889543250311
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9155323845350367
============= xn0: 0.89 =============
new_qn: 0.8692656573107734
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9165090875207789
============= xn0: 0.891 =============
new_qn: 0.8702423602965159
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9174857905065215
============= xn0: 0.892 =============
new_qn: 0.8712190632822584
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.918462493492264
============= xn0: 0.893 =============
new_qn: 0.8721957662680008
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9194391964780062
============= xn0: 0.894 =============
new_qn: 0.8731724692537433
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9204158994637488
============= xn0: 0.895 =============
new_qn: 0.8741491722394856
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.921392602449491
============= xn0: 0.896 =============
new_qn: 0.8751258752252281
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9223693054352335
============= xn0: 0.897 =============
new_qn: 0.8761025782109706
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9233460084209761
============= xn0: 0.898 =============
new_qn: 0.877079281196713
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9243227114067183
============= xn0: 0.899 =============
new_qn: 0.8780559841824555
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9252994143924609
============= xn0: 0.9 =============
new_qn: 0.8790326871681979
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9262761173782035
============= xn0: 0.901 =============
new_qn: 0.8800093901539403
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9272528203639456
============= xn0: 0.902 =============
new_qn: 0.8809860931396828
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9282295233496882
============= xn0: 0.903 =============
new_qn: 0.8819627961254252
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9292062263354308
============= xn0: 0.904 =============
new_qn: 0.8829394991111676
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.930182929321173
============= xn0: 0.905 =============
new_qn: 0.8839162020969101
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9311596323069156
============= xn0: 0.906 =============
new_qn: 0.8848929050826525
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9321363352926582
============= xn0: 0.907 =============
new_qn: 0.885869608068395
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9331130382784003
============= xn0: 0.908 =============
new_qn: 0.8868463110541375
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.934089741264143
============= xn0: 0.909 =============
new_qn: 0.8878230140398798
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9350664442498855
============= xn0: 0.91 =============
new_qn: 0.8887997170256223
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9360431472356276
============= xn0: 0.911 =============
new_qn: 0.8897764200113648
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9370198502213702
============= xn0: 0.912 =============
new_qn: 0.8907531229971072
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9379965532071128
============= xn0: 0.913 =============
new_qn: 0.8917298259828497
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.938973256192855
============= xn0: 0.914 =============
new_qn: 0.892706528968592
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9399499591785976
============= xn0: 0.915 =============
new_qn: 0.8936832319543345
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9409266621643402
============= xn0: 0.916 =============
new_qn: 0.894659934940077
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9419033651500823
============= xn0: 0.917 =============
new_qn: 0.8956366379258194
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.942880068135825
============= xn0: 0.918 =============
new_qn: 0.8966133409115619
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9438567711215675
============= xn0: 0.919 =============
new_qn: 0.8975900438973043
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9448334741073097
============= xn0: 0.92 =============
new_qn: 0.8985667468830467
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9458101770930523
============= xn0: 0.921 =============
new_qn: 0.8995434498687892
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9467868800787949
============= xn0: 0.922 =============
new_qn: 0.9005201528545317
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.947763583064537
============= xn0: 0.923 =============
new_qn: 0.9014968558402741
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9487402860502796
============= xn0: 0.924 =============
new_qn: 0.9024735588260165
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9497169890360222
============= xn0: 0.925 =============
new_qn: 0.9034502618117589
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9506936920217643
============= xn0: 0.926 =============
new_qn: 0.9044269647975014
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.951670395007507
============= xn0: 0.927 =============
new_qn: 0.9054036677832439
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9526470979932495
============= xn0: 0.928 =============
new_qn: 0.9063803707689863
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9536238009789917
============= xn0: 0.929 =============
new_qn: 0.9073570737547287
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9546005039647343
============= xn0: 0.93 =============
new_qn: 0.9083337767404712
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9555772069504769
============= xn0: 0.931 =============
new_qn: 0.9093104797262136
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.956553909936219
============= xn0: 0.932 =============
new_qn: 0.9102871827119561
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9575306129219616
============= xn0: 0.933 =============
new_qn: 0.9112638856976986
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9585073159077042
============= xn0: 0.934 =============
new_qn: 0.9122405886834409
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9594840188934464
============= xn0: 0.935 =============
new_qn: 0.9132172916691834
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.960460721879189
============= xn0: 0.936 =============
new_qn: 0.9141939946549258
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9614374248649311
============= xn0: 0.937 =============
new_qn: 0.9151706976406683
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9624141278506737
============= xn0: 0.9380000000000001 =============
new_qn: 0.9161474006264108
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9633908308364163
============= xn0: 0.9390000000000001 =============
new_qn: 0.9171241036121531
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9643675338221585
============= xn0: 0.9400000000000001 =============
new_qn: 0.9181008065978956
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.965344236807901
============= xn0: 0.9410000000000001 =============
new_qn: 0.9190775095836381
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9663209397936436
============= xn0: 0.9420000000000001 =============
new_qn: 0.9200542125693805
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9672976427793858
============= xn0: 0.9430000000000001 =============
new_qn: 0.921030915555123
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9682743457651284
============= xn0: 0.9440000000000001 =============
new_qn: 0.9220076185408653
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.969251048750871
============= xn0: 0.9450000000000001 =============
new_qn: 0.9229843215266078
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9702277517366131
============= xn0: 0.9460000000000001 =============
new_qn: 0.9239610245123503
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9712044547223557
============= xn0: 0.9470000000000001 =============
new_qn: 0.9249377274980927
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9721811577080983
============= xn0: 0.9480000000000001 =============
new_qn: 0.9259144304838351
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9731578606938405
============= xn0: 0.9490000000000001 =============
new_qn: 0.9268911334695776
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.974134563679583
============= xn0: 0.9500000000000001 =============
new_qn: 0.92786783645532
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9751112666653257
============= xn0: 0.9510000000000001 =============
new_qn: 0.9288445394410625
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9760879696510678
============= xn0: 0.9520000000000001 =============
new_qn: 0.929821242426805
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9770646726368104
============= xn0: 0.9530000000000001 =============
new_qn: 0.9307979454125473
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.978041375622553
============= xn0: 0.9540000000000001 =============
new_qn: 0.9317746483982898
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9790180786082952
============= xn0: 0.9550000000000001 =============
new_qn: 0.9327513513840322
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9799947815940377
============= xn0: 0.9560000000000001 =============
new_qn: 0.9337280543697747
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9809714845797803
============= xn0: 0.9570000000000001 =============
new_qn: 0.9347047573555172
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9819481875655225
============= xn0: 0.9580000000000001 =============
new_qn: 0.9356814603412595
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.982924890551265
============= xn0: 0.9590000000000001 =============
new_qn: 0.936658163327002
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9839015935370077
============= xn0: 0.96 =============
new_qn: 0.9376348663127444
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9848782965227498
============= xn0: 0.961 =============
new_qn: 0.9386115692984868
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9858549995084924
============= xn0: 0.962 =============
new_qn: 0.9395882722842293
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9868317024942346
============= xn0: 0.963 =============
new_qn: 0.9405649752699717
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9878084054799772
============= xn0: 0.964 =============
new_qn: 0.9415416782557141
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9887851084657198
============= xn0: 0.965 =============
new_qn: 0.9425183812414566
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.989761811451462
============= xn0: 0.966 =============
new_qn: 0.943495084227199
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9907385144372045
============= xn0: 0.967 =============
new_qn: 0.9444717872129414
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.991715217422947
============= xn0: 0.968 =============
new_qn: 0.9454484901986839
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9926919204086893
============= xn0: 0.969 =============
new_qn: 0.9464251931844263
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9936686233944318
============= xn0: 0.97 =============
new_qn: 0.9474018961701688
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9946453263801744
============= xn0: 0.971 =============
new_qn: 0.9483785991559113
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9956220293659166
============= xn0: 0.972 =============
new_qn: 0.9493553021416536
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9965987323516592
============= xn0: 0.973 =============
new_qn: 0.9503320051273961
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9975754353374018
============= xn0: 0.974 =============
new_qn: 0.9513087081131386
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.998552138323144
============= xn0: 0.975 =============
new_qn: 0.952285411098881
best_Eta: 1.3508086611311048
sum(new_qn_list): 1.9995288413088865
============= xn0: 0.976 =============
new_qn: 0.9532621140846235
best_Eta: 1.3508086611311048
sum(new_qn_list): 2.0005055442946293
============= xn0: 0.977 =============
new_qn: 0.9542388170703658
best_Eta: 1.3508086611311048
sum(new_qn_list): 2.0014822472803715
============= xn0: 0.978 =============
new_qn: 0.9552155200561083
best_Eta: 1.3508086611311048
sum(new_qn_list): 2.002458950266114
============= xn0: 0.979 =============
new_qn: 0.9561922230418508
best_Eta: 1.3508086611311048
sum(new_qn_list): 2.0034356532518567
============= xn0: 0.98 =============
new_qn: 0.9571689260275932
best_Eta: 1.3508086611311048
sum(new_qn_list): 2.004412356237599
============= xn0: 0.981 =============
new_qn: 0.9581456290133357
best_Eta: 1.3508086611311048
sum(new_qn_list): 2.0053890592233414
============= xn0: 0.982 =============
new_qn: 0.9591223319990781
best_Eta: 1.3508086611311048
sum(new_qn_list): 2.006365762209084
============= xn0: 0.983 =============
new_qn: 0.9600990349848205
best_Eta: 1.3508086611311048
sum(new_qn_list): 2.007342465194826
============= xn0: 0.984 =============
new_qn: 0.961075737970563
best_Eta: 1.3508086611311048
sum(new_qn_list): 2.0083191681805688
============= xn0: 0.985 =============
new_qn: 0.9620524409563054
best_Eta: 1.3508086611311048
sum(new_qn_list): 2.009295871166311
============= xn0: 0.986 =============
new_qn: 0.9630291439420479
best_Eta: 1.3508086611311048
sum(new_qn_list): 2.0102725741520535
============= xn0: 0.987 =============
new_qn: 0.9640058469277903
best_Eta: 1.3508086611311048
sum(new_qn_list): 2.011249277137796
============= xn0: 0.988 =============
new_qn: 0.9649825499135327
best_Eta: 1.3508086611311048
sum(new_qn_list): 2.0122259801235383
============= xn0: 0.989 =============
new_qn: 0.9659592528992752
best_Eta: 1.3508086611311048
sum(new_qn_list): 2.013202683109281
============= xn0: 0.99 =============
new_qn: 0.9669359558850177
best_Eta: 1.3508086611311048
sum(new_qn_list): 2.0141793860950234
============= xn0: 0.991 =============
new_qn: 0.9679126588707601
best_Eta: 1.3508086611311048
sum(new_qn_list): 2.0151560890807656
============= xn0: 0.992 =============
new_qn: 0.9688893618565025
best_Eta: 1.3508086611311048
sum(new_qn_list): 2.016132792066508
============= xn0: 0.993 =============
new_qn: 0.969866064842245
best_Eta: 1.3508086611311048
sum(new_qn_list): 2.017109495052251
============= xn0: 0.994 =============
new_qn: 0.9708427678279874
best_Eta: 1.3508086611311048
sum(new_qn_list): 2.018086198037993
============= xn0: 0.995 =============
new_qn: 0.9718194708137299
best_Eta: 1.3508086611311048
sum(new_qn_list): 2.0190629010237355
============= xn0: 0.996 =============
new_qn: 0.9727961737994723
best_Eta: 1.3508086611311048
sum(new_qn_list): 2.020039604009478
============= xn0: 0.997 =============
new_qn: 0.9737728767852147
best_Eta: 1.3508086611311048
sum(new_qn_list): 2.0210163069952203
============= xn0: 0.998 =============
new_qn: 0.9747495797709572
best_Eta: 1.3508086611311048
sum(new_qn_list): 2.021993009980963
============= xn0: 0.999 =============
new_qn: 0.9757262827566996
best_Eta: 1.3508086611311048
sum(new_qn_list): 2.0229697129667055
============= xn0: 1.0 =============
new_qn: 0.9767029857424421
best_Eta: 1.3508086611311048
sum(new_qn_list): 2.0239464159524476
DONE
最终的列表：
[0.0, 0.000258646739088271, 0.0005149501161196435, 0.000768916669352698, 0.0010205529127452597, 0.0012698653360671839, 0.0015168604050125323, 0.001761544561311115, 0.0020039242228394056, 0.0022440057837308444, 0.002481795614485558, 0.0027173000620794203, 0.002950525450072544, 0.003181478078717187, 0.003410164225065, 0.0036365901430737597, 0.0038607620637134583, 0.004082686195071836, 0.0043023687224593085, 0.004519815808513367, 0.004735033593302341, 0.004948028194428605, 0.005158805707131276, 0.0053673722043883106, 0.005573733737017989, 0.005777896333779946, 0.005979866001475591, 0.006179648725047952, 0.006377250467681058, 0.006572677170898721, 0.006765934754662778, 0.0069570291174708004, 0.007145966136453373, 0.007332751667470612, 0.007517391545208439, 0.007699891583274138, 0.007880257574291442, 0.008058495289995186, 0.008234610481325275, 0.0084086088785204, 0.008580496191210929, 0.008750278108511596, 0.008917960299113549, 0.009083548411375833, 0.009247048073416572, 0.009408464893203482, 0.009567804458644018, 0.009725072337674917, 0.009880274078351452, 0.010033415208935997, 0.010184501237986206, 0.010333537654442837, 0.010480529927716899, 0.01062548350777643, 0.010768403825232843, 0.01090929629142686, 0.011048166298513788, 0.011185019219548516, 0.011319860408570075, 0.011452695200685571, 0.011583528912153931, 0.011712366840468919, 0.011839214264441955, 0.011964076444284391, 0.01208695862168932, 0.012207866019913013, 0.012326803843855913, 0.012443777280143273, 0.01255879149720511, 0.012671851645356086, 0.01278296285687476, 0.012892130246082462, 0.012999358909421749, 0.013104653925534476, 0.013208020355339509, 0.013309463242109826, 0.013408987611549447, 0.013506598471869982, 0.013602300813866375, 0.013696099610992815, 0.013787999819437916, 0.013878006378199553, 0.013966124209159228, 0.014052358217156419, 0.014136713290062025, 0.01421919429885174, 0.014299806097679044, 0.014378553523947832, 0.014455441398384367, 0.014530474525109277, 0.014603657691708988, 0.014674995669306704, 0.014744493212633117, 0.014812155060096907, 0.014877985933854476, 0.01494199053987974, 0.015004173568033394, 0.015064539692131715, 0.015123093570015123, 0.015179839843616524, 0.015234783139029012, 0.015287928066573406, 0.015339279220865462, 0.015388841180882684, 0.015436618510030711, 0.015482615756209558, 0.015526837451879341, 0.015569288114125862, 0.0156099722447256, 0.015648894330210578, 0.015686058841932846, 0.015721470236128618, 0.01575513295398205, 0.01578705142168889, 0.015817230050519424, 0.015845673236881477, 0.015872385362382987, 0.01589737079389418, 0.015920633883609356, 0.015942178969108836, 0.015962010373419766, 0.01598013240507748, 0.01599654935818612, 0.01601126551247886, 0.016024285133378002, 0.016035612472054955, 0.016045251765489377, 0.01605320723652859, 0.016059483093946447, 0.016064083532501577, 0.01606701273299624, 0.01606827486233392, 0.016067874073576988, 0.016065814506004472, 0.016062100285168912, 0.016056735522953114, 0.016049724317627273, 0.016041070753904485, 0.016030778902997367, 0.01601885282267354, 0.01600529655731109, 0.015990114137953787, 0.015973309582365952, 0.0159548868950872, 0.015934850067486628, 0.015913203077817184, 0.01588994989126913, 0.015865094460023987, 0.015838640723307595, 0.015810592607443275, 0.01578095402590446, 0.01574972887936754, 0.015716921055763694, 0.01568253443033127, 0.015646572865667396, 0.015609040211779363, 0.01556994030613612, 0.015529276973719114, 0.015487054027073066, 0.015443275266356499, 0.015397944479392106, 0.015351065441716383, 0.015302641916629972, 0.015252677655246683, 0.015201176396543087, 0.0151481418674074, 0.015093577782688467, 0.015037487845244085, 0.014979875745989663, 0.014920745163946036, 0.014860099766287521, 0.014797943208389536, 0.014734279133875927, 0.01466911117466621, 0.014602442951022643, 0.014534278071596768, 0.01446462013347588, 0.014393472722229739, 0.014320839411956027, 0.01424672376532668, 0.014171129333633237, 0.014094059656832414, 0.01401551826359107, 0.01393550867133142, 0.013854034386275693, 0.01377109890349057, 0.013686705706931784, 0.013600858269487842, 0.013513560053024343, 0.013424814508427446, 0.01333462507564731, 0.013242995183741557, 0.013149928250918486, 0.01305542768457929, 0.012959496881361526, 0.012862139227181157, 0.012763358097274746, 0.01266315685624192, 0.012561538858086918, 0.01245850744626023, 0.012354065953700338, 0.01224821770287482, 0.012140966005821374, 0.012032314164189012, 0.011922265469278326, 0.011810823202082377, 0.011697990633326882, 0.011583771023510242, 0.011468167622943504, 0.011351183671790499, 0.011232822400106784, 0.011113087027879415, 0.010991980765066273, 0.010869506811634622, 0.010745668357600352, 0.010620468583066534, 0.010493910658261663, 0.010365997743578353, 0.010236732989611141, 0.010106119537194536, 0.009974160517440767, 0.00984085905177745, 0.009706218251984866, 0.009570241220233339, 0.009432931049120186, 0.009294290821706686, 0.009154323611554743, 0.009013032482763722, 0.00887042049000622, 0.00872649067856493, 0.008581246084368582, 0.00843468973402725, 0.008286824644868879, 0.008137653824974417, 0.007987180273213074, 0.007835406979278065, 0.007682336923721139, 0.007527973077987776, 0.0073723184044519885, 0.007215375856450629, 0.00705714837831814, 0.006897638905420667, 0.0067368503641901945, 0.006574785672158412, 0.006411447737990905, 0.006246839461520326, 0.006080963733780087, 0.005913823437037891, 0.005745421444828702, 0.00557576062198778, 0.005404843824683703, 0.00523267390045129, 0.005059253688223575, 0.004884586018365, 0.004708673712703282, 0.004531519584561661, 0.0043531264387909885, 0.004173497071801424, 0.0039926342715944085, 0.003810540817794139, 0.003627219481678934, 0.003442673026212484, 0.0032569042060753284, 0.003069915767695386, 0.0028817104492794288, 0.0026922909808433926, 0.002501660084242796, 0.002309820473203994, 0.002116774853353709, 0.0019225259222494517, 0.0017270763694099411, 0.0015304288763447471, 0.0013325861165836006, 0.0011335507557069802, 0.0009333254513753109, 0.0007319128533577746, 0.0005293156035622304, 0.0003255363360644692, 0.00012057767713624656, -8.555775472451899e-05, -0.000292867348765935, -0.0005013485019535469, -0.0007109986189409723, -0.0009218151120425344, -0.001133795401204507, -0.0013469369139769705, -0.0015612370854857227, -0.0017766933584054123, -0.001993303182930173, -0.002211064016747033, -0.0024299733250084943, -0.002650028580304664, -0.002871227262636278, -0.00309356685938722, -0.0033170448652980467, -0.003541658782438839, -0.003767406120182115, -0.003994284395176462, -0.004222291131320277, -0.004451423859734682, -0.00468168011873743, -0.0049130574538172045, -0.005145553417606863, -0.00537916556985768, -0.005613891477413591, -0.0058497287141854315, -0.006086674861125296, -0.006324727506200611, -0.0065638842443696, -0.006804142677555136, -0.007045500414619765, -0.007287955071341057, -0.007531504270385847, -0.007776145641286258, -0.00802187682041422, -0.008268695450957098, -0.008516599182893214, -0.008765585672967424, -0.0090156525846663, -0.009266797588194764, -0.009519018360451159, -0.00977231258500355, -0.010026677952066076, -0.010282112158474521, -0.010538612907663114, -0.01079617790964088, -0.011054804880967717, -0.011314491544731187, -0.011575235630523817, -0.01183703487441884, -0.012099887018947653, -0.012363789813076953, -0.012628741012185585, -0.012894738378041504, -0.01316177967877974, -0.01342986268887908, -0.013698985189139534, -0.013969144966660962, -0.014240339814819203, -0.014512567533244813, -0.014785825927801088, -0.015060112810561188, -0.0153354259997866, -0.015611763319905714, -0.015889122601491334, -0.016167501681239094, -0.01644689840194663, -0.016727310612491164, -0.017008736167808347, -0.0172911729288715, -0.017574618762669636, -0.017859071542186578, -0.018144529146380317, -0.018430989460161307, -0.01871845037437242, -0.019006909785767467, -0.019296365596990772, -0.019586815716556683, -0.01987825805882898, -0.02017069054400028, -0.020464111098072002, -0.020758517652833597, -0.02105390814584296, -0.021350280520406162, -0.021647632725556865, -0.02194596271603727, -0.022245268452277867, -0.022545547900377494, -0.02284679903208381, -0.023149019824773964, -0.02345220826143468, -0.023756362330642933, -0.02406148002654679, -0.02436755934884599, -0.024674598302772466, -0.024982594899071953, -0.02529154715398435, -0.025601453089224624, -0.025912310731964705, -0.02622411811481401, -0.026536873275800843, -0.026850574258354076, -0.027165219111284056, -0.02748080588876467, -0.027797332650314532, -0.028114797460778662, -0.028433198390310666, -0.028752533514353973, -0.029072800913623742, -0.029393998674089705, -0.029716124886956907, -0.030039177648648663, -0.030363155060788738, -0.03068805523018303, -0.03101387626880231, -0.031340616293765056, -0.03166827342731876, -0.03199684579682349, -0.03232633153473413, -0.032656728778583166, -0.032988035670963145, -0.03332025035951003, -0.03365337099688542, -0.03398739574076021, -0.03432232275379726, -0.03465815020363433, -0.03499487626286768, -0.035332499109035076, -0.03567101692459895, -0.03601042789692993, -0.03635073021829072, -0.03669192208581873, -0.03703400170151, -0.03737696727220369, -0.0377208170095642, -0.03806554913006627, -0.03841116185497828, -0.03875765341034598, -0.03910502202697724, -0.039453265940424975, -0.03980238339097181, -0.04015237262361493, -0.040503231888048996, -0.04085495943865097, -0.04120755353446515, -0.041561012439186584, -0.04191533442114587, -0.0422705177532941, -0.042626560713187045, -0.04298346158296967, -0.04334121864936147, -0.043699830203640766, -0.044059294541629435, -0.04441960996367844, -0.044780774774652254, -0.0451427872839138, -0.04550564580531036, -0.04586934865715764, -0.046233894162225564, -0.04659928064772367, -0.046965506445286076, -0.047332569890956855, -0.04770046932517602, -0.048069203092764134, -0.048438769542908655, -0.04880916702914928, -0.04918039390936324, -0.04955244854575158, -0.049925329304824906, -0.050299034557388544, -0.050673562678529294, -0.05104891204760098, -0.051425081048210086, -0.05180206806820281, -0.05217987149964992, -0.05255848973883398, -0.05293792118623508, -0.0533181642465172, -0.05369921732851435, -0.05408107884521729, -0.054463747213759905, -0.05484722085540511, -0.05523149819553269, -0.05561657766362438, -0.056002457693251295, -0.05638913672206086, -0.05677661319176319, -0.05716488554811755, -0.05755395224092019, -0.05794381172399016, -0.058334462455157055, -0.05872590289624785, -0.05911813151307349, -0.05951114677541641, -0.05990494715701816, -0.06029953113556552, -0.06069489719267834, -0.061091043813897294, -0.06148796948867041, -0.06188567271034079, -0.06228415197613435, -0.06268340578714693, -0.06308343264833227, -0.06348423106848894, -0.06388579956024837, -0.06428813664006316, -0.06469124082819366, -0.06509511064869616, -0.06549974462941166, -0.06590514130195224, -0.06631129920168988, -0.06671821686774448, -0.06712589284297166, -0.06753432567395051, -0.06794351391097253, -0.06835345610802873, -0.06876415082279885, -0.06917559661663902, -0.06958779205457005, -0.07000073570526594, -0.07041442614104254, -0.07082886193784521, -0.07124404167523785, -0.07165996393639162, -0.0720766273080729, -0.07249403038063207, -0.07291217174799258, -0.07333105000763923, -0.07375066376060663, -0.074171011611469, -0.07459209216832763, -0.07501390404280084, -0.07543644585001247, -0.07585971620858073, -0.07628371374060716, -0.07670843707166602, -0.07713388483079292, -0.07756005565047425, -0.0779869481666362, -0.07841456101863359, -0.07884289284924012, -0.07927194230463669, -0.07970170803440052, -0.08013218869149591, -0.08056338293226212, -0.08099528941640338, -0.08142790680697887, -0.08186123377039106, -0.08229526897637651, -0.08273001109799472, -0.0831654588116178, -0.08360161079692036, -0.0840384657368694, -0.08447602231771328, -0.08491427922897243, -0.08535323516342869, -0.08579288881711505, -0.08623323888930584, -0.08667428408250655, -0.08711602310244387, -0.08755845465805567, -0.0880015774614808, -0.08844539022804959, -0.08888989167627381, -0.08933508052783662, -0.08978095550758303, -0.09022751534351009, -0.09067475876675701, -0.09112268451159555, -0.09157129131542058, -0.09202057791873991, -0.09247054306516522, -0.09292118550140244, -0.09337250397724178, -0.09382449724554881, -0.09427716406225489, -0.09473050318634735, -0.09518451337986034, -0.09563919340786609, -0.09609454203846451, -0.09655055804277451, -0.09700724019492513, -0.09746458727204566, -0.0979225980542563, -0.09838127132466012, -0.09884060586933308, -0.0993006004773146, -0.09976125394059993, -0.10022256505412958, -0.10068453261578114, -0.10114715542636094, -0.10161043228959332, -0.10207436201211356, -0.10253894340345854, -0.1030041752760571, -0.10347005644522211, -0.10393658572914205, -0.10440376194887069, -0.10487158392832074, -0.10534005049425277, -0.10580916047626826, -0.10627891270680084, -0.10674930602110655, -0.1072203392572566, -0.10769201125612882, -0.10816432086139793, -0.10863726691952841, -0.10911084827976603, -0.10958506379412825, -0.11005991231739726, -0.1105353927071111, -0.11101150382355535, -0.1114882445297547, -0.1119656136914653, -0.11244361017716598, -0.11292223285805025, -0.11340148060801852, -0.11388135230366953, -0.11436184682429223, -0.11484296305185837, -0.11532469987101379, -0.11580705616907067, -0.1162900308359997, -0.11677362276442199, -0.11725783084960117, -0.11774265398943567, -0.11822809108445059, -0.11871414103779016, -0.11920080275520983, -0.11968807514506818, -0.12017595711832002, -0.12066444758850797, -0.12115354547175466, -0.12164324968675577, -0.12213355915477198, -0.12262447279962102, -0.12311598954767095, -0.12360810832783198, -0.12410082807154882, -0.12459414771279403, -0.1250880661880595, -0.12558258243634968, -0.12607769539917418, -0.1265734040205399, -0.12706970724694394, -0.12756660402736691, -0.12806409331326407, -0.12856217405855946, -0.1290608452196385, -0.1295601057553396, -0.13005995462694836, -0.1305603907981896, -0.13106141323522036, -0.13156302090662286, -0.13206521278339733, -0.13256798783895507, -0.1330713450491109, -0.1335752833920767, -0.13407980184845436, -0.1345848994012282, -0.13509057503575894, -0.13559682773977588, -0.1361036565033703, -0.13661106031898906, -0.13711903818142723, -0.1376275890878207, -0.1381367120376411, -0.13864640603268719, -0.13915667007707855, -0.1396675031772503, -0.1401789043419438, -0.14069087258220225, -0.14120340691136302, -0.14171650634505073, -0.1422301699011711, -0.14274439659990468, -0.1432591854636993, -0.14377453551726405, -0.14429044578756334, -0.14480691530380907, -0.1453239430974551, -0.14584152820219076, -0.146359669653934, -0.14687836649082509, -0.14739761775322036, -0.14791742248368578, -0.14843777972699046, -0.14895868853010064, -0.1494801479421729, -0.1500021570145481, -0.15052471480074558, -0.15104782035645603, -0.15157147273953586, -0.15209567101000088, -0.15262041423002004, -0.15314570146390927, -0.15367153177812554, -0.15419790424126034, -0.15472481792403414, -0.1552522718992897, -0.15578026524198618, -0.15630879702919387, -0.15683786634008712, -0.15736747225593828, -0.15789761386011325, -0.15842829023806382, -0.1589595004773221, -0.15949124366749579, -0.1600235189002609, -0.16055632526935604, -0.16108966187057772, -0.16162352780177314, -0.16215792216283476, -0.16269284405569528, -0.16322829258432103, -0.16376426685470602, -0.16430076597486754, -0.1648377890548387, -0.1653753352066638, -0.16591340354439288, -0.1664519931840751, -0.16699110324375388, -0.16753073284346076, -0.1680708811052105, -0.16861154715299453, -0.16915273011277632, -0.1696944291124851, -0.17023664328201127, -0.17077937175319957, -0.17132261365984447, -0.17186636813768508, -0.1724106343243984, -0.17295541135959513, -0.17350069838481375, -0.17404649454351495, -0.1745927989810765, -0.1751396108447879, -0.1756869292838451, -0.1762347534493447, -0.17678308249427943, -0.177331915573532, -0.1778812518438706, -0.178431090463943, -0.17898143059427196, -0.17953227139724937, -0.18008361203713152, -0.1806354516800336, -0.18118778949392478, -0.18174062464862306, -0.1822939563157897, -0.18284778366892496, -0.18340210588336203, -0.18395692213626236, -0.18451223160661112, -0.18506803347521128, -0.185624326924679, -0.1861811111394388, -0.1867383853057183, -0.18729614861154276, -0.1878544002467316, -0.18841313940289184, -0.18897236527341377, -0.18953207705346675, -0.19009227393999273, -0.19065295513170288, -0.19121411982907244, -0.19177576723433465, -0.19233789655147737, -0.19290050698623817, -0.19346359774609811, -0.19402716804027842, -0.1945912170797356, -0.19515574407715564, -0.19572074824695018, -0.19628622880525182, -0.19685218496990864, -0.19741861596048038, -0.1979855209982334, -0.19855289930613573, -0.19912075010885266, -0.19968907263274271, -0.20025786610585183, -0.20082712975790973, -0.20139686282032465, -0.20196706452617974, -0.20253773411022724, -0.20310887080888507, -0.20368047386023125, -0.2042525425040006, -0.20482507598157906, -0.20539807353600004, -0.20597153441193972, -0.20654545785571232, -0.20711984311526566, -0.20769468944017755, -0.2082699960816503, -0.20884576229250673, -0.20942198732718653, -0.2099986704417406, -0.21057581089382704, -0.2111534079427081, -0.2117314608492441, -0.2123099688758896, -0.2128889312866904, -0.21346834734727715, -0.21404821632486282, -0.21462853748823751, -0.21520931010776445, -0.2157905334553757, -0.2163722068045687, -0.21695432943040038, -0.2175369006094846, -0.21811991961998756, -0.21870338574162274, -0.21928729825564797, -0.21987165644486084, -0.22045645959359417, -0.22104170698771242, -0.22162739791460773, -0.2222135316631949, -0.22280010752390844, -0.22338712478869804, -0.22397458275102433, -0.22456248070585494, -0.22515081794966108, -0.22573959378041242, -0.22632880749757434, -0.22691845840210267, -0.2275085457964411, -0.22809906898451582, -0.22869002727173293, -0.2292814199649733, -0.22987324637258977, -0.23046550580440217, -0.231058197571694, -0.23165132098720898, -0.2322448753651466, -0.2328388600211576, -0.23343327427234195, -0.2340281174372435, -0.23462338883584632, -0.2352190877895718, -0.23581521362127433, -0.23641176565523647, -0.23700874321716747, -0.23760614563419746, -0.23820397223487444, -0.23880222234916104, -0.23940089530842978, -0.23999999044546017, -0.24059950709443512, -0.24119944459093634, -0.2417998022719411, -0.24240057947581928, -0.24300177554232882, -0.24360338981261243, -0.24420542162919368, -0.24480787033597373, -0.245410735278228, -0.24601401580260152, -0.24661771125710652, -0.24722182099111822, -0.24782634435537132, -0.24843128070195641, -0.24903662938431692, -0.24964238975724473, -0.25024856117687777, -0.25085514300069534, -0.25146213458751543, -0.2520695352974909, -0.2526773444921062, -0.2532855615341736, -0.2538941857878301, -0.2545032166185336, -0.25511265339306, -0.25572249547949943, -0.25633274224725267, -0.25694339306702774, -0.25755444731083765, -0.25816590435199505, -0.2587777635651106, -0.25939002432608893, -0.26000268601212484, -0.26061574800170084, -0.2612292096745834, -0.26184307041181953, -0.2624573295957333, -0.2630719866099238, -0.26368704083925987, -0.2643024916698783, -0.2649183384891802, -0.2655345806858278, -0.26615121764974026, -0.2667682487720926, -0.2673856734453097, -0.26800349106306565, -0.26862170102027905, -0.26924030271311006, -0.26985929553895704, -0.2704786788964547, -0.2710984521854689, -0.2717186148070948, -0.27233916616365383, -0.27296010565868967, -0.27358143269696567, -0.274203146684462, -0.27482524702837174, -0.2754477331370987, -0.2760706044202538, -0.27669386028865184, -0.27731750015430867, -0.27794152343043854, -0.27856592953145054, -0.279190717872945, -0.2798158878717123, -0.2804414389457275, -0.2810673705141493, -0.2816936819973159, -0.2823203728167425, -0.28294744239511793, -0.28357489015630233, -0.28420271552532306, -0.28483091792837323, -0.28545949679280735, -0.28608845154713936, -0.2867177816210391, -0.28734748644533015, -0.28797756545198516, -0.28860801807412584, -0.28923884374601727, -0.2898700419030662, -0.2905016119818187, -0.2911335534199563, -0.2917658656562935, -0.29239854813077526, -0.2930316002844735, -0.2936650215595846, -0.29429881139942726, -0.29493296924843804, -0.29556749455217013, -0.29620238675729016, -0.2968376453115744, -0.2974732696639073, -0.29810925926427845, -0.2987456135637788, -0.2993823320145992, -0.30001941407002675, -0.30065685918444274, -0.3012946668133193, -0.3019328364132172, -0.3025713674417827, -0.3032102593577449, -0.30384951162091356, -0.3044891236921756, -0.30512909503349306, -0.3057694251079002, -0.3064101133795005, -0.3070511593134648, -0.3076925623760277, -0.3083343220344855, -0.30897643775719397, -0.30961890901356437, -0.310261735274062, -0.3109049160102032, -0.31154845069455306, -0.31219233880072184, -0.312836579803364, -0.3134811731781739, -0.3141261184018844, -0.3147714149522637, -0.3154170623081134, -0.31606305994926476, -0.3167094073565778, -0.3173561040119375, -0.31800314939825136, -0.31865054299944773, -0.31929828430047236, -0.3199463727872862, -0.3205948079468637, -0.32124358926718855, -0.3218927162372527, -0.32254218834705406, -0.32319200508759205, -0.3238421659508679, -0.3244926704298796, -0.32514351801862107, -0.3257947082120797, -0.3264462405062326, -0.3270981143980455, -0.32775032938547044, -0.32840288496744163, -0.3290557806438751, -0.32970901591566515, -0.3303625902846823, -0.3310165032537702, -0.3316707543267452, -0.33232534300839145, -0.33298026880445997, -0.33363553122166667, -0.3342911297676887, -0.3349470639511629, -0.3356033332816839, -0.33625993726980086, -0.3369168754270152, -0.3375741472657794, -0.3382317522994932, -0.3388896900425026, -0.33954796001009624, -0.3402065617185047, -0.3408654946848968, -0.34152475842737806, -0.342184352464988, -0.34284427631769876, -0.3435045295064115, -0.34416511155295537, -0.3448260219800844, -0.34548726031147603, -0.3461488260717279, -0.34681071878635705, -0.347472937981796, -0.34813548318539167]
**** log-parameter_analysis 运行时间： 2025-02-10 18:47:59 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.038874811478408114
DataOwner2: noise random: 0.051458755245256976
DataOwner3: noise random: 0.03699899855103233
DataOwner4: noise random: 0.03595627525128105
DataOwner5: noise random: 0.02590285127223021
DataOwner6: noise random: 0.020666690840403103
DataOwner7: noise random: 0.03751042411214474
DataOwner8: noise random: 0.031544285346762235
DataOwner9: noise random: 0.04880764671107989
DataOwner10: noise random: 0.09106067132167066
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9993869507521783, 0.9989329530622298, 0.9994460764731271, 0.999478012821188, 0.9997293870126162, 0.9998272903926713, 0.9994303309440331, 0.9995975626322727, 0.9990359615807548, 0.9966435147136081]
归一化后的数据质量列表avg_f_list: [0.986169263010938, 0.9719095369588165, 0.9880263574456247, 0.989029454123286, 0.9969249284521227, 1.0, 0.9875318022168269, 0.9927844237925036, 0.9751449570671601, 0.9]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3638
DataOwner1的最优x_1 = 0.1309
DataOwner2的最优x_2 = 0.1166
DataOwner3的最优x_3 = 0.1327
DataOwner4的最优x_4 = 0.1336
DataOwner5的最优x_5 = 0.1411
DataOwner6的最优x_6 = 0.1439
DataOwner7的最优x_7 = 0.1322
DataOwner8的最优x_8 = 0.1372
DataOwner9的最优x_9 = 0.1199
DataOwner10的最优x_10 = 0.0294
每个DataOwner应该贡献数据比例 xn_list = [0.13087282269648248, 0.1166211548302469, 0.13266479683575474, 0.13362673991745394, 0.1410542794085665, 0.14387922107335654, 0.13218899114871793, 0.13719080074056214, 0.11993264775669457, 0.029439328608638217]
ModelOwner的最大效用 U(Eta) = 0.6077
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3638078754300729
DataOwner1的分配到的支付 ： 0.1466
DataOwner2的分配到的支付 ： 0.1288
DataOwner3的分配到的支付 ： 0.1489
DataOwner4的分配到的支付 ： 0.1502
DataOwner5的分配到的支付 ： 0.1598
DataOwner6的分配到的支付 ： 0.1635
DataOwner7的分配到的支付 ： 0.1483
DataOwner8的分配到的支付 ： 0.1548
DataOwner9的分配到的支付 ： 0.1329
DataOwner10的分配到的支付 ： 0.0301
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC8', 'DataOwner2': 'CPC9', 'DataOwner5': 'CPC10', 'DataOwner4': 'CPC2', 'DataOwner3': 'CPC4', 'DataOwner7': 'CPC6', 'DataOwner9': 'CPC7', 'DataOwner6': 'CPC5', 'DataOwner8': 'CPC3', 'DataOwner10': 'CPC1'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC8
DataOwner2 把数据交给 CPC9
DataOwner5 把数据交给 CPC10
DataOwner4 把数据交给 CPC2
DataOwner3 把数据交给 CPC4
DataOwner7 把数据交给 CPC6
DataOwner9 把数据交给 CPC7
DataOwner6 把数据交给 CPC5
DataOwner8 把数据交给 CPC3
DataOwner10 把数据交给 CPC1
DONE
最终Um的列表：
[('DataOwner1', 'CPC8', 0.8, 513.798111274726, 515, 0.026174564539296494), ('DataOwner2', 'CPC9', 0.9, 326.2272339653968, 344, 0.01166211548302469), ('DataOwner5', 'CPC10', 1.0, 138.65635665862087, 138, 0.0), ('DataOwner4', 'CPC2', 0.2, 1639.2233751199135, 394, 0.10690139193396316), ('DataOwner3', 'CPC4', 0.4, 1264.0816205049878, 1826, 0.07959887810145284), ('DataOwner7', 'CPC6', 0.6, 888.9398658894411, 910, 0.05287559645948718), ('DataOwner9', 'CPC7', 0.7, 701.3689885822145, 589, 0.035979794327008374), ('DataOwner6', 'CPC5', 0.5, 1076.5107431971444, 1273, 0.07193961053667827), ('DataOwner8', 'CPC3', 0.3, 1451.6524978118475, 404, 0.0960335605183935), ('DataOwner10', 'CPC1', 0.1, 1826.794252428343, 347, 0.026495395747774396)]
('DataOwner1', 'CPC8', 0.8, 513.798111274726, 515, 0.026174564539296494)
('DataOwner2', 'CPC9', 0.9, 326.2272339653968, 344, 0.01166211548302469)
('DataOwner5', 'CPC10', 1.0, 138.65635665862087, 138, 0.0)
('DataOwner4', 'CPC2', 0.2, 1639.2233751199135, 394, 0.10690139193396316)
('DataOwner3', 'CPC4', 0.4, 1264.0816205049878, 1826, 0.07959887810145284)
('DataOwner7', 'CPC6', 0.6, 888.9398658894411, 910, 0.05287559645948718)
('DataOwner9', 'CPC7', 0.7, 701.3689885822145, 589, 0.035979794327008374)
('DataOwner6', 'CPC5', 0.5, 1076.5107431971444, 1273, 0.07193961053667827)
('DataOwner8', 'CPC3', 0.3, 1451.6524978118475, 404, 0.0960335605183935)
('DataOwner10', 'CPC1', 0.1, 1826.794252428343, 347, 0.026495395747774396)
**** log-parameter_analysis 运行时间： 2025-02-10 18:50:26 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
**** log-parameter_analysis 运行时间： 2025-02-10 21:57:17 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
**** log-parameter_analysis 运行时间： 2025-02-10 22:01:32 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
**** log-parameter_analysis 运行时间： 2025-02-10 22:04:10 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.03214300985982782
DataOwner2: noise random: 0.0996408070601267
DataOwner3: noise random: 0.017954849631530934
DataOwner4: noise random: 0.001782774797326303
DataOwner5: noise random: 0.037850809456505846
DataOwner6: noise random: 0.020348657000185757
DataOwner7: noise random: 0.09668293833312307
DataOwner8: noise random: 0.05160953431665415
DataOwner9: noise random: 0.0743973188284039
DataOwner10: noise random: 0.05513182805670679
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9995821244568306, 0.9959739884427624, 0.999869457227804, 0.9999987147755363, 0.9994212496789494, 0.9998322220289936, 0.9962187854488557, 0.9989218497246706, 0.9977549569532547, 0.9987692446242107]
归一化后的数据质量列表avg_f_list: [0.9896492262017091, 0.9, 0.9967884139927792, 1.0, 0.9856520655358731, 0.9958632529822734, 0.9060823267435563, 0.9732437695925649, 0.9442506735424371, 0.9694520807212693]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3483
DataOwner1的最优x_1 = 0.1451
DataOwner2的最优x_2 = 0.0458
DataOwner3的最优x_3 = 0.1514
DataOwner4的最优x_4 = 0.1542
DataOwner5的最优x_5 = 0.1414
DataOwner6的最优x_6 = 0.1506
DataOwner7的最优x_7 = 0.0539
DataOwner8的最优x_8 = 0.1297
DataOwner9的最优x_9 = 0.0997
DataOwner10的最优x_10 = 0.1260
每个DataOwner应该贡献数据比例 xn_list = [0.14506551831154293, 0.04583142450397306, 0.15140695143241126, 0.15419588190422642, 0.14142768208755868, 0.15059627196679062, 0.05389157123270837, 0.12972051067812873, 0.09973963748818405, 0.12601403078060092]
ModelOwner的最大效用 U(Eta) = 0.5893
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3483099368022515
DataOwner1的分配到的支付 ： 0.1653
DataOwner2的分配到的支付 ： 0.0475
DataOwner3的分配到的支付 ： 0.1738
DataOwner4的分配到的支付 ： 0.1776
DataOwner5的分配到的支付 ： 0.1605
DataOwner6的分配到的支付 ： 0.1727
DataOwner7的分配到的支付 ： 0.0562
DataOwner8的分配到的支付 ： 0.1454
DataOwner9的分配到的支付 ： 0.1085
DataOwner10的分配到的支付 ： 0.1407
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC4', 'DataOwner2': 'CPC9', 'DataOwner7': 'CPC1', 'DataOwner9': 'CPC10', 'DataOwner3': 'CPC6', 'DataOwner4': 'CPC5', 'DataOwner5': 'CPC7', 'DataOwner10': 'CPC8', 'DataOwner6': 'CPC3', 'DataOwner8': 'CPC2'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC4
DataOwner2 把数据交给 CPC9
DataOwner7 把数据交给 CPC1
DataOwner9 把数据交给 CPC10
DataOwner3 把数据交给 CPC6
DataOwner4 把数据交给 CPC5
DataOwner5 把数据交给 CPC7
DataOwner10 把数据交给 CPC8
DataOwner6 把数据交给 CPC3
DataOwner8 把数据交给 CPC2
DONE
最终Um的列表：
[('DataOwner1', 'CPC4', 0.4, 1064.2683970659027, 1451, 0.08703931098692576), ('DataOwner2', 'CPC9', 0.9, 315.8499295570433, 229, 0.004583142450397315), ('DataOwner7', 'CPC1', 0.1, 1513.3194775710497, 179, 0.048502414109437536), ('DataOwner9', 'CPC10', 1.0, 166.16623605531464, 166, 1.3877787807814457e-17), ('DataOwner3', 'CPC6', 0.6, 764.9010100624546, 1009, 0.06056278057296448), ('DataOwner4', 'CPC5', 0.5, 914.5847035641502, 513, 0.07709794095211324), ('DataOwner5', 'CPC7', 0.7, 615.2173165606578, 707, 0.04242830462626761), ('DataOwner10', 'CPC8', 0.8, 465.53362305949213, 420, 0.02520280615612018), ('DataOwner6', 'CPC3', 0.3, 1213.952090567215, 1505, 0.10541739037675343), ('DataOwner8', 'CPC2', 0.2, 1363.635784068844, 1513, 0.10377640854250299)]
('DataOwner1', 'CPC4', 0.4, 1064.2683970659027, 1451, 0.08703931098692576)
('DataOwner2', 'CPC9', 0.9, 315.8499295570433, 229, 0.004583142450397315)
('DataOwner7', 'CPC1', 0.1, 1513.3194775710497, 179, 0.048502414109437536)
('DataOwner9', 'CPC10', 1.0, 166.16623605531464, 166, 1.3877787807814457e-17)
('DataOwner3', 'CPC6', 0.6, 764.9010100624546, 1009, 0.06056278057296448)
('DataOwner4', 'CPC5', 0.5, 914.5847035641502, 513, 0.07709794095211324)
('DataOwner5', 'CPC7', 0.7, 615.2173165606578, 707, 0.04242830462626761)
('DataOwner10', 'CPC8', 0.8, 465.53362305949213, 420, 0.02520280615612018)
('DataOwner6', 'CPC3', 0.3, 1213.952090567215, 1505, 0.10541739037675343)
('DataOwner8', 'CPC2', 0.2, 1363.635784068844, 1513, 0.10377640854250299)
**** log-parameter_analysis 运行时间： 2025-02-10 22:04:38 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
**** log-parameter_analysis 运行时间： 2025-02-10 22:05:07 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.06441805047689118
DataOwner2: noise random: 0.08022591460798684
DataOwner3: noise random: 0.04310057537341813
DataOwner4: noise random: 0.0967558869437257
DataOwner5: noise random: 0.08996524863526455
DataOwner6: noise random: 0.074340171540813
DataOwner7: noise random: 0.057681448835857366
DataOwner8: noise random: 0.08720439417799021
DataOwner9: noise random: 0.020351608568098856
DataOwner10: noise random: 0.069973585785196
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9983285590676614, 0.9973935808677655, 0.9992499591592993, 0.9962042318993491, 0.9967395341303296, 0.9977651922049068, 0.9986491333282361, 0.996918989439314, 0.9998325113515341, 0.9980167187002841]
归一化后的数据质量列表avg_f_list: [0.9585491607332778, 0.9327799714462504, 0.9839441200736622, 0.9, 0.9147536108515033, 0.9430220529076845, 0.9673845954014013, 0.9196996275888959, 1.0, 0.9499544432787136]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3255
DataOwner1的最优x_1 = 0.1315
DataOwner2的最优x_2 = 0.1055
DataOwner3的最优x_3 = 0.1544
DataOwner4的最优x_4 = 0.0676
DataOwner5的最优x_5 = 0.0854
DataOwner6的最优x_6 = 0.1162
DataOwner7的最优x_7 = 0.1398
DataOwner8的最优x_8 = 0.0911
DataOwner9的最优x_9 = 0.1676
DataOwner10的最优x_10 = 0.1232
每个DataOwner应该贡献数据比例 xn_list = [0.1315436129152857, 0.10548468342461487, 0.1544462803869056, 0.06763298704352104, 0.08536883414930946, 0.11620466442058681, 0.13980762857620313, 0.0910528005653523, 0.16764808051359623, 0.1231852017023597]
ModelOwner的最大效用 U(Eta) = 0.5632
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3255480154355044
DataOwner1的分配到的支付 ： 0.1481
DataOwner2的分配到的支付 ： 0.1156
DataOwner3的分配到的支付 ： 0.1785
DataOwner4的分配到的支付 ： 0.0715
DataOwner5的分配到的支付 ： 0.0917
DataOwner6的分配到的支付 ： 0.1287
DataOwner7的分配到的支付 ： 0.1588
DataOwner8的分配到的支付 ： 0.0983
DataOwner9的分配到的支付 ： 0.1969
DataOwner10的分配到的支付 ： 0.1374
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC1', 'DataOwner5': 'CPC10', 'DataOwner2': 'CPC2', 'DataOwner8': 'CPC9', 'DataOwner3': 'CPC6', 'DataOwner4': 'CPC8', 'DataOwner10': 'CPC7', 'DataOwner6': 'CPC5', 'DataOwner7': 'CPC3', 'DataOwner9': 'CPC4'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner5 把数据交给 CPC10
DataOwner2 把数据交给 CPC2
DataOwner8 把数据交给 CPC9
DataOwner3 把数据交给 CPC6
DataOwner4 把数据交给 CPC8
DataOwner10 把数据交给 CPC7
DataOwner6 把数据交给 CPC5
DataOwner7 把数据交给 CPC3
DataOwner9 把数据交给 CPC4
DONE
最终Um的列表：
[('DataOwner1', 'CPC1', 0.1, 2573.062739722675, 184, 0.11838925162375714), ('DataOwner5', 'CPC10', 1.0, 119.08952363828669, 119, 0.0), ('DataOwner2', 'CPC2', 0.2, 2300.3990490455985, 294, 0.0843877467396919), ('DataOwner8', 'CPC9', 0.9, 391.75321431326176, 381, 0.009105280056535228), ('DataOwner3', 'CPC6', 0.6, 1209.744286342411, 1077, 0.06177851215476225), ('DataOwner4', 'CPC8', 0.8, 664.4169049902517, 660, 0.013526597408704204), ('DataOwner10', 'CPC7', 0.7, 937.0805956670389, 1031, 0.03695556051070792), ('DataOwner6', 'CPC5', 0.5, 1482.407977018282, 648, 0.05810233221029341), ('DataOwner7', 'CPC3', 0.3, 2027.7353583694053, 585, 0.09786534000334218), ('DataOwner9', 'CPC4', 0.4, 1755.0716676946004, 2573, 0.10058884830815773)]
('DataOwner1', 'CPC1', 0.1, 2573.062739722675, 184, 0.11838925162375714)
('DataOwner5', 'CPC10', 1.0, 119.08952363828669, 119, 0.0)
('DataOwner2', 'CPC2', 0.2, 2300.3990490455985, 294, 0.0843877467396919)
('DataOwner8', 'CPC9', 0.9, 391.75321431326176, 381, 0.009105280056535228)
('DataOwner3', 'CPC6', 0.6, 1209.744286342411, 1077, 0.06177851215476225)
('DataOwner4', 'CPC8', 0.8, 664.4169049902517, 660, 0.013526597408704204)
('DataOwner10', 'CPC7', 0.7, 937.0805956670389, 1031, 0.03695556051070792)
('DataOwner6', 'CPC5', 0.5, 1482.407977018282, 648, 0.05810233221029341)
('DataOwner7', 'CPC3', 0.3, 2027.7353583694053, 585, 0.09786534000334218)
('DataOwner9', 'CPC4', 0.4, 1755.0716676946004, 2573, 0.10058884830815773)
**** log-parameter_analysis 运行时间： 2025-02-10 22:06:24 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.06809316390184027
DataOwner2: noise random: 0.06368444535067523
DataOwner3: noise random: 0.0352704879682304
DataOwner4: noise random: 0.0012684099249348436
DataOwner5: noise random: 0.04711536872461423
DataOwner6: noise random: 0.005149476430493238
DataOwner7: noise random: 0.07867683428158406
DataOwner8: noise random: 0.025261005266280556
DataOwner9: noise random: 0.00208962024506586
DataOwner10: noise random: 0.06468573719644914
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9981222897142952, 0.9983589032604295, 0.9994960007822745, 0.9999993487175345, 0.9991073424374927, 0.9999892859688941, 0.9975038505948072, 0.999741368000141, 0.9999982327830409, 0.9983094403251596]
归一化后的数据质量列表avg_f_list: [0.924782191333092, 0.9342638072068705, 0.9798297610134099, 1.0, 0.9642553816443286, 0.9995967639266604, 0.9, 0.9896621554212381, 0.9999552820944474, 0.9322817205517254]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3440
DataOwner1的最优x_1 = 0.0812
DataOwner2的最优x_2 = 0.0922
DataOwner3的最优x_3 = 0.1390
DataOwner4的最优x_4 = 0.1569
DataOwner5的最优x_5 = 0.1240
DataOwner6的最优x_6 = 0.1566
DataOwner7的最优x_7 = 0.0502
DataOwner8的最优x_8 = 0.1479
DataOwner9的最优x_9 = 0.1569
DataOwner10的最优x_10 = 0.0899
每个DataOwner应该贡献数据比例 xn_list = [0.08119958550448028, 0.09218925815129289, 0.13899416627614739, 0.1568941436981883, 0.12404583796490184, 0.15655155843826143, 0.050161934245047474, 0.14791754692197975, 0.15685618154574651, 0.08993030827046015]
ModelOwner的最大效用 U(Eta) = 0.5843
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.343999303618168
DataOwner1的分配到的支付 ： 0.0868
DataOwner2的分配到的支付 ： 0.0996
DataOwner3的分配到的支付 ： 0.1574
DataOwner4的分配到的支付 ： 0.1814
DataOwner5的分配到的支付 ： 0.1383
DataOwner6的分配到的支付 ： 0.1809
DataOwner7的分配到的支付 ： 0.0522
DataOwner8的分配到的支付 ： 0.1692
DataOwner9的分配到的支付 ： 0.1813
DataOwner10的分配到的支付 ： 0.0969
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC1', 'DataOwner7': 'CPC10', 'DataOwner2': 'CPC6', 'DataOwner4': 'CPC7', 'DataOwner5': 'CPC9', 'DataOwner3': 'CPC4', 'DataOwner8': 'CPC8', 'DataOwner6': 'CPC5', 'DataOwner9': 'CPC3', 'DataOwner10': 'CPC2'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner7 把数据交给 CPC10
DataOwner2 把数据交给 CPC6
DataOwner4 把数据交给 CPC7
DataOwner5 把数据交给 CPC9
DataOwner3 把数据交给 CPC4
DataOwner8 把数据交给 CPC8
DataOwner6 把数据交给 CPC5
DataOwner9 把数据交给 CPC3
DataOwner10 把数据交给 CPC2
DONE
最终Um的列表：
[('DataOwner1', 'CPC1', 0.1, 1969.7999278514847, 113, 0.07307962695403225), ('DataOwner7', 'CPC10', 1.0, 69.97589827184123, 69, 0.0), ('DataOwner2', 'CPC6', 0.6, 914.3421336404913, 900, 0.03687570326051716), ('DataOwner4', 'CPC7', 0.7, 703.250574798413, 656, 0.04706824310945652), ('DataOwner5', 'CPC9', 0.9, 281.0674571161125, 346, 0.012404583796490162), ('DataOwner3', 'CPC4', 0.4, 1336.5252513245994, 1163, 0.08339649976568843), ('DataOwner8', 'CPC8', 0.8, 492.15901595636205, 619, 0.02958350938439594), ('DataOwner6', 'CPC5', 0.5, 1125.4336924823185, 1092, 0.07827577921913072), ('DataOwner9', 'CPC3', 0.3, 1547.616810165808, 1969, 0.10979932708202256), ('DataOwner10', 'CPC2', 0.2, 1758.708369008425, 752, 0.07194424661636813)]
('DataOwner1', 'CPC1', 0.1, 1969.7999278514847, 113, 0.07307962695403225)
('DataOwner7', 'CPC10', 1.0, 69.97589827184123, 69, 0.0)
('DataOwner2', 'CPC6', 0.6, 914.3421336404913, 900, 0.03687570326051716)
('DataOwner4', 'CPC7', 0.7, 703.250574798413, 656, 0.04706824310945652)
('DataOwner5', 'CPC9', 0.9, 281.0674571161125, 346, 0.012404583796490162)
('DataOwner3', 'CPC4', 0.4, 1336.5252513245994, 1163, 0.08339649976568843)
('DataOwner8', 'CPC8', 0.8, 492.15901595636205, 619, 0.02958350938439594)
('DataOwner6', 'CPC5', 0.5, 1125.4336924823185, 1092, 0.07827577921913072)
('DataOwner9', 'CPC3', 0.3, 1547.616810165808, 1969, 0.10979932708202256)
('DataOwner10', 'CPC2', 0.2, 1758.708369008425, 752, 0.07194424661636813)
**** log-parameter_analysis 运行时间： 2025-02-10 22:06:57 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
**** log-parameter_analysis 运行时间： 2025-02-10 22:07:46 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
**** log-parameter_analysis 运行时间： 2025-02-10 22:09:07 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
**** log-parameter_analysis 运行时间： 2025-02-10 22:09:55 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
**** log-parameter_analysis 运行时间： 2025-02-10 22:10:19 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
**** log-parameter_analysis 运行时间： 2025-02-10 22:11:08 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
**** log-parameter_analysis 运行时间： 2025-02-10 22:12:16 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from ../../../data/model/initial/mnist_cnn_initial_model
Model saved to ../../../data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.0787122439291883
DataOwner2: noise random: 0.003173389782050995
DataOwner3: noise random: 0.09757090392900056
DataOwner4: noise random: 0.057948784073346894
DataOwner5: noise random: 0.0017879146960766202
DataOwner6: noise random: 0.09900856495259658
DataOwner7: noise random: 0.09327027361030052
DataOwner8: noise random: 0.042945077347161124
DataOwner9: noise random: 0.06853044131225954
DataOwner10: noise random: 0.05638311138683588
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9974923765969953, 0.9999959246736501, 0.9961349636147636, 0.9986430939913169, 0.9999987097876963, 0.9960242710712289, 0.9964794575752768, 0.9992534317641947, 0.9980957608114369, 0.9987137808376221]
归一化后的数据质量列表avg_f_list: [0.9369386882148548, 0.9999299243429082, 0.9027851113435477, 0.9658916417364124, 1.0, 0.9, 0.911452849987644, 0.9812482195180485, 0.9521203090042658, 0.9676701783134717]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3309
DataOwner1的最优x_1 = 0.1058
DataOwner2的最优x_2 = 0.1646
DataOwner3的最优x_3 = 0.0662
DataOwner4的最优x_4 = 0.1348
DataOwner5的最优x_5 = 0.1646
DataOwner6的最优x_6 = 0.0627
DataOwner7的最优x_7 = 0.0768
DataOwner8的最优x_8 = 0.1488
DataOwner9的最优x_9 = 0.1215
DataOwner10的最优x_10 = 0.1365
每个DataOwner应该贡献数据比例 xn_list = [0.10575247057631382, 0.1645731623881295, 0.0661967684730249, 0.13483929725497648, 0.16462987844741175, 0.06269251078154726, 0.07682234968939426, 0.1488097436711154, 0.1214730687696797, 0.1365065002775077]
ModelOwner的最大效用 U(Eta) = 0.5693
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.330949823585275
DataOwner1的分配到的支付 ： 0.1158
DataOwner2的分配到的支付 ： 0.1924
DataOwner3的分配到的支付 ： 0.0699
DataOwner4的分配到的支付 ： 0.1523
DataOwner5的分配到的支付 ： 0.1925
DataOwner6的分配到的支付 ： 0.0660
DataOwner7的分配到的支付 ： 0.0819
DataOwner8的分配到的支付 ： 0.1707
DataOwner9的分配到的支付 ： 0.1352
DataOwner10的分配到的支付 ： 0.1544
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC5', 'DataOwner3': 'CPC1', 'DataOwner6': 'CPC10', 'DataOwner2': 'CPC4', 'DataOwner5': 'CPC6', 'DataOwner8': 'CPC9', 'DataOwner7': 'CPC2', 'DataOwner9': 'CPC8', 'DataOwner4': 'CPC3', 'DataOwner10': 'CPC7'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC5
DataOwner3 把数据交给 CPC1
DataOwner6 把数据交给 CPC10
DataOwner2 把数据交给 CPC4
DataOwner5 把数据交给 CPC6
DataOwner8 把数据交给 CPC9
DataOwner7 把数据交给 CPC2
DataOwner9 把数据交给 CPC8
DataOwner4 把数据交给 CPC3
DataOwner10 把数据交给 CPC7
DONE
最终Um的列表：
[('DataOwner1', 'CPC5', 0.5, 1051.2045044146146, 1360, 0.05287623528815691), ('DataOwner3', 'CPC1', 0.1, 1849.1861225547475, 113, 0.05957709162572241), ('DataOwner6', 'CPC10', 1.0, 53.72748173978601, 53, 0.0), ('DataOwner2', 'CPC4', 0.4, 1250.6999089495941, 1410, 0.09874389743287772), ('DataOwner5', 'CPC6', 0.6, 851.7090998799763, 705, 0.0658519513789647), ('DataOwner8', 'CPC9', 0.9, 253.22288627745533, 255, 0.014880974367111499), ('DataOwner7', 'CPC2', 0.2, 1649.690718019046, 526, 0.06145787975151541), ('DataOwner9', 'CPC8', 0.8, 452.7182908116122, 520, 0.024294613753935934), ('DataOwner4', 'CPC3', 0.3, 1450.195313483885, 1849, 0.09438750807848353), ('DataOwner10', 'CPC7', 0.7, 652.2136953453582, 701, 0.04095195008325231)]
('DataOwner1', 'CPC5', 0.5, 1051.2045044146146, 1360, 0.05287623528815691)
('DataOwner3', 'CPC1', 0.1, 1849.1861225547475, 113, 0.05957709162572241)
('DataOwner6', 'CPC10', 1.0, 53.72748173978601, 53, 0.0)
('DataOwner2', 'CPC4', 0.4, 1250.6999089495941, 1410, 0.09874389743287772)
('DataOwner5', 'CPC6', 0.6, 851.7090998799763, 705, 0.0658519513789647)
('DataOwner8', 'CPC9', 0.9, 253.22288627745533, 255, 0.014880974367111499)
('DataOwner7', 'CPC2', 0.2, 1649.690718019046, 526, 0.06145787975151541)
('DataOwner9', 'CPC8', 0.8, 452.7182908116122, 520, 0.024294613753935934)
('DataOwner4', 'CPC3', 0.3, 1450.195313483885, 1849, 0.09438750807848353)
('DataOwner10', 'CPC7', 0.7, 652.2136953453582, 701, 0.04095195008325231)
**** log-parameter_analysis 运行时间： 2025-02-10 22:12:27 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
**** log-parameter_analysis 运行时间： 2025-02-10 22:13:37 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Epoch 1/5 started...
Epoch [1/5], Batch [1/1], Loss: 5.9886
Epoch 1 completed. Average Loss: 5.9886
Epoch 2/5 started...
Epoch [2/5], Batch [1/1], Loss: 43.8707
Epoch 2 completed. Average Loss: 43.8707
Epoch 3/5 started...
Epoch [3/5], Batch [1/1], Loss: 38.3645
Epoch 3 completed. Average Loss: 38.3645
Epoch 4/5 started...
Epoch [4/5], Batch [1/1], Loss: 17.4969
Epoch 4 completed. Average Loss: 17.4969
Epoch 5/5 started...
Epoch [5/5], Batch [1/1], Loss: 7.7702
Epoch 5 completed. Average Loss: 7.7702
**** log-parameter_analysis 运行时间： 2025-02-10 22:14:41 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/initial/mnist_cnn_initial_model
Model saved to D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.0387614121255461
DataOwner2: noise random: 0.07888489965989229
DataOwner3: noise random: 0.03501894385015455
DataOwner4: noise random: 0.013622698523819532
DataOwner5: noise random: 0.04502683575858294
DataOwner6: noise random: 0.04615457661187893
DataOwner7: noise random: 0.03699725501330119
DataOwner8: noise random: 0.07814587465032832
DataOwner9: noise random: 0.0460005026638412
DataOwner10: noise random: 0.09484688935378123
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9993904988027155, 0.9974817051924405, 0.999503469223037, 0.999924642691819, 0.9991789811823775, 0.9991365928094157, 0.9994460176651082, 0.9975295630563817, 0.9991489102007642, 0.9963540239603487]
归一化后的数据质量列表avg_f_list: [0.98504057897877, 0.9315822359344277, 0.9882044681760639, 1.0, 0.9791167423486155, 0.9779295987147074, 0.9865954597030382, 0.9329225600502297, 0.9782745638951112, 0.9]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3485
DataOwner1的最优x_1 = 0.1407
DataOwner2的最优x_2 = 0.0852
DataOwner3的最优x_3 = 0.1436
DataOwner4的最优x_4 = 0.1541
DataOwner5的最优x_5 = 0.1352
DataOwner6的最优x_6 = 0.1341
DataOwner7的最优x_7 = 0.1421
DataOwner8的最优x_8 = 0.0868
DataOwner9的最优x_9 = 0.1344
DataOwner10的最优x_10 = 0.0456
每个DataOwner应该贡献数据比例 xn_list = [0.14071902563985983, 0.08519232911210202, 0.14361370532782555, 0.1540597705420014, 0.1351902602486148, 0.13406492428102784, 0.14214660997378903, 0.0867512436369459, 0.13439253309548355, 0.04561348053547588]
ModelOwner的最大效用 U(Eta) = 0.5896
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3485243787249463
DataOwner1的分配到的支付 ： 0.1596
DataOwner2的分配到的支付 ： 0.0914
DataOwner3的分配到的支付 ： 0.1634
DataOwner4的分配到的支付 ： 0.1774
DataOwner5的分配到的支付 ： 0.1524
DataOwner6的分配到的支付 ： 0.1510
DataOwner7的分配到的支付 ： 0.1615
DataOwner8的分配到的支付 ： 0.0932
DataOwner9的分配到的支付 ： 0.1514
DataOwner10的分配到的支付 ： 0.0473
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC7', 'DataOwner4': 'CPC10', 'DataOwner2': 'CPC5', 'DataOwner3': 'CPC6', 'DataOwner5': 'CPC2', 'DataOwner6': 'CPC8', 'DataOwner8': 'CPC9', 'DataOwner7': 'CPC3', 'DataOwner10': 'CPC4', 'DataOwner9': 'CPC1'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC7
DataOwner4 把数据交给 CPC10
DataOwner2 把数据交给 CPC5
DataOwner3 把数据交给 CPC6
DataOwner5 把数据交给 CPC2
DataOwner6 把数据交给 CPC8
DataOwner8 把数据交给 CPC9
DataOwner7 把数据交给 CPC3
DataOwner10 把数据交给 CPC4
DataOwner9 把数据交给 CPC1
DONE
最终Um的列表：
[('DataOwner1', 'CPC7', 0.7, 768.5376131152057, 1002, 0.042215707691957954), ('DataOwner4', 'CPC10', 1.0, 156.52472687067342, 156, 0.0), ('DataOwner2', 'CPC5', 0.5, 1176.5462039441597, 1992, 0.04259616455605101), ('DataOwner3', 'CPC6', 0.6, 972.5419085295571, 1168, 0.057445482131130224), ('DataOwner5', 'CPC2', 0.2, 1788.5590901879382, 274, 0.10815220819889185), ('DataOwner6', 'CPC8', 0.8, 564.5333177003503, 408, 0.026812984856205563), ('DataOwner8', 'CPC9', 0.9, 360.52902228414234, 352, 0.008675124363694595), ('DataOwner7', 'CPC3', 0.3, 1584.5547947725036, 288, 0.09950262698165233), ('DataOwner10', 'CPC4', 0.4, 1380.5504993591248, 324, 0.027368088321285525), ('DataOwner9', 'CPC1', 0.1, 1992.5633856029542, 273, 0.12095327978593519)]
('DataOwner1', 'CPC7', 0.7, 768.5376131152057, 1002, 0.042215707691957954)
('DataOwner4', 'CPC10', 1.0, 156.52472687067342, 156, 0.0)
('DataOwner2', 'CPC5', 0.5, 1176.5462039441597, 1992, 0.04259616455605101)
('DataOwner3', 'CPC6', 0.6, 972.5419085295571, 1168, 0.057445482131130224)
('DataOwner5', 'CPC2', 0.2, 1788.5590901879382, 274, 0.10815220819889185)
('DataOwner6', 'CPC8', 0.8, 564.5333177003503, 408, 0.026812984856205563)
('DataOwner8', 'CPC9', 0.9, 360.52902228414234, 352, 0.008675124363694595)
('DataOwner7', 'CPC3', 0.3, 1584.5547947725036, 288, 0.09950262698165233)
('DataOwner10', 'CPC4', 0.4, 1380.5504993591248, 324, 0.027368088321285525)
('DataOwner9', 'CPC1', 0.1, 1992.5633856029542, 273, 0.12095327978593519)
**** log-parameter_analysis 运行时间： 2025-02-10 22:24:46 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
**** log-parameter_analysis 运行时间： 2025-02-10 22:25:14 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
**** log-parameter_analysis 运行时间： 2025-02-10 22:26:00 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Epoch 1/5 started...
Epoch [1/5], Batch [1/1], Loss: 13.0876
Epoch 1 completed. Average Loss: 13.0876
Epoch 2/5 started...
Epoch [2/5], Batch [1/1], Loss: 80.1481
Epoch 2 completed. Average Loss: 80.1481
Epoch 3/5 started...
Epoch [3/5], Batch [1/1], Loss: 79.6098
Epoch 3 completed. Average Loss: 79.6098
Epoch 4/5 started...
Epoch [4/5], Batch [1/1], Loss: 65.5717
Epoch 4 completed. Average Loss: 65.5717
Epoch 5/5 started...
Epoch [5/5], Batch [1/1], Loss: 30.4744
Epoch 5 completed. Average Loss: 30.4744
**** log-parameter_analysis 运行时间： 2025-02-10 22:26:29 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/initial/mnist_cnn_initial_model
Model saved to D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/mnist_cnn_model
DONE
----- 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.04662299036646842
DataOwner2: noise random: 0.049632342602278806
DataOwner3: noise random: 0.0632674452871921
DataOwner4: noise random: 0.04389177811667599
DataOwner5: noise random: 0.056948808774163376
DataOwner6: noise random: 0.04276454889408926
DataOwner7: noise random: 0.014954060883780141
DataOwner8: noise random: 0.09087131567265681
DataOwner9: noise random: 0.038209277538461255
DataOwner10: noise random: 0.06035843454283096
DONE
----- 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9991189993828156, 0.9990044963325301, 0.9983793256484484, 0.9992206943715987, 0.9986903378935615, 0.9992593359602169, 0.9999092606713745, 0.9966595842856604, 0.9994110691884102, 0.9985199074722375]
归一化后的数据质量列表avg_f_list: [0.9756818465976176, 0.9721583249697764, 0.9529203883299948, 0.9788112347800888, 0.9624909488473538, 0.9800003251396116, 1.0, 0.9, 0.9846695047804003, 0.9572464136661506]
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.1468
DataOwner1的最优x_1 = 0.0143
DataOwner2的最优x_2 = 0.0139
DataOwner3的最优x_3 = 0.0118
DataOwner4的最优x_4 = 0.0146
DataOwner5的最优x_5 = 0.0129
DataOwner6的最优x_6 = 0.0147
DataOwner7的最优x_7 = 0.0167
DataOwner8的最优x_8 = 0.0049
DataOwner9的最优x_9 = 0.0152
DataOwner10的最优x_10 = 0.0123
每个DataOwner应该贡献数据比例 xn_list = [0.014280322852357835, 0.013908425063474908, 0.01177304808648649, 0.01460583586945225, 0.012857986440762767, 0.014728357027178446, 0.016696433303558654, 0.0048637984783448706, 0.015203335531523558, 0.012269135062515334]
ModelOwner的最大效用 U(Eta) = -0.1468
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.1468
DataOwner1的最优x_1 = 0.0143
DataOwner2的最优x_2 = 0.0139
DataOwner3的最优x_3 = 0.0118
DataOwner4的最优x_4 = 0.0146
DataOwner5的最优x_5 = 0.0129
DataOwner6的最优x_6 = 0.0147
DataOwner7的最优x_7 = 0.0167
DataOwner8的最优x_8 = 0.0049
DataOwner9的最优x_9 = 0.0152
DataOwner10的最优x_10 = 0.0123
每个DataOwner应该贡献数据比例 xn_list = [0.014280322852357835, 0.013908425063474908, 0.01177304808648649, 0.01460583586945225, 0.012857986440762767, 0.014728357027178446, 0.016696433303558654, 0.0048637984783448706, 0.015203335531523558, 0.012269135062515334]
ModelOwner的最大效用 U(Eta) = -0.1408
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.1468
DataOwner1的最优x_1 = 0.0143
DataOwner2的最优x_2 = 0.0139
DataOwner3的最优x_3 = 0.0118
DataOwner4的最优x_4 = 0.0146
DataOwner5的最优x_5 = 0.0129
DataOwner6的最优x_6 = 0.0147
DataOwner7的最优x_7 = 0.0167
DataOwner8的最优x_8 = 0.0049
DataOwner9的最优x_9 = 0.0152
DataOwner10的最优x_10 = 0.0123
每个DataOwner应该贡献数据比例 xn_list = [0.014280322852357835, 0.013908425063474908, 0.01177304808648649, 0.01460583586945225, 0.012857986440762767, 0.014728357027178446, 0.016696433303558654, 0.0048637984783448706, 0.015203335531523558, 0.012269135062515334]
ModelOwner的最大效用 U(Eta) = -0.1348
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.1468
DataOwner1的最优x_1 = 0.0143
DataOwner2的最优x_2 = 0.0139
DataOwner3的最优x_3 = 0.0118
DataOwner4的最优x_4 = 0.0146
DataOwner5的最优x_5 = 0.0129
DataOwner6的最优x_6 = 0.0147
DataOwner7的最优x_7 = 0.0167
DataOwner8的最优x_8 = 0.0049
DataOwner9的最优x_9 = 0.0152
DataOwner10的最优x_10 = 0.0123
每个DataOwner应该贡献数据比例 xn_list = [0.014280322852357835, 0.013908425063474908, 0.01177304808648649, 0.01460583586945225, 0.012857986440762767, 0.014728357027178446, 0.016696433303558654, 0.0048637984783448706, 0.015203335531523558, 0.012269135062515334]
ModelOwner的最大效用 U(Eta) = -0.1288
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.1468
DataOwner1的最优x_1 = 0.0143
DataOwner2的最优x_2 = 0.0139
DataOwner3的最优x_3 = 0.0118
DataOwner4的最优x_4 = 0.0146
DataOwner5的最优x_5 = 0.0129
DataOwner6的最优x_6 = 0.0147
DataOwner7的最优x_7 = 0.0167
DataOwner8的最优x_8 = 0.0049
DataOwner9的最优x_9 = 0.0152
DataOwner10的最优x_10 = 0.0123
每个DataOwner应该贡献数据比例 xn_list = [0.014280322852357835, 0.013908425063474908, 0.01177304808648649, 0.01460583586945225, 0.012857986440762767, 0.014728357027178446, 0.016696433303558654, 0.0048637984783448706, 0.015203335531523558, 0.012269135062515334]
ModelOwner的最大效用 U(Eta) = -0.1228
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.1468
DataOwner1的最优x_1 = 0.0143
DataOwner2的最优x_2 = 0.0139
DataOwner3的最优x_3 = 0.0118
DataOwner4的最优x_4 = 0.0146
DataOwner5的最优x_5 = 0.0129
DataOwner6的最优x_6 = 0.0147
DataOwner7的最优x_7 = 0.0167
DataOwner8的最优x_8 = 0.0049
DataOwner9的最优x_9 = 0.0152
DataOwner10的最优x_10 = 0.0123
每个DataOwner应该贡献数据比例 xn_list = [0.014280322852357835, 0.013908425063474908, 0.01177304808648649, 0.01460583586945225, 0.012857986440762767, 0.014728357027178446, 0.016696433303558654, 0.0048637984783448706, 0.015203335531523558, 0.012269135062515334]
ModelOwner的最大效用 U(Eta) = -0.1168
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.1468
DataOwner1的最优x_1 = 0.0143
DataOwner2的最优x_2 = 0.0139
DataOwner3的最优x_3 = 0.0118
DataOwner4的最优x_4 = 0.0146
DataOwner5的最优x_5 = 0.0129
DataOwner6的最优x_6 = 0.0147
DataOwner7的最优x_7 = 0.0167
DataOwner8的最优x_8 = 0.0049
DataOwner9的最优x_9 = 0.0152
DataOwner10的最优x_10 = 0.0123
每个DataOwner应该贡献数据比例 xn_list = [0.014280322852357835, 0.013908425063474908, 0.01177304808648649, 0.01460583586945225, 0.012857986440762767, 0.014728357027178446, 0.016696433303558654, 0.0048637984783448706, 0.015203335531523558, 0.012269135062515334]
ModelOwner的最大效用 U(Eta) = -0.1108
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.1468
DataOwner1的最优x_1 = 0.0143
DataOwner2的最优x_2 = 0.0139
DataOwner3的最优x_3 = 0.0118
DataOwner4的最优x_4 = 0.0146
DataOwner5的最优x_5 = 0.0129
DataOwner6的最优x_6 = 0.0147
DataOwner7的最优x_7 = 0.0167
DataOwner8的最优x_8 = 0.0049
DataOwner9的最优x_9 = 0.0152
DataOwner10的最优x_10 = 0.0123
每个DataOwner应该贡献数据比例 xn_list = [0.014280322852357835, 0.013908425063474908, 0.01177304808648649, 0.01460583586945225, 0.012857986440762767, 0.014728357027178446, 0.016696433303558654, 0.0048637984783448706, 0.015203335531523558, 0.012269135062515334]
ModelOwner的最大效用 U(Eta) = -0.1048
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.1468
DataOwner1的最优x_1 = 0.0143
DataOwner2的最优x_2 = 0.0139
DataOwner3的最优x_3 = 0.0118
DataOwner4的最优x_4 = 0.0146
DataOwner5的最优x_5 = 0.0129
DataOwner6的最优x_6 = 0.0147
DataOwner7的最优x_7 = 0.0167
DataOwner8的最优x_8 = 0.0049
DataOwner9的最优x_9 = 0.0152
DataOwner10的最优x_10 = 0.0123
每个DataOwner应该贡献数据比例 xn_list = [0.014280322852357835, 0.013908425063474908, 0.01177304808648649, 0.01460583586945225, 0.012857986440762767, 0.014728357027178446, 0.016696433303558654, 0.0048637984783448706, 0.015203335531523558, 0.012269135062515334]
ModelOwner的最大效用 U(Eta) = -0.0988
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.1468
DataOwner1的最优x_1 = 0.0143
DataOwner2的最优x_2 = 0.0139
DataOwner3的最优x_3 = 0.0118
DataOwner4的最优x_4 = 0.0146
DataOwner5的最优x_5 = 0.0129
DataOwner6的最优x_6 = 0.0147
DataOwner7的最优x_7 = 0.0167
DataOwner8的最优x_8 = 0.0049
DataOwner9的最优x_9 = 0.0152
DataOwner10的最优x_10 = 0.0123
每个DataOwner应该贡献数据比例 xn_list = [0.014280322852357835, 0.013908425063474908, 0.01177304808648649, 0.01460583586945225, 0.012857986440762767, 0.014728357027178446, 0.016696433303558654, 0.0048637984783448706, 0.015203335531523558, 0.012269135062515334]
ModelOwner的最大效用 U(Eta) = -0.0928
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.1468
DataOwner1的最优x_1 = 0.0143
DataOwner2的最优x_2 = 0.0139
DataOwner3的最优x_3 = 0.0118
DataOwner4的最优x_4 = 0.0146
DataOwner5的最优x_5 = 0.0129
DataOwner6的最优x_6 = 0.0147
DataOwner7的最优x_7 = 0.0167
DataOwner8的最优x_8 = 0.0049
DataOwner9的最优x_9 = 0.0152
DataOwner10的最优x_10 = 0.0123
每个DataOwner应该贡献数据比例 xn_list = [0.014280322852357835, 0.013908425063474908, 0.01177304808648649, 0.01460583586945225, 0.012857986440762767, 0.014728357027178446, 0.016696433303558654, 0.0048637984783448706, 0.015203335531523558, 0.012269135062515334]
ModelOwner的最大效用 U(Eta) = -0.0867
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.1468
DataOwner1的最优x_1 = 0.0143
DataOwner2的最优x_2 = 0.0139
DataOwner3的最优x_3 = 0.0118
DataOwner4的最优x_4 = 0.0146
DataOwner5的最优x_5 = 0.0129
DataOwner6的最优x_6 = 0.0147
DataOwner7的最优x_7 = 0.0167
DataOwner8的最优x_8 = 0.0049
DataOwner9的最优x_9 = 0.0152
DataOwner10的最优x_10 = 0.0123
每个DataOwner应该贡献数据比例 xn_list = [0.014280322852357835, 0.013908425063474908, 0.01177304808648649, 0.01460583586945225, 0.012857986440762767, 0.014728357027178446, 0.016696433303558654, 0.0048637984783448706, 0.015203335531523558, 0.012269135062515334]
ModelOwner的最大效用 U(Eta) = -0.0807
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.1468
DataOwner1的最优x_1 = 0.0143
DataOwner2的最优x_2 = 0.0139
DataOwner3的最优x_3 = 0.0118
DataOwner4的最优x_4 = 0.0146
DataOwner5的最优x_5 = 0.0129
DataOwner6的最优x_6 = 0.0147
DataOwner7的最优x_7 = 0.0167
DataOwner8的最优x_8 = 0.0049
DataOwner9的最优x_9 = 0.0152
DataOwner10的最优x_10 = 0.0123
每个DataOwner应该贡献数据比例 xn_list = [0.014280322852357835, 0.013908425063474908, 0.01177304808648649, 0.01460583586945225, 0.012857986440762767, 0.014728357027178446, 0.016696433303558654, 0.0048637984783448706, 0.015203335531523558, 0.012269135062515334]
ModelOwner的最大效用 U(Eta) = -0.0747
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.1468
DataOwner1的最优x_1 = 0.0143
DataOwner2的最优x_2 = 0.0139
DataOwner3的最优x_3 = 0.0118
DataOwner4的最优x_4 = 0.0146
DataOwner5的最优x_5 = 0.0129
DataOwner6的最优x_6 = 0.0147
DataOwner7的最优x_7 = 0.0167
DataOwner8的最优x_8 = 0.0049
DataOwner9的最优x_9 = 0.0152
DataOwner10的最优x_10 = 0.0123
每个DataOwner应该贡献数据比例 xn_list = [0.014280322852357835, 0.013908425063474908, 0.01177304808648649, 0.01460583586945225, 0.012857986440762767, 0.014728357027178446, 0.016696433303558654, 0.0048637984783448706, 0.015203335531523558, 0.012269135062515334]
ModelOwner的最大效用 U(Eta) = -0.0687
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.1468
DataOwner1的最优x_1 = 0.0143
DataOwner2的最优x_2 = 0.0139
DataOwner3的最优x_3 = 0.0118
DataOwner4的最优x_4 = 0.0146
DataOwner5的最优x_5 = 0.0129
DataOwner6的最优x_6 = 0.0147
DataOwner7的最优x_7 = 0.0167
DataOwner8的最优x_8 = 0.0049
DataOwner9的最优x_9 = 0.0152
DataOwner10的最优x_10 = 0.0123
每个DataOwner应该贡献数据比例 xn_list = [0.014280322852357835, 0.013908425063474908, 0.01177304808648649, 0.01460583586945225, 0.012857986440762767, 0.014728357027178446, 0.016696433303558654, 0.0048637984783448706, 0.015203335531523558, 0.012269135062515334]
ModelOwner的最大效用 U(Eta) = -0.0627
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.1468
DataOwner1的最优x_1 = 0.0143
DataOwner2的最优x_2 = 0.0139
DataOwner3的最优x_3 = 0.0118
DataOwner4的最优x_4 = 0.0146
DataOwner5的最优x_5 = 0.0129
DataOwner6的最优x_6 = 0.0147
DataOwner7的最优x_7 = 0.0167
DataOwner8的最优x_8 = 0.0049
DataOwner9的最优x_9 = 0.0152
DataOwner10的最优x_10 = 0.0123
每个DataOwner应该贡献数据比例 xn_list = [0.014280322852357835, 0.013908425063474908, 0.01177304808648649, 0.01460583586945225, 0.012857986440762767, 0.014728357027178446, 0.016696433303558654, 0.0048637984783448706, 0.015203335531523558, 0.012269135062515334]
ModelOwner的最大效用 U(Eta) = -0.0567
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.1468
DataOwner1的最优x_1 = 0.0143
DataOwner2的最优x_2 = 0.0139
DataOwner3的最优x_3 = 0.0118
DataOwner4的最优x_4 = 0.0146
DataOwner5的最优x_5 = 0.0129
DataOwner6的最优x_6 = 0.0147
DataOwner7的最优x_7 = 0.0167
DataOwner8的最优x_8 = 0.0049
DataOwner9的最优x_9 = 0.0152
DataOwner10的最优x_10 = 0.0123
每个DataOwner应该贡献数据比例 xn_list = [0.014280322852357835, 0.013908425063474908, 0.01177304808648649, 0.01460583586945225, 0.012857986440762767, 0.014728357027178446, 0.016696433303558654, 0.0048637984783448706, 0.015203335531523558, 0.012269135062515334]
ModelOwner的最大效用 U(Eta) = -0.0507
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.1468
DataOwner1的最优x_1 = 0.0143
DataOwner2的最优x_2 = 0.0139
DataOwner3的最优x_3 = 0.0118
DataOwner4的最优x_4 = 0.0146
DataOwner5的最优x_5 = 0.0129
DataOwner6的最优x_6 = 0.0147
DataOwner7的最优x_7 = 0.0167
DataOwner8的最优x_8 = 0.0049
DataOwner9的最优x_9 = 0.0152
DataOwner10的最优x_10 = 0.0123
每个DataOwner应该贡献数据比例 xn_list = [0.014280322852357835, 0.013908425063474908, 0.01177304808648649, 0.01460583586945225, 0.012857986440762767, 0.014728357027178446, 0.016696433303558654, 0.0048637984783448706, 0.015203335531523558, 0.012269135062515334]
ModelOwner的最大效用 U(Eta) = -0.0447
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.1468
DataOwner1的最优x_1 = 0.0143
DataOwner2的最优x_2 = 0.0139
DataOwner3的最优x_3 = 0.0118
DataOwner4的最优x_4 = 0.0146
DataOwner5的最优x_5 = 0.0129
DataOwner6的最优x_6 = 0.0147
DataOwner7的最优x_7 = 0.0167
DataOwner8的最优x_8 = 0.0049
DataOwner9的最优x_9 = 0.0152
DataOwner10的最优x_10 = 0.0123
每个DataOwner应该贡献数据比例 xn_list = [0.014280322852357835, 0.013908425063474908, 0.01177304808648649, 0.01460583586945225, 0.012857986440762767, 0.014728357027178446, 0.016696433303558654, 0.0048637984783448706, 0.015203335531523558, 0.012269135062515334]
ModelOwner的最大效用 U(Eta) = -0.0387
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.1468
DataOwner1的最优x_1 = 0.0143
DataOwner2的最优x_2 = 0.0139
DataOwner3的最优x_3 = 0.0118
DataOwner4的最优x_4 = 0.0146
DataOwner5的最优x_5 = 0.0129
DataOwner6的最优x_6 = 0.0147
DataOwner7的最优x_7 = 0.0167
DataOwner8的最优x_8 = 0.0049
DataOwner9的最优x_9 = 0.0152
DataOwner10的最优x_10 = 0.0123
每个DataOwner应该贡献数据比例 xn_list = [0.014280322852357835, 0.013908425063474908, 0.01177304808648649, 0.01460583586945225, 0.012857986440762767, 0.014728357027178446, 0.016696433303558654, 0.0048637984783448706, 0.015203335531523558, 0.012269135062515334]
ModelOwner的最大效用 U(Eta) = -0.0327
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.1468
DataOwner1的最优x_1 = 0.0143
DataOwner2的最优x_2 = 0.0139
DataOwner3的最优x_3 = 0.0118
DataOwner4的最优x_4 = 0.0146
DataOwner5的最优x_5 = 0.0129
DataOwner6的最优x_6 = 0.0147
DataOwner7的最优x_7 = 0.0167
DataOwner8的最优x_8 = 0.0049
DataOwner9的最优x_9 = 0.0152
DataOwner10的最优x_10 = 0.0123
每个DataOwner应该贡献数据比例 xn_list = [0.014280322852357835, 0.013908425063474908, 0.01177304808648649, 0.01460583586945225, 0.012857986440762767, 0.014728357027178446, 0.016696433303558654, 0.0048637984783448706, 0.015203335531523558, 0.012269135062515334]
ModelOwner的最大效用 U(Eta) = -0.0267
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.1468
DataOwner1的最优x_1 = 0.0143
DataOwner2的最优x_2 = 0.0139
DataOwner3的最优x_3 = 0.0118
DataOwner4的最优x_4 = 0.0146
DataOwner5的最优x_5 = 0.0129
DataOwner6的最优x_6 = 0.0147
DataOwner7的最优x_7 = 0.0167
DataOwner8的最优x_8 = 0.0049
DataOwner9的最优x_9 = 0.0152
DataOwner10的最优x_10 = 0.0123
每个DataOwner应该贡献数据比例 xn_list = [0.014280322852357835, 0.013908425063474908, 0.01177304808648649, 0.01460583586945225, 0.012857986440762767, 0.014728357027178446, 0.016696433303558654, 0.0048637984783448706, 0.015203335531523558, 0.012269135062515334]
ModelOwner的最大效用 U(Eta) = -0.0207
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.1468
DataOwner1的最优x_1 = 0.0143
DataOwner2的最优x_2 = 0.0139
DataOwner3的最优x_3 = 0.0118
DataOwner4的最优x_4 = 0.0146
DataOwner5的最优x_5 = 0.0129
DataOwner6的最优x_6 = 0.0147
DataOwner7的最优x_7 = 0.0167
DataOwner8的最优x_8 = 0.0049
DataOwner9的最优x_9 = 0.0152
DataOwner10的最优x_10 = 0.0123
每个DataOwner应该贡献数据比例 xn_list = [0.014280322852357835, 0.013908425063474908, 0.01177304808648649, 0.01460583586945225, 0.012857986440762767, 0.014728357027178446, 0.016696433303558654, 0.0048637984783448706, 0.015203335531523558, 0.012269135062515334]
ModelOwner的最大效用 U(Eta) = -0.0147
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.1468
DataOwner1的最优x_1 = 0.0143
DataOwner2的最优x_2 = 0.0139
DataOwner3的最优x_3 = 0.0118
DataOwner4的最优x_4 = 0.0146
DataOwner5的最优x_5 = 0.0129
DataOwner6的最优x_6 = 0.0147
DataOwner7的最优x_7 = 0.0167
DataOwner8的最优x_8 = 0.0049
DataOwner9的最优x_9 = 0.0152
DataOwner10的最优x_10 = 0.0123
每个DataOwner应该贡献数据比例 xn_list = [0.014280322852357835, 0.013908425063474908, 0.01177304808648649, 0.01460583586945225, 0.012857986440762767, 0.014728357027178446, 0.016696433303558654, 0.0048637984783448706, 0.015203335531523558, 0.012269135062515334]
ModelOwner的最大效用 U(Eta) = -0.0087
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.1468
DataOwner1的最优x_1 = 0.0143
DataOwner2的最优x_2 = 0.0139
DataOwner3的最优x_3 = 0.0118
DataOwner4的最优x_4 = 0.0146
DataOwner5的最优x_5 = 0.0129
DataOwner6的最优x_6 = 0.0147
DataOwner7的最优x_7 = 0.0167
DataOwner8的最优x_8 = 0.0049
DataOwner9的最优x_9 = 0.0152
DataOwner10的最优x_10 = 0.0123
每个DataOwner应该贡献数据比例 xn_list = [0.014280322852357835, 0.013908425063474908, 0.01177304808648649, 0.01460583586945225, 0.012857986440762767, 0.014728357027178446, 0.016696433303558654, 0.0048637984783448706, 0.015203335531523558, 0.012269135062515334]
ModelOwner的最大效用 U(Eta) = -0.0027
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.1645
DataOwner1的最优x_1 = 0.0160
DataOwner2的最优x_2 = 0.0156
DataOwner3的最优x_3 = 0.0132
DataOwner4的最优x_4 = 0.0164
DataOwner5的最优x_5 = 0.0144
DataOwner6的最优x_6 = 0.0165
DataOwner7的最优x_7 = 0.0187
DataOwner8的最优x_8 = 0.0054
DataOwner9的最优x_9 = 0.0170
DataOwner10的最优x_10 = 0.0137
每个DataOwner应该贡献数据比例 xn_list = [0.016000506051606016, 0.015583810097355192, 0.01319120926895719, 0.016365229814745454, 0.01440683743255233, 0.016502509653604405, 0.01870765702459099, 0.005449683301016999, 0.017034703251467616, 0.013747054028708395]
ModelOwner的最大效用 U(Eta) = 0.0025
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.2446
DataOwner1的最优x_1 = 0.0238
DataOwner2的最优x_2 = 0.0232
DataOwner3的最优x_3 = 0.0196
DataOwner4的最优x_4 = 0.0243
DataOwner5的最优x_5 = 0.0214
DataOwner6的最优x_6 = 0.0245
DataOwner7的最优x_7 = 0.0278
DataOwner8的最优x_8 = 0.0081
DataOwner9的最优x_9 = 0.0253
DataOwner10的最优x_10 = 0.0204
每个DataOwner应该贡献数据比例 xn_list = [0.023798508184240442, 0.023178731407402443, 0.019620073313317492, 0.024340983617530173, 0.021428149676568465, 0.024545168119946923, 0.027825015489216925, 0.008105639418917251, 0.025336731439129654, 0.02044681441884317]
ModelOwner的最大效用 U(Eta) = 0.0060
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.1994
DataOwner1的最优x_1 = 0.0194
DataOwner2的最优x_2 = 0.0189
DataOwner3的最优x_3 = 0.0160
DataOwner4的最优x_4 = 0.0198
DataOwner5的最优x_5 = 0.0175
DataOwner6的最优x_6 = 0.0200
DataOwner7的最优x_7 = 0.0227
DataOwner8的最优x_8 = 0.0066
DataOwner9的最优x_9 = 0.0207
DataOwner10的最优x_10 = 0.0167
每个DataOwner应该贡献数据比例 xn_list = [0.019400617650963032, 0.01889537370308129, 0.015994344587644548, 0.019842845308202333, 0.017468294046764216, 0.020009297147648143, 0.02268303889391347, 0.006607742387068769, 0.020654582019714356, 0.016668306502654197]
ModelOwner的最大效用 U(Eta) = 0.0164
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.2494
DataOwner1的最优x_1 = 0.0243
DataOwner2的最优x_2 = 0.0236
DataOwner3的最优x_3 = 0.0200
DataOwner4的最优x_4 = 0.0248
DataOwner5的最优x_5 = 0.0218
DataOwner6的最优x_6 = 0.0250
DataOwner7的最优x_7 = 0.0284
DataOwner8的最优x_8 = 0.0083
DataOwner9的最优x_9 = 0.0258
DataOwner10的最优x_10 = 0.0208
每个DataOwner应该贡献数据比例 xn_list = [0.02426515754760105, 0.02363322797324056, 0.020004790481086616, 0.02481827002698398, 0.02184832023041166, 0.02502645824982566, 0.02837061800320097, 0.008264577598809598, 0.02583354281673665, 0.020847742611801334]
ModelOwner的最大效用 U(Eta) = 0.0253
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.2994
DataOwner1的最优x_1 = 0.0291
DataOwner2的最优x_2 = 0.0284
DataOwner3的最优x_3 = 0.0240
DataOwner4的最优x_4 = 0.0298
DataOwner5的最优x_5 = 0.0262
DataOwner6的最优x_6 = 0.0300
DataOwner7的最优x_7 = 0.0341
DataOwner8的最优x_8 = 0.0099
DataOwner9的最优x_9 = 0.0310
DataOwner10的最优x_10 = 0.0250
每个DataOwner应该贡献数据比例 xn_list = [0.02912979531926125, 0.028371177571935156, 0.02401531706420963, 0.029793794853629076, 0.02622843454171011, 0.03004372029949914, 0.03405831154785753, 0.009921446146855112, 0.031012607816233287, 0.025027262813313796]
ModelOwner的最大效用 U(Eta) = 0.0360
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.3495
DataOwner1的最优x_1 = 0.0340
DataOwner2的最优x_2 = 0.0331
DataOwner3的最优x_3 = 0.0280
DataOwner4的最优x_4 = 0.0348
DataOwner5的最优x_5 = 0.0306
DataOwner6的最优x_6 = 0.0351
DataOwner7的最优x_7 = 0.0398
DataOwner8的最优x_8 = 0.0116
DataOwner9的最优x_9 = 0.0362
DataOwner10的最优x_10 = 0.0292
每个DataOwner应该贡献数据比例 xn_list = [0.03399927544576898, 0.03311384341960299, 0.028029835857532763, 0.0347742724247411, 0.03061290890081848, 0.03506597663073383, 0.039751666794775355, 0.011579963998365705, 0.036196828168116146, 0.029210943426849705]
ModelOwner的最大效用 U(Eta) = 0.0484
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.4183
DataOwner1的最优x_1 = 0.0407
DataOwner2的最优x_2 = 0.0396
DataOwner3的最优x_3 = 0.0336
DataOwner4的最优x_4 = 0.0416
DataOwner5的最优x_5 = 0.0366
DataOwner6的最优x_6 = 0.0420
DataOwner7的最优x_7 = 0.0476
DataOwner8的最优x_8 = 0.0139
DataOwner9的最优x_9 = 0.0433
DataOwner10的最优x_10 = 0.0350
每个DataOwner应该贡献数据比例 xn_list = [0.04070156910419395, 0.039641591431463945, 0.033555370935543394, 0.04162934160061926, 0.03664764648630222, 0.04197854960885422, 0.04758793207881134, 0.013862727888082278, 0.043332326447400364, 0.034969310832750955]
ModelOwner的最大效用 U(Eta) = 0.0623
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.4494
DataOwner1的最优x_1 = 0.0437
DataOwner2的最优x_2 = 0.0426
DataOwner3的最优x_3 = 0.0360
DataOwner4的最优x_4 = 0.0447
DataOwner5的最优x_5 = 0.0394
DataOwner6的最优x_6 = 0.0451
DataOwner7的最优x_7 = 0.0511
DataOwner8的最优x_8 = 0.0149
DataOwner9的最优x_9 = 0.0465
DataOwner10的最优x_10 = 0.0376
每个DataOwner应该贡献数据比例 xn_list = [0.04372372052843618, 0.0425850379526995, 0.036046906619518405, 0.044720381498981276, 0.03936878818540448, 0.045095518716285914, 0.05112140608887104, 0.014892055842143277, 0.04654981547771349, 0.037565833637420734]
ModelOwner的最大效用 U(Eta) = 0.0782
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.5563
DataOwner1的最优x_1 = 0.0541
DataOwner2的最优x_2 = 0.0527
DataOwner3的最优x_3 = 0.0446
DataOwner4的最优x_4 = 0.0554
DataOwner5的最优x_5 = 0.0487
DataOwner6的最优x_6 = 0.0558
DataOwner7的最优x_7 = 0.0633
DataOwner8的最优x_8 = 0.0184
DataOwner9的最优x_9 = 0.0576
DataOwner10的最优x_10 = 0.0465
每个DataOwner应该贡献数据比例 xn_list = [0.05412574411291639, 0.05271616503361734, 0.04462258975585683, 0.05535951415834564, 0.048734758378081035, 0.05582389781032552, 0.0632833645258916, 0.018434927176473753, 0.05762417677640785, 0.04650287473818199]
ModelOwner的最大效用 U(Eta) = 0.0945
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.5494
DataOwner1的最优x_1 = 0.0535
DataOwner2的最优x_2 = 0.0521
DataOwner3的最优x_3 = 0.0441
DataOwner4的最优x_4 = 0.0547
DataOwner5的最优x_5 = 0.0481
DataOwner6的最优x_6 = 0.0551
DataOwner7的最优x_7 = 0.0625
DataOwner8的最优x_8 = 0.0182
DataOwner9的最优x_9 = 0.0569
DataOwner10的最优x_10 = 0.0459
每个DataOwner应该贡献数据比例 xn_list = [0.05345258308447217, 0.052060534957346666, 0.044067619312509264, 0.05467100878785405, 0.04812864496915547, 0.05512961686679335, 0.062496310347366905, 0.01820565228633933, 0.05690750580957727, 0.04592451925340429]
ModelOwner的最大效用 U(Eta) = 0.1142
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.5994
DataOwner1的最优x_1 = 0.0583
DataOwner2的最优x_2 = 0.0568
DataOwner3的最优x_3 = 0.0481
DataOwner4的最优x_4 = 0.0596
DataOwner5的最优x_5 = 0.0525
DataOwner6的最优x_6 = 0.0601
DataOwner7的最优x_7 = 0.0682
DataOwner8的最优x_8 = 0.0199
DataOwner9的最优x_9 = 0.0621
DataOwner10的最优x_10 = 0.0501
每个DataOwner应该贡献数据比例 xn_list = [0.05831741266751772, 0.05679867133264488, 0.048078304026010925, 0.05964672976131123, 0.05250893197250104, 0.06014707674979566, 0.06818422812544289, 0.019862586154948368, 0.062086775004536514, 0.050104204215179374]
ModelOwner的最大效用 U(Eta) = 0.1344
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.6494
DataOwner1的最优x_1 = 0.0632
DataOwner2的最优x_2 = 0.0615
DataOwner3的最优x_3 = 0.0521
DataOwner4的最优x_4 = 0.0646
DataOwner5的最优x_5 = 0.0569
DataOwner6的最优x_6 = 0.0652
DataOwner7的最优x_7 = 0.0739
DataOwner8的最优x_8 = 0.0215
DataOwner9的最优x_9 = 0.0673
DataOwner10的最优x_10 = 0.0543
每个DataOwner应该贡献数据比例 xn_list = [0.06318193898711194, 0.06153651238173844, 0.05208873872510784, 0.06462214059543483, 0.05688894593234237, 0.06516422385073935, 0.07387179136165567, 0.021519416743218788, 0.06726572134835726, 0.0542836286609393]
ModelOwner的最大效用 U(Eta) = 0.1561
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.6994
DataOwner1的最优x_1 = 0.0680
DataOwner2的最优x_2 = 0.0663
DataOwner3的最优x_3 = 0.0561
DataOwner4的最优x_4 = 0.0696
DataOwner5的最优x_5 = 0.0613
DataOwner6的最优x_6 = 0.0702
DataOwner7的最优x_7 = 0.0796
DataOwner8的最优x_8 = 0.0232
DataOwner9的最优x_9 = 0.0724
DataOwner10的最优x_10 = 0.0585
每个DataOwner应该贡献数据比例 xn_list = [0.06804641420966688, 0.06627430366458512, 0.056099131298991016, 0.06959749916770237, 0.06126891388476244, 0.07018131825139554, 0.07955929485486735, 0.02317622992773849, 0.07244461329221137, 0.0584630092063592]
ModelOwner的最大效用 U(Eta) = 0.1792
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.7494
DataOwner1的最优x_1 = 0.0729
DataOwner2的最优x_2 = 0.0710
DataOwner3的最优x_3 = 0.0601
DataOwner4的最优x_4 = 0.0746
DataOwner5的最优x_5 = 0.0656
DataOwner6的最优x_6 = 0.0752
DataOwner7的最优x_7 = 0.0852
DataOwner8的最优x_8 = 0.0248
DataOwner9的最优x_9 = 0.0776
DataOwner10的最优x_10 = 0.0626
每个DataOwner应该贡献数据比例 xn_list = [0.07291110748748725, 0.07101230732055658, 0.06010970364680927, 0.0745730807649874, 0.06564907817643245, 0.07519863754617179, 0.08524705330040801, 0.024833117380437433, 0.07762373738106085, 0.0626425770996123]
ModelOwner的最大效用 U(Eta) = 0.2036
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.7994
DataOwner1的最优x_1 = 0.0778
DataOwner2的最优x_2 = 0.0758
DataOwner3的最优x_3 = 0.0641
DataOwner4的最优x_4 = 0.0795
DataOwner5的最优x_5 = 0.0700
DataOwner6的最优x_6 = 0.0802
DataOwner7的最优x_7 = 0.0909
DataOwner8的最优x_8 = 0.0265
DataOwner9的最优x_9 = 0.0828
DataOwner10的最优x_10 = 0.0668
每个DataOwner应该贡献数据比例 xn_list = [0.0777757306411625, 0.07575024268588279, 0.06412021817424972, 0.07954859064189035, 0.0700291793228901, 0.08021588452135227, 0.09093472974975736, 0.02648998095021466, 0.08280278682218056, 0.06682208473877554]
ModelOwner的最大效用 U(Eta) = 0.2293
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.8494
DataOwner1的最优x_1 = 0.0826
DataOwner2的最优x_2 = 0.0805
DataOwner3的最优x_3 = 0.0681
DataOwner4的最优x_4 = 0.0845
DataOwner5的最优x_5 = 0.0744
DataOwner6的最优x_6 = 0.0852
DataOwner7的最优x_7 = 0.0966
DataOwner8的最优x_8 = 0.0281
DataOwner9的最优x_9 = 0.0880
DataOwner10的最优x_10 = 0.0710
每个DataOwner应该贡献数据比例 xn_list = [0.08264023502504149, 0.08048806235153973, 0.06813063478592184, 0.08452397905045476, 0.07440917352407793, 0.08523300900625896, 0.09662226733901154, 0.028146804067252448, 0.08798170984506629, 0.07100149037096964]
ModelOwner的最大效用 U(Eta) = 0.2563
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.8994
DataOwner1的最优x_1 = 0.0875
DataOwner2的最优x_2 = 0.0852
DataOwner3的最优x_3 = 0.0721
DataOwner4的最优x_4 = 0.0895
DataOwner5的最优x_5 = 0.0788
DataOwner6的最优x_6 = 0.0903
DataOwner7的最优x_7 = 0.1023
DataOwner8的最优x_8 = 0.0298
DataOwner9的最优x_9 = 0.0932
DataOwner10的最优x_10 = 0.0752
每个DataOwner应该贡献数据比例 xn_list = [0.08750484810754873, 0.08522598792301883, 0.07214114101753233, 0.08949947861459956, 0.07878926561405927, 0.09025024558449665, 0.10230993201761841, 0.02980366420604765, 0.09316074852702978, 0.07518098932798688]
ModelOwner的最大效用 U(Eta) = 0.2846
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.9494
DataOwner1的最优x_1 = 0.0924
DataOwner2的最优x_2 = 0.0900
DataOwner3的最优x_3 = 0.0762
DataOwner4的最优x_4 = 0.0945
DataOwner5的最优x_5 = 0.0832
DataOwner6的最优x_6 = 0.0953
DataOwner7的最优x_7 = 0.1080
DataOwner8的最优x_8 = 0.0315
DataOwner9的最优x_9 = 0.0983
DataOwner10的最优x_10 = 0.0794
每个DataOwner应该贡献数据比例 xn_list = [0.09236944042516793, 0.08996389325118391, 0.07615163012851933, 0.09447495695044665, 0.08316933899849738, 0.09526746075372142, 0.10799757241251756, 0.03146051727160227, 0.09833976513330332, 0.07936047047781461]
ModelOwner的最大效用 U(Eta) = 0.3141
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.9994
DataOwner1的最优x_1 = 0.0972
DataOwner2的最优x_2 = 0.0947
DataOwner3的最优x_3 = 0.0802
DataOwner4的最优x_4 = 0.0995
DataOwner5的最优x_5 = 0.0875
DataOwner6的最优x_6 = 0.1003
DataOwner7的最优x_7 = 0.1137
DataOwner8的最优x_8 = 0.0331
DataOwner9的最优x_9 = 0.1035
DataOwner10的最优x_10 = 0.0835
每个DataOwner应该贡献数据比例 xn_list = [0.09723399477407761, 0.09470176160011066, 0.08016208793544359, 0.09945039645279581, 0.08754937819612502, 0.1002846367655088, 0.1136851684257945, 0.03311735740759977, 0.10351874131998186, 0.08353991900580222]
ModelOwner的最大效用 U(Eta) = 0.3448
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.0494
DataOwner1的最优x_1 = 0.1021
DataOwner2的最优x_2 = 0.0994
DataOwner3的最优x_3 = 0.0842
DataOwner4的最优x_4 = 0.1044
DataOwner5的最优x_5 = 0.0919
DataOwner6的最优x_6 = 0.1053
DataOwner7的最优x_7 = 0.1194
DataOwner8的最优x_8 = 0.0348
DataOwner9的最优x_9 = 0.1087
DataOwner10的最优x_10 = 0.0877
每个DataOwner应该贡献数据比例 xn_list = [0.10209871258595833, 0.09943978915505074, 0.08417268047891617, 0.10442600312782024, 0.09192956458654225, 0.10530198140546314, 0.11937295553401094, 0.03477425321914101, 0.10869789154206111, 0.08771950798250887]
ModelOwner的最大效用 U(Eta) = 0.3766
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.0994
DataOwner1的最优x_1 = 0.1070
DataOwner2的最优x_2 = 0.1042
DataOwner3的最优x_3 = 0.0882
DataOwner4的最优x_4 = 0.1094
DataOwner5的最优x_5 = 0.0963
DataOwner6的最优x_6 = 0.1103
DataOwner7的最优x_7 = 0.1251
DataOwner8的最优x_8 = 0.0364
DataOwner9的最优x_9 = 0.1139
DataOwner10的最优x_10 = 0.0919
每个DataOwner应该贡献数据比例 xn_list = [0.10696322996594741, 0.10417762149620523, 0.08818310783485961, 0.10940140483510762, 0.0963095704856498, 0.11031911924567048, 0.1250605083357173, 0.0364310807600009, 0.11387682835852081, 0.09189892473944883]
ModelOwner的最大效用 U(Eta) = 0.4096
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.1494
DataOwner1的最优x_1 = 0.1118
DataOwner2的最优x_2 = 0.1089
DataOwner3的最优x_3 = 0.0922
DataOwner4的最优x_4 = 0.1144
DataOwner5的最优x_5 = 0.1007
DataOwner6的最优x_6 = 0.1153
DataOwner7的最优x_7 = 0.1307
DataOwner8的最优x_8 = 0.0381
DataOwner9的最优x_9 = 0.1191
DataOwner10的最优x_10 = 0.0961
每个DataOwner应该贡献数据比例 xn_list = [0.11182779625746442, 0.1089155014762591, 0.09219357548343243, 0.11437685655262056, 0.10068962043498793, 0.11533630757416585, 0.13074811830199712, 0.038087924962893746, 0.11905581725731021, 0.09607838352513394]
ModelOwner的最大效用 U(Eta) = 0.4436
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.1994
DataOwner1的最优x_1 = 0.1167
DataOwner2的最优x_2 = 0.1137
DataOwner3的最优x_3 = 0.0962
DataOwner4的最优x_4 = 0.1194
DataOwner5的最优x_5 = 0.1051
DataOwner6的最优x_6 = 0.1204
DataOwner7的最优x_7 = 0.1364
DataOwner8的最优x_8 = 0.0397
DataOwner9的最优x_9 = 0.1242
DataOwner10的最优x_10 = 0.1003
每个DataOwner应该贡献数据比例 xn_list = [0.1166924169229862, 0.11365343441433935, 0.09620408796840141, 0.11935236388274317, 0.10506971934514962, 0.12035355197997087, 0.1364357918509614, 0.03974478768421238, 0.12423486404609937, 0.10025788903263497]
ModelOwner的最大效用 U(Eta) = 0.4788
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.2494
DataOwner1的最优x_1 = 0.1216
DataOwner2的最优x_2 = 0.1184
DataOwner3的最优x_3 = 0.1002
DataOwner4的最优x_4 = 0.1243
DataOwner5的最优x_5 = 0.1094
DataOwner6的最优x_6 = 0.1254
DataOwner7的最优x_7 = 0.1421
DataOwner8的最优x_8 = 0.0414
DataOwner9的最优x_9 = 0.1294
DataOwner10的最优x_10 = 0.1044
每个DataOwner应该贡献数据比例 xn_list = [0.12155710626025415, 0.11839143423571592, 0.10021465706343811, 0.12432794145050895, 0.10944988008613799, 0.12537086721254276, 0.14212354568556132, 0.04140167379518757, 0.1294139839435353, 0.10443745353713485]
ModelOwner的最大效用 U(Eta) = 0.5151
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.2994
DataOwner1的最优x_1 = 0.1264
DataOwner2的最优x_2 = 0.1231
DataOwner3的最优x_3 = 0.1042
DataOwner4的最优x_4 = 0.1293
DataOwner5的最优x_5 = 0.1138
DataOwner6的最优x_6 = 0.1304
DataOwner7的最优x_7 = 0.1478
DataOwner8的最优x_8 = 0.0431
DataOwner9的最优x_9 = 0.1346
DataOwner10的最优x_10 = 0.1086
每个DataOwner应该贡献数据比例 xn_list = [0.12642175101962597, 0.12312939062893608, 0.10422518940857466, 0.12930347342063364, 0.11383000064907904, 0.13038813649649456, 0.1478112473715873, 0.043058544717208345, 0.134593056364633, 0.10861697972070851]
ModelOwner的最大效用 U(Eta) = 0.5523
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3494
DataOwner1的最优x_1 = 0.1313
DataOwner2的最优x_2 = 0.1279
DataOwner3的最优x_3 = 0.1082
DataOwner4的最优x_4 = 0.1343
DataOwner5的最优x_5 = 0.1182
DataOwner6的最优x_6 = 0.1354
DataOwner7的最优x_7 = 0.1535
DataOwner8的最优x_8 = 0.0447
DataOwner9的最优x_9 = 0.1398
DataOwner10的最优x_10 = 0.1128
每个DataOwner应该贡献数据比例 xn_list = [0.1312860117002423, 0.12786697296836558, 0.10823540510766998, 0.134278612563996, 0.11820977546794936, 0.1354050095973909, 0.15349850005332316, 0.04471528483580842, 0.13977171991761017, 0.1127961759607742]
ModelOwner的最大效用 U(Eta) = 0.5906
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3994
DataOwner1的最优x_1 = 0.1362
DataOwner2的最优x_2 = 0.1326
DataOwner3的最优x_3 = 0.1122
DataOwner4的最优x_4 = 0.1393
DataOwner5的最优x_5 = 0.1226
DataOwner6的最优x_6 = 0.1404
DataOwner7的最优x_7 = 0.1592
DataOwner8的最优x_8 = 0.0464
DataOwner9的最优x_9 = 0.1450
DataOwner10的最优x_10 = 0.1170
每个DataOwner应该贡献数据比例 xn_list = [0.1361508573239482, 0.1326051249990502, 0.11224610300123071, 0.13925435002030037, 0.12259007691945042, 0.14042248602385543, 0.15918643660341178, 0.04637222417291799, 0.14495100619366436, 0.11697587474958115]
ModelOwner的最大效用 U(Eta) = 0.6299
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.4494
DataOwner1的最优x_1 = 0.1410
DataOwner2的最优x_2 = 0.1373
DataOwner3的最优x_3 = 0.1163
DataOwner4的最优x_4 = 0.1442
DataOwner5的最优x_5 = 0.1270
DataOwner6的最优x_6 = 0.1454
DataOwner7的最优x_7 = 0.1649
DataOwner8的最优x_8 = 0.0480
DataOwner9的最优x_9 = 0.1501
DataOwner10的最优x_10 = 0.1212
每个DataOwner应该贡献数据比例 xn_list = [0.14101535882433114, 0.13734294188223292, 0.11625651728651766, 0.14422973542980383, 0.1269700685420685, 0.14543960752274243, 0.16487397083414018, 0.048029046312244125, 0.15012992612382595, 0.12115527786346746]
ModelOwner的最大效用 U(Eta) = 0.6702
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.4994
DataOwner1的最优x_1 = 0.1459
DataOwner2的最优x_2 = 0.1421
DataOwner3的最优x_3 = 0.1203
DataOwner4的最优x_4 = 0.1492
DataOwner5的最优x_5 = 0.1313
DataOwner6的最优x_6 = 0.1505
DataOwner7的最优x_7 = 0.1706
DataOwner8的最优x_8 = 0.0497
DataOwner9的最优x_9 = 0.1553
DataOwner10的最优x_10 = 0.1253
每个DataOwner应该贡献数据比例 xn_list = [0.14587976958273183, 0.14208067041624972, 0.12026685673668815, 0.14920502809487818, 0.1313499784505441, 0.15045663544027466, 0.17056139895091588, 0.049685837542338324, 0.15530874944401904, 0.12533460303413457]
ModelOwner的最大效用 U(Eta) = 0.7114
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.5494
DataOwner1的最优x_1 = 0.1507
DataOwner2的最优x_2 = 0.1468
DataOwner3的最优x_3 = 0.1243
DataOwner4的最优x_4 = 0.1542
DataOwner5的最优x_5 = 0.1357
DataOwner6的最优x_6 = 0.1555
DataOwner7的最优x_7 = 0.1762
DataOwner8的最优x_8 = 0.0513
DataOwner9的最优x_9 = 0.1605
DataOwner10的最优x_10 = 0.1295
每个DataOwner应该贡献数据比例 xn_list = [0.15074442181035552, 0.14681863405783846, 0.12427739521542207, 0.15418056768054922, 0.1357301057781514, 0.15547391239584385, 0.17624910940316685, 0.05134271101256541, 0.1604878298266906, 0.1295141356449649]
ModelOwner的最大效用 U(Eta) = 0.7536
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.5994
DataOwner1的最优x_1 = 0.1556
DataOwner2的最优x_2 = 0.1516
DataOwner3的最优x_3 = 0.1283
DataOwner4的最优x_4 = 0.1592
DataOwner5的最优x_5 = 0.1401
DataOwner6的最优x_6 = 0.1605
DataOwner7的最优x_7 = 0.1819
DataOwner8的最优x_8 = 0.0530
DataOwner9的最优x_9 = 0.1657
DataOwner10的最优x_10 = 0.1337
每个DataOwner应该贡献数据比例 xn_list = [0.1556090452938947, 0.1515565697401651, 0.12828791002077172, 0.15915607789247777, 0.1401102072256354, 0.16049115970752093, 0.18193678624436393, 0.05299957469380139, 0.16566687961429527, 0.1336936435711491]
ModelOwner的最大效用 U(Eta) = 0.7967
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.6494
DataOwner1的最优x_1 = 0.1605
DataOwner2的最优x_2 = 0.1563
DataOwner3的最优x_3 = 0.1323
DataOwner4的最优x_4 = 0.1641
DataOwner5的最优x_5 = 0.1445
DataOwner6的最优x_6 = 0.1655
DataOwner7的最优x_7 = 0.1876
DataOwner8的最优x_8 = 0.0547
DataOwner9的最优x_9 = 0.1708
DataOwner10的最优x_10 = 0.1379
每个DataOwner应该贡献数据比例 xn_list = [0.1604736506426177, 0.15629448776089472, 0.13229840987260577, 0.1641315695581233, 0.14449029234245425, 0.16550838831911954, 0.18762444187781377, 0.054656432200387954, 0.17084591009539782, 0.13787313591587166]
ModelOwner的最大效用 U(Eta) = 0.8408
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.6994
DataOwner1的最优x_1 = 0.1653
DataOwner2的最优x_2 = 0.1610
DataOwner3的最优x_3 = 0.1363
DataOwner4的最优x_4 = 0.1691
DataOwner5的最优x_5 = 0.1489
DataOwner6的最优x_6 = 0.1705
DataOwner7的最优x_7 = 0.1933
DataOwner8的最优x_8 = 0.0563
DataOwner9的最优x_9 = 0.1760
DataOwner10的最优x_10 = 0.1421
每个DataOwner应该贡献数据比例 xn_list = [0.16533824478510034, 0.16103239486670837, 0.13630890048956662, 0.1691070497610865, 0.14887036737123646, 0.17052560537042244, 0.19331208441611342, 0.056313285887758545, 0.17602492864497726, 0.14205261863491694]
ModelOwner的最大效用 U(Eta) = 0.8857
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.7494
DataOwner1的最优x_1 = 0.1702
DataOwner2的最优x_2 = 0.1658
DataOwner3的最优x_3 = 0.1403
DataOwner4的最优x_4 = 0.1741
DataOwner5的最优x_5 = 0.1533
DataOwner6的最优x_6 = 0.1755
DataOwner7的最优x_7 = 0.1990
DataOwner8的最优x_8 = 0.0580
DataOwner9的最优x_9 = 0.1812
DataOwner10的最优x_10 = 0.1462
每个DataOwner应该贡献数据比例 xn_list = [0.17020282846310558, 0.16577029177761965, 0.14031938244710826, 0.17408251926786245, 0.15325043295362237, 0.17554281163571817, 0.19899971467420677, 0.057970136005712464, 0.18120393605467155, 0.1462320923353861]
ModelOwner的最大效用 U(Eta) = 0.9314
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.7994
DataOwner1的最优x_1 = 0.1751
DataOwner2的最优x_2 = 0.1705
DataOwner3的最优x_3 = 0.1443
DataOwner4的最优x_4 = 0.1791
DataOwner5的最优x_5 = 0.1576
DataOwner6的最优x_6 = 0.1806
DataOwner7的最优x_7 = 0.2047
DataOwner8的最优x_8 = 0.0596
DataOwner9的最优x_9 = 0.1864
DataOwner10的最优x_10 = 0.1504
每个DataOwner应该贡献数据比例 xn_list = [0.17506754378086611, 0.17050831690508658, 0.14432997298253095, 0.17905812340351915, 0.1576306171051532, 0.18056015366013, 0.2046874989343972, 0.059627031028861105, 0.1863830836197115, 0.15041167917996207]
ModelOwner的最大效用 U(Eta) = 0.9781
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.8494
DataOwner1的最优x_1 = 0.1799
DataOwner2的最优x_2 = 0.1752
DataOwner3的最优x_3 = 0.1483
DataOwner4的最优x_4 = 0.1840
DataOwner5的最优x_5 = 0.1620
DataOwner6的最优x_6 = 0.1856
DataOwner7的最优x_7 = 0.2104
DataOwner8的最优x_8 = 0.0613
DataOwner9的最优x_9 = 0.1916
DataOwner10的最优x_10 = 0.1546
每个DataOwner应该贡献数据比例 xn_list = [0.17993204538230612, 0.1752461338802847, 0.14834038730856586, 0.1840335089539548, 0.16201060881299323, 0.18557727526528334, 0.21037503325793633, 0.061283853140321316, 0.19156200364117562, 0.15459108239426997]
ModelOwner的最大效用 U(Eta) = 1.0256
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.8994
DataOwner1的最优x_1 = 0.1848
DataOwner2的最优x_2 = 0.1800
DataOwner3的最优x_3 = 0.1524
DataOwner4的最优x_4 = 0.1890
DataOwner5的最优x_5 = 0.1664
DataOwner6的最优x_6 = 0.1906
DataOwner7的最优x_7 = 0.2161
DataOwner8的最优x_8 = 0.0629
DataOwner9的最优x_9 = 0.1967
DataOwner10的最优x_10 = 0.1588
每个DataOwner应该贡献数据比例 xn_list = [0.18479659279388083, 0.179983995431809, 0.15235083931592827, 0.1890089413420592, 0.16639064171809495, 0.19059444407413031, 0.2160626210407021, 0.06294069086521603, 0.19674097239782537, 0.15877052483276932]
ModelOwner的最大效用 U(Eta) = 1.0739
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.9494
DataOwner1的最优x_1 = 0.1897
DataOwner2的最优x_2 = 0.1847
DataOwner3的最优x_3 = 0.1564
DataOwner4的最优x_4 = 0.1940
DataOwner5的最优x_5 = 0.1708
DataOwner6的最优x_6 = 0.1956
DataOwner7的最优x_7 = 0.2218
DataOwner8的最优x_8 = 0.0646
DataOwner9的最优x_9 = 0.2019
DataOwner10的最优x_10 = 0.1629
每个DataOwner应该贡献数据比例 xn_list = [0.18966109543600865, 0.18472181346068217, 0.15636125457559596, 0.1939843279776205, 0.17077063440511253, 0.1956115668006312, 0.22175015670667417, 0.06459751343596952, 0.20191989357614687, 0.16294992906473854]
ModelOwner的最大效用 U(Eta) = 1.1231
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.9994
DataOwner1的最优x_1 = 0.1945
DataOwner2的最优x_2 = 0.1895
DataOwner3的最优x_3 = 0.1604
DataOwner4的最优x_4 = 0.1990
DataOwner5的最优x_5 = 0.1752
DataOwner6的最优x_6 = 0.2006
DataOwner7的最优x_7 = 0.2274
DataOwner8的最优x_8 = 0.0663
DataOwner9的最优x_9 = 0.2071
DataOwner10的最优x_10 = 0.1671
每个DataOwner应该贡献数据比例 xn_list = [0.19452569385910312, 0.18945972473487324, 0.16037174872029977, 0.1989598125587119, 0.17515071328695958, 0.20062878826701488, 0.22743780424594032, 0.06625436858196497, 0.20709891668356914, 0.16712941545904855]
ModelOwner的最大效用 U(Eta) = 1.1730
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 2.0494
DataOwner1的最优x_1 = 0.1994
DataOwner2的最优x_2 = 0.1942
DataOwner3的最优x_3 = 0.1644
DataOwner4的最优x_4 = 0.2039
DataOwner5的最优x_5 = 0.1795
DataOwner6的最优x_6 = 0.2056
DataOwner7的最优x_7 = 0.2331
DataOwner8的最优x_8 = 0.0679
DataOwner9的最优x_9 = 0.2123
DataOwner10的最优x_10 = 0.1713
每个DataOwner应该贡献数据比例 xn_list = [0.19939038966243494, 0.19419773085178643, 0.16438232314613213, 0.20393539674044087, 0.17953087984956473, 0.2056461101692329, 0.23312556564208928, 0.0679112568966342, 0.21227804346556117, 0.17130898552446422]
ModelOwner的最大效用 U(Eta) = 1.2238
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 2.0994
DataOwner1的最优x_1 = 0.2043
DataOwner2的最优x_2 = 0.1989
DataOwner3的最优x_3 = 0.1684
DataOwner4的最优x_4 = 0.2089
DataOwner5的最优x_5 = 0.1839
DataOwner6的最优x_6 = 0.2107
DataOwner7的最优x_7 = 0.2388
DataOwner8的最优x_8 = 0.0696
DataOwner9的最优x_9 = 0.2175
DataOwner10的最优x_10 = 0.1755
每个DataOwner应该贡献数据比例 xn_list = [0.20425496132313062, 0.1989356160634548, 0.16839279522228986, 0.20891085395450318, 0.18391093463081482, 0.2106633040388472, 0.23881318188404282, 0.06956810292786271, 0.2174570380880504, 0.17548844892466187]
ModelOwner的最大效用 U(Eta) = 1.2753
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 2.1494
DataOwner1的最优x_1 = 0.2091
DataOwner2的最优x_2 = 0.2037
DataOwner3的最优x_3 = 0.1724
DataOwner4的最优x_4 = 0.2139
DataOwner5的最优x_5 = 0.1883
DataOwner6的最优x_6 = 0.2157
DataOwner7的最优x_7 = 0.2445
DataOwner8的最优x_8 = 0.0712
DataOwner9的最优x_9 = 0.2226
DataOwner10的最优x_10 = 0.1797
每个DataOwner应该贡献数据比例 xn_list = [0.209119220696919, 0.20367319716634844, 0.17240300972413478, 0.21388599180970483, 0.18829070823682803, 0.2156801758211138, 0.24450043289581622, 0.0712248425204341, 0.2226357003243529, 0.17966764382385675]
ModelOwner的最大效用 U(Eta) = 1.3276
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 2.1994
DataOwner1的最优x_1 = 0.2140
DataOwner2的最优x_2 = 0.2084
DataOwner3的最优x_3 = 0.1764
DataOwner4的最优x_4 = 0.2189
DataOwner5的最优x_5 = 0.1927
DataOwner6的最优x_6 = 0.2207
DataOwner7的最优x_7 = 0.2502
DataOwner8的最优x_8 = 0.0729
DataOwner9的最优x_9 = 0.2278
DataOwner10的最优x_10 = 0.1838
每个DataOwner应该贡献数据比例 xn_list = [0.2139841294734149, 0.20841141066431337, 0.1764137598521657, 0.21886179376009543, 0.19267106656032934, 0.22069771736929492, 0.25018844341361407, 0.07288180344484474, 0.22781505374463198, 0.18384739705536565]
ModelOwner的最大效用 U(Eta) = 1.3807
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 2.2494
DataOwner1的最优x_1 = 0.2188
DataOwner2的最优x_2 = 0.2131
DataOwner3的最优x_3 = 0.1804
DataOwner4的最优x_4 = 0.2238
DataOwner5的最优x_5 = 0.1971
DataOwner6的最优x_6 = 0.2257
DataOwner7的最优x_7 = 0.2559
DataOwner8的最优x_8 = 0.0745
DataOwner9的最优x_9 = 0.2330
DataOwner10的最优x_10 = 0.1880
每个DataOwner应该贡献数据比例 xn_list = [0.21884871259619637, 0.2131493070378894, 0.1804242413820078, 0.22383726269302634, 0.1970511316662368, 0.22571492305675683, 0.2558760730639108, 0.07453865337996894, 0.2329940605640295, 0.1880268703048117]
ModelOwner的最大效用 U(Eta) = 1.4345
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 2.2994
DataOwner1的最优x_1 = 0.2237
DataOwner2的最优x_2 = 0.2179
DataOwner3的最优x_3 = 0.1844
DataOwner4的最优x_4 = 0.2288
DataOwner5的最优x_5 = 0.2014
DataOwner6的最优x_6 = 0.2307
DataOwner7的最优x_7 = 0.2616
DataOwner8的最优x_8 = 0.0762
DataOwner9的最优x_9 = 0.2382
DataOwner10的最优x_10 = 0.1922
每个DataOwner应该贡献数据比例 xn_list = [0.22371319680191717, 0.21788710707018843, 0.18443464136574964, 0.22881263045319644, 0.20143110770886688, 0.23073202672246654, 0.26156358706519717, 0.07619546962393685, 0.2381729620720239, 0.1922062585711298]
ModelOwner的最大效用 U(Eta) = 1.4890
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 2.3494
DataOwner1的最优x_1 = 0.2286
DataOwner2的最优x_2 = 0.2226
DataOwner3的最优x_3 = 0.1884
DataOwner4的最优x_4 = 0.2338
DataOwner5的最优x_5 = 0.2058
DataOwner6的最优x_6 = 0.2357
DataOwner7的最优x_7 = 0.2673
DataOwner8的最优x_8 = 0.0779
DataOwner9的最优x_9 = 0.2434
DataOwner10的最优x_10 = 0.1964
每个DataOwner应该贡献数据比例 xn_list = [0.22857784373734574, 0.22262506559505416, 0.18844517549794226, 0.23378816465304622, 0.20581123027043627, 0.2357492982224956, 0.26725129131899955, 0.07785234129367373, 0.24335203682794807, 0.19638578664255596]
ModelOwner的最大效用 U(Eta) = 1.5443
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 2.3994
DataOwner1的最优x_1 = 0.2334
DataOwner2的最优x_2 = 0.2274
DataOwner3的最优x_3 = 0.1925
DataOwner4的最优x_4 = 0.2388
DataOwner5的最优x_5 = 0.2102
DataOwner6的最优x_6 = 0.2408
DataOwner7的最优x_7 = 0.2729
DataOwner8的最优x_8 = 0.0795
DataOwner9的最优x_9 = 0.2485
DataOwner10的最优x_10 = 0.2006
每个DataOwner应该贡献数据比例 xn_list = [0.2334425389457544, 0.22736307113374135, 0.1924557494370519, 0.23876374822508903, 0.21019139629777164, 0.24076661951192962, 0.27293905202045776, 0.07950922940371608, 0.24853116297603844, 0.2005653561934618]
ModelOwner的最大效用 U(Eta) = 1.6003
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 2.4494
DataOwner1的最优x_1 = 0.2383
DataOwner2的最优x_2 = 0.2321
DataOwner3的最优x_3 = 0.1965
DataOwner4的最优x_4 = 0.2437
DataOwner5的最优x_5 = 0.2146
DataOwner6的最优x_6 = 0.2458
DataOwner7的最优x_7 = 0.2786
DataOwner8的最优x_8 = 0.0812
DataOwner9的最优x_9 = 0.2537
DataOwner10的最优x_10 = 0.2047
每个DataOwner应该贡献数据比例 xn_list = [0.2383070456496368, 0.23210089307951176, 0.19646616798290745, 0.24373913899687277, 0.21457139260337257, 0.24578374638275388, 0.27862659234615306, 0.08116605331040048, 0.2537100884381778, 0.20474476379976075]
ModelOwner的最大效用 U(Eta) = 1.6570
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 2.4994
DataOwner1的最优x_1 = 0.2432
DataOwner2的最优x_2 = 0.2368
DataOwner3的最优x_3 = 0.2005
DataOwner4的最优x_4 = 0.2487
DataOwner5的最优x_5 = 0.2190
DataOwner6的最优x_6 = 0.2508
DataOwner7的最优x_7 = 0.2843
DataOwner8的最优x_8 = 0.0828
DataOwner9的最优x_9 = 0.2589
DataOwner10的最优x_10 = 0.2089
每个DataOwner应该贡献数据比例 xn_list = [0.24317163445900078, 0.2368387949890716, 0.20047665417986452, 0.24871461374506018, 0.21895146282040134, 0.25080095793282425, 0.2843142286183916, 0.08282290518305538, 0.25888910130869297, 0.2089242419197737]
ModelOwner的最大效用 U(Eta) = 1.7143
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 2.5494
DataOwner1的最优x_1 = 0.2480
DataOwner2的最优x_2 = 0.2416
DataOwner3的最优x_3 = 0.2045
DataOwner4的最优x_4 = 0.2537
DataOwner5的最优x_5 = 0.2233
DataOwner6的最优x_6 = 0.2558
DataOwner7的最优x_7 = 0.2900
DataOwner8的最优x_8 = 0.0845
DataOwner9的最优x_9 = 0.2641
DataOwner10的最优x_10 = 0.2131
每个DataOwner应该贡献数据比例 xn_list = [0.24803655054176207, 0.24157701576250676, 0.20448741013194083, 0.2536904233655548, 0.22333182774200255, 0.25581850714300935, 0.2900022475582052, 0.0844798685612281, 0.26406846268648687, 0.2131040012577763]
ModelOwner的最大效用 U(Eta) = 1.7724
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 2.5994
DataOwner1的最优x_1 = 0.2529
DataOwner2的最优x_2 = 0.2463
DataOwner3的最优x_3 = 0.2085
DataOwner4的最优x_4 = 0.2587
DataOwner5的最优x_5 = 0.2277
DataOwner6的最优x_6 = 0.2608
DataOwner7的最优x_7 = 0.2957
DataOwner8的最优x_8 = 0.0861
DataOwner9的最优x_9 = 0.2692
DataOwner10的最优x_10 = 0.2173
每个DataOwner应该贡献数据比例 xn_list = [0.2529008649409812, 0.24631465029706556, 0.2084976701955895, 0.25866561730826465, 0.2277116508684815, 0.2608354355544474, 0.29568956302130106, 0.08613662693099353, 0.26924718333253284, 0.2172832436072527]
ModelOwner的最大效用 U(Eta) = 1.8311
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 2.6494
DataOwner1的最优x_1 = 0.2578
DataOwner2的最优x_2 = 0.2511
DataOwner3的最优x_3 = 0.2125
DataOwner4的最优x_4 = 0.2636
DataOwner5的最优x_5 = 0.2321
DataOwner6的最优x_6 = 0.2659
DataOwner7的最优x_7 = 0.3014
DataOwner8的最优x_8 = 0.0878
DataOwner9的最优x_9 = 0.2744
DataOwner10的最优x_10 = 0.2215
每个DataOwner应该贡献数据比例 xn_list = [0.25776556783985405, 0.251052663328736, 0.21250825047136154, 0.26364120874729086, 0.23209182382157545, 0.2658527647782272, 0.30137733271258266, 0.08779351766149576, 0.27442631767046316, 0.22146281976431087]
ModelOwner的最大效用 U(Eta) = 1.8905
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 2.6994
DataOwner1的最优x_1 = 0.2626
DataOwner2的最优x_2 = 0.2558
DataOwner3的最优x_3 = 0.2165
DataOwner4的最优x_4 = 0.2686
DataOwner5的最优x_5 = 0.2365
DataOwner6的最优x_6 = 0.2709
DataOwner7的最优x_7 = 0.3071
DataOwner8的最优x_8 = 0.0895
DataOwner9的最优x_9 = 0.2796
DataOwner10的最优x_10 = 0.2256
每个DataOwner应该贡献数据比例 xn_list = [0.2626298870788646, 0.25579030268309383, 0.21651851445137432, 0.26861640777758333, 0.23647165131858403, 0.27086969829426155, 0.30706465382690423, 0.08945027771618609, 0.27960504354095794, 0.22564206628777722]
ModelOwner的最大效用 U(Eta) = 1.9506
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 2.7494
DataOwner1的最优x_1 = 0.2675
DataOwner2的最优x_2 = 0.2605
DataOwner3的最优x_3 = 0.2205
DataOwner4的最优x_4 = 0.2736
DataOwner5的最优x_5 = 0.2409
DataOwner6的最优x_6 = 0.2759
DataOwner7的最优x_7 = 0.3128
DataOwner8的最优x_8 = 0.0911
DataOwner9的最优x_9 = 0.2848
DataOwner10的最优x_10 = 0.2298
每个DataOwner应该贡献数据比例 xn_list = [0.26749452558366316, 0.26052825300273763, 0.22052904163677778, 0.2735919333575943, 0.24085176629779373, 0.27588696110642175, 0.31275234823402814, 0.09110714651681764, 0.284784109328429, 0.22982158712523376]
ModelOwner的最大效用 U(Eta) = 2.0113
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 2.7994
DataOwner1的最优x_1 = 0.2724
DataOwner2的最优x_2 = 0.2653
DataOwner3的最优x_3 = 0.2245
DataOwner4的最优x_4 = 0.2786
DataOwner5的最优x_5 = 0.2452
DataOwner6的最优x_6 = 0.2809
DataOwner7的最优x_7 = 0.3184
DataOwner8的最优x_8 = 0.0928
DataOwner9的最优x_9 = 0.2900
DataOwner10的最优x_10 = 0.2340
每个DataOwner应该贡献数据比例 xn_list = [0.27235913687892493, 0.2652661767993096, 0.22453954636414417, 0.2785674310959823, 0.2452318567415111, 0.28090419584425314, 0.31844001078803696, 0.09276400604639061, 0.28996314612726615, 0.23400108454932245]
ModelOwner的最大效用 U(Eta) = 2.0727
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 2.8494
DataOwner1的最优x_1 = 0.2772
DataOwner2的最优x_2 = 0.2700
DataOwner3的最优x_3 = 0.2286
DataOwner4的最优x_4 = 0.2835
DataOwner5的最优x_5 = 0.2496
DataOwner6的最优x_6 = 0.2859
DataOwner7的最优x_7 = 0.3241
DataOwner8的最优x_8 = 0.0944
DataOwner9的最优x_9 = 0.2951
DataOwner10的最优x_10 = 0.2382
每个DataOwner应该贡献数据比例 xn_list = [0.2772237373874205, 0.27000409012946425, 0.22855004224725453, 0.2835429178293638, 0.24961193752954489, 0.28592141948582156, 0.324127660795617, 0.09442086190887529, 0.29514217147934696, 0.23818057276099405]
ModelOwner的最大效用 U(Eta) = 2.1347
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 2.8994
DataOwner1的最优x_1 = 0.2821
DataOwner2的最优x_2 = 0.2747
DataOwner3的最优x_3 = 0.2326
DataOwner4的最优x_4 = 0.2885
DataOwner5的最优x_5 = 0.2540
DataOwner6的最优x_6 = 0.2909
DataOwner7的最优x_7 = 0.3298
DataOwner8的最优x_8 = 0.0961
DataOwner9的最优x_9 = 0.3003
DataOwner10的最优x_10 = 0.2424
每个DataOwner应该贡献数据比例 xn_list = [0.28208833237746866, 0.2747419980415805, 0.2325605334265279, 0.28851839886179426, 0.253992013211954, 0.2909386373614819, 0.3298153041875435, 0.09607771588790168, 0.30032119093400544, 0.24236005614379724]
ModelOwner的最大效用 U(Eta) = 2.1973
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 2.9494
DataOwner1的最优x_1 = 0.2870
DataOwner2的最优x_2 = 0.2795
DataOwner3的最优x_3 = 0.2366
DataOwner4的最优x_4 = 0.2935
DataOwner5的最优x_5 = 0.2584
DataOwner6的最优x_6 = 0.2960
DataOwner7的最优x_7 = 0.3355
DataOwner8的最优x_8 = 0.0977
DataOwner9的最优x_9 = 0.3055
DataOwner10的最优x_10 = 0.2465
每个DataOwner应该贡献数据比例 xn_list = [0.2869529233970072, 0.2794799021469923, 0.23657102144138825, 0.2934938760008827, 0.2583720855793315, 0.2959558512288694, 0.3355029431058524, 0.097734568536387, 0.3055002062072836, 0.246539536171279]
ModelOwner的最大效用 U(Eta) = 2.2605
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 2.9994
DataOwner1的最优x_1 = 0.2918
DataOwner2的最优x_2 = 0.2842
DataOwner3的最优x_3 = 0.2406
DataOwner4的最优x_4 = 0.2985
DataOwner5的最优x_5 = 0.2628
DataOwner6的最优x_6 = 0.3010
DataOwner7的最优x_7 = 0.3412
DataOwner8的最优x_8 = 0.0994
DataOwner9的最优x_9 = 0.3107
DataOwner10的最优x_10 = 0.2507
每个DataOwner应该贡献数据比例 xn_list = [0.2918176720409344, 0.28421795968644153, 0.24058163955592166, 0.2984695141358462, 0.26275229965070934, 0.30097322761583056, 0.3411907663492979, 0.0993914748159312, 0.31067938915795473, 0.25071915168249953]
ModelOwner的最大效用 U(Eta) = 2.3243
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 3.0494
DataOwner1的最优x_1 = 0.2967
DataOwner2的最优x_2 = 0.2890
DataOwner3的最优x_3 = 0.2446
DataOwner4的最优x_4 = 0.3034
DataOwner5的最优x_5 = 0.2671
DataOwner6的最优x_6 = 0.3060
DataOwner7的最优x_7 = 0.3469
DataOwner8的最优x_8 = 0.1010
DataOwner9的最优x_9 = 0.3159
DataOwner10的最优x_10 = 0.2549
每个DataOwner应该贡献数据比例 xn_list = [0.2966821522170928, 0.2889557558066819, 0.24459203625365572, 0.3034448777622578, 0.26713227208889156, 0.3059903271330459, 0.34687827566794727, 0.10104828968738057, 0.3158582864020796, 0.25489853650497546]
ModelOwner的最大效用 U(Eta) = 2.3888
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 3.0994
DataOwner1的最优x_1 = 0.3015
DataOwner2的最优x_2 = 0.2937
DataOwner3的最优x_3 = 0.2486
DataOwner4的最优x_4 = 0.3084
DataOwner5的最优x_5 = 0.2715
DataOwner6的最优x_6 = 0.3110
DataOwner7的最优x_7 = 0.3526
DataOwner8的最优x_8 = 0.1027
DataOwner9的最优x_9 = 0.3210
DataOwner10的最优x_10 = 0.2591
每个DataOwner应该贡献数据比例 xn_list = [0.3015467459366524, 0.2936936625006318, 0.24860252651895468, 0.308420357532484, 0.27151234673576197, 0.3110075437486769, 0.35256591770700557, 0.10270514323152861, 0.32103730450212975, 0.2590780188580954]
ModelOwner的最大效用 U(Eta) = 2.4538
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 3.1494
DataOwner1的最优x_1 = 0.3064
DataOwner2的最优x_2 = 0.2984
DataOwner3的最优x_3 = 0.2526
DataOwner4的最优x_4 = 0.3134
DataOwner5的最优x_5 = 0.2759
DataOwner6的最优x_6 = 0.3160
DataOwner7的最优x_7 = 0.3583
DataOwner8的最优x_8 = 0.1044
DataOwner9的最优x_9 = 0.3262
DataOwner10的最优x_10 = 0.2633
每个DataOwner应该贡献数据比例 xn_list = [0.3064112679118817, 0.29843149933724183, 0.25261295766080694, 0.31339576394427193, 0.27589235680471286, 0.3160246863745962, 0.35825347589064593, 0.10436197234404924, 0.32621624624335593, 0.26325743958675074]
ModelOwner的最大效用 U(Eta) = 2.5194
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 3.1994
DataOwner1的最优x_1 = 0.3113
DataOwner2的最优x_2 = 0.3032
DataOwner3的最优x_3 = 0.2566
DataOwner4的最优x_4 = 0.3184
DataOwner5的最优x_5 = 0.2803
DataOwner6的最优x_6 = 0.3210
DataOwner7的最优x_7 = 0.3639
DataOwner8的最优x_8 = 0.1060
DataOwner9的最优x_9 = 0.3314
DataOwner10的最优x_10 = 0.2674
每个DataOwner应该贡献数据比例 xn_list = [0.3112758608819099, 0.30316940516323054, 0.2566234474190426, 0.3183712428592315, 0.2802724308331449, 0.32104190214123024, 0.36394111709205945, 0.10601882563599443, 0.33139526353917065, 0.26743692141571895]
ModelOwner的最大效用 U(Eta) = 2.5857
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 3.2494
DataOwner1的最优x_1 = 0.3161
DataOwner2的最优x_2 = 0.3079
DataOwner3的最优x_3 = 0.2606
DataOwner4的最优x_4 = 0.3233
DataOwner5的最优x_5 = 0.2847
DataOwner6的最优x_6 = 0.3261
DataOwner7的最优x_7 = 0.3696
DataOwner8的最优x_8 = 0.1077
DataOwner9的最优x_9 = 0.3366
DataOwner10的最优x_10 = 0.2716
每个DataOwner应该贡献数据比例 xn_list = [0.31614045943814506, 0.30790731697259127, 0.26063394213709634, 0.32334672816914933, 0.28465250997607067, 0.32605912389364694, 0.3696287651297909, 0.10767568088311417, 0.33657428724047944, 0.2716164080555286]
ModelOwner的最大效用 U(Eta) = 2.6524
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 3.2994
DataOwner1的最优x_1 = 0.3210
DataOwner2的最优x_2 = 0.3126
DataOwner3的最优x_3 = 0.2646
DataOwner4的最优x_4 = 0.3283
DataOwner5的最优x_5 = 0.2890
DataOwner6的最优x_6 = 0.3311
DataOwner7的最优x_7 = 0.3753
DataOwner8的最优x_8 = 0.1093
DataOwner9的最优x_9 = 0.3418
DataOwner10的最优x_10 = 0.2758
每个DataOwner应该贡献数据比例 xn_list = [0.3210050604651885, 0.31264523049994297, 0.26464443782943947, 0.3283222148880493, 0.28903259098693773, 0.3310763479683029, 0.37531641530707754, 0.10933253685355306, 0.3417533126533187, 0.2757958964231785]
ModelOwner的最大效用 U(Eta) = 2.7198
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 3.3494
DataOwner1的最优x_1 = 0.3259
DataOwner2的最优x_2 = 0.3174
DataOwner3的最优x_3 = 0.2687
DataOwner4的最优x_4 = 0.3333
DataOwner5的最优x_5 = 0.2934
DataOwner6的最优x_6 = 0.3361
DataOwner7的最优x_7 = 0.3810
DataOwner8的最优x_8 = 0.1110
DataOwner9的最优x_9 = 0.3469
DataOwner10的最优x_10 = 0.2800
每个DataOwner应该贡献数据比例 xn_list = [0.32586966033792014, 0.31738314318429445, 0.2686549331685719, 0.33329770095511124, 0.293412671177155, 0.33609357093467046, 0.381004064542142, 0.11098939249383206, 0.3469323373033817, 0.2799753840641674]
ModelOwner的最大效用 U(Eta) = 2.7877
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 3.3994
DataOwner1的最优x_1 = 0.3307
DataOwner2的最优x_2 = 0.3221
DataOwner3的最优x_3 = 0.2727
DataOwner4的最优x_4 = 0.3383
DataOwner5的最优x_5 = 0.2978
DataOwner6的最优x_6 = 0.3411
DataOwner7的最优x_7 = 0.3867
DataOwner8的最优x_8 = 0.1126
DataOwner9的最优x_9 = 0.3521
DataOwner10的最优x_10 = 0.2842
每个DataOwner应该贡献数据比例 xn_list = [0.3307339273749705, 0.3221207321458008, 0.2726651561511496, 0.33827284706367267, 0.29779245198815424, 0.3411104499012071, 0.3866913255965068, 0.11264613479635129, 0.3521110089011518, 0.284154585420522]
ModelOwner的最大效用 U(Eta) = 2.8562
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 3.4494
DataOwner1的最优x_1 = 0.3356
DataOwner2的最优x_2 = 0.3269
DataOwner3的最优x_3 = 0.2767
DataOwner4的最优x_4 = 0.3432
DataOwner5的最优x_5 = 0.3022
DataOwner6的最优x_6 = 0.3461
DataOwner7的最优x_7 = 0.3924
DataOwner8的最优x_8 = 0.1143
DataOwner9的最优x_9 = 0.3573
DataOwner10的最优x_10 = 0.2883
每个DataOwner应该贡献数据比例 xn_list = [0.3355988403613408, 0.326858951357423, 0.2766759078276773, 0.3432486555370529, 0.30217281493079634, 0.346127996811089, 0.39237934120632495, 0.1143030974132284, 0.3572903661116759, 0.2883343434020731]
ModelOwner的最大效用 U(Eta) = 2.9252
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 3.4994
DataOwner1的最优x_1 = 0.3405
DataOwner2的最优x_2 = 0.3316
DataOwner3的最优x_3 = 0.2807
DataOwner4的最优x_4 = 0.3482
DataOwner5的最优x_5 = 0.3066
DataOwner6的最优x_6 = 0.3511
DataOwner7的最优x_7 = 0.3981
DataOwner8的最优x_8 = 0.1160
DataOwner9的最优x_9 = 0.3625
DataOwner10的最优x_10 = 0.2925
每个DataOwner应该贡献数据比例 xn_list = [0.34046342595107776, 0.331596848125387, 0.2806863911466937, 0.34822412436497424, 0.30655288111472984, 0.3511452047426537, 0.39806697248054485, 0.11595994783098831, 0.36246937505275956, 0.2925138177673949]
ModelOwner的最大效用 U(Eta) = 2.9947
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 3.5494
DataOwner1的最优x_1 = 0.3453
DataOwner2的最优x_2 = 0.3363
DataOwner3的最优x_3 = 0.2847
DataOwner4的最优x_4 = 0.3532
DataOwner5的最优x_5 = 0.3109
DataOwner6的最优x_6 = 0.3562
DataOwner7的最优x_7 = 0.4038
DataOwner8的最优x_8 = 0.1176
DataOwner9的最优x_9 = 0.3676
DataOwner10的最优x_10 = 0.2967
每个DataOwner应该贡献数据比例 xn_list = [0.345328003173091, 0.33633473875221964, 0.2846968678108396, 0.3531995872610741, 0.31093294090738716, 0.35616240434330604, 0.4037545952308306, 0.11761679575629827, 0.3676483755902194, 0.2966932859470418]
ModelOwner的最大效用 U(Eta) = 3.0648
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 3.5994
DataOwner1的最优x_1 = 0.3502
DataOwner2的最优x_2 = 0.3411
DataOwner3的最优x_3 = 0.2887
DataOwner4的最优x_4 = 0.3582
DataOwner5的最优x_5 = 0.3153
DataOwner6的最优x_6 = 0.3612
DataOwner7的最优x_7 = 0.4094
DataOwner8的最优x_8 = 0.1193
DataOwner9的最优x_9 = 0.3728
DataOwner10的最优x_10 = 0.3009
每个DataOwner应该贡献数据比例 xn_list = [0.3501925805134497, 0.34107262949315514, 0.2887073445724736, 0.35817505027914764, 0.31531300080596025, 0.36117960406581895, 0.4094422181215866, 0.11927364372249935, 0.3728273762523673, 0.3008727542266268]
ModelOwner的最大效用 U(Eta) = 3.1355
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 3.6494
DataOwner1的最优x_1 = 0.3551
DataOwner2的最优x_2 = 0.3458
DataOwner3的最优x_3 = 0.2927
DataOwner4的最优x_4 = 0.3632
DataOwner5的最优x_5 = 0.3197
DataOwner6的最优x_6 = 0.3662
DataOwner7的最优x_7 = 0.4151
DataOwner8的最优x_8 = 0.1209
DataOwner9的最优x_9 = 0.3780
DataOwner10的最优x_10 = 0.3051
每个DataOwner应该贡献数据比例 xn_list = [0.355057159290916, 0.34581052163381515, 0.2927178225192262, 0.36315051476772575, 0.3196930619982946, 0.3661968052702438, 0.4151298426903553, 0.12093049217737863, 0.37800637844414214, 0.30505222374242175]
ModelOwner的最大效用 U(Eta) = 3.2066
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 3.6994
DataOwner1的最优x_1 = 0.3599
DataOwner2的最优x_2 = 0.3505
DataOwner3的最优x_3 = 0.2967
DataOwner4的最优x_4 = 0.3681
DataOwner5的最优x_5 = 0.3241
DataOwner6的最优x_6 = 0.3712
DataOwner7的最优x_7 = 0.4208
DataOwner8的最优x_8 = 0.1226
DataOwner9的最优x_9 = 0.3832
DataOwner10的最优x_10 = 0.3092
每个DataOwner应该贡献数据比例 xn_list = [0.35992173956839346, 0.35054841523287933, 0.2967283017007997, 0.36812598078914777, 0.32407312454298837, 0.3712140080222811, 0.420817469010204, 0.12258734114252659, 0.38318538223432663, 0.3092316945501262]
ModelOwner的最大效用 U(Eta) = 3.2783
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 3.7494
DataOwner1的最优x_1 = 0.3648
DataOwner2的最优x_2 = 0.3553
DataOwner3的最优x_3 = 0.3007
DataOwner4的最优x_4 = 0.3731
DataOwner5的最优x_5 = 0.3285
DataOwner6的最优x_6 = 0.3762
DataOwner7的最优x_7 = 0.4265
DataOwner8的最优x_8 = 0.1242
DataOwner9的最优x_9 = 0.3884
DataOwner10的最优x_10 = 0.3134
每个DataOwner应该贡献数据比例 xn_list = [0.36478627319053847, 0.35528626339325176, 0.30073874242173493, 0.37310139908779505, 0.32845314507687395, 0.3762311626474455, 0.42650504078333024, 0.12424417421655806, 0.3883643363481377, 0.31341112526485704]
ModelOwner的最大效用 U(Eta) = 3.3505
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 3.7994
DataOwner1的最优x_1 = 0.3697
DataOwner2的最优x_2 = 0.3600
DataOwner3的最优x_3 = 0.3047
DataOwner4的最优x_4 = 0.3781
DataOwner5的最优x_5 = 0.3328
DataOwner6的最优x_6 = 0.3812
DataOwner7的最优x_7 = 0.4322
DataOwner8的最优x_8 = 0.1259
DataOwner9的最优x_9 = 0.3935
DataOwner10的最优x_10 = 0.3176
每个DataOwner应该贡献数据比例 xn_list = [0.3696507516155926, 0.36002405765342393, 0.3047491376465754, 0.3780767609443184, 0.33283311587115405, 0.38124826021699904, 0.43219254795982975, 0.125900988479496, 0.393543231802702, 0.31759050854236975]
ModelOwner的最大效用 U(Eta) = 3.4232
DONE
---------------------------------- 定义参数值 ----------------------------------
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 3.8494
DataOwner1的最优x_1 = 0.3745
DataOwner2的最优x_2 = 0.3648
DataOwner3的最优x_3 = 0.3088
DataOwner4的最优x_4 = 0.3831
DataOwner5的最优x_5 = 0.3372
DataOwner6的最优x_6 = 0.3863
DataOwner7的最优x_7 = 0.4379
DataOwner8的最优x_8 = 0.1276
DataOwner9的最优x_9 = 0.3987
DataOwner10的最优x_10 = 0.3218
每个DataOwner应该贡献数据比例 xn_list = [0.3745158049254095, 0.36476241209605637, 0.3087600066714011, 0.3830527106764558, 0.3372136044907578, 0.3862659506649768, 0.4378807273738268, 0.12755799855665714, 0.39872273918722867, 0.32177038573010286]
ModelOwner的最大效用 U(Eta) = 3.4964
DONE
最终的列表 alpha_U_Eta_list：
[-0.14677836719699666, -0.14077523142023157, -0.13477209564346648, -0.1287689598667014, -0.12276582408993633, -0.11676268831317124, -0.11075955253640615, -0.10475641675964106, -0.09875328098287599, -0.09275014520611091, -0.08674700942934582, -0.08074387365258073, -0.07474073787581564, -0.06873760209905057, -0.06273446632228548, -0.0567313305455204, -0.050728194768755314, -0.044725058991990224, -0.03872192321522515, -0.03271878743846006, -0.026715651661694984, -0.020712515884929894, -0.014709380108164805, -0.008706244331399715, -0.0027031085546346256, 0.002539411075359488, 0.005981188563049161, 0.016359600147988984, 0.025265651898180225, 0.03595779774437596, 0.04837442254011748, 0.062343256628922594, 0.07815480857563928, 0.09445466415135995, 0.11418926917120165, 0.13443499153585348, 0.15610947976148204, 0.17917303554296782, 0.2035881072420881, 0.2293191203260233, 0.2563323252113848, 0.28459566033868466, 0.3140786286010282, 0.34475218555812626, 0.376588638047757, 0.40956155202203326, 0.44364566857474586, 0.47881682727895813, 0.5150518960501209, 0.5523287068549241, 0.5906259966637168, 0.6299233531566919, 0.6702011646269876, 0.7114405738421248, 0.7536234352606626, 0.7967322755373674, 0.8407502568165495, 0.8856611426980332, 0.9314492665694691, 0.9780995020875347, 1.0255972358690209, 1.073928341561896, 1.1230791560642284, 1.173036457168735, 1.2237874423964623, 1.2753197094177628, 1.3276212375909449, 1.3806803704387218, 1.4344857996889062, 1.489026549561534, 1.5442919624847335, 1.6002716854195778, 1.656955656992095, 1.7143340953364041, 1.7723974866219088, 1.831136574127755, 1.8905423480652663, 1.950606035725388, 2.0113190922887823, 2.072673192055271, 2.1346602201168876, 2.1972722644559797, 2.2605016084270857, 2.324340723609899, 2.388782263006847, 2.4538190545597005, 2.5194440949769024, 2.585650543821221, 2.6524317180051917, 2.719781086227628, 2.7876922640167123, 2.856159008794478, 2.925175215161823, 2.9947349105259424, 3.0648322506432395, 3.135461515688161, 3.2066171062421853, 3.2782935395484154, 3.3504854459037228, 3.4231875651950263, 3.4963947435816882]
最终的列表 alpha_U_qn_list：
[0.0015591689481341542, 0.0015591689481341542, 0.0015591689481341542, 0.0015591689481341542, 0.0015591689481341542, 0.0015591689481341542, 0.0015591689481341542, 0.0015591689481341542, 0.0015591689481341542, 0.0015591689481341542, 0.0015591689481341542, 0.0015591689481341542, 0.0015591689481341542, 0.0015591689481341542, 0.0015591689481341542, 0.0015591689481341542, 0.0015591689481341542, 0.0015591689481341542, 0.0015591689481341542, 0.0015591689481341542, 0.0015591689481341542, 0.0015591689481341542, 0.0015591689481341542, 0.0015591689481341542, 0.0015591689481341542, 0.0017469837655601866, 0.0025983932825418667, 0.0021182182595190836, 0.00264934347499515, 0.0031804793768854867, 0.0037121439830679836, 0.004443920725456019, 0.004773888381226127, 0.005909612855949359, 0.005836115095315925, 0.006367271938218522, 0.006898395671436996, 0.007429513825720324, 0.007960655787987792, 0.008491790093792095, 0.009022911432726832, 0.009554044638298276, 0.010085175577265649, 0.010616302370946417, 0.011147447011960132, 0.01167856976896715, 0.012209697866300929, 0.012740831900683047, 0.013271973432610773, 0.013803110096067361, 0.014334204827372733, 0.014865363422275225, 0.015396484446572956, 0.01592759556329699, 0.016458733042692165, 0.016989867384592162, 0.01752099974653103, 0.018052130884917326, 0.018583260878971775, 0.019114405252124555, 0.019645526283574855, 0.02017665231139454, 0.020707773467723546, 0.02123890507342354, 0.0217700473115362, 0.02230117599525383, 0.02283227057616095, 0.023363436073291322, 0.023894566008498284, 0.024425685143752672, 0.02495682204601879, 0.02548796421909425, 0.02601908581139285, 0.02655021636657944, 0.02708138265947424, 0.027612483250292995, 0.028143626263095457, 0.02867472738607151, 0.02920586336856581, 0.029736996377742075, 0.03026812821354463, 0.03079925943798778, 0.03133039024051602, 0.03186153824670215, 0.03239265694368929, 0.03292378803582818, 0.033454911296609116, 0.03398604230898683, 0.03451717396045924, 0.03504830581541283, 0.035579437579517315, 0.036110533056886406, 0.03664169906803725, 0.03717282915841049, 0.03770395844932865, 0.0382350877531092, 0.038766217213816165, 0.039297346838321624, 0.039828471368556294, 0.040359589869039515, 0.040890771138652984]
**** log-parameter_analysis 运行时间： 2025-02-10 22:31:40 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/initial/mnist_cnn_initial_model
Model saved to D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.057034378570483116
DataOwner2: noise random: 0.007135665369946854
DataOwner3: noise random: 0.007837088727799047
DataOwner4: noise random: 0.03169105890965286
DataOwner5: noise random: 0.0408933379515371
DataOwner6: noise random: 0.06316966735104547
DataOwner7: noise random: 0.07235792346507247
DataOwner8: noise random: 0.05620953730362241
DataOwner9: noise random: 0.0274685897642302
DataOwner10: noise random: 0.056914965655936234
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9986854515626515, 0.9999793431740112, 0.9999750154819679, 0.9995935669581084, 0.9993246572429144, 0.9983862588193292, 0.997879231428616, 0.998720962767666, 0.9996946223536863, 0.998689533949942]
归一化后的数据质量列表avg_f_list: [0.938389392174169, 1.0, 0.9997939303919056, 0.9816306814745157, 0.9688261382980095, 0.92414287676953, 0.9, 0.9400803119593806, 0.9864425871171321, 0.9385837812251036]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3386
DataOwner1的最优x_1 = 0.1013
DataOwner2的最优x_2 = 0.1602
DataOwner3的最优x_3 = 0.1600
DataOwner4的最优x_4 = 0.1443
DataOwner5的最优x_5 = 0.1324
DataOwner6的最优x_6 = 0.0852
DataOwner7的最优x_7 = 0.0555
DataOwner8的最优x_8 = 0.1031
DataOwner9的最优x_9 = 0.1486
DataOwner10的最优x_10 = 0.1015
每个DataOwner应该贡献数据比例 xn_list = [0.10127874756249833, 0.16019088987523847, 0.16001933433727586, 0.14427156100974212, 0.13238784925771924, 0.08519989011620244, 0.05548024258905641, 0.10311990111327309, 0.1485667701328675, 0.10149111664664573]
ModelOwner的最大效用 U(Eta) = 0.5780
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3385693389787798
DataOwner1的分配到的支付 ： 0.1104
DataOwner2的分配到的支付 ： 0.1861
DataOwner3的分配到的支付 ： 0.1858
DataOwner4的分配到的支付 ： 0.1645
DataOwner5的分配到的支付 ： 0.1490
DataOwner6的分配到的支付 ： 0.0914
DataOwner7的分配到的支付 ： 0.0580
DataOwner8的分配到的支付 ： 0.1126
DataOwner9的分配到的支付 ： 0.1702
DataOwner10的分配到的支付 ： 0.1106
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC10', 'DataOwner2': 'CPC4', 'DataOwner6': 'CPC8', 'DataOwner9': 'CPC9', 'DataOwner3': 'CPC3', 'DataOwner4': 'CPC5', 'DataOwner5': 'CPC6', 'DataOwner10': 'CPC7', 'DataOwner7': 'CPC2', 'DataOwner8': 'CPC1'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC10
DataOwner2 把数据交给 CPC4
DataOwner6 把数据交给 CPC8
DataOwner9 把数据交给 CPC9
DataOwner3 把数据交给 CPC3
DataOwner4 把数据交给 CPC5
DataOwner5 把数据交给 CPC6
DataOwner10 把数据交给 CPC7
DataOwner7 把数据交给 CPC2
DataOwner8 把数据交给 CPC1
DONE
最终Um的列表：
[('DataOwner1', 'CPC10', 1.0, 145.0311665094976, 145, -1.3877787807814457e-17), ('DataOwner2', 'CPC4', 0.4, 1971.9645356331825, 228, 0.09611453392514308), ('DataOwner6', 'CPC8', 0.8, 754.0089562175597, 608, 0.01703997802324049), ('DataOwner9', 'CPC9', 0.9, 449.52006136677045, 424, 0.014856677013286745), ('DataOwner3', 'CPC3', 0.3, 2276.453430485873, 228, 0.1120135340360931), ('DataOwner4', 'CPC5', 0.5, 1667.4756407788568, 2885, 0.07213578050487106), ('DataOwner5', 'CPC6', 0.6, 1362.9867459251616, 1512, 0.0529551397030877), ('DataOwner10', 'CPC7', 0.7, 1058.4978510708188, 1014, 0.030447334993993727), ('DataOwner7', 'CPC2', 0.2, 2580.9423253397435, 158, 0.04438419407124512), ('DataOwner8', 'CPC1', 0.1, 2885.4312201948424, 147, 0.09280791100194577)]
('DataOwner1', 'CPC10', 1.0, 145.0311665094976, 145, -1.3877787807814457e-17)
('DataOwner2', 'CPC4', 0.4, 1971.9645356331825, 228, 0.09611453392514308)
('DataOwner6', 'CPC8', 0.8, 754.0089562175597, 608, 0.01703997802324049)
('DataOwner9', 'CPC9', 0.9, 449.52006136677045, 424, 0.014856677013286745)
('DataOwner3', 'CPC3', 0.3, 2276.453430485873, 228, 0.1120135340360931)
('DataOwner4', 'CPC5', 0.5, 1667.4756407788568, 2885, 0.07213578050487106)
('DataOwner5', 'CPC6', 0.6, 1362.9867459251616, 1512, 0.0529551397030877)
('DataOwner10', 'CPC7', 0.7, 1058.4978510708188, 1014, 0.030447334993993727)
('DataOwner7', 'CPC2', 0.2, 2580.9423253397435, 158, 0.04438419407124512)
('DataOwner8', 'CPC1', 0.1, 2885.4312201948424, 147, 0.09280791100194577)
**** log-parameter_analysis 运行时间： 2025-02-10 22:32:44 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/initial/mnist_cnn_initial_model
Model saved to D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/mnist_cnn_model
DONE
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.022687057108548927
DataOwner2: noise random: 0.042148889619390985
DataOwner3: noise random: 0.020232481120566415
DataOwner4: noise random: 0.09264150363690846
DataOwner5: noise random: 0.06320057069035231
DataOwner6: noise random: 0.022383004206508284
DataOwner7: noise random: 0.0885112524724359
DataOwner8: noise random: 0.019723642256875108
DataOwner9: noise random: 0.04032475168087313
DataOwner10: noise random: 0.08309289056757672
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9997918524481907, 0.9992821527793715, 0.9998341047623024, 0.996526353394101, 0.9983809167678582, 0.9997975410141198, 0.9968416689218947, 0.9998430590928681, 0.9993414491619523, 0.9972034000612006]
归一化后的数据质量列表avg_f_list: [0.998456099234357, 0.9830884508774748, 0.9997300233611617, 0.9, 0.9559158255870162, 0.9986276117665421, 0.909506888956441, 1.0, 0.9848762604682649, 0.9204132271172337]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3468
DataOwner1的最优x_1 = 0.1538
DataOwner2的最优x_2 = 0.1401
DataOwner3的最优x_3 = 0.1549
DataOwner4的最优x_4 = 0.0474
DataOwner5的最优x_5 = 0.1135
DataOwner6的最优x_6 = 0.1540
DataOwner7的最优x_7 = 0.0598
DataOwner8的最优x_8 = 0.1552
DataOwner9的最优x_9 = 0.1418
DataOwner10的最优x_10 = 0.0734
每个DataOwner应该贡献数据比例 xn_list = [0.153831705354458, 0.14010880943178625, 0.15492846500946064, 0.047376720288317624, 0.11345945841786037, 0.15397972101648885, 0.05981333968344817, 0.1551601143049675, 0.14175311092574033, 0.07343912297837643]
ModelOwner的最大效用 U(Eta) = 0.5876
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3467828145487108
DataOwner1的分配到的支付 ： 0.1771
DataOwner2的分配到的支付 ： 0.1588
DataOwner3的分配到的支付 ： 0.1786
DataOwner4的分配到的支付 ： 0.0492
DataOwner5的分配到的支付 ： 0.1251
DataOwner6的分配到的支付 ： 0.1773
DataOwner7的分配到的支付 ： 0.0627
DataOwner8的分配到的支付 ： 0.1789
DataOwner9的分配到的支付 ： 0.1610
DataOwner10的分配到的支付 ： 0.0780
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC6', 'DataOwner3': 'CPC9', 'DataOwner4': 'CPC10', 'DataOwner2': 'CPC4', 'DataOwner9': 'CPC8', 'DataOwner5': 'CPC3', 'DataOwner6': 'CPC5', 'DataOwner10': 'CPC7', 'DataOwner7': 'CPC2', 'DataOwner8': 'CPC1'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC6
DataOwner3 把数据交给 CPC9
DataOwner4 把数据交给 CPC10
DataOwner2 把数据交给 CPC4
DataOwner9 把数据交给 CPC8
DataOwner5 把数据交给 CPC3
DataOwner6 把数据交给 CPC5
DataOwner10 把数据交给 CPC7
DataOwner7 把数据交给 CPC2
DataOwner8 把数据交给 CPC1
DONE
最终Um的列表：
[('DataOwner1', 'CPC6', 0.6, 1353.2837716595634, 551, 0.0615326821417832), ('DataOwner3', 'CPC9', 0.9, 433.761345937936, 416, 0.015492846500946067), ('DataOwner4', 'CPC10', 1.0, 127.25387069442114, 127, 0.0), ('DataOwner2', 'CPC4', 0.4, 1966.2987221424146, 2885, 0.08406528565907175), ('DataOwner9', 'CPC8', 0.8, 740.2688211775095, 634, 0.028350622185148056), ('DataOwner5', 'CPC3', 0.3, 2272.8061973824183, 304, 0.07942162089250226), ('DataOwner6', 'CPC5', 0.5, 1659.791246900808, 551, 0.07698986050824443), ('DataOwner10', 'CPC7', 0.7, 1046.776296418547, 1183, 0.022031736893512933), ('DataOwner7', 'CPC2', 0.2, 2579.3136726243815, 160, 0.04785067174675854), ('DataOwner8', 'CPC1', 0.1, 2885.821147866501, 138, 0.13964410287447077)]
('DataOwner1', 'CPC6', 0.6, 1353.2837716595634, 551, 0.0615326821417832)
('DataOwner3', 'CPC9', 0.9, 433.761345937936, 416, 0.015492846500946067)
('DataOwner4', 'CPC10', 1.0, 127.25387069442114, 127, 0.0)
('DataOwner2', 'CPC4', 0.4, 1966.2987221424146, 2885, 0.08406528565907175)
('DataOwner9', 'CPC8', 0.8, 740.2688211775095, 634, 0.028350622185148056)
('DataOwner5', 'CPC3', 0.3, 2272.8061973824183, 304, 0.07942162089250226)
('DataOwner6', 'CPC5', 0.5, 1659.791246900808, 551, 0.07698986050824443)
('DataOwner10', 'CPC7', 0.7, 1046.776296418547, 1183, 0.022031736893512933)
('DataOwner7', 'CPC2', 0.2, 2579.3136726243815, 160, 0.04785067174675854)
('DataOwner8', 'CPC1', 0.1, 2885.821147866501, 138, 0.13964410287447077)
**** log-parameter_analysis 运行时间： 2025-02-11 11:09:27 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
**** log-parameter_analysis 运行时间： 2025-02-11 11:10:01 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
**** log-parameter_analysis 运行时间： 2025-02-11 11:10:28 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Epoch 1/5 started...
Epoch [1/5], Batch [1/1], Loss: 4.9231
Epoch 1 completed. Average Loss: 4.9231
Epoch 2/5 started...
Epoch [2/5], Batch [1/1], Loss: 70.9829
Epoch 2 completed. Average Loss: 70.9829
Epoch 3/5 started...
Epoch [3/5], Batch [1/1], Loss: 59.4186
Epoch 3 completed. Average Loss: 59.4186
Epoch 4/5 started...
Epoch [4/5], Batch [1/1], Loss: 36.6876
Epoch 4 completed. Average Loss: 36.6876
Epoch 5/5 started...
Epoch [5/5], Batch [1/1], Loss: 15.9389
Epoch 5 completed. Average Loss: 15.9389
**** log-parameter_analysis 运行时间： 2025-02-11 11:11:03 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/initial/mnist_cnn_initial_model
Model saved to D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/mnist_cnn_model
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.04227823734790551
DataOwner2: noise random: 0.014978137102866652
DataOwner3: noise random: 0.005396927726359036
DataOwner4: noise random: 0.046859221928135665
DataOwner5: noise random: 0.07808597690290293
DataOwner6: noise random: 0.07853146634150689
DataOwner7: noise random: 0.04916941479154999
DataOwner8: noise random: 0.01624509199997277
DataOwner9: noise random: 0.08741561301758899
DataOwner10: noise random: 0.035335587132675365
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9992767993064403, 0.9999094343373216, 0.999988220640965, 0.9991104543007976, 0.9975321776701334, 0.9975052969537441, 0.9990196795640809, 0.9998932277687045, 0.9969080611921379, 0.9994954760254684]
归一化后的数据质量列表avg_f_list: [0.9769031004289199, 0.9974421355467997, 1.0, 0.9715025681380978, 0.9202624730428512, 0.9193897676899038, 0.9685554889941506, 0.9969159754928685, 0.9, 0.9840026263677917]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3452
DataOwner1的最优x_1 = 0.1354
DataOwner2的最优x_2 = 0.1539
DataOwner3的最优x_3 = 0.1561
DataOwner4的最优x_4 = 0.1302
DataOwner5的最优x_5 = 0.0747
DataOwner6的最优x_6 = 0.0736
DataOwner7的最优x_7 = 0.1274
DataOwner8的最优x_8 = 0.1535
DataOwner9的最优x_9 = 0.0489
DataOwner10的最优x_10 = 0.1420
每个DataOwner应该贡献数据比例 xn_list = [0.13539783656692503, 0.15392914872229624, 0.15612250771752548, 0.13024144190181497, 0.07466369891920499, 0.07360274937531217, 0.12737546148314022, 0.15347492133062943, 0.04892152227828695, 0.14199288356274561]
ModelOwner的最大效用 U(Eta) = 0.5858
Eta开始变化：
eta:0.0
eta:0.01
eta:0.02
eta:0.03
eta:0.04
eta:0.05
eta:0.06
eta:0.07
eta:0.08
eta:0.09
eta:0.1
eta:0.11
new: DataOwner1的最优x_1 = 0.0111
new: DataOwner2的最优x_2 = 0.0126
new: DataOwner3的最优x_3 = 0.0128
new: DataOwner4的最优x_4 = 0.0106
new: DataOwner5的最优x_5 = 0.0061
new: DataOwner6的最优x_6 = 0.0060
new: DataOwner7的最优x_7 = 0.0104
new: DataOwner8的最优x_8 = 0.0125
new: DataOwner9的最优x_9 = 0.0040
new: DataOwner10的最优x_10 = 0.0116
eta:0.12
eta:0.13
eta:0.14
eta:0.15
eta:0.16
eta:0.17
eta:0.18
eta:0.19
eta:0.2
new: DataOwner1的最优x_1 = 0.0201
new: DataOwner2的最优x_2 = 0.0229
new: DataOwner3的最优x_3 = 0.0232
new: DataOwner4的最优x_4 = 0.0194
new: DataOwner5的最优x_5 = 0.0111
new: DataOwner6的最优x_6 = 0.0109
new: DataOwner7的最优x_7 = 0.0189
new: DataOwner8的最优x_8 = 0.0228
new: DataOwner9的最优x_9 = 0.0073
new: DataOwner10的最优x_10 = 0.0211
eta:0.21
new: DataOwner1的最优x_1 = 0.0211
new: DataOwner2的最优x_2 = 0.0240
new: DataOwner3的最优x_3 = 0.0244
new: DataOwner4的最优x_4 = 0.0203
new: DataOwner5的最优x_5 = 0.0117
new: DataOwner6的最优x_6 = 0.0115
new: DataOwner7的最优x_7 = 0.0199
new: DataOwner8的最优x_8 = 0.0240
new: DataOwner9的最优x_9 = 0.0076
new: DataOwner10的最优x_10 = 0.0222
eta:0.22
new: DataOwner1的最优x_1 = 0.0221
new: DataOwner2的最优x_2 = 0.0252
new: DataOwner3的最优x_3 = 0.0255
new: DataOwner4的最优x_4 = 0.0213
new: DataOwner5的最优x_5 = 0.0122
new: DataOwner6的最优x_6 = 0.0120
new: DataOwner7的最优x_7 = 0.0208
new: DataOwner8的最优x_8 = 0.0251
new: DataOwner9的最优x_9 = 0.0080
new: DataOwner10的最优x_10 = 0.0232
eta:0.23
new: DataOwner1的最优x_1 = 0.0231
new: DataOwner2的最优x_2 = 0.0263
new: DataOwner3的最优x_3 = 0.0267
new: DataOwner4的最优x_4 = 0.0223
new: DataOwner5的最优x_5 = 0.0128
new: DataOwner6的最优x_6 = 0.0126
new: DataOwner7的最优x_7 = 0.0218
new: DataOwner8的最优x_8 = 0.0262
new: DataOwner9的最优x_9 = 0.0084
new: DataOwner10的最优x_10 = 0.0243
eta:0.24
new: DataOwner1的最优x_1 = 0.0242
new: DataOwner2的最优x_2 = 0.0275
new: DataOwner3的最优x_3 = 0.0279
new: DataOwner4的最优x_4 = 0.0232
new: DataOwner5的最优x_5 = 0.0133
new: DataOwner6的最优x_6 = 0.0131
new: DataOwner7的最优x_7 = 0.0227
new: DataOwner8的最优x_8 = 0.0274
new: DataOwner9的最优x_9 = 0.0087
new: DataOwner10的最优x_10 = 0.0253
eta:0.25
eta:0.26
eta:0.27
new: DataOwner1的最优x_1 = 0.0272
new: DataOwner2的最优x_2 = 0.0309
new: DataOwner3的最优x_3 = 0.0313
new: DataOwner4的最优x_4 = 0.0261
new: DataOwner5的最优x_5 = 0.0150
new: DataOwner6的最优x_6 = 0.0148
new: DataOwner7的最优x_7 = 0.0256
new: DataOwner8的最优x_8 = 0.0308
new: DataOwner9的最优x_9 = 0.0098
new: DataOwner10的最优x_10 = 0.0285
eta:0.28
new: DataOwner1的最优x_1 = 0.0282
new: DataOwner2的最优x_2 = 0.0320
new: DataOwner3的最优x_3 = 0.0325
new: DataOwner4的最优x_4 = 0.0271
new: DataOwner5的最优x_5 = 0.0155
new: DataOwner6的最优x_6 = 0.0153
new: DataOwner7的最优x_7 = 0.0265
new: DataOwner8的最优x_8 = 0.0319
new: DataOwner9的最优x_9 = 0.0102
new: DataOwner10的最优x_10 = 0.0296
eta:0.29
new: DataOwner1的最优x_1 = 0.0292
new: DataOwner2的最优x_2 = 0.0332
new: DataOwner3的最优x_3 = 0.0337
new: DataOwner4的最优x_4 = 0.0281
new: DataOwner5的最优x_5 = 0.0161
new: DataOwner6的最优x_6 = 0.0159
new: DataOwner7的最优x_7 = 0.0275
new: DataOwner8的最优x_8 = 0.0331
new: DataOwner9的最优x_9 = 0.0105
new: DataOwner10的最优x_10 = 0.0306
eta:0.3
eta:0.31
eta:0.32
eta:0.33
new: DataOwner1的最优x_1 = 0.0332
new: DataOwner2的最优x_2 = 0.0378
new: DataOwner3的最优x_3 = 0.0383
new: DataOwner4的最优x_4 = 0.0319
new: DataOwner5的最优x_5 = 0.0183
new: DataOwner6的最优x_6 = 0.0181
new: DataOwner7的最优x_7 = 0.0312
new: DataOwner8的最优x_8 = 0.0376
new: DataOwner9的最优x_9 = 0.0120
new: DataOwner10的最优x_10 = 0.0348
eta:0.34
new: DataOwner1的最优x_1 = 0.0342
new: DataOwner2的最优x_2 = 0.0389
new: DataOwner3的最优x_3 = 0.0395
new: DataOwner4的最优x_4 = 0.0329
new: DataOwner5的最优x_5 = 0.0189
new: DataOwner6的最优x_6 = 0.0186
new: DataOwner7的最优x_7 = 0.0322
new: DataOwner8的最优x_8 = 0.0388
new: DataOwner9的最优x_9 = 0.0124
new: DataOwner10的最优x_10 = 0.0359
eta:0.35000000000000003
new: DataOwner1的最优x_1 = 0.0352
new: DataOwner2的最优x_2 = 0.0400
new: DataOwner3的最优x_3 = 0.0406
new: DataOwner4的最优x_4 = 0.0339
new: DataOwner5的最优x_5 = 0.0194
new: DataOwner6的最优x_6 = 0.0191
new: DataOwner7的最优x_7 = 0.0331
new: DataOwner8的最优x_8 = 0.0399
new: DataOwner9的最优x_9 = 0.0127
new: DataOwner10的最优x_10 = 0.0369
eta:0.36
new: DataOwner1的最优x_1 = 0.0362
new: DataOwner2的最优x_2 = 0.0412
new: DataOwner3的最优x_3 = 0.0418
new: DataOwner4的最优x_4 = 0.0349
new: DataOwner5的最优x_5 = 0.0200
new: DataOwner6的最优x_6 = 0.0197
new: DataOwner7的最优x_7 = 0.0341
new: DataOwner8的最优x_8 = 0.0411
new: DataOwner9的最优x_9 = 0.0131
new: DataOwner10的最优x_10 = 0.0380
eta:0.37
new: DataOwner1的最优x_1 = 0.0372
new: DataOwner2的最优x_2 = 0.0423
new: DataOwner3的最优x_3 = 0.0429
new: DataOwner4的最优x_4 = 0.0358
new: DataOwner5的最优x_5 = 0.0205
new: DataOwner6的最优x_6 = 0.0202
new: DataOwner7的最优x_7 = 0.0350
new: DataOwner8的最优x_8 = 0.0422
new: DataOwner9的最优x_9 = 0.0135
new: DataOwner10的最优x_10 = 0.0391
eta:0.38
new: DataOwner1的最优x_1 = 0.0382
new: DataOwner2的最优x_2 = 0.0435
new: DataOwner3的最优x_3 = 0.0441
new: DataOwner4的最优x_4 = 0.0368
new: DataOwner5的最优x_5 = 0.0211
new: DataOwner6的最优x_6 = 0.0208
new: DataOwner7的最优x_7 = 0.0360
new: DataOwner8的最优x_8 = 0.0434
new: DataOwner9的最优x_9 = 0.0138
new: DataOwner10的最优x_10 = 0.0401
eta:0.39
eta:0.4
new: DataOwner1的最优x_1 = 0.0403
new: DataOwner2的最优x_2 = 0.0458
new: DataOwner3的最优x_3 = 0.0464
new: DataOwner4的最优x_4 = 0.0387
new: DataOwner5的最优x_5 = 0.0222
new: DataOwner6的最优x_6 = 0.0219
new: DataOwner7的最优x_7 = 0.0379
new: DataOwner8的最优x_8 = 0.0456
new: DataOwner9的最优x_9 = 0.0145
new: DataOwner10的最优x_10 = 0.0422
eta:0.41000000000000003
new: DataOwner1的最优x_1 = 0.0413
new: DataOwner2的最优x_2 = 0.0469
new: DataOwner3的最优x_3 = 0.0476
new: DataOwner4的最优x_4 = 0.0397
new: DataOwner5的最优x_5 = 0.0228
new: DataOwner6的最优x_6 = 0.0224
new: DataOwner7的最优x_7 = 0.0388
new: DataOwner8的最优x_8 = 0.0468
new: DataOwner9的最优x_9 = 0.0149
new: DataOwner10的最优x_10 = 0.0433
eta:0.42
new: DataOwner1的最优x_1 = 0.0423
new: DataOwner2的最优x_2 = 0.0481
new: DataOwner3的最优x_3 = 0.0487
new: DataOwner4的最优x_4 = 0.0407
new: DataOwner5的最优x_5 = 0.0233
new: DataOwner6的最优x_6 = 0.0230
new: DataOwner7的最优x_7 = 0.0398
new: DataOwner8的最优x_8 = 0.0479
new: DataOwner9的最优x_9 = 0.0153
new: DataOwner10的最优x_10 = 0.0443
eta:0.43
new: DataOwner1的最优x_1 = 0.0433
new: DataOwner2的最优x_2 = 0.0492
new: DataOwner3的最优x_3 = 0.0499
new: DataOwner4的最优x_4 = 0.0416
new: DataOwner5的最优x_5 = 0.0239
new: DataOwner6的最优x_6 = 0.0235
new: DataOwner7的最优x_7 = 0.0407
new: DataOwner8的最优x_8 = 0.0491
new: DataOwner9的最优x_9 = 0.0156
new: DataOwner10的最优x_10 = 0.0454
eta:0.44
new: DataOwner1的最优x_1 = 0.0443
new: DataOwner2的最优x_2 = 0.0503
new: DataOwner3的最优x_3 = 0.0511
new: DataOwner4的最优x_4 = 0.0426
new: DataOwner5的最优x_5 = 0.0244
new: DataOwner6的最优x_6 = 0.0241
new: DataOwner7的最优x_7 = 0.0417
new: DataOwner8的最优x_8 = 0.0502
new: DataOwner9的最优x_9 = 0.0160
new: DataOwner10的最优x_10 = 0.0464
eta:0.45
new: DataOwner1的最优x_1 = 0.0453
new: DataOwner2的最优x_2 = 0.0515
new: DataOwner3的最优x_3 = 0.0522
new: DataOwner4的最优x_4 = 0.0436
new: DataOwner5的最优x_5 = 0.0250
new: DataOwner6的最优x_6 = 0.0246
new: DataOwner7的最优x_7 = 0.0426
new: DataOwner8的最优x_8 = 0.0513
new: DataOwner9的最优x_9 = 0.0164
new: DataOwner10的最优x_10 = 0.0475
eta:0.46
new: DataOwner1的最优x_1 = 0.0463
new: DataOwner2的最优x_2 = 0.0526
new: DataOwner3的最优x_3 = 0.0534
new: DataOwner4的最优x_4 = 0.0445
new: DataOwner5的最优x_5 = 0.0255
new: DataOwner6的最优x_6 = 0.0252
new: DataOwner7的最优x_7 = 0.0436
new: DataOwner8的最优x_8 = 0.0525
new: DataOwner9的最优x_9 = 0.0167
new: DataOwner10的最优x_10 = 0.0486
eta:0.47000000000000003
new: DataOwner1的最优x_1 = 0.0473
new: DataOwner2的最优x_2 = 0.0538
new: DataOwner3的最优x_3 = 0.0545
new: DataOwner4的最优x_4 = 0.0455
new: DataOwner5的最优x_5 = 0.0261
new: DataOwner6的最优x_6 = 0.0257
new: DataOwner7的最优x_7 = 0.0445
new: DataOwner8的最优x_8 = 0.0536
new: DataOwner9的最优x_9 = 0.0171
new: DataOwner10的最优x_10 = 0.0496
eta:0.48
new: DataOwner1的最优x_1 = 0.0483
new: DataOwner2的最优x_2 = 0.0549
new: DataOwner3的最优x_3 = 0.0557
new: DataOwner4的最优x_4 = 0.0465
new: DataOwner5的最优x_5 = 0.0266
new: DataOwner6的最优x_6 = 0.0263
new: DataOwner7的最优x_7 = 0.0454
new: DataOwner8的最优x_8 = 0.0548
new: DataOwner9的最优x_9 = 0.0175
new: DataOwner10的最优x_10 = 0.0507
eta:0.49
new: DataOwner1的最优x_1 = 0.0493
new: DataOwner2的最优x_2 = 0.0561
new: DataOwner3的最优x_3 = 0.0569
new: DataOwner4的最优x_4 = 0.0474
new: DataOwner5的最优x_5 = 0.0272
new: DataOwner6的最优x_6 = 0.0268
new: DataOwner7的最优x_7 = 0.0464
new: DataOwner8的最优x_8 = 0.0559
new: DataOwner9的最优x_9 = 0.0178
new: DataOwner10的最优x_10 = 0.0517
eta:0.5
new: DataOwner1的最优x_1 = 0.0503
new: DataOwner2的最优x_2 = 0.0572
new: DataOwner3的最优x_3 = 0.0580
new: DataOwner4的最优x_4 = 0.0484
new: DataOwner5的最优x_5 = 0.0278
new: DataOwner6的最优x_6 = 0.0274
new: DataOwner7的最优x_7 = 0.0473
new: DataOwner8的最优x_8 = 0.0570
new: DataOwner9的最优x_9 = 0.0182
new: DataOwner10的最优x_10 = 0.0528
eta:0.51
new: DataOwner1的最优x_1 = 0.0513
new: DataOwner2的最优x_2 = 0.0584
new: DataOwner3的最优x_3 = 0.0592
new: DataOwner4的最优x_4 = 0.0494
new: DataOwner5的最优x_5 = 0.0283
new: DataOwner6的最优x_6 = 0.0279
new: DataOwner7的最优x_7 = 0.0483
new: DataOwner8的最优x_8 = 0.0582
new: DataOwner9的最优x_9 = 0.0185
new: DataOwner10的最优x_10 = 0.0538
eta:0.52
new: DataOwner1的最优x_1 = 0.0523
new: DataOwner2的最优x_2 = 0.0595
new: DataOwner3的最优x_3 = 0.0603
new: DataOwner4的最优x_4 = 0.0503
new: DataOwner5的最优x_5 = 0.0289
new: DataOwner6的最优x_6 = 0.0285
new: DataOwner7的最优x_7 = 0.0492
new: DataOwner8的最优x_8 = 0.0593
new: DataOwner9的最优x_9 = 0.0189
new: DataOwner10的最优x_10 = 0.0549
eta:0.53
new: DataOwner1的最优x_1 = 0.0533
new: DataOwner2的最优x_2 = 0.0606
new: DataOwner3的最优x_3 = 0.0615
new: DataOwner4的最优x_4 = 0.0513
new: DataOwner5的最优x_5 = 0.0294
new: DataOwner6的最优x_6 = 0.0290
new: DataOwner7的最优x_7 = 0.0502
new: DataOwner8的最优x_8 = 0.0605
new: DataOwner9的最优x_9 = 0.0193
new: DataOwner10的最优x_10 = 0.0559
eta:0.54
new: DataOwner1的最优x_1 = 0.0544
new: DataOwner2的最优x_2 = 0.0618
new: DataOwner3的最优x_3 = 0.0627
new: DataOwner4的最优x_4 = 0.0523
new: DataOwner5的最优x_5 = 0.0300
new: DataOwner6的最优x_6 = 0.0295
new: DataOwner7的最优x_7 = 0.0511
new: DataOwner8的最优x_8 = 0.0616
new: DataOwner9的最优x_9 = 0.0196
new: DataOwner10的最优x_10 = 0.0570
eta:0.55
new: DataOwner1的最优x_1 = 0.0554
new: DataOwner2的最优x_2 = 0.0629
new: DataOwner3的最优x_3 = 0.0638
new: DataOwner4的最优x_4 = 0.0532
new: DataOwner5的最优x_5 = 0.0305
new: DataOwner6的最优x_6 = 0.0301
new: DataOwner7的最优x_7 = 0.0521
new: DataOwner8的最优x_8 = 0.0627
new: DataOwner9的最优x_9 = 0.0200
new: DataOwner10的最优x_10 = 0.0581
eta:0.56
new: DataOwner1的最优x_1 = 0.0564
new: DataOwner2的最优x_2 = 0.0641
new: DataOwner3的最优x_3 = 0.0650
new: DataOwner4的最优x_4 = 0.0542
new: DataOwner5的最优x_5 = 0.0311
new: DataOwner6的最优x_6 = 0.0306
new: DataOwner7的最优x_7 = 0.0530
new: DataOwner8的最优x_8 = 0.0639
new: DataOwner9的最优x_9 = 0.0204
new: DataOwner10的最优x_10 = 0.0591
eta:0.5700000000000001
new: DataOwner1的最优x_1 = 0.0574
new: DataOwner2的最优x_2 = 0.0652
new: DataOwner3的最优x_3 = 0.0662
new: DataOwner4的最优x_4 = 0.0552
new: DataOwner5的最优x_5 = 0.0316
new: DataOwner6的最优x_6 = 0.0312
new: DataOwner7的最优x_7 = 0.0540
new: DataOwner8的最优x_8 = 0.0650
new: DataOwner9的最优x_9 = 0.0207
new: DataOwner10的最优x_10 = 0.0602
eta:0.58
new: DataOwner1的最优x_1 = 0.0584
new: DataOwner2的最优x_2 = 0.0664
new: DataOwner3的最优x_3 = 0.0673
new: DataOwner4的最优x_4 = 0.0562
new: DataOwner5的最优x_5 = 0.0322
new: DataOwner6的最优x_6 = 0.0317
new: DataOwner7的最优x_7 = 0.0549
new: DataOwner8的最优x_8 = 0.0662
new: DataOwner9的最优x_9 = 0.0211
new: DataOwner10的最优x_10 = 0.0612
eta:0.59
new: DataOwner1的最优x_1 = 0.0594
new: DataOwner2的最优x_2 = 0.0675
new: DataOwner3的最优x_3 = 0.0685
new: DataOwner4的最优x_4 = 0.0571
new: DataOwner5的最优x_5 = 0.0327
new: DataOwner6的最优x_6 = 0.0323
new: DataOwner7的最优x_7 = 0.0559
new: DataOwner8的最优x_8 = 0.0673
new: DataOwner9的最优x_9 = 0.0215
new: DataOwner10的最优x_10 = 0.0623
eta:0.6
new: DataOwner1的最优x_1 = 0.0604
new: DataOwner2的最优x_2 = 0.0687
new: DataOwner3的最优x_3 = 0.0696
new: DataOwner4的最优x_4 = 0.0581
new: DataOwner5的最优x_5 = 0.0333
new: DataOwner6的最优x_6 = 0.0328
new: DataOwner7的最优x_7 = 0.0568
new: DataOwner8的最优x_8 = 0.0685
new: DataOwner9的最优x_9 = 0.0218
new: DataOwner10的最优x_10 = 0.0633
eta:0.61
new: DataOwner1的最优x_1 = 0.0614
new: DataOwner2的最优x_2 = 0.0698
new: DataOwner3的最优x_3 = 0.0708
new: DataOwner4的最优x_4 = 0.0591
new: DataOwner5的最优x_5 = 0.0339
new: DataOwner6的最优x_6 = 0.0334
new: DataOwner7的最优x_7 = 0.0578
new: DataOwner8的最优x_8 = 0.0696
new: DataOwner9的最优x_9 = 0.0222
new: DataOwner10的最优x_10 = 0.0644
eta:0.62
eta:0.63
new: DataOwner1的最优x_1 = 0.0634
new: DataOwner2的最优x_2 = 0.0721
new: DataOwner3的最优x_3 = 0.0731
new: DataOwner4的最优x_4 = 0.0610
new: DataOwner5的最优x_5 = 0.0350
new: DataOwner6的最优x_6 = 0.0345
new: DataOwner7的最优x_7 = 0.0597
new: DataOwner8的最优x_8 = 0.0719
new: DataOwner9的最优x_9 = 0.0229
new: DataOwner10的最优x_10 = 0.0665
eta:0.64
eta:0.65
eta:0.66
eta:0.67
new: DataOwner1的最优x_1 = 0.0674
new: DataOwner2的最优x_2 = 0.0767
new: DataOwner3的最优x_3 = 0.0778
new: DataOwner4的最优x_4 = 0.0649
new: DataOwner5的最优x_5 = 0.0372
new: DataOwner6的最优x_6 = 0.0367
new: DataOwner7的最优x_7 = 0.0634
new: DataOwner8的最优x_8 = 0.0764
new: DataOwner9的最优x_9 = 0.0244
new: DataOwner10的最优x_10 = 0.0707
eta:0.68
new: DataOwner1的最优x_1 = 0.0684
new: DataOwner2的最优x_2 = 0.0778
new: DataOwner3的最优x_3 = 0.0789
new: DataOwner4的最优x_4 = 0.0658
new: DataOwner5的最优x_5 = 0.0377
new: DataOwner6的最优x_6 = 0.0372
new: DataOwner7的最优x_7 = 0.0644
new: DataOwner8的最优x_8 = 0.0776
new: DataOwner9的最优x_9 = 0.0247
new: DataOwner10的最优x_10 = 0.0718
eta:0.6900000000000001
new: DataOwner1的最优x_1 = 0.0694
new: DataOwner2的最优x_2 = 0.0790
new: DataOwner3的最优x_3 = 0.0801
new: DataOwner4的最优x_4 = 0.0668
new: DataOwner5的最优x_5 = 0.0383
new: DataOwner6的最优x_6 = 0.0378
new: DataOwner7的最优x_7 = 0.0653
new: DataOwner8的最优x_8 = 0.0787
new: DataOwner9的最优x_9 = 0.0251
new: DataOwner10的最优x_10 = 0.0728
eta:0.7000000000000001
new: DataOwner1的最优x_1 = 0.0705
new: DataOwner2的最优x_2 = 0.0801
new: DataOwner3的最优x_3 = 0.0812
new: DataOwner4的最优x_4 = 0.0678
new: DataOwner5的最优x_5 = 0.0389
new: DataOwner6的最优x_6 = 0.0383
new: DataOwner7的最优x_7 = 0.0663
new: DataOwner8的最优x_8 = 0.0799
new: DataOwner9的最优x_9 = 0.0255
new: DataOwner10的最优x_10 = 0.0739
eta:0.71
new: DataOwner1的最优x_1 = 0.0715
new: DataOwner2的最优x_2 = 0.0812
new: DataOwner3的最优x_3 = 0.0824
new: DataOwner4的最优x_4 = 0.0687
new: DataOwner5的最优x_5 = 0.0394
new: DataOwner6的最优x_6 = 0.0388
new: DataOwner7的最优x_7 = 0.0672
new: DataOwner8的最优x_8 = 0.0810
new: DataOwner9的最优x_9 = 0.0258
new: DataOwner10的最优x_10 = 0.0749
eta:0.72
new: DataOwner1的最优x_1 = 0.0725
new: DataOwner2的最优x_2 = 0.0824
new: DataOwner3的最优x_3 = 0.0836
new: DataOwner4的最优x_4 = 0.0697
new: DataOwner5的最优x_5 = 0.0400
new: DataOwner6的最优x_6 = 0.0394
new: DataOwner7的最优x_7 = 0.0682
new: DataOwner8的最优x_8 = 0.0821
new: DataOwner9的最优x_9 = 0.0262
new: DataOwner10的最优x_10 = 0.0760
eta:0.73
new: DataOwner1的最优x_1 = 0.0735
new: DataOwner2的最优x_2 = 0.0835
new: DataOwner3的最优x_3 = 0.0847
new: DataOwner4的最优x_4 = 0.0707
new: DataOwner5的最优x_5 = 0.0405
new: DataOwner6的最优x_6 = 0.0399
new: DataOwner7的最优x_7 = 0.0691
new: DataOwner8的最优x_8 = 0.0833
new: DataOwner9的最优x_9 = 0.0265
new: DataOwner10的最优x_10 = 0.0771
eta:0.74
new: DataOwner1的最优x_1 = 0.0745
new: DataOwner2的最优x_2 = 0.0847
new: DataOwner3的最优x_3 = 0.0859
new: DataOwner4的最优x_4 = 0.0716
new: DataOwner5的最优x_5 = 0.0411
new: DataOwner6的最优x_6 = 0.0405
new: DataOwner7的最优x_7 = 0.0701
new: DataOwner8的最优x_8 = 0.0844
new: DataOwner9的最优x_9 = 0.0269
new: DataOwner10的最优x_10 = 0.0781
eta:0.75
new: DataOwner1的最优x_1 = 0.0755
new: DataOwner2的最优x_2 = 0.0858
new: DataOwner3的最优x_3 = 0.0870
new: DataOwner4的最优x_4 = 0.0726
new: DataOwner5的最优x_5 = 0.0416
new: DataOwner6的最优x_6 = 0.0410
new: DataOwner7的最优x_7 = 0.0710
new: DataOwner8的最优x_8 = 0.0856
new: DataOwner9的最优x_9 = 0.0273
new: DataOwner10的最优x_10 = 0.0792
eta:0.76
eta:0.77
new: DataOwner1的最优x_1 = 0.0775
new: DataOwner2的最优x_2 = 0.0881
new: DataOwner3的最优x_3 = 0.0894
new: DataOwner4的最优x_4 = 0.0745
new: DataOwner5的最优x_5 = 0.0427
new: DataOwner6的最优x_6 = 0.0421
new: DataOwner7的最优x_7 = 0.0729
new: DataOwner8的最优x_8 = 0.0878
new: DataOwner9的最优x_9 = 0.0280
new: DataOwner10的最优x_10 = 0.0813
eta:0.78
new: DataOwner1的最优x_1 = 0.0785
new: DataOwner2的最优x_2 = 0.0893
new: DataOwner3的最优x_3 = 0.0905
new: DataOwner4的最优x_4 = 0.0755
new: DataOwner5的最优x_5 = 0.0433
new: DataOwner6的最优x_6 = 0.0427
new: DataOwner7的最优x_7 = 0.0739
new: DataOwner8的最优x_8 = 0.0890
new: DataOwner9的最优x_9 = 0.0284
new: DataOwner10的最优x_10 = 0.0823
eta:0.79
new: DataOwner1的最优x_1 = 0.0795
new: DataOwner2的最优x_2 = 0.0904
new: DataOwner3的最优x_3 = 0.0917
new: DataOwner4的最优x_4 = 0.0765
new: DataOwner5的最优x_5 = 0.0438
new: DataOwner6的最优x_6 = 0.0432
new: DataOwner7的最优x_7 = 0.0748
new: DataOwner8的最优x_8 = 0.0901
new: DataOwner9的最优x_9 = 0.0287
new: DataOwner10的最优x_10 = 0.0834
eta:0.8
new: DataOwner1的最优x_1 = 0.0805
new: DataOwner2的最优x_2 = 0.0915
new: DataOwner3的最优x_3 = 0.0928
new: DataOwner4的最优x_4 = 0.0775
new: DataOwner5的最优x_5 = 0.0444
new: DataOwner6的最优x_6 = 0.0438
new: DataOwner7的最优x_7 = 0.0757
new: DataOwner8的最优x_8 = 0.0913
new: DataOwner9的最优x_9 = 0.0291
new: DataOwner10的最优x_10 = 0.0844
eta:0.81
new: DataOwner1的最优x_1 = 0.0815
new: DataOwner2的最优x_2 = 0.0927
new: DataOwner3的最优x_3 = 0.0940
new: DataOwner4的最优x_4 = 0.0784
new: DataOwner5的最优x_5 = 0.0450
new: DataOwner6的最优x_6 = 0.0443
new: DataOwner7的最优x_7 = 0.0767
new: DataOwner8的最优x_8 = 0.0924
new: DataOwner9的最优x_9 = 0.0295
new: DataOwner10的最优x_10 = 0.0855
eta:0.8200000000000001
new: DataOwner1的最优x_1 = 0.0825
new: DataOwner2的最优x_2 = 0.0938
new: DataOwner3的最优x_3 = 0.0952
new: DataOwner4的最优x_4 = 0.0794
new: DataOwner5的最优x_5 = 0.0455
new: DataOwner6的最优x_6 = 0.0449
new: DataOwner7的最优x_7 = 0.0776
new: DataOwner8的最优x_8 = 0.0936
new: DataOwner9的最优x_9 = 0.0298
new: DataOwner10的最优x_10 = 0.0866
eta:0.8300000000000001
eta:0.84
eta:0.85
eta:0.86
eta:0.87
new: DataOwner1的最优x_1 = 0.0876
new: DataOwner2的最优x_2 = 0.0995
new: DataOwner3的最优x_3 = 0.1010
new: DataOwner4的最优x_4 = 0.0842
new: DataOwner5的最优x_5 = 0.0483
new: DataOwner6的最优x_6 = 0.0476
new: DataOwner7的最优x_7 = 0.0824
new: DataOwner8的最优x_8 = 0.0993
new: DataOwner9的最优x_9 = 0.0316
new: DataOwner10的最优x_10 = 0.0918
eta:0.88
new: DataOwner1的最优x_1 = 0.0886
new: DataOwner2的最优x_2 = 0.1007
new: DataOwner3的最优x_3 = 0.1021
new: DataOwner4的最优x_4 = 0.0852
new: DataOwner5的最优x_5 = 0.0488
new: DataOwner6的最优x_6 = 0.0481
new: DataOwner7的最优x_7 = 0.0833
new: DataOwner8的最优x_8 = 0.1004
new: DataOwner9的最优x_9 = 0.0320
new: DataOwner10的最优x_10 = 0.0929
eta:0.89
new: DataOwner1的最优x_1 = 0.0896
new: DataOwner2的最优x_2 = 0.1018
new: DataOwner3的最优x_3 = 0.1033
new: DataOwner4的最优x_4 = 0.0862
new: DataOwner5的最优x_5 = 0.0494
new: DataOwner6的最优x_6 = 0.0487
new: DataOwner7的最优x_7 = 0.0843
new: DataOwner8的最优x_8 = 0.1015
new: DataOwner9的最优x_9 = 0.0324
new: DataOwner10的最优x_10 = 0.0939
eta:0.9
new: DataOwner1的最优x_1 = 0.0906
new: DataOwner2的最优x_2 = 0.1030
new: DataOwner3的最优x_3 = 0.1044
new: DataOwner4的最优x_4 = 0.0871
new: DataOwner5的最优x_5 = 0.0500
new: DataOwner6的最优x_6 = 0.0492
new: DataOwner7的最优x_7 = 0.0852
new: DataOwner8的最优x_8 = 0.1027
new: DataOwner9的最优x_9 = 0.0327
new: DataOwner10的最优x_10 = 0.0950
eta:0.91
new: DataOwner1的最优x_1 = 0.0916
new: DataOwner2的最优x_2 = 0.1041
new: DataOwner3的最优x_3 = 0.1056
new: DataOwner4的最优x_4 = 0.0881
new: DataOwner5的最优x_5 = 0.0505
new: DataOwner6的最优x_6 = 0.0498
new: DataOwner7的最优x_7 = 0.0862
new: DataOwner8的最优x_8 = 0.1038
new: DataOwner9的最优x_9 = 0.0331
new: DataOwner10的最优x_10 = 0.0961
eta:0.92
new: DataOwner1的最优x_1 = 0.0926
new: DataOwner2的最优x_2 = 0.1053
new: DataOwner3的最优x_3 = 0.1068
new: DataOwner4的最优x_4 = 0.0891
new: DataOwner5的最优x_5 = 0.0511
new: DataOwner6的最优x_6 = 0.0503
new: DataOwner7的最优x_7 = 0.0871
new: DataOwner8的最优x_8 = 0.1050
new: DataOwner9的最优x_9 = 0.0335
new: DataOwner10的最优x_10 = 0.0971
eta:0.93
new: DataOwner1的最优x_1 = 0.0936
new: DataOwner2的最优x_2 = 0.1064
new: DataOwner3的最优x_3 = 0.1079
new: DataOwner4的最优x_4 = 0.0900
new: DataOwner5的最优x_5 = 0.0516
new: DataOwner6的最优x_6 = 0.0509
new: DataOwner7的最优x_7 = 0.0881
new: DataOwner8的最优x_8 = 0.1061
new: DataOwner9的最优x_9 = 0.0338
new: DataOwner10的最优x_10 = 0.0982
eta:0.9400000000000001
new: DataOwner1的最优x_1 = 0.0946
new: DataOwner2的最优x_2 = 0.1076
new: DataOwner3的最优x_3 = 0.1091
new: DataOwner4的最优x_4 = 0.0910
new: DataOwner5的最优x_5 = 0.0522
new: DataOwner6的最优x_6 = 0.0514
new: DataOwner7的最优x_7 = 0.0890
new: DataOwner8的最优x_8 = 0.1072
new: DataOwner9的最优x_9 = 0.0342
new: DataOwner10的最优x_10 = 0.0992
eta:0.9500000000000001
new: DataOwner1的最优x_1 = 0.0956
new: DataOwner2的最优x_2 = 0.1087
new: DataOwner3的最优x_3 = 0.1103
new: DataOwner4的最优x_4 = 0.0920
new: DataOwner5的最优x_5 = 0.0527
new: DataOwner6的最优x_6 = 0.0520
new: DataOwner7的最优x_7 = 0.0900
new: DataOwner8的最优x_8 = 0.1084
new: DataOwner9的最优x_9 = 0.0345
new: DataOwner10的最优x_10 = 0.1003
eta:0.96
new: DataOwner1的最优x_1 = 0.0966
new: DataOwner2的最优x_2 = 0.1098
new: DataOwner3的最优x_3 = 0.1114
new: DataOwner4的最优x_4 = 0.0929
new: DataOwner5的最优x_5 = 0.0533
new: DataOwner6的最优x_6 = 0.0525
new: DataOwner7的最优x_7 = 0.0909
new: DataOwner8的最优x_8 = 0.1095
new: DataOwner9的最优x_9 = 0.0349
new: DataOwner10的最优x_10 = 0.1013
eta:0.97
new: DataOwner1的最优x_1 = 0.0976
new: DataOwner2的最优x_2 = 0.1110
new: DataOwner3的最优x_3 = 0.1126
new: DataOwner4的最优x_4 = 0.0939
new: DataOwner5的最优x_5 = 0.0538
new: DataOwner6的最优x_6 = 0.0531
new: DataOwner7的最优x_7 = 0.0918
new: DataOwner8的最优x_8 = 0.1107
new: DataOwner9的最优x_9 = 0.0353
new: DataOwner10的最优x_10 = 0.1024
eta:0.98
new: DataOwner1的最优x_1 = 0.0986
new: DataOwner2的最优x_2 = 0.1121
new: DataOwner3的最优x_3 = 0.1137
new: DataOwner4的最优x_4 = 0.0949
new: DataOwner5的最优x_5 = 0.0544
new: DataOwner6的最优x_6 = 0.0536
new: DataOwner7的最优x_7 = 0.0928
new: DataOwner8的最优x_8 = 0.1118
new: DataOwner9的最优x_9 = 0.0356
new: DataOwner10的最优x_10 = 0.1034
eta:0.99
new: DataOwner1的最优x_1 = 0.0996
new: DataOwner2的最优x_2 = 0.1133
new: DataOwner3的最优x_3 = 0.1149
new: DataOwner4的最优x_4 = 0.0958
new: DataOwner5的最优x_5 = 0.0549
new: DataOwner6的最优x_6 = 0.0542
new: DataOwner7的最优x_7 = 0.0937
new: DataOwner8的最优x_8 = 0.1129
new: DataOwner9的最优x_9 = 0.0360
new: DataOwner10的最优x_10 = 0.1045
eta:1.0
new: DataOwner1的最优x_1 = 0.1006
new: DataOwner2的最优x_2 = 0.1144
new: DataOwner3的最优x_3 = 0.1161
new: DataOwner4的最优x_4 = 0.0968
new: DataOwner5的最优x_5 = 0.0555
new: DataOwner6的最优x_6 = 0.0547
new: DataOwner7的最优x_7 = 0.0947
new: DataOwner8的最优x_8 = 0.1141
new: DataOwner9的最优x_9 = 0.0364
new: DataOwner10的最优x_10 = 0.1056
eta:1.01
new: DataOwner1的最优x_1 = 0.1017
new: DataOwner2的最优x_2 = 0.1156
new: DataOwner3的最优x_3 = 0.1172
new: DataOwner4的最优x_4 = 0.0978
new: DataOwner5的最优x_5 = 0.0561
new: DataOwner6的最优x_6 = 0.0553
new: DataOwner7的最优x_7 = 0.0956
new: DataOwner8的最优x_8 = 0.1152
new: DataOwner9的最优x_9 = 0.0367
new: DataOwner10的最优x_10 = 0.1066
eta:1.02
new: DataOwner1的最优x_1 = 0.1027
new: DataOwner2的最优x_2 = 0.1167
new: DataOwner3的最优x_3 = 0.1184
new: DataOwner4的最优x_4 = 0.0988
new: DataOwner5的最优x_5 = 0.0566
new: DataOwner6的最优x_6 = 0.0558
new: DataOwner7的最优x_7 = 0.0966
new: DataOwner8的最优x_8 = 0.1164
new: DataOwner9的最优x_9 = 0.0371
new: DataOwner10的最优x_10 = 0.1077
eta:1.03
new: DataOwner1的最优x_1 = 0.1037
new: DataOwner2的最优x_2 = 0.1179
new: DataOwner3的最优x_3 = 0.1195
new: DataOwner4的最优x_4 = 0.0997
new: DataOwner5的最优x_5 = 0.0572
new: DataOwner6的最优x_6 = 0.0564
new: DataOwner7的最优x_7 = 0.0975
new: DataOwner8的最优x_8 = 0.1175
new: DataOwner9的最优x_9 = 0.0375
new: DataOwner10的最优x_10 = 0.1087
eta:1.04
new: DataOwner1的最优x_1 = 0.1047
new: DataOwner2的最优x_2 = 0.1190
new: DataOwner3的最优x_3 = 0.1207
new: DataOwner4的最优x_4 = 0.1007
new: DataOwner5的最优x_5 = 0.0577
new: DataOwner6的最优x_6 = 0.0569
new: DataOwner7的最优x_7 = 0.0985
new: DataOwner8的最优x_8 = 0.1187
new: DataOwner9的最优x_9 = 0.0378
new: DataOwner10的最优x_10 = 0.1098
eta:1.05
new: DataOwner1的最优x_1 = 0.1057
new: DataOwner2的最优x_2 = 0.1201
new: DataOwner3的最优x_3 = 0.1219
new: DataOwner4的最优x_4 = 0.1017
new: DataOwner5的最优x_5 = 0.0583
new: DataOwner6的最优x_6 = 0.0574
new: DataOwner7的最优x_7 = 0.0994
new: DataOwner8的最优x_8 = 0.1198
new: DataOwner9的最优x_9 = 0.0382
new: DataOwner10的最优x_10 = 0.1108
eta:1.06
new: DataOwner1的最优x_1 = 0.1067
new: DataOwner2的最优x_2 = 0.1213
new: DataOwner3的最优x_3 = 0.1230
new: DataOwner4的最优x_4 = 0.1026
new: DataOwner5的最优x_5 = 0.0588
new: DataOwner6的最优x_6 = 0.0580
new: DataOwner7的最优x_7 = 0.1004
new: DataOwner8的最优x_8 = 0.1209
new: DataOwner9的最优x_9 = 0.0385
new: DataOwner10的最优x_10 = 0.1119
eta:1.07
new: DataOwner1的最优x_1 = 0.1077
new: DataOwner2的最优x_2 = 0.1224
new: DataOwner3的最优x_3 = 0.1242
new: DataOwner4的最优x_4 = 0.1036
new: DataOwner5的最优x_5 = 0.0594
new: DataOwner6的最优x_6 = 0.0585
new: DataOwner7的最优x_7 = 0.1013
new: DataOwner8的最优x_8 = 0.1221
new: DataOwner9的最优x_9 = 0.0389
new: DataOwner10的最优x_10 = 0.1129
eta:1.08
new: DataOwner1的最优x_1 = 0.1087
new: DataOwner2的最优x_2 = 0.1236
new: DataOwner3的最优x_3 = 0.1253
new: DataOwner4的最优x_4 = 0.1046
new: DataOwner5的最优x_5 = 0.0599
new: DataOwner6的最优x_6 = 0.0591
new: DataOwner7的最优x_7 = 0.1023
new: DataOwner8的最优x_8 = 0.1232
new: DataOwner9的最优x_9 = 0.0393
new: DataOwner10的最优x_10 = 0.1140
eta:1.09
new: DataOwner1的最优x_1 = 0.1097
new: DataOwner2的最优x_2 = 0.1247
new: DataOwner3的最优x_3 = 0.1265
new: DataOwner4的最优x_4 = 0.1055
new: DataOwner5的最优x_5 = 0.0605
new: DataOwner6的最优x_6 = 0.0596
new: DataOwner7的最优x_7 = 0.1032
new: DataOwner8的最优x_8 = 0.1244
new: DataOwner9的最优x_9 = 0.0396
new: DataOwner10的最优x_10 = 0.1151
eta:1.1
new: DataOwner1的最优x_1 = 0.1107
new: DataOwner2的最优x_2 = 0.1259
new: DataOwner3的最优x_3 = 0.1277
new: DataOwner4的最优x_4 = 0.1065
new: DataOwner5的最优x_5 = 0.0611
new: DataOwner6的最优x_6 = 0.0602
new: DataOwner7的最优x_7 = 0.1042
new: DataOwner8的最优x_8 = 0.1255
new: DataOwner9的最优x_9 = 0.0400
new: DataOwner10的最优x_10 = 0.1161
eta:1.11
new: DataOwner1的最优x_1 = 0.1117
new: DataOwner2的最优x_2 = 0.1270
new: DataOwner3的最优x_3 = 0.1288
new: DataOwner4的最优x_4 = 0.1075
new: DataOwner5的最优x_5 = 0.0616
new: DataOwner6的最优x_6 = 0.0607
new: DataOwner7的最优x_7 = 0.1051
new: DataOwner8的最优x_8 = 0.1266
new: DataOwner9的最优x_9 = 0.0404
new: DataOwner10的最优x_10 = 0.1172
eta:1.12
new: DataOwner1的最优x_1 = 0.1127
new: DataOwner2的最优x_2 = 0.1282
new: DataOwner3的最优x_3 = 0.1300
new: DataOwner4的最优x_4 = 0.1084
new: DataOwner5的最优x_5 = 0.0622
new: DataOwner6的最优x_6 = 0.0613
new: DataOwner7的最优x_7 = 0.1060
new: DataOwner8的最优x_8 = 0.1278
new: DataOwner9的最优x_9 = 0.0407
new: DataOwner10的最优x_10 = 0.1182
eta:1.1300000000000001
new: DataOwner1的最优x_1 = 0.1137
new: DataOwner2的最优x_2 = 0.1293
new: DataOwner3的最优x_3 = 0.1311
new: DataOwner4的最优x_4 = 0.1094
new: DataOwner5的最优x_5 = 0.0627
new: DataOwner6的最优x_6 = 0.0618
new: DataOwner7的最优x_7 = 0.1070
new: DataOwner8的最优x_8 = 0.1289
new: DataOwner9的最优x_9 = 0.0411
new: DataOwner10的最优x_10 = 0.1193
eta:1.1400000000000001
new: DataOwner1的最优x_1 = 0.1147
new: DataOwner2的最优x_2 = 0.1304
new: DataOwner3的最优x_3 = 0.1323
new: DataOwner4的最优x_4 = 0.1104
new: DataOwner5的最优x_5 = 0.0633
new: DataOwner6的最优x_6 = 0.0624
new: DataOwner7的最优x_7 = 0.1079
new: DataOwner8的最优x_8 = 0.1301
new: DataOwner9的最优x_9 = 0.0415
new: DataOwner10的最优x_10 = 0.1203
eta:1.1500000000000001
new: DataOwner1的最优x_1 = 0.1157
new: DataOwner2的最优x_2 = 0.1316
new: DataOwner3的最优x_3 = 0.1335
new: DataOwner4的最优x_4 = 0.1113
new: DataOwner5的最优x_5 = 0.0638
new: DataOwner6的最优x_6 = 0.0629
new: DataOwner7的最优x_7 = 0.1089
new: DataOwner8的最优x_8 = 0.1312
new: DataOwner9的最优x_9 = 0.0418
new: DataOwner10的最优x_10 = 0.1214
eta:1.16
new: DataOwner1的最优x_1 = 0.1168
new: DataOwner2的最优x_2 = 0.1327
new: DataOwner3的最优x_3 = 0.1346
new: DataOwner4的最优x_4 = 0.1123
new: DataOwner5的最优x_5 = 0.0644
new: DataOwner6的最优x_6 = 0.0635
new: DataOwner7的最优x_7 = 0.1098
new: DataOwner8的最优x_8 = 0.1323
new: DataOwner9的最优x_9 = 0.0422
new: DataOwner10的最优x_10 = 0.1224
eta:1.17
new: DataOwner1的最优x_1 = 0.1178
new: DataOwner2的最优x_2 = 0.1339
new: DataOwner3的最优x_3 = 0.1358
new: DataOwner4的最优x_4 = 0.1133
new: DataOwner5的最优x_5 = 0.0649
new: DataOwner6的最优x_6 = 0.0640
new: DataOwner7的最优x_7 = 0.1108
new: DataOwner8的最优x_8 = 0.1335
new: DataOwner9的最优x_9 = 0.0425
new: DataOwner10的最优x_10 = 0.1235
eta:1.18
new: DataOwner1的最优x_1 = 0.1188
new: DataOwner2的最优x_2 = 0.1350
new: DataOwner3的最优x_3 = 0.1369
new: DataOwner4的最优x_4 = 0.1142
new: DataOwner5的最优x_5 = 0.0655
new: DataOwner6的最优x_6 = 0.0646
new: DataOwner7的最优x_7 = 0.1117
new: DataOwner8的最优x_8 = 0.1346
new: DataOwner9的最优x_9 = 0.0429
new: DataOwner10的最优x_10 = 0.1246
eta:1.19
new: DataOwner1的最优x_1 = 0.1198
new: DataOwner2的最优x_2 = 0.1362
new: DataOwner3的最优x_3 = 0.1381
new: DataOwner4的最优x_4 = 0.1152
new: DataOwner5的最优x_5 = 0.0660
new: DataOwner6的最优x_6 = 0.0651
new: DataOwner7的最优x_7 = 0.1127
new: DataOwner8的最优x_8 = 0.1358
new: DataOwner9的最优x_9 = 0.0433
new: DataOwner10的最优x_10 = 0.1256
eta:1.2
new: DataOwner1的最优x_1 = 0.1208
new: DataOwner2的最优x_2 = 0.1373
new: DataOwner3的最优x_3 = 0.1393
new: DataOwner4的最优x_4 = 0.1162
new: DataOwner5的最优x_5 = 0.0666
new: DataOwner6的最优x_6 = 0.0657
new: DataOwner7的最优x_7 = 0.1136
new: DataOwner8的最优x_8 = 0.1369
new: DataOwner9的最优x_9 = 0.0436
new: DataOwner10的最优x_10 = 0.1267
eta:1.21
new: DataOwner1的最优x_1 = 0.1218
new: DataOwner2的最优x_2 = 0.1385
new: DataOwner3的最优x_3 = 0.1404
new: DataOwner4的最优x_4 = 0.1171
new: DataOwner5的最优x_5 = 0.0672
new: DataOwner6的最优x_6 = 0.0662
new: DataOwner7的最优x_7 = 0.1146
new: DataOwner8的最优x_8 = 0.1380
new: DataOwner9的最优x_9 = 0.0440
new: DataOwner10的最优x_10 = 0.1277
eta:1.22
new: DataOwner1的最优x_1 = 0.1228
new: DataOwner2的最优x_2 = 0.1396
new: DataOwner3的最优x_3 = 0.1416
new: DataOwner4的最优x_4 = 0.1181
new: DataOwner5的最优x_5 = 0.0677
new: DataOwner6的最优x_6 = 0.0668
new: DataOwner7的最优x_7 = 0.1155
new: DataOwner8的最优x_8 = 0.1392
new: DataOwner9的最优x_9 = 0.0444
new: DataOwner10的最优x_10 = 0.1288
eta:1.23
new: DataOwner1的最优x_1 = 0.1238
new: DataOwner2的最优x_2 = 0.1407
new: DataOwner3的最优x_3 = 0.1427
new: DataOwner4的最优x_4 = 0.1191
new: DataOwner5的最优x_5 = 0.0683
new: DataOwner6的最优x_6 = 0.0673
new: DataOwner7的最优x_7 = 0.1165
new: DataOwner8的最优x_8 = 0.1403
new: DataOwner9的最优x_9 = 0.0447
new: DataOwner10的最优x_10 = 0.1298
eta:1.24
new: DataOwner1的最优x_1 = 0.1248
new: DataOwner2的最优x_2 = 0.1419
new: DataOwner3的最优x_3 = 0.1439
new: DataOwner4的最优x_4 = 0.1201
new: DataOwner5的最优x_5 = 0.0688
new: DataOwner6的最优x_6 = 0.0678
new: DataOwner7的最优x_7 = 0.1174
new: DataOwner8的最优x_8 = 0.1415
new: DataOwner9的最优x_9 = 0.0451
new: DataOwner10的最优x_10 = 0.1309
eta:1.25
new: DataOwner1的最优x_1 = 0.1258
new: DataOwner2的最优x_2 = 0.1430
new: DataOwner3的最优x_3 = 0.1451
new: DataOwner4的最优x_4 = 0.1210
new: DataOwner5的最优x_5 = 0.0694
new: DataOwner6的最优x_6 = 0.0684
new: DataOwner7的最优x_7 = 0.1184
new: DataOwner8的最优x_8 = 0.1426
new: DataOwner9的最优x_9 = 0.0455
new: DataOwner10的最优x_10 = 0.1319
eta:1.26
new: DataOwner1的最优x_1 = 0.1268
new: DataOwner2的最优x_2 = 0.1442
new: DataOwner3的最优x_3 = 0.1462
new: DataOwner4的最优x_4 = 0.1220
new: DataOwner5的最优x_5 = 0.0699
new: DataOwner6的最优x_6 = 0.0689
new: DataOwner7的最优x_7 = 0.1193
new: DataOwner8的最优x_8 = 0.1437
new: DataOwner9的最优x_9 = 0.0458
new: DataOwner10的最优x_10 = 0.1330
eta:1.27
new: DataOwner1的最优x_1 = 0.1278
new: DataOwner2的最优x_2 = 0.1453
new: DataOwner3的最优x_3 = 0.1474
new: DataOwner4的最优x_4 = 0.1230
new: DataOwner5的最优x_5 = 0.0705
new: DataOwner6的最优x_6 = 0.0695
new: DataOwner7的最优x_7 = 0.1203
new: DataOwner8的最优x_8 = 0.1449
new: DataOwner9的最优x_9 = 0.0462
new: DataOwner10的最优x_10 = 0.1341
eta:1.28
new: DataOwner1的最优x_1 = 0.1288
new: DataOwner2的最优x_2 = 0.1465
new: DataOwner3的最优x_3 = 0.1486
new: DataOwner4的最优x_4 = 0.1239
new: DataOwner5的最优x_5 = 0.0710
new: DataOwner6的最优x_6 = 0.0700
new: DataOwner7的最优x_7 = 0.1212
new: DataOwner8的最优x_8 = 0.1460
new: DataOwner9的最优x_9 = 0.0465
new: DataOwner10的最优x_10 = 0.1351
eta:1.29
new: DataOwner1的最优x_1 = 0.1298
new: DataOwner2的最优x_2 = 0.1476
new: DataOwner3的最优x_3 = 0.1497
new: DataOwner4的最优x_4 = 0.1249
new: DataOwner5的最优x_5 = 0.0716
new: DataOwner6的最优x_6 = 0.0706
new: DataOwner7的最优x_7 = 0.1221
new: DataOwner8的最优x_8 = 0.1472
new: DataOwner9的最优x_9 = 0.0469
new: DataOwner10的最优x_10 = 0.1362
eta:1.3
new: DataOwner1的最优x_1 = 0.1308
new: DataOwner2的最优x_2 = 0.1488
new: DataOwner3的最优x_3 = 0.1509
new: DataOwner4的最优x_4 = 0.1259
new: DataOwner5的最优x_5 = 0.0722
new: DataOwner6的最优x_6 = 0.0711
new: DataOwner7的最优x_7 = 0.1231
new: DataOwner8的最优x_8 = 0.1483
new: DataOwner9的最优x_9 = 0.0473
new: DataOwner10的最优x_10 = 0.1372
eta:1.31
new: DataOwner1的最优x_1 = 0.1319
new: DataOwner2的最优x_2 = 0.1499
new: DataOwner3的最优x_3 = 0.1520
new: DataOwner4的最优x_4 = 0.1268
new: DataOwner5的最优x_5 = 0.0727
new: DataOwner6的最优x_6 = 0.0717
new: DataOwner7的最优x_7 = 0.1240
new: DataOwner8的最优x_8 = 0.1495
new: DataOwner9的最优x_9 = 0.0476
new: DataOwner10的最优x_10 = 0.1383
eta:1.32
new: DataOwner1的最优x_1 = 0.1329
new: DataOwner2的最优x_2 = 0.1510
new: DataOwner3的最优x_3 = 0.1532
new: DataOwner4的最优x_4 = 0.1278
new: DataOwner5的最优x_5 = 0.0733
new: DataOwner6的最优x_6 = 0.0722
new: DataOwner7的最优x_7 = 0.1250
new: DataOwner8的最优x_8 = 0.1506
new: DataOwner9的最优x_9 = 0.0480
new: DataOwner10的最优x_10 = 0.1393
eta:1.33
new: DataOwner1的最优x_1 = 0.1339
new: DataOwner2的最优x_2 = 0.1522
new: DataOwner3的最优x_3 = 0.1544
new: DataOwner4的最优x_4 = 0.1288
new: DataOwner5的最优x_5 = 0.0738
new: DataOwner6的最优x_6 = 0.0728
new: DataOwner7的最优x_7 = 0.1259
new: DataOwner8的最优x_8 = 0.1517
new: DataOwner9的最优x_9 = 0.0484
new: DataOwner10的最优x_10 = 0.1404
eta:1.34
new: DataOwner1的最优x_1 = 0.1349
new: DataOwner2的最优x_2 = 0.1533
new: DataOwner3的最优x_3 = 0.1555
new: DataOwner4的最优x_4 = 0.1297
new: DataOwner5的最优x_5 = 0.0744
new: DataOwner6的最优x_6 = 0.0733
new: DataOwner7的最优x_7 = 0.1269
new: DataOwner8的最优x_8 = 0.1529
new: DataOwner9的最优x_9 = 0.0487
new: DataOwner10的最优x_10 = 0.1414
eta:1.35
new: DataOwner1的最优x_1 = 0.1359
new: DataOwner2的最优x_2 = 0.1545
new: DataOwner3的最优x_3 = 0.1567
new: DataOwner4的最优x_4 = 0.1307
new: DataOwner5的最优x_5 = 0.0749
new: DataOwner6的最优x_6 = 0.0739
new: DataOwner7的最优x_7 = 0.1278
new: DataOwner8的最优x_8 = 0.1540
new: DataOwner9的最优x_9 = 0.0491
new: DataOwner10的最优x_10 = 0.1425
eta:1.36
new: DataOwner1的最优x_1 = 0.1369
new: DataOwner2的最优x_2 = 0.1556
new: DataOwner3的最优x_3 = 0.1578
new: DataOwner4的最优x_4 = 0.1317
new: DataOwner5的最优x_5 = 0.0755
new: DataOwner6的最优x_6 = 0.0744
new: DataOwner7的最优x_7 = 0.1288
new: DataOwner8的最优x_8 = 0.1552
new: DataOwner9的最优x_9 = 0.0495
new: DataOwner10的最优x_10 = 0.1436
eta:1.37
new: DataOwner1的最优x_1 = 0.1379
new: DataOwner2的最优x_2 = 0.1568
new: DataOwner3的最优x_3 = 0.1590
new: DataOwner4的最优x_4 = 0.1326
new: DataOwner5的最优x_5 = 0.0760
new: DataOwner6的最优x_6 = 0.0750
new: DataOwner7的最优x_7 = 0.1297
new: DataOwner8的最优x_8 = 0.1563
new: DataOwner9的最优x_9 = 0.0498
new: DataOwner10的最优x_10 = 0.1446
eta:1.3800000000000001
new: DataOwner1的最优x_1 = 0.1389
new: DataOwner2的最优x_2 = 0.1579
new: DataOwner3的最优x_3 = 0.1602
new: DataOwner4的最优x_4 = 0.1336
new: DataOwner5的最优x_5 = 0.0766
new: DataOwner6的最优x_6 = 0.0755
new: DataOwner7的最优x_7 = 0.1307
new: DataOwner8的最优x_8 = 0.1574
new: DataOwner9的最优x_9 = 0.0502
new: DataOwner10的最优x_10 = 0.1457
eta:1.3900000000000001
new: DataOwner1的最优x_1 = 0.1399
new: DataOwner2的最优x_2 = 0.1591
new: DataOwner3的最优x_3 = 0.1613
new: DataOwner4的最优x_4 = 0.1346
new: DataOwner5的最优x_5 = 0.0771
new: DataOwner6的最优x_6 = 0.0761
new: DataOwner7的最优x_7 = 0.1316
new: DataOwner8的最优x_8 = 0.1586
new: DataOwner9的最优x_9 = 0.0505
new: DataOwner10的最优x_10 = 0.1467
eta:1.4000000000000001
new: DataOwner1的最优x_1 = 0.1409
new: DataOwner2的最优x_2 = 0.1602
new: DataOwner3的最优x_3 = 0.1625
new: DataOwner4的最优x_4 = 0.1355
new: DataOwner5的最优x_5 = 0.0777
new: DataOwner6的最优x_6 = 0.0766
new: DataOwner7的最优x_7 = 0.1326
new: DataOwner8的最优x_8 = 0.1597
new: DataOwner9的最优x_9 = 0.0509
new: DataOwner10的最优x_10 = 0.1478
eta:1.41
new: DataOwner1的最优x_1 = 0.1419
new: DataOwner2的最优x_2 = 0.1613
new: DataOwner3的最优x_3 = 0.1636
new: DataOwner4的最优x_4 = 0.1365
new: DataOwner5的最优x_5 = 0.0783
new: DataOwner6的最优x_6 = 0.0771
new: DataOwner7的最优x_7 = 0.1335
new: DataOwner8的最优x_8 = 0.1609
new: DataOwner9的最优x_9 = 0.0513
new: DataOwner10的最优x_10 = 0.1488
eta:1.42
new: DataOwner1的最优x_1 = 0.1429
new: DataOwner2的最优x_2 = 0.1625
new: DataOwner3的最优x_3 = 0.1648
new: DataOwner4的最优x_4 = 0.1375
new: DataOwner5的最优x_5 = 0.0788
new: DataOwner6的最优x_6 = 0.0777
new: DataOwner7的最优x_7 = 0.1345
new: DataOwner8的最优x_8 = 0.1620
new: DataOwner9的最优x_9 = 0.0516
new: DataOwner10的最优x_10 = 0.1499
eta:1.43
new: DataOwner1的最优x_1 = 0.1439
new: DataOwner2的最优x_2 = 0.1636
new: DataOwner3的最优x_3 = 0.1660
new: DataOwner4的最优x_4 = 0.1384
new: DataOwner5的最优x_5 = 0.0794
new: DataOwner6的最优x_6 = 0.0782
new: DataOwner7的最优x_7 = 0.1354
new: DataOwner8的最优x_8 = 0.1631
new: DataOwner9的最优x_9 = 0.0520
new: DataOwner10的最优x_10 = 0.1509
eta:1.44
new: DataOwner1的最优x_1 = 0.1449
new: DataOwner2的最优x_2 = 0.1648
new: DataOwner3的最优x_3 = 0.1671
new: DataOwner4的最优x_4 = 0.1394
new: DataOwner5的最优x_5 = 0.0799
new: DataOwner6的最优x_6 = 0.0788
new: DataOwner7的最优x_7 = 0.1363
new: DataOwner8的最优x_8 = 0.1643
new: DataOwner9的最优x_9 = 0.0524
new: DataOwner10的最优x_10 = 0.1520
eta:1.45
new: DataOwner1的最优x_1 = 0.1459
new: DataOwner2的最优x_2 = 0.1659
new: DataOwner3的最优x_3 = 0.1683
new: DataOwner4的最优x_4 = 0.1404
new: DataOwner5的最优x_5 = 0.0805
new: DataOwner6的最优x_6 = 0.0793
new: DataOwner7的最优x_7 = 0.1373
new: DataOwner8的最优x_8 = 0.1654
new: DataOwner9的最优x_9 = 0.0527
new: DataOwner10的最优x_10 = 0.1531
eta:1.46
new: DataOwner1的最优x_1 = 0.1469
new: DataOwner2的最优x_2 = 0.1671
new: DataOwner3的最优x_3 = 0.1694
new: DataOwner4的最优x_4 = 0.1414
new: DataOwner5的最优x_5 = 0.0810
new: DataOwner6的最优x_6 = 0.0799
new: DataOwner7的最优x_7 = 0.1382
new: DataOwner8的最优x_8 = 0.1666
new: DataOwner9的最优x_9 = 0.0531
new: DataOwner10的最优x_10 = 0.1541
eta:1.47
new: DataOwner1的最优x_1 = 0.1480
new: DataOwner2的最优x_2 = 0.1682
new: DataOwner3的最优x_3 = 0.1706
new: DataOwner4的最优x_4 = 0.1423
new: DataOwner5的最优x_5 = 0.0816
new: DataOwner6的最优x_6 = 0.0804
new: DataOwner7的最优x_7 = 0.1392
new: DataOwner8的最优x_8 = 0.1677
new: DataOwner9的最优x_9 = 0.0535
new: DataOwner10的最优x_10 = 0.1552
eta:1.48
new: DataOwner1的最优x_1 = 0.1490
new: DataOwner2的最优x_2 = 0.1693
new: DataOwner3的最优x_3 = 0.1718
new: DataOwner4的最优x_4 = 0.1433
new: DataOwner5的最优x_5 = 0.0821
new: DataOwner6的最优x_6 = 0.0810
new: DataOwner7的最优x_7 = 0.1401
new: DataOwner8的最优x_8 = 0.1688
new: DataOwner9的最优x_9 = 0.0538
new: DataOwner10的最优x_10 = 0.1562
eta:1.49
new: DataOwner1的最优x_1 = 0.1500
new: DataOwner2的最优x_2 = 0.1705
new: DataOwner3的最优x_3 = 0.1729
new: DataOwner4的最优x_4 = 0.1443
new: DataOwner5的最优x_5 = 0.0827
new: DataOwner6的最优x_6 = 0.0815
new: DataOwner7的最优x_7 = 0.1411
new: DataOwner8的最优x_8 = 0.1700
new: DataOwner9的最优x_9 = 0.0542
new: DataOwner10的最优x_10 = 0.1573
eta:1.5
new: DataOwner1的最优x_1 = 0.1510
new: DataOwner2的最优x_2 = 0.1716
new: DataOwner3的最优x_3 = 0.1741
new: DataOwner4的最优x_4 = 0.1452
new: DataOwner5的最优x_5 = 0.0833
new: DataOwner6的最优x_6 = 0.0821
new: DataOwner7的最优x_7 = 0.1420
new: DataOwner8的最优x_8 = 0.1711
new: DataOwner9的最优x_9 = 0.0545
new: DataOwner10的最优x_10 = 0.1583
eta:1.51
new: DataOwner1的最优x_1 = 0.1520
new: DataOwner2的最优x_2 = 0.1728
new: DataOwner3的最优x_3 = 0.1752
new: DataOwner4的最优x_4 = 0.1462
new: DataOwner5的最优x_5 = 0.0838
new: DataOwner6的最优x_6 = 0.0826
new: DataOwner7的最优x_7 = 0.1430
new: DataOwner8的最优x_8 = 0.1723
new: DataOwner9的最优x_9 = 0.0549
new: DataOwner10的最优x_10 = 0.1594
eta:1.52
new: DataOwner1的最优x_1 = 0.1530
new: DataOwner2的最优x_2 = 0.1739
new: DataOwner3的最优x_3 = 0.1764
new: DataOwner4的最优x_4 = 0.1472
new: DataOwner5的最优x_5 = 0.0844
new: DataOwner6的最优x_6 = 0.0832
new: DataOwner7的最优x_7 = 0.1439
new: DataOwner8的最优x_8 = 0.1734
new: DataOwner9的最优x_9 = 0.0553
new: DataOwner10的最优x_10 = 0.1604
eta:1.53
new: DataOwner1的最优x_1 = 0.1540
new: DataOwner2的最优x_2 = 0.1751
new: DataOwner3的最优x_3 = 0.1776
new: DataOwner4的最优x_4 = 0.1481
new: DataOwner5的最优x_5 = 0.0849
new: DataOwner6的最优x_6 = 0.0837
new: DataOwner7的最优x_7 = 0.1449
new: DataOwner8的最优x_8 = 0.1746
new: DataOwner9的最优x_9 = 0.0556
new: DataOwner10的最优x_10 = 0.1615
eta:1.54
new: DataOwner1的最优x_1 = 0.1550
new: DataOwner2的最优x_2 = 0.1762
new: DataOwner3的最优x_3 = 0.1787
new: DataOwner4的最优x_4 = 0.1491
new: DataOwner5的最优x_5 = 0.0855
new: DataOwner6的最优x_6 = 0.0843
new: DataOwner7的最优x_7 = 0.1458
new: DataOwner8的最优x_8 = 0.1757
new: DataOwner9的最优x_9 = 0.0560
new: DataOwner10的最优x_10 = 0.1625
eta:1.55
new: DataOwner1的最优x_1 = 0.1560
new: DataOwner2的最优x_2 = 0.1774
new: DataOwner3的最优x_3 = 0.1799
new: DataOwner4的最优x_4 = 0.1501
new: DataOwner5的最优x_5 = 0.0860
new: DataOwner6的最优x_6 = 0.0848
new: DataOwner7的最优x_7 = 0.1468
new: DataOwner8的最优x_8 = 0.1768
new: DataOwner9的最优x_9 = 0.0564
new: DataOwner10的最优x_10 = 0.1636
eta:1.56
new: DataOwner1的最优x_1 = 0.1570
new: DataOwner2的最优x_2 = 0.1785
new: DataOwner3的最优x_3 = 0.1810
new: DataOwner4的最优x_4 = 0.1510
new: DataOwner5的最优x_5 = 0.0866
new: DataOwner6的最优x_6 = 0.0854
new: DataOwner7的最优x_7 = 0.1477
new: DataOwner8的最优x_8 = 0.1780
new: DataOwner9的最优x_9 = 0.0567
new: DataOwner10的最优x_10 = 0.1647
eta:1.57
new: DataOwner1的最优x_1 = 0.1580
new: DataOwner2的最优x_2 = 0.1796
new: DataOwner3的最优x_3 = 0.1822
new: DataOwner4的最优x_4 = 0.1520
new: DataOwner5的最优x_5 = 0.0871
new: DataOwner6的最优x_6 = 0.0859
new: DataOwner7的最优x_7 = 0.1487
new: DataOwner8的最优x_8 = 0.1791
new: DataOwner9的最优x_9 = 0.0571
new: DataOwner10的最优x_10 = 0.1657
eta:1.58
new: DataOwner1的最优x_1 = 0.1590
new: DataOwner2的最优x_2 = 0.1808
new: DataOwner3的最优x_3 = 0.1834
new: DataOwner4的最优x_4 = 0.1530
new: DataOwner5的最优x_5 = 0.0877
new: DataOwner6的最优x_6 = 0.0864
new: DataOwner7的最优x_7 = 0.1496
new: DataOwner8的最优x_8 = 0.1803
new: DataOwner9的最优x_9 = 0.0575
new: DataOwner10的最优x_10 = 0.1668
eta:1.59
new: DataOwner1的最优x_1 = 0.1600
new: DataOwner2的最优x_2 = 0.1819
new: DataOwner3的最优x_3 = 0.1845
new: DataOwner4的最优x_4 = 0.1539
new: DataOwner5的最优x_5 = 0.0882
new: DataOwner6的最优x_6 = 0.0870
new: DataOwner7的最优x_7 = 0.1506
new: DataOwner8的最优x_8 = 0.1814
new: DataOwner9的最优x_9 = 0.0578
new: DataOwner10的最优x_10 = 0.1678
eta:1.6
new: DataOwner1的最优x_1 = 0.1610
new: DataOwner2的最优x_2 = 0.1831
new: DataOwner3的最优x_3 = 0.1857
new: DataOwner4的最优x_4 = 0.1549
new: DataOwner5的最优x_5 = 0.0888
new: DataOwner6的最优x_6 = 0.0875
new: DataOwner7的最优x_7 = 0.1515
new: DataOwner8的最优x_8 = 0.1825
new: DataOwner9的最优x_9 = 0.0582
new: DataOwner10的最优x_10 = 0.1689
eta:1.61
new: DataOwner1的最优x_1 = 0.1620
new: DataOwner2的最优x_2 = 0.1842
new: DataOwner3的最优x_3 = 0.1868
new: DataOwner4的最优x_4 = 0.1559
new: DataOwner5的最优x_5 = 0.0894
new: DataOwner6的最优x_6 = 0.0881
new: DataOwner7的最优x_7 = 0.1524
new: DataOwner8的最优x_8 = 0.1837
new: DataOwner9的最优x_9 = 0.0585
new: DataOwner10的最优x_10 = 0.1699
eta:1.62
new: DataOwner1的最优x_1 = 0.1631
new: DataOwner2的最优x_2 = 0.1854
new: DataOwner3的最优x_3 = 0.1880
new: DataOwner4的最优x_4 = 0.1568
new: DataOwner5的最优x_5 = 0.0899
new: DataOwner6的最优x_6 = 0.0886
new: DataOwner7的最优x_7 = 0.1534
new: DataOwner8的最优x_8 = 0.1848
new: DataOwner9的最优x_9 = 0.0589
new: DataOwner10的最优x_10 = 0.1710
eta:1.6300000000000001
new: DataOwner1的最优x_1 = 0.1641
new: DataOwner2的最优x_2 = 0.1865
new: DataOwner3的最优x_3 = 0.1892
new: DataOwner4的最优x_4 = 0.1578
new: DataOwner5的最优x_5 = 0.0905
new: DataOwner6的最优x_6 = 0.0892
new: DataOwner7的最优x_7 = 0.1543
new: DataOwner8的最优x_8 = 0.1860
new: DataOwner9的最优x_9 = 0.0593
new: DataOwner10的最优x_10 = 0.1720
eta:1.6400000000000001
new: DataOwner1的最优x_1 = 0.1651
new: DataOwner2的最优x_2 = 0.1877
new: DataOwner3的最优x_3 = 0.1903
new: DataOwner4的最优x_4 = 0.1588
new: DataOwner5的最优x_5 = 0.0910
new: DataOwner6的最优x_6 = 0.0897
new: DataOwner7的最优x_7 = 0.1553
new: DataOwner8的最优x_8 = 0.1871
new: DataOwner9的最优x_9 = 0.0596
new: DataOwner10的最优x_10 = 0.1731
eta:1.6500000000000001
new: DataOwner1的最优x_1 = 0.1661
new: DataOwner2的最优x_2 = 0.1888
new: DataOwner3的最优x_3 = 0.1915
new: DataOwner4的最优x_4 = 0.1597
new: DataOwner5的最优x_5 = 0.0916
new: DataOwner6的最优x_6 = 0.0903
new: DataOwner7的最优x_7 = 0.1562
new: DataOwner8的最优x_8 = 0.1882
new: DataOwner9的最优x_9 = 0.0600
new: DataOwner10的最优x_10 = 0.1742
eta:1.6600000000000001
new: DataOwner1的最优x_1 = 0.1671
new: DataOwner2的最优x_2 = 0.1899
new: DataOwner3的最优x_3 = 0.1927
new: DataOwner4的最优x_4 = 0.1607
new: DataOwner5的最优x_5 = 0.0921
new: DataOwner6的最优x_6 = 0.0908
new: DataOwner7的最优x_7 = 0.1572
new: DataOwner8的最优x_8 = 0.1894
new: DataOwner9的最优x_9 = 0.0604
new: DataOwner10的最优x_10 = 0.1752
eta:1.67
new: DataOwner1的最优x_1 = 0.1681
new: DataOwner2的最优x_2 = 0.1911
new: DataOwner3的最优x_3 = 0.1938
new: DataOwner4的最优x_4 = 0.1617
new: DataOwner5的最优x_5 = 0.0927
new: DataOwner6的最优x_6 = 0.0914
new: DataOwner7的最优x_7 = 0.1581
new: DataOwner8的最优x_8 = 0.1905
new: DataOwner9的最优x_9 = 0.0607
new: DataOwner10的最优x_10 = 0.1763
eta:1.68
new: DataOwner1的最优x_1 = 0.1691
new: DataOwner2的最优x_2 = 0.1922
new: DataOwner3的最优x_3 = 0.1950
new: DataOwner4的最优x_4 = 0.1627
new: DataOwner5的最优x_5 = 0.0932
new: DataOwner6的最优x_6 = 0.0919
new: DataOwner7的最优x_7 = 0.1591
new: DataOwner8的最优x_8 = 0.1917
new: DataOwner9的最优x_9 = 0.0611
new: DataOwner10的最优x_10 = 0.1773
eta:1.69
new: DataOwner1的最优x_1 = 0.1701
new: DataOwner2的最优x_2 = 0.1934
new: DataOwner3的最优x_3 = 0.1961
new: DataOwner4的最优x_4 = 0.1636
new: DataOwner5的最优x_5 = 0.0938
new: DataOwner6的最优x_6 = 0.0925
new: DataOwner7的最优x_7 = 0.1600
new: DataOwner8的最优x_8 = 0.1928
new: DataOwner9的最优x_9 = 0.0615
new: DataOwner10的最优x_10 = 0.1784
eta:1.7
new: DataOwner1的最优x_1 = 0.1711
new: DataOwner2的最优x_2 = 0.1945
new: DataOwner3的最优x_3 = 0.1973
new: DataOwner4的最优x_4 = 0.1646
new: DataOwner5的最优x_5 = 0.0944
new: DataOwner6的最优x_6 = 0.0930
new: DataOwner7的最优x_7 = 0.1610
new: DataOwner8的最优x_8 = 0.1939
new: DataOwner9的最优x_9 = 0.0618
new: DataOwner10的最优x_10 = 0.1794
eta:1.71
new: DataOwner1的最优x_1 = 0.1721
new: DataOwner2的最优x_2 = 0.1957
new: DataOwner3的最优x_3 = 0.1985
new: DataOwner4的最优x_4 = 0.1656
new: DataOwner5的最优x_5 = 0.0949
new: DataOwner6的最优x_6 = 0.0936
new: DataOwner7的最优x_7 = 0.1619
new: DataOwner8的最优x_8 = 0.1951
new: DataOwner9的最优x_9 = 0.0622
new: DataOwner10的最优x_10 = 0.1805
eta:1.72
new: DataOwner1的最优x_1 = 0.1731
new: DataOwner2的最优x_2 = 0.1968
new: DataOwner3的最优x_3 = 0.1996
new: DataOwner4的最优x_4 = 0.1665
new: DataOwner5的最优x_5 = 0.0955
new: DataOwner6的最优x_6 = 0.0941
new: DataOwner7的最优x_7 = 0.1629
new: DataOwner8的最优x_8 = 0.1962
new: DataOwner9的最优x_9 = 0.0626
new: DataOwner10的最优x_10 = 0.1815
eta:1.73
new: DataOwner1的最优x_1 = 0.1741
new: DataOwner2的最优x_2 = 0.1980
new: DataOwner3的最优x_3 = 0.2008
new: DataOwner4的最优x_4 = 0.1675
new: DataOwner5的最优x_5 = 0.0960
new: DataOwner6的最优x_6 = 0.0947
new: DataOwner7的最优x_7 = 0.1638
new: DataOwner8的最优x_8 = 0.1974
new: DataOwner9的最优x_9 = 0.0629
new: DataOwner10的最优x_10 = 0.1826
eta:1.74
new: DataOwner1的最优x_1 = 0.1751
new: DataOwner2的最优x_2 = 0.1991
new: DataOwner3的最优x_3 = 0.2019
new: DataOwner4的最优x_4 = 0.1685
new: DataOwner5的最优x_5 = 0.0966
new: DataOwner6的最优x_6 = 0.0952
new: DataOwner7的最优x_7 = 0.1648
new: DataOwner8的最优x_8 = 0.1985
new: DataOwner9的最优x_9 = 0.0633
new: DataOwner10的最优x_10 = 0.1837
eta:1.75
new: DataOwner1的最优x_1 = 0.1761
new: DataOwner2的最优x_2 = 0.2002
new: DataOwner3的最优x_3 = 0.2031
new: DataOwner4的最优x_4 = 0.1694
new: DataOwner5的最优x_5 = 0.0971
new: DataOwner6的最优x_6 = 0.0957
new: DataOwner7的最优x_7 = 0.1657
new: DataOwner8的最优x_8 = 0.1997
new: DataOwner9的最优x_9 = 0.0636
new: DataOwner10的最优x_10 = 0.1847
eta:1.76
new: DataOwner1的最优x_1 = 0.1771
new: DataOwner2的最优x_2 = 0.2014
new: DataOwner3的最优x_3 = 0.2043
new: DataOwner4的最优x_4 = 0.1704
new: DataOwner5的最优x_5 = 0.0977
new: DataOwner6的最优x_6 = 0.0963
new: DataOwner7的最优x_7 = 0.1666
new: DataOwner8的最优x_8 = 0.2008
new: DataOwner9的最优x_9 = 0.0640
new: DataOwner10的最优x_10 = 0.1858
eta:1.77
new: DataOwner1的最优x_1 = 0.1781
new: DataOwner2的最优x_2 = 0.2025
new: DataOwner3的最优x_3 = 0.2054
new: DataOwner4的最优x_4 = 0.1714
new: DataOwner5的最优x_5 = 0.0982
new: DataOwner6的最优x_6 = 0.0968
new: DataOwner7的最优x_7 = 0.1676
new: DataOwner8的最优x_8 = 0.2019
new: DataOwner9的最优x_9 = 0.0644
new: DataOwner10的最优x_10 = 0.1868
eta:1.78
new: DataOwner1的最优x_1 = 0.1792
new: DataOwner2的最优x_2 = 0.2037
new: DataOwner3的最优x_3 = 0.2066
new: DataOwner4的最优x_4 = 0.1723
new: DataOwner5的最优x_5 = 0.0988
new: DataOwner6的最优x_6 = 0.0974
new: DataOwner7的最优x_7 = 0.1685
new: DataOwner8的最优x_8 = 0.2031
new: DataOwner9的最优x_9 = 0.0647
new: DataOwner10的最优x_10 = 0.1879
eta:1.79
new: DataOwner1的最优x_1 = 0.1802
new: DataOwner2的最优x_2 = 0.2048
new: DataOwner3的最优x_3 = 0.2077
new: DataOwner4的最优x_4 = 0.1733
new: DataOwner5的最优x_5 = 0.0993
new: DataOwner6的最优x_6 = 0.0979
new: DataOwner7的最优x_7 = 0.1695
new: DataOwner8的最优x_8 = 0.2042
new: DataOwner9的最优x_9 = 0.0651
new: DataOwner10的最优x_10 = 0.1889
eta:1.8
new: DataOwner1的最优x_1 = 0.1812
new: DataOwner2的最优x_2 = 0.2060
new: DataOwner3的最优x_3 = 0.2089
new: DataOwner4的最优x_4 = 0.1743
new: DataOwner5的最优x_5 = 0.0999
new: DataOwner6的最优x_6 = 0.0985
new: DataOwner7的最优x_7 = 0.1704
new: DataOwner8的最优x_8 = 0.2054
new: DataOwner9的最优x_9 = 0.0655
new: DataOwner10的最优x_10 = 0.1900
eta:1.81
new: DataOwner1的最优x_1 = 0.1822
new: DataOwner2的最优x_2 = 0.2071
new: DataOwner3的最优x_3 = 0.2101
new: DataOwner4的最优x_4 = 0.1752
new: DataOwner5的最优x_5 = 0.1005
new: DataOwner6的最优x_6 = 0.0990
new: DataOwner7的最优x_7 = 0.1714
new: DataOwner8的最优x_8 = 0.2065
new: DataOwner9的最优x_9 = 0.0658
new: DataOwner10的最优x_10 = 0.1910
eta:1.82
new: DataOwner1的最优x_1 = 0.1832
new: DataOwner2的最优x_2 = 0.2083
new: DataOwner3的最优x_3 = 0.2112
new: DataOwner4的最优x_4 = 0.1762
new: DataOwner5的最优x_5 = 0.1010
new: DataOwner6的最优x_6 = 0.0996
new: DataOwner7的最优x_7 = 0.1723
new: DataOwner8的最优x_8 = 0.2076
new: DataOwner9的最优x_9 = 0.0662
new: DataOwner10的最优x_10 = 0.1921
eta:1.83
new: DataOwner1的最优x_1 = 0.1842
new: DataOwner2的最优x_2 = 0.2094
new: DataOwner3的最优x_3 = 0.2124
new: DataOwner4的最优x_4 = 0.1772
new: DataOwner5的最优x_5 = 0.1016
new: DataOwner6的最优x_6 = 0.1001
new: DataOwner7的最优x_7 = 0.1733
new: DataOwner8的最优x_8 = 0.2088
new: DataOwner9的最优x_9 = 0.0666
new: DataOwner10的最优x_10 = 0.1932
eta:1.84
new: DataOwner1的最优x_1 = 0.1852
new: DataOwner2的最优x_2 = 0.2105
new: DataOwner3的最优x_3 = 0.2135
new: DataOwner4的最优x_4 = 0.1781
new: DataOwner5的最优x_5 = 0.1021
new: DataOwner6的最优x_6 = 0.1007
new: DataOwner7的最优x_7 = 0.1742
new: DataOwner8的最优x_8 = 0.2099
new: DataOwner9的最优x_9 = 0.0669
new: DataOwner10的最优x_10 = 0.1942
eta:1.85
new: DataOwner1的最优x_1 = 0.1862
new: DataOwner2的最优x_2 = 0.2117
new: DataOwner3的最优x_3 = 0.2147
new: DataOwner4的最优x_4 = 0.1791
new: DataOwner5的最优x_5 = 0.1027
new: DataOwner6的最优x_6 = 0.1012
new: DataOwner7的最优x_7 = 0.1752
new: DataOwner8的最优x_8 = 0.2111
new: DataOwner9的最优x_9 = 0.0673
new: DataOwner10的最优x_10 = 0.1953
eta:1.86
new: DataOwner1的最优x_1 = 0.1872
new: DataOwner2的最优x_2 = 0.2128
new: DataOwner3的最优x_3 = 0.2159
new: DataOwner4的最优x_4 = 0.1801
new: DataOwner5的最优x_5 = 0.1032
new: DataOwner6的最优x_6 = 0.1018
new: DataOwner7的最优x_7 = 0.1761
new: DataOwner8的最优x_8 = 0.2122
new: DataOwner9的最优x_9 = 0.0676
new: DataOwner10的最优x_10 = 0.1963
eta:1.87
new: DataOwner1的最优x_1 = 0.1882
new: DataOwner2的最优x_2 = 0.2140
new: DataOwner3的最优x_3 = 0.2170
new: DataOwner4的最优x_4 = 0.1810
new: DataOwner5的最优x_5 = 0.1038
new: DataOwner6的最优x_6 = 0.1023
new: DataOwner7的最优x_7 = 0.1771
new: DataOwner8的最优x_8 = 0.2133
new: DataOwner9的最优x_9 = 0.0680
new: DataOwner10的最优x_10 = 0.1974
eta:1.8800000000000001
new: DataOwner1的最优x_1 = 0.1892
new: DataOwner2的最优x_2 = 0.2151
new: DataOwner3的最优x_3 = 0.2182
new: DataOwner4的最优x_4 = 0.1820
new: DataOwner5的最优x_5 = 0.1043
new: DataOwner6的最优x_6 = 0.1029
new: DataOwner7的最优x_7 = 0.1780
new: DataOwner8的最优x_8 = 0.2145
new: DataOwner9的最优x_9 = 0.0684
new: DataOwner10的最优x_10 = 0.1984
eta:1.8900000000000001
new: DataOwner1的最优x_1 = 0.1902
new: DataOwner2的最优x_2 = 0.2163
new: DataOwner3的最优x_3 = 0.2193
new: DataOwner4的最优x_4 = 0.1830
new: DataOwner5的最优x_5 = 0.1049
new: DataOwner6的最优x_6 = 0.1034
new: DataOwner7的最优x_7 = 0.1790
new: DataOwner8的最优x_8 = 0.2156
new: DataOwner9的最优x_9 = 0.0687
new: DataOwner10的最优x_10 = 0.1995
eta:1.9000000000000001
new: DataOwner1的最优x_1 = 0.1912
new: DataOwner2的最优x_2 = 0.2174
new: DataOwner3的最优x_3 = 0.2205
new: DataOwner4的最优x_4 = 0.1840
new: DataOwner5的最优x_5 = 0.1055
new: DataOwner6的最优x_6 = 0.1040
new: DataOwner7的最优x_7 = 0.1799
new: DataOwner8的最优x_8 = 0.2168
new: DataOwner9的最优x_9 = 0.0691
new: DataOwner10的最优x_10 = 0.2005
eta:1.9100000000000001
new: DataOwner1的最优x_1 = 0.1922
new: DataOwner2的最优x_2 = 0.2186
new: DataOwner3的最优x_3 = 0.2217
new: DataOwner4的最优x_4 = 0.1849
new: DataOwner5的最优x_5 = 0.1060
new: DataOwner6的最优x_6 = 0.1045
new: DataOwner7的最优x_7 = 0.1808
new: DataOwner8的最优x_8 = 0.2179
new: DataOwner9的最优x_9 = 0.0695
new: DataOwner10的最优x_10 = 0.2016
eta:1.92
new: DataOwner1的最优x_1 = 0.1932
new: DataOwner2的最优x_2 = 0.2197
new: DataOwner3的最优x_3 = 0.2228
new: DataOwner4的最优x_4 = 0.1859
new: DataOwner5的最优x_5 = 0.1066
new: DataOwner6的最优x_6 = 0.1050
new: DataOwner7的最优x_7 = 0.1818
new: DataOwner8的最优x_8 = 0.2190
new: DataOwner9的最优x_9 = 0.0698
new: DataOwner10的最优x_10 = 0.2027
eta:1.93
new: DataOwner1的最优x_1 = 0.1943
new: DataOwner2的最优x_2 = 0.2208
new: DataOwner3的最优x_3 = 0.2240
new: DataOwner4的最优x_4 = 0.1869
new: DataOwner5的最优x_5 = 0.1071
new: DataOwner6的最优x_6 = 0.1056
new: DataOwner7的最优x_7 = 0.1827
new: DataOwner8的最优x_8 = 0.2202
new: DataOwner9的最优x_9 = 0.0702
new: DataOwner10的最优x_10 = 0.2037
eta:1.94
new: DataOwner1的最优x_1 = 0.1953
new: DataOwner2的最优x_2 = 0.2220
new: DataOwner3的最优x_3 = 0.2251
new: DataOwner4的最优x_4 = 0.1878
new: DataOwner5的最优x_5 = 0.1077
new: DataOwner6的最优x_6 = 0.1061
new: DataOwner7的最优x_7 = 0.1837
new: DataOwner8的最优x_8 = 0.2213
new: DataOwner9的最优x_9 = 0.0706
new: DataOwner10的最优x_10 = 0.2048
eta:1.95
new: DataOwner1的最优x_1 = 0.1963
new: DataOwner2的最优x_2 = 0.2231
new: DataOwner3的最优x_3 = 0.2263
new: DataOwner4的最优x_4 = 0.1888
new: DataOwner5的最优x_5 = 0.1082
new: DataOwner6的最优x_6 = 0.1067
new: DataOwner7的最优x_7 = 0.1846
new: DataOwner8的最优x_8 = 0.2225
new: DataOwner9的最优x_9 = 0.0709
new: DataOwner10的最优x_10 = 0.2058
eta:1.96
new: DataOwner1的最优x_1 = 0.1973
new: DataOwner2的最优x_2 = 0.2243
new: DataOwner3的最优x_3 = 0.2275
new: DataOwner4的最优x_4 = 0.1898
new: DataOwner5的最优x_5 = 0.1088
new: DataOwner6的最优x_6 = 0.1072
new: DataOwner7的最优x_7 = 0.1856
new: DataOwner8的最优x_8 = 0.2236
new: DataOwner9的最优x_9 = 0.0713
new: DataOwner10的最优x_10 = 0.2069
eta:1.97
new: DataOwner1的最优x_1 = 0.1983
new: DataOwner2的最优x_2 = 0.2254
new: DataOwner3的最优x_3 = 0.2286
new: DataOwner4的最优x_4 = 0.1907
new: DataOwner5的最优x_5 = 0.1093
new: DataOwner6的最优x_6 = 0.1078
new: DataOwner7的最优x_7 = 0.1865
new: DataOwner8的最优x_8 = 0.2248
new: DataOwner9的最优x_9 = 0.0716
new: DataOwner10的最优x_10 = 0.2079
eta:1.98
new: DataOwner1的最优x_1 = 0.1993
new: DataOwner2的最优x_2 = 0.2266
new: DataOwner3的最优x_3 = 0.2298
new: DataOwner4的最优x_4 = 0.1917
new: DataOwner5的最优x_5 = 0.1099
new: DataOwner6的最优x_6 = 0.1083
new: DataOwner7的最优x_7 = 0.1875
new: DataOwner8的最优x_8 = 0.2259
new: DataOwner9的最优x_9 = 0.0720
new: DataOwner10的最优x_10 = 0.2090
eta:1.99
new: DataOwner1的最优x_1 = 0.2003
new: DataOwner2的最优x_2 = 0.2277
new: DataOwner3的最优x_3 = 0.2309
new: DataOwner4的最优x_4 = 0.1927
new: DataOwner5的最优x_5 = 0.1104
new: DataOwner6的最优x_6 = 0.1089
new: DataOwner7的最优x_7 = 0.1884
new: DataOwner8的最优x_8 = 0.2270
new: DataOwner9的最优x_9 = 0.0724
new: DataOwner10的最优x_10 = 0.2100
eta:2.0
new: DataOwner1的最优x_1 = 0.2013
new: DataOwner2的最优x_2 = 0.2288
new: DataOwner3的最优x_3 = 0.2321
new: DataOwner4的最优x_4 = 0.1936
new: DataOwner5的最优x_5 = 0.1110
new: DataOwner6的最优x_6 = 0.1094
new: DataOwner7的最优x_7 = 0.1894
new: DataOwner8的最优x_8 = 0.2282
new: DataOwner9的最优x_9 = 0.0727
new: DataOwner10的最优x_10 = 0.2111
eta:2.0100000000000002
new: DataOwner1的最优x_1 = 0.2023
new: DataOwner2的最优x_2 = 0.2300
new: DataOwner3的最优x_3 = 0.2333
new: DataOwner4的最优x_4 = 0.1946
new: DataOwner5的最优x_5 = 0.1116
new: DataOwner6的最优x_6 = 0.1100
new: DataOwner7的最优x_7 = 0.1903
new: DataOwner8的最优x_8 = 0.2293
new: DataOwner9的最优x_9 = 0.0731
new: DataOwner10的最优x_10 = 0.2122
eta:2.02
new: DataOwner1的最优x_1 = 0.2033
new: DataOwner2的最优x_2 = 0.2311
new: DataOwner3的最优x_3 = 0.2344
new: DataOwner4的最优x_4 = 0.1956
new: DataOwner5的最优x_5 = 0.1121
new: DataOwner6的最优x_6 = 0.1105
new: DataOwner7的最优x_7 = 0.1913
new: DataOwner8的最优x_8 = 0.2305
new: DataOwner9的最优x_9 = 0.0735
new: DataOwner10的最优x_10 = 0.2132
eta:2.0300000000000002
new: DataOwner1的最优x_1 = 0.2043
new: DataOwner2的最优x_2 = 0.2323
new: DataOwner3的最优x_3 = 0.2356
new: DataOwner4的最优x_4 = 0.1965
new: DataOwner5的最优x_5 = 0.1127
new: DataOwner6的最优x_6 = 0.1111
new: DataOwner7的最优x_7 = 0.1922
new: DataOwner8的最优x_8 = 0.2316
new: DataOwner9的最优x_9 = 0.0738
new: DataOwner10的最优x_10 = 0.2143
eta:2.04
new: DataOwner1的最优x_1 = 0.2053
new: DataOwner2的最优x_2 = 0.2334
new: DataOwner3的最优x_3 = 0.2368
new: DataOwner4的最优x_4 = 0.1975
new: DataOwner5的最优x_5 = 0.1132
new: DataOwner6的最优x_6 = 0.1116
new: DataOwner7的最优x_7 = 0.1932
new: DataOwner8的最优x_8 = 0.2327
new: DataOwner9的最优x_9 = 0.0742
new: DataOwner10的最优x_10 = 0.2153
eta:2.05
new: DataOwner1的最优x_1 = 0.2063
new: DataOwner2的最优x_2 = 0.2346
new: DataOwner3的最优x_3 = 0.2379
new: DataOwner4的最优x_4 = 0.1985
new: DataOwner5的最优x_5 = 0.1138
new: DataOwner6的最优x_6 = 0.1122
new: DataOwner7的最优x_7 = 0.1941
new: DataOwner8的最优x_8 = 0.2339
new: DataOwner9的最优x_9 = 0.0746
new: DataOwner10的最优x_10 = 0.2164
eta:2.06
new: DataOwner1的最优x_1 = 0.2073
new: DataOwner2的最优x_2 = 0.2357
new: DataOwner3的最优x_3 = 0.2391
new: DataOwner4的最优x_4 = 0.1994
new: DataOwner5的最优x_5 = 0.1143
new: DataOwner6的最优x_6 = 0.1127
new: DataOwner7的最优x_7 = 0.1951
new: DataOwner8的最优x_8 = 0.2350
new: DataOwner9的最优x_9 = 0.0749
new: DataOwner10的最优x_10 = 0.2174
eta:2.07
new: DataOwner1的最优x_1 = 0.2083
new: DataOwner2的最优x_2 = 0.2369
new: DataOwner3的最优x_3 = 0.2402
new: DataOwner4的最优x_4 = 0.2004
new: DataOwner5的最优x_5 = 0.1149
new: DataOwner6的最优x_6 = 0.1133
new: DataOwner7的最优x_7 = 0.1960
new: DataOwner8的最优x_8 = 0.2362
new: DataOwner9的最优x_9 = 0.0753
new: DataOwner10的最优x_10 = 0.2185
eta:2.08
new: DataOwner1的最优x_1 = 0.2094
new: DataOwner2的最优x_2 = 0.2380
new: DataOwner3的最优x_3 = 0.2414
new: DataOwner4的最优x_4 = 0.2014
new: DataOwner5的最优x_5 = 0.1154
new: DataOwner6的最优x_6 = 0.1138
new: DataOwner7的最优x_7 = 0.1969
new: DataOwner8的最优x_8 = 0.2373
new: DataOwner9的最优x_9 = 0.0756
new: DataOwner10的最优x_10 = 0.2195
eta:2.09
new: DataOwner1的最优x_1 = 0.2104
new: DataOwner2的最优x_2 = 0.2391
new: DataOwner3的最优x_3 = 0.2426
new: DataOwner4的最优x_4 = 0.2023
new: DataOwner5的最优x_5 = 0.1160
new: DataOwner6的最优x_6 = 0.1144
new: DataOwner7的最优x_7 = 0.1979
new: DataOwner8的最优x_8 = 0.2384
new: DataOwner9的最优x_9 = 0.0760
new: DataOwner10的最优x_10 = 0.2206
eta:2.1
new: DataOwner1的最优x_1 = 0.2114
new: DataOwner2的最优x_2 = 0.2403
new: DataOwner3的最优x_3 = 0.2437
new: DataOwner4的最优x_4 = 0.2033
new: DataOwner5的最优x_5 = 0.1166
new: DataOwner6的最优x_6 = 0.1149
new: DataOwner7的最优x_7 = 0.1988
new: DataOwner8的最优x_8 = 0.2396
new: DataOwner9的最优x_9 = 0.0764
new: DataOwner10的最优x_10 = 0.2217
eta:2.11
new: DataOwner1的最优x_1 = 0.2124
new: DataOwner2的最优x_2 = 0.2414
new: DataOwner3的最优x_3 = 0.2449
new: DataOwner4的最优x_4 = 0.2043
new: DataOwner5的最优x_5 = 0.1171
new: DataOwner6的最优x_6 = 0.1154
new: DataOwner7的最优x_7 = 0.1998
new: DataOwner8的最优x_8 = 0.2407
new: DataOwner9的最优x_9 = 0.0767
new: DataOwner10的最优x_10 = 0.2227
eta:2.12
new: DataOwner1的最优x_1 = 0.2134
new: DataOwner2的最优x_2 = 0.2426
new: DataOwner3的最优x_3 = 0.2460
new: DataOwner4的最优x_4 = 0.2053
new: DataOwner5的最优x_5 = 0.1177
new: DataOwner6的最优x_6 = 0.1160
new: DataOwner7的最优x_7 = 0.2007
new: DataOwner8的最优x_8 = 0.2419
new: DataOwner9的最优x_9 = 0.0771
new: DataOwner10的最优x_10 = 0.2238
eta:2.13
new: DataOwner1的最优x_1 = 0.2144
new: DataOwner2的最优x_2 = 0.2437
new: DataOwner3的最优x_3 = 0.2472
new: DataOwner4的最优x_4 = 0.2062
new: DataOwner5的最优x_5 = 0.1182
new: DataOwner6的最优x_6 = 0.1165
new: DataOwner7的最优x_7 = 0.2017
new: DataOwner8的最优x_8 = 0.2430
new: DataOwner9的最优x_9 = 0.0775
new: DataOwner10的最优x_10 = 0.2248
eta:2.14
new: DataOwner1的最优x_1 = 0.2154
new: DataOwner2的最优x_2 = 0.2449
new: DataOwner3的最优x_3 = 0.2484
new: DataOwner4的最优x_4 = 0.2072
new: DataOwner5的最优x_5 = 0.1188
new: DataOwner6的最优x_6 = 0.1171
new: DataOwner7的最优x_7 = 0.2026
new: DataOwner8的最优x_8 = 0.2441
new: DataOwner9的最优x_9 = 0.0778
new: DataOwner10的最优x_10 = 0.2259
eta:2.15
new: DataOwner1的最优x_1 = 0.2164
new: DataOwner2的最优x_2 = 0.2460
new: DataOwner3的最优x_3 = 0.2495
new: DataOwner4的最优x_4 = 0.2082
new: DataOwner5的最优x_5 = 0.1193
new: DataOwner6的最优x_6 = 0.1176
new: DataOwner7的最优x_7 = 0.2036
new: DataOwner8的最优x_8 = 0.2453
new: DataOwner9的最优x_9 = 0.0782
new: DataOwner10的最优x_10 = 0.2269
eta:2.16
new: DataOwner1的最优x_1 = 0.2174
new: DataOwner2的最优x_2 = 0.2472
new: DataOwner3的最优x_3 = 0.2507
new: DataOwner4的最优x_4 = 0.2091
new: DataOwner5的最优x_5 = 0.1199
new: DataOwner6的最优x_6 = 0.1182
new: DataOwner7的最优x_7 = 0.2045
new: DataOwner8的最优x_8 = 0.2464
new: DataOwner9的最优x_9 = 0.0786
new: DataOwner10的最优x_10 = 0.2280
eta:2.17
new: DataOwner1的最优x_1 = 0.2184
new: DataOwner2的最优x_2 = 0.2483
new: DataOwner3的最优x_3 = 0.2518
new: DataOwner4的最优x_4 = 0.2101
new: DataOwner5的最优x_5 = 0.1204
new: DataOwner6的最优x_6 = 0.1187
new: DataOwner7的最优x_7 = 0.2055
new: DataOwner8的最优x_8 = 0.2476
new: DataOwner9的最优x_9 = 0.0789
new: DataOwner10的最优x_10 = 0.2290
eta:2.18
new: DataOwner1的最优x_1 = 0.2194
new: DataOwner2的最优x_2 = 0.2494
new: DataOwner3的最优x_3 = 0.2530
new: DataOwner4的最优x_4 = 0.2111
new: DataOwner5的最优x_5 = 0.1210
new: DataOwner6的最优x_6 = 0.1193
new: DataOwner7的最优x_7 = 0.2064
new: DataOwner8的最优x_8 = 0.2487
new: DataOwner9的最优x_9 = 0.0793
new: DataOwner10的最优x_10 = 0.2301
eta:2.19
new: DataOwner1的最优x_1 = 0.2204
new: DataOwner2的最优x_2 = 0.2506
new: DataOwner3的最优x_3 = 0.2542
new: DataOwner4的最优x_4 = 0.2120
new: DataOwner5的最优x_5 = 0.1215
new: DataOwner6的最优x_6 = 0.1198
new: DataOwner7的最优x_7 = 0.2074
new: DataOwner8的最优x_8 = 0.2499
new: DataOwner9的最优x_9 = 0.0796
new: DataOwner10的最优x_10 = 0.2312
eta:2.2
new: DataOwner1的最优x_1 = 0.2214
new: DataOwner2的最优x_2 = 0.2517
new: DataOwner3的最优x_3 = 0.2553
new: DataOwner4的最优x_4 = 0.2130
new: DataOwner5的最优x_5 = 0.1221
new: DataOwner6的最优x_6 = 0.1204
new: DataOwner7的最优x_7 = 0.2083
new: DataOwner8的最优x_8 = 0.2510
new: DataOwner9的最优x_9 = 0.0800
new: DataOwner10的最优x_10 = 0.2322
eta:2.21
new: DataOwner1的最优x_1 = 0.2224
new: DataOwner2的最优x_2 = 0.2529
new: DataOwner3的最优x_3 = 0.2565
new: DataOwner4的最优x_4 = 0.2140
new: DataOwner5的最优x_5 = 0.1227
new: DataOwner6的最优x_6 = 0.1209
new: DataOwner7的最优x_7 = 0.2093
new: DataOwner8的最优x_8 = 0.2521
new: DataOwner9的最优x_9 = 0.0804
new: DataOwner10的最优x_10 = 0.2333
eta:2.22
new: DataOwner1的最优x_1 = 0.2234
new: DataOwner2的最优x_2 = 0.2540
new: DataOwner3的最优x_3 = 0.2576
new: DataOwner4的最优x_4 = 0.2149
new: DataOwner5的最优x_5 = 0.1232
new: DataOwner6的最优x_6 = 0.1215
new: DataOwner7的最优x_7 = 0.2102
new: DataOwner8的最优x_8 = 0.2533
new: DataOwner9的最优x_9 = 0.0807
new: DataOwner10的最优x_10 = 0.2343
eta:2.23
new: DataOwner1的最优x_1 = 0.2244
new: DataOwner2的最优x_2 = 0.2552
new: DataOwner3的最优x_3 = 0.2588
new: DataOwner4的最优x_4 = 0.2159
new: DataOwner5的最优x_5 = 0.1238
new: DataOwner6的最优x_6 = 0.1220
new: DataOwner7的最优x_7 = 0.2111
new: DataOwner8的最优x_8 = 0.2544
new: DataOwner9的最优x_9 = 0.0811
new: DataOwner10的最优x_10 = 0.2354
eta:2.24
new: DataOwner1的最优x_1 = 0.2255
new: DataOwner2的最优x_2 = 0.2563
new: DataOwner3的最优x_3 = 0.2600
new: DataOwner4的最优x_4 = 0.2169
new: DataOwner5的最优x_5 = 0.1243
new: DataOwner6的最优x_6 = 0.1226
new: DataOwner7的最优x_7 = 0.2121
new: DataOwner8的最优x_8 = 0.2556
new: DataOwner9的最优x_9 = 0.0815
new: DataOwner10的最优x_10 = 0.2364
eta:2.25
new: DataOwner1的最优x_1 = 0.2265
new: DataOwner2的最优x_2 = 0.2575
new: DataOwner3的最优x_3 = 0.2611
new: DataOwner4的最优x_4 = 0.2178
new: DataOwner5的最优x_5 = 0.1249
new: DataOwner6的最优x_6 = 0.1231
new: DataOwner7的最优x_7 = 0.2130
new: DataOwner8的最优x_8 = 0.2567
new: DataOwner9的最优x_9 = 0.0818
new: DataOwner10的最优x_10 = 0.2375
eta:2.2600000000000002
new: DataOwner1的最优x_1 = 0.2275
new: DataOwner2的最优x_2 = 0.2586
new: DataOwner3的最优x_3 = 0.2623
new: DataOwner4的最优x_4 = 0.2188
new: DataOwner5的最优x_5 = 0.1254
new: DataOwner6的最优x_6 = 0.1237
new: DataOwner7的最优x_7 = 0.2140
new: DataOwner8的最优x_8 = 0.2578
new: DataOwner9的最优x_9 = 0.0822
new: DataOwner10的最优x_10 = 0.2385
eta:2.27
new: DataOwner1的最优x_1 = 0.2285
new: DataOwner2的最优x_2 = 0.2597
new: DataOwner3的最优x_3 = 0.2634
new: DataOwner4的最优x_4 = 0.2198
new: DataOwner5的最优x_5 = 0.1260
new: DataOwner6的最优x_6 = 0.1242
new: DataOwner7的最优x_7 = 0.2149
new: DataOwner8的最优x_8 = 0.2590
new: DataOwner9的最优x_9 = 0.0826
new: DataOwner10的最优x_10 = 0.2396
eta:2.2800000000000002
new: DataOwner1的最优x_1 = 0.2295
new: DataOwner2的最优x_2 = 0.2609
new: DataOwner3的最优x_3 = 0.2646
new: DataOwner4的最优x_4 = 0.2207
new: DataOwner5的最优x_5 = 0.1265
new: DataOwner6的最优x_6 = 0.1247
new: DataOwner7的最优x_7 = 0.2159
new: DataOwner8的最优x_8 = 0.2601
new: DataOwner9的最优x_9 = 0.0829
new: DataOwner10的最优x_10 = 0.2407
eta:2.29
new: DataOwner1的最优x_1 = 0.2305
new: DataOwner2的最优x_2 = 0.2620
new: DataOwner3的最优x_3 = 0.2658
new: DataOwner4的最优x_4 = 0.2217
new: DataOwner5的最优x_5 = 0.1271
new: DataOwner6的最优x_6 = 0.1253
new: DataOwner7的最优x_7 = 0.2168
new: DataOwner8的最优x_8 = 0.2613
new: DataOwner9的最优x_9 = 0.0833
new: DataOwner10的最优x_10 = 0.2417
eta:2.3000000000000003
new: DataOwner1的最优x_1 = 0.2315
new: DataOwner2的最优x_2 = 0.2632
new: DataOwner3的最优x_3 = 0.2669
new: DataOwner4的最优x_4 = 0.2227
new: DataOwner5的最优x_5 = 0.1277
new: DataOwner6的最优x_6 = 0.1258
new: DataOwner7的最优x_7 = 0.2178
new: DataOwner8的最优x_8 = 0.2624
new: DataOwner9的最优x_9 = 0.0836
new: DataOwner10的最优x_10 = 0.2428
eta:2.31
new: DataOwner1的最优x_1 = 0.2325
new: DataOwner2的最优x_2 = 0.2643
new: DataOwner3的最优x_3 = 0.2681
new: DataOwner4的最优x_4 = 0.2236
new: DataOwner5的最优x_5 = 0.1282
new: DataOwner6的最优x_6 = 0.1264
new: DataOwner7的最优x_7 = 0.2187
new: DataOwner8的最优x_8 = 0.2635
new: DataOwner9的最优x_9 = 0.0840
new: DataOwner10的最优x_10 = 0.2438
eta:2.32
new: DataOwner1的最优x_1 = 0.2335
new: DataOwner2的最优x_2 = 0.2655
new: DataOwner3的最优x_3 = 0.2692
new: DataOwner4的最优x_4 = 0.2246
new: DataOwner5的最优x_5 = 0.1288
new: DataOwner6的最优x_6 = 0.1269
new: DataOwner7的最优x_7 = 0.2197
new: DataOwner8的最优x_8 = 0.2647
new: DataOwner9的最优x_9 = 0.0844
new: DataOwner10的最优x_10 = 0.2449
eta:2.33
new: DataOwner1的最优x_1 = 0.2345
new: DataOwner2的最优x_2 = 0.2666
new: DataOwner3的最优x_3 = 0.2704
new: DataOwner4的最优x_4 = 0.2256
new: DataOwner5的最优x_5 = 0.1293
new: DataOwner6的最优x_6 = 0.1275
new: DataOwner7的最优x_7 = 0.2206
new: DataOwner8的最优x_8 = 0.2658
new: DataOwner9的最优x_9 = 0.0847
new: DataOwner10的最优x_10 = 0.2459
eta:2.34
new: DataOwner1的最优x_1 = 0.2355
new: DataOwner2的最优x_2 = 0.2678
new: DataOwner3的最优x_3 = 0.2716
new: DataOwner4的最优x_4 = 0.2265
new: DataOwner5的最优x_5 = 0.1299
new: DataOwner6的最优x_6 = 0.1280
new: DataOwner7的最优x_7 = 0.2216
new: DataOwner8的最优x_8 = 0.2670
new: DataOwner9的最优x_9 = 0.0851
new: DataOwner10的最优x_10 = 0.2470
eta:2.35
new: DataOwner1的最优x_1 = 0.2365
new: DataOwner2的最优x_2 = 0.2689
new: DataOwner3的最优x_3 = 0.2727
new: DataOwner4的最优x_4 = 0.2275
new: DataOwner5的最优x_5 = 0.1304
new: DataOwner6的最优x_6 = 0.1286
new: DataOwner7的最优x_7 = 0.2225
new: DataOwner8的最优x_8 = 0.2681
new: DataOwner9的最优x_9 = 0.0855
new: DataOwner10的最优x_10 = 0.2480
eta:2.36
new: DataOwner1的最优x_1 = 0.2375
new: DataOwner2的最优x_2 = 0.2700
new: DataOwner3的最优x_3 = 0.2739
new: DataOwner4的最优x_4 = 0.2285
new: DataOwner5的最优x_5 = 0.1310
new: DataOwner6的最优x_6 = 0.1291
new: DataOwner7的最优x_7 = 0.2235
new: DataOwner8的最优x_8 = 0.2692
new: DataOwner9的最优x_9 = 0.0858
new: DataOwner10的最优x_10 = 0.2491
eta:2.37
new: DataOwner1的最优x_1 = 0.2385
new: DataOwner2的最优x_2 = 0.2712
new: DataOwner3的最优x_3 = 0.2751
new: DataOwner4的最优x_4 = 0.2295
new: DataOwner5的最优x_5 = 0.1315
new: DataOwner6的最优x_6 = 0.1297
new: DataOwner7的最优x_7 = 0.2244
new: DataOwner8的最优x_8 = 0.2704
new: DataOwner9的最优x_9 = 0.0862
new: DataOwner10的最优x_10 = 0.2502
eta:2.38
new: DataOwner1的最优x_1 = 0.2395
new: DataOwner2的最优x_2 = 0.2723
new: DataOwner3的最优x_3 = 0.2762
new: DataOwner4的最优x_4 = 0.2304
new: DataOwner5的最优x_5 = 0.1321
new: DataOwner6的最优x_6 = 0.1302
new: DataOwner7的最优x_7 = 0.2254
new: DataOwner8的最优x_8 = 0.2715
new: DataOwner9的最优x_9 = 0.0866
new: DataOwner10的最优x_10 = 0.2512
eta:2.39
new: DataOwner1的最优x_1 = 0.2406
new: DataOwner2的最优x_2 = 0.2735
new: DataOwner3的最优x_3 = 0.2774
new: DataOwner4的最优x_4 = 0.2314
new: DataOwner5的最优x_5 = 0.1326
new: DataOwner6的最优x_6 = 0.1308
new: DataOwner7的最优x_7 = 0.2263
new: DataOwner8的最优x_8 = 0.2727
new: DataOwner9的最优x_9 = 0.0869
new: DataOwner10的最优x_10 = 0.2523
eta:2.4
new: DataOwner1的最优x_1 = 0.2416
new: DataOwner2的最优x_2 = 0.2746
new: DataOwner3的最优x_3 = 0.2785
new: DataOwner4的最优x_4 = 0.2324
new: DataOwner5的最优x_5 = 0.1332
new: DataOwner6的最优x_6 = 0.1313
new: DataOwner7的最优x_7 = 0.2272
new: DataOwner8的最优x_8 = 0.2738
new: DataOwner9的最优x_9 = 0.0873
new: DataOwner10的最优x_10 = 0.2533
eta:2.41
new: DataOwner1的最优x_1 = 0.2426
new: DataOwner2的最优x_2 = 0.2758
new: DataOwner3的最优x_3 = 0.2797
new: DataOwner4的最优x_4 = 0.2333
new: DataOwner5的最优x_5 = 0.1338
new: DataOwner6的最优x_6 = 0.1319
new: DataOwner7的最优x_7 = 0.2282
new: DataOwner8的最优x_8 = 0.2749
new: DataOwner9的最优x_9 = 0.0876
new: DataOwner10的最优x_10 = 0.2544
eta:2.42
new: DataOwner1的最优x_1 = 0.2436
new: DataOwner2的最优x_2 = 0.2769
new: DataOwner3的最优x_3 = 0.2809
new: DataOwner4的最优x_4 = 0.2343
new: DataOwner5的最优x_5 = 0.1343
new: DataOwner6的最优x_6 = 0.1324
new: DataOwner7的最优x_7 = 0.2291
new: DataOwner8的最优x_8 = 0.2761
new: DataOwner9的最优x_9 = 0.0880
new: DataOwner10的最优x_10 = 0.2554
eta:2.43
new: DataOwner1的最优x_1 = 0.2446
new: DataOwner2的最优x_2 = 0.2781
new: DataOwner3的最优x_3 = 0.2820
new: DataOwner4的最优x_4 = 0.2353
new: DataOwner5的最优x_5 = 0.1349
new: DataOwner6的最优x_6 = 0.1330
new: DataOwner7的最优x_7 = 0.2301
new: DataOwner8的最优x_8 = 0.2772
new: DataOwner9的最优x_9 = 0.0884
new: DataOwner10的最优x_10 = 0.2565
eta:2.44
new: DataOwner1的最优x_1 = 0.2456
new: DataOwner2的最优x_2 = 0.2792
new: DataOwner3的最优x_3 = 0.2832
new: DataOwner4的最优x_4 = 0.2362
new: DataOwner5的最优x_5 = 0.1354
new: DataOwner6的最优x_6 = 0.1335
new: DataOwner7的最优x_7 = 0.2310
new: DataOwner8的最优x_8 = 0.2784
new: DataOwner9的最优x_9 = 0.0887
new: DataOwner10的最优x_10 = 0.2575
eta:2.45
new: DataOwner1的最优x_1 = 0.2466
new: DataOwner2的最优x_2 = 0.2803
new: DataOwner3的最优x_3 = 0.2843
new: DataOwner4的最优x_4 = 0.2372
new: DataOwner5的最优x_5 = 0.1360
new: DataOwner6的最优x_6 = 0.1340
new: DataOwner7的最优x_7 = 0.2320
new: DataOwner8的最优x_8 = 0.2795
new: DataOwner9的最优x_9 = 0.0891
new: DataOwner10的最优x_10 = 0.2586
eta:2.46
new: DataOwner1的最优x_1 = 0.2476
new: DataOwner2的最优x_2 = 0.2815
new: DataOwner3的最优x_3 = 0.2855
new: DataOwner4的最优x_4 = 0.2382
new: DataOwner5的最优x_5 = 0.1365
new: DataOwner6的最优x_6 = 0.1346
new: DataOwner7的最优x_7 = 0.2329
new: DataOwner8的最优x_8 = 0.2807
new: DataOwner9的最优x_9 = 0.0895
new: DataOwner10的最优x_10 = 0.2597
eta:2.47
new: DataOwner1的最优x_1 = 0.2486
new: DataOwner2的最优x_2 = 0.2826
new: DataOwner3的最优x_3 = 0.2867
new: DataOwner4的最优x_4 = 0.2391
new: DataOwner5的最优x_5 = 0.1371
new: DataOwner6的最优x_6 = 0.1351
new: DataOwner7的最优x_7 = 0.2339
new: DataOwner8的最优x_8 = 0.2818
new: DataOwner9的最优x_9 = 0.0898
new: DataOwner10的最优x_10 = 0.2607
eta:2.48
new: DataOwner1的最优x_1 = 0.2496
new: DataOwner2的最优x_2 = 0.2838
new: DataOwner3的最优x_3 = 0.2878
new: DataOwner4的最优x_4 = 0.2401
new: DataOwner5的最优x_5 = 0.1376
new: DataOwner6的最优x_6 = 0.1357
new: DataOwner7的最优x_7 = 0.2348
new: DataOwner8的最优x_8 = 0.2829
new: DataOwner9的最优x_9 = 0.0902
new: DataOwner10的最优x_10 = 0.2618
eta:2.49
new: DataOwner1的最优x_1 = 0.2506
new: DataOwner2的最优x_2 = 0.2849
new: DataOwner3的最优x_3 = 0.2890
new: DataOwner4的最优x_4 = 0.2411
new: DataOwner5的最优x_5 = 0.1382
new: DataOwner6的最优x_6 = 0.1362
new: DataOwner7的最优x_7 = 0.2358
new: DataOwner8的最优x_8 = 0.2841
new: DataOwner9的最优x_9 = 0.0906
new: DataOwner10的最优x_10 = 0.2628
eta:2.5
new: DataOwner1的最优x_1 = 0.2516
new: DataOwner2的最优x_2 = 0.2861
new: DataOwner3的最优x_3 = 0.2901
new: DataOwner4的最优x_4 = 0.2420
new: DataOwner5的最优x_5 = 0.1388
new: DataOwner6的最优x_6 = 0.1368
new: DataOwner7的最优x_7 = 0.2367
new: DataOwner8的最优x_8 = 0.2852
new: DataOwner9的最优x_9 = 0.0909
new: DataOwner10的最优x_10 = 0.2639
eta:2.5100000000000002
new: DataOwner1的最优x_1 = 0.2526
new: DataOwner2的最优x_2 = 0.2872
new: DataOwner3的最优x_3 = 0.2913
new: DataOwner4的最优x_4 = 0.2430
new: DataOwner5的最优x_5 = 0.1393
new: DataOwner6的最优x_6 = 0.1373
new: DataOwner7的最优x_7 = 0.2377
new: DataOwner8的最优x_8 = 0.2864
new: DataOwner9的最优x_9 = 0.0913
new: DataOwner10的最优x_10 = 0.2649
eta:2.52
new: DataOwner1的最优x_1 = 0.2536
new: DataOwner2的最优x_2 = 0.2884
new: DataOwner3的最优x_3 = 0.2925
new: DataOwner4的最优x_4 = 0.2440
new: DataOwner5的最优x_5 = 0.1399
new: DataOwner6的最优x_6 = 0.1379
new: DataOwner7的最优x_7 = 0.2386
new: DataOwner8的最优x_8 = 0.2875
new: DataOwner9的最优x_9 = 0.0916
new: DataOwner10的最优x_10 = 0.2660
eta:2.5300000000000002
new: DataOwner1的最优x_1 = 0.2546
new: DataOwner2的最优x_2 = 0.2895
new: DataOwner3的最优x_3 = 0.2936
new: DataOwner4的最优x_4 = 0.2449
new: DataOwner5的最优x_5 = 0.1404
new: DataOwner6的最优x_6 = 0.1384
new: DataOwner7的最优x_7 = 0.2396
new: DataOwner8的最优x_8 = 0.2886
new: DataOwner9的最优x_9 = 0.0920
new: DataOwner10的最优x_10 = 0.2670
eta:2.54
new: DataOwner1的最优x_1 = 0.2556
new: DataOwner2的最优x_2 = 0.2906
new: DataOwner3的最优x_3 = 0.2948
new: DataOwner4的最优x_4 = 0.2459
new: DataOwner5的最优x_5 = 0.1410
new: DataOwner6的最优x_6 = 0.1390
new: DataOwner7的最优x_7 = 0.2405
new: DataOwner8的最优x_8 = 0.2898
new: DataOwner9的最优x_9 = 0.0924
new: DataOwner10的最优x_10 = 0.2681
eta:2.5500000000000003
new: DataOwner1的最优x_1 = 0.2567
new: DataOwner2的最优x_2 = 0.2918
new: DataOwner3的最优x_3 = 0.2959
new: DataOwner4的最优x_4 = 0.2469
new: DataOwner5的最优x_5 = 0.1415
new: DataOwner6的最优x_6 = 0.1395
new: DataOwner7的最优x_7 = 0.2414
new: DataOwner8的最优x_8 = 0.2909
new: DataOwner9的最优x_9 = 0.0927
new: DataOwner10的最优x_10 = 0.2692
eta:2.56
new: DataOwner1的最优x_1 = 0.2577
new: DataOwner2的最优x_2 = 0.2929
new: DataOwner3的最优x_3 = 0.2971
new: DataOwner4的最优x_4 = 0.2478
new: DataOwner5的最优x_5 = 0.1421
new: DataOwner6的最优x_6 = 0.1401
new: DataOwner7的最优x_7 = 0.2424
new: DataOwner8的最优x_8 = 0.2921
new: DataOwner9的最优x_9 = 0.0931
new: DataOwner10的最优x_10 = 0.2702
eta:2.57
new: DataOwner1的最优x_1 = 0.2587
new: DataOwner2的最优x_2 = 0.2941
new: DataOwner3的最优x_3 = 0.2983
new: DataOwner4的最优x_4 = 0.2488
new: DataOwner5的最优x_5 = 0.1426
new: DataOwner6的最优x_6 = 0.1406
new: DataOwner7的最优x_7 = 0.2433
new: DataOwner8的最优x_8 = 0.2932
new: DataOwner9的最优x_9 = 0.0935
new: DataOwner10的最优x_10 = 0.2713
eta:2.58
new: DataOwner1的最优x_1 = 0.2597
new: DataOwner2的最优x_2 = 0.2952
new: DataOwner3的最优x_3 = 0.2994
new: DataOwner4的最优x_4 = 0.2498
new: DataOwner5的最优x_5 = 0.1432
new: DataOwner6的最优x_6 = 0.1412
new: DataOwner7的最优x_7 = 0.2443
new: DataOwner8的最优x_8 = 0.2943
new: DataOwner9的最优x_9 = 0.0938
new: DataOwner10的最优x_10 = 0.2723
eta:2.59
new: DataOwner1的最优x_1 = 0.2607
new: DataOwner2的最优x_2 = 0.2964
new: DataOwner3的最优x_3 = 0.3006
new: DataOwner4的最优x_4 = 0.2508
new: DataOwner5的最优x_5 = 0.1438
new: DataOwner6的最优x_6 = 0.1417
new: DataOwner7的最优x_7 = 0.2452
new: DataOwner8的最优x_8 = 0.2955
new: DataOwner9的最优x_9 = 0.0942
new: DataOwner10的最优x_10 = 0.2734
eta:2.6
new: DataOwner1的最优x_1 = 0.2617
new: DataOwner2的最优x_2 = 0.2975
new: DataOwner3的最优x_3 = 0.3017
new: DataOwner4的最优x_4 = 0.2517
new: DataOwner5的最优x_5 = 0.1443
new: DataOwner6的最优x_6 = 0.1423
new: DataOwner7的最优x_7 = 0.2462
new: DataOwner8的最优x_8 = 0.2966
new: DataOwner9的最优x_9 = 0.0946
new: DataOwner10的最优x_10 = 0.2744
eta:2.61
new: DataOwner1的最优x_1 = 0.2627
new: DataOwner2的最优x_2 = 0.2986
new: DataOwner3的最优x_3 = 0.3029
new: DataOwner4的最优x_4 = 0.2527
new: DataOwner5的最优x_5 = 0.1449
new: DataOwner6的最优x_6 = 0.1428
new: DataOwner7的最优x_7 = 0.2471
new: DataOwner8的最优x_8 = 0.2978
new: DataOwner9的最优x_9 = 0.0949
new: DataOwner10的最优x_10 = 0.2755
eta:2.62
new: DataOwner1的最优x_1 = 0.2637
new: DataOwner2的最优x_2 = 0.2998
new: DataOwner3的最优x_3 = 0.3041
new: DataOwner4的最优x_4 = 0.2537
new: DataOwner5的最优x_5 = 0.1454
new: DataOwner6的最优x_6 = 0.1433
new: DataOwner7的最优x_7 = 0.2481
new: DataOwner8的最优x_8 = 0.2989
new: DataOwner9的最优x_9 = 0.0953
new: DataOwner10的最优x_10 = 0.2765
eta:2.63
new: DataOwner1的最优x_1 = 0.2647
new: DataOwner2的最优x_2 = 0.3009
new: DataOwner3的最优x_3 = 0.3052
new: DataOwner4的最优x_4 = 0.2546
new: DataOwner5的最优x_5 = 0.1460
new: DataOwner6的最优x_6 = 0.1439
new: DataOwner7的最优x_7 = 0.2490
new: DataOwner8的最优x_8 = 0.3000
new: DataOwner9的最优x_9 = 0.0956
new: DataOwner10的最优x_10 = 0.2776
eta:2.64
new: DataOwner1的最优x_1 = 0.2657
new: DataOwner2的最优x_2 = 0.3021
new: DataOwner3的最优x_3 = 0.3064
new: DataOwner4的最优x_4 = 0.2556
new: DataOwner5的最优x_5 = 0.1465
new: DataOwner6的最优x_6 = 0.1444
new: DataOwner7的最优x_7 = 0.2500
new: DataOwner8的最优x_8 = 0.3012
new: DataOwner9的最优x_9 = 0.0960
new: DataOwner10的最优x_10 = 0.2787
eta:2.65
new: DataOwner1的最优x_1 = 0.2667
new: DataOwner2的最优x_2 = 0.3032
new: DataOwner3的最优x_3 = 0.3075
new: DataOwner4的最优x_4 = 0.2566
new: DataOwner5的最优x_5 = 0.1471
new: DataOwner6的最优x_6 = 0.1450
new: DataOwner7的最优x_7 = 0.2509
new: DataOwner8的最优x_8 = 0.3023
new: DataOwner9的最优x_9 = 0.0964
new: DataOwner10的最优x_10 = 0.2797
eta:2.66
new: DataOwner1的最优x_1 = 0.2677
new: DataOwner2的最优x_2 = 0.3044
new: DataOwner3的最优x_3 = 0.3087
new: DataOwner4的最优x_4 = 0.2575
new: DataOwner5的最优x_5 = 0.1476
new: DataOwner6的最优x_6 = 0.1455
new: DataOwner7的最优x_7 = 0.2519
new: DataOwner8的最优x_8 = 0.3035
new: DataOwner9的最优x_9 = 0.0967
new: DataOwner10的最优x_10 = 0.2808
eta:2.67
new: DataOwner1的最优x_1 = 0.2687
new: DataOwner2的最优x_2 = 0.3055
new: DataOwner3的最优x_3 = 0.3099
new: DataOwner4的最优x_4 = 0.2585
new: DataOwner5的最优x_5 = 0.1482
new: DataOwner6的最优x_6 = 0.1461
new: DataOwner7的最优x_7 = 0.2528
new: DataOwner8的最优x_8 = 0.3046
new: DataOwner9的最优x_9 = 0.0971
new: DataOwner10的最优x_10 = 0.2818
eta:2.68
new: DataOwner1的最优x_1 = 0.2697
new: DataOwner2的最优x_2 = 0.3067
new: DataOwner3的最优x_3 = 0.3110
new: DataOwner4的最优x_4 = 0.2595
new: DataOwner5的最优x_5 = 0.1487
new: DataOwner6的最优x_6 = 0.1466
new: DataOwner7的最优x_7 = 0.2538
new: DataOwner8的最优x_8 = 0.3058
new: DataOwner9的最优x_9 = 0.0975
new: DataOwner10的最优x_10 = 0.2829
eta:2.69
new: DataOwner1的最优x_1 = 0.2707
new: DataOwner2的最优x_2 = 0.3078
new: DataOwner3的最优x_3 = 0.3122
new: DataOwner4的最优x_4 = 0.2604
new: DataOwner5的最优x_5 = 0.1493
new: DataOwner6的最优x_6 = 0.1472
new: DataOwner7的最优x_7 = 0.2547
new: DataOwner8的最优x_8 = 0.3069
new: DataOwner9的最优x_9 = 0.0978
new: DataOwner10的最优x_10 = 0.2839
eta:2.7
new: DataOwner1的最优x_1 = 0.2718
new: DataOwner2的最优x_2 = 0.3089
new: DataOwner3的最优x_3 = 0.3133
new: DataOwner4的最优x_4 = 0.2614
new: DataOwner5的最优x_5 = 0.1499
new: DataOwner6的最优x_6 = 0.1477
new: DataOwner7的最优x_7 = 0.2557
new: DataOwner8的最优x_8 = 0.3080
new: DataOwner9的最优x_9 = 0.0982
new: DataOwner10的最优x_10 = 0.2850
eta:2.71
new: DataOwner1的最优x_1 = 0.2728
new: DataOwner2的最优x_2 = 0.3101
new: DataOwner3的最优x_3 = 0.3145
new: DataOwner4的最优x_4 = 0.2624
new: DataOwner5的最优x_5 = 0.1504
new: DataOwner6的最优x_6 = 0.1483
new: DataOwner7的最优x_7 = 0.2566
new: DataOwner8的最优x_8 = 0.3092
new: DataOwner9的最优x_9 = 0.0986
new: DataOwner10的最优x_10 = 0.2860
eta:2.72
new: DataOwner1的最优x_1 = 0.2738
new: DataOwner2的最优x_2 = 0.3112
new: DataOwner3的最优x_3 = 0.3157
new: DataOwner4的最优x_4 = 0.2633
new: DataOwner5的最优x_5 = 0.1510
new: DataOwner6的最优x_6 = 0.1488
new: DataOwner7的最优x_7 = 0.2575
new: DataOwner8的最优x_8 = 0.3103
new: DataOwner9的最优x_9 = 0.0989
new: DataOwner10的最优x_10 = 0.2871
eta:2.73
new: DataOwner1的最优x_1 = 0.2748
new: DataOwner2的最优x_2 = 0.3124
new: DataOwner3的最优x_3 = 0.3168
new: DataOwner4的最优x_4 = 0.2643
new: DataOwner5的最优x_5 = 0.1515
new: DataOwner6的最优x_6 = 0.1494
new: DataOwner7的最优x_7 = 0.2585
new: DataOwner8的最优x_8 = 0.3115
new: DataOwner9的最优x_9 = 0.0993
new: DataOwner10的最优x_10 = 0.2882
eta:2.74
new: DataOwner1的最优x_1 = 0.2758
new: DataOwner2的最优x_2 = 0.3135
new: DataOwner3的最优x_3 = 0.3180
new: DataOwner4的最优x_4 = 0.2653
new: DataOwner5的最优x_5 = 0.1521
new: DataOwner6的最优x_6 = 0.1499
new: DataOwner7的最优x_7 = 0.2594
new: DataOwner8的最优x_8 = 0.3126
new: DataOwner9的最优x_9 = 0.0996
new: DataOwner10的最优x_10 = 0.2892
eta:2.75
new: DataOwner1的最优x_1 = 0.2768
new: DataOwner2的最优x_2 = 0.3147
new: DataOwner3的最优x_3 = 0.3192
new: DataOwner4的最优x_4 = 0.2662
new: DataOwner5的最优x_5 = 0.1526
new: DataOwner6的最优x_6 = 0.1505
new: DataOwner7的最优x_7 = 0.2604
new: DataOwner8的最优x_8 = 0.3137
new: DataOwner9的最优x_9 = 0.1000
new: DataOwner10的最优x_10 = 0.2903
eta:2.7600000000000002
new: DataOwner1的最优x_1 = 0.2778
new: DataOwner2的最优x_2 = 0.3158
new: DataOwner3的最优x_3 = 0.3203
new: DataOwner4的最优x_4 = 0.2672
new: DataOwner5的最优x_5 = 0.1532
new: DataOwner6的最优x_6 = 0.1510
new: DataOwner7的最优x_7 = 0.2613
new: DataOwner8的最优x_8 = 0.3149
new: DataOwner9的最优x_9 = 0.1004
new: DataOwner10的最优x_10 = 0.2913
eta:2.77
new: DataOwner1的最优x_1 = 0.2788
new: DataOwner2的最优x_2 = 0.3170
new: DataOwner3的最优x_3 = 0.3215
new: DataOwner4的最优x_4 = 0.2682
new: DataOwner5的最优x_5 = 0.1537
new: DataOwner6的最优x_6 = 0.1516
new: DataOwner7的最优x_7 = 0.2623
new: DataOwner8的最优x_8 = 0.3160
new: DataOwner9的最优x_9 = 0.1007
new: DataOwner10的最优x_10 = 0.2924
eta:2.7800000000000002
new: DataOwner1的最优x_1 = 0.2798
new: DataOwner2的最优x_2 = 0.3181
new: DataOwner3的最优x_3 = 0.3226
new: DataOwner4的最优x_4 = 0.2691
new: DataOwner5的最优x_5 = 0.1543
new: DataOwner6的最优x_6 = 0.1521
new: DataOwner7的最优x_7 = 0.2632
new: DataOwner8的最优x_8 = 0.3172
new: DataOwner9的最优x_9 = 0.1011
new: DataOwner10的最优x_10 = 0.2934
eta:2.79
new: DataOwner1的最优x_1 = 0.2808
new: DataOwner2的最优x_2 = 0.3192
new: DataOwner3的最优x_3 = 0.3238
new: DataOwner4的最优x_4 = 0.2701
new: DataOwner5的最优x_5 = 0.1549
new: DataOwner6的最优x_6 = 0.1527
new: DataOwner7的最优x_7 = 0.2642
new: DataOwner8的最优x_8 = 0.3183
new: DataOwner9的最优x_9 = 0.1015
new: DataOwner10的最优x_10 = 0.2945
eta:2.8000000000000003
new: DataOwner1的最优x_1 = 0.2818
new: DataOwner2的最优x_2 = 0.3204
new: DataOwner3的最优x_3 = 0.3250
new: DataOwner4的最优x_4 = 0.2711
new: DataOwner5的最优x_5 = 0.1554
new: DataOwner6的最优x_6 = 0.1532
new: DataOwner7的最优x_7 = 0.2651
new: DataOwner8的最优x_8 = 0.3194
new: DataOwner9的最优x_9 = 0.1018
new: DataOwner10的最优x_10 = 0.2955
eta:2.81
new: DataOwner1的最优x_1 = 0.2828
new: DataOwner2的最优x_2 = 0.3215
new: DataOwner3的最优x_3 = 0.3261
new: DataOwner4的最优x_4 = 0.2721
new: DataOwner5的最优x_5 = 0.1560
new: DataOwner6的最优x_6 = 0.1537
new: DataOwner7的最优x_7 = 0.2661
new: DataOwner8的最优x_8 = 0.3206
new: DataOwner9的最优x_9 = 0.1022
new: DataOwner10的最优x_10 = 0.2966
eta:2.82
new: DataOwner1的最优x_1 = 0.2838
new: DataOwner2的最优x_2 = 0.3227
new: DataOwner3的最优x_3 = 0.3273
new: DataOwner4的最优x_4 = 0.2730
new: DataOwner5的最优x_5 = 0.1565
new: DataOwner6的最优x_6 = 0.1543
new: DataOwner7的最优x_7 = 0.2670
new: DataOwner8的最优x_8 = 0.3217
new: DataOwner9的最优x_9 = 0.1026
new: DataOwner10的最优x_10 = 0.2977
eta:2.83
new: DataOwner1的最优x_1 = 0.2848
new: DataOwner2的最优x_2 = 0.3238
new: DataOwner3的最优x_3 = 0.3284
new: DataOwner4的最优x_4 = 0.2740
new: DataOwner5的最优x_5 = 0.1571
new: DataOwner6的最优x_6 = 0.1548
new: DataOwner7的最优x_7 = 0.2680
new: DataOwner8的最优x_8 = 0.3229
new: DataOwner9的最优x_9 = 0.1029
new: DataOwner10的最优x_10 = 0.2987
eta:2.84
new: DataOwner1的最优x_1 = 0.2858
new: DataOwner2的最优x_2 = 0.3250
new: DataOwner3的最优x_3 = 0.3296
new: DataOwner4的最优x_4 = 0.2750
new: DataOwner5的最优x_5 = 0.1576
new: DataOwner6的最优x_6 = 0.1554
new: DataOwner7的最优x_7 = 0.2689
new: DataOwner8的最优x_8 = 0.3240
new: DataOwner9的最优x_9 = 0.1033
new: DataOwner10的最优x_10 = 0.2998
eta:2.85
new: DataOwner1的最优x_1 = 0.2869
new: DataOwner2的最优x_2 = 0.3261
new: DataOwner3的最优x_3 = 0.3308
new: DataOwner4的最优x_4 = 0.2759
new: DataOwner5的最优x_5 = 0.1582
new: DataOwner6的最优x_6 = 0.1559
new: DataOwner7的最优x_7 = 0.2699
new: DataOwner8的最优x_8 = 0.3251
new: DataOwner9的最优x_9 = 0.1036
new: DataOwner10的最优x_10 = 0.3008
eta:2.86
new: DataOwner1的最优x_1 = 0.2879
new: DataOwner2的最优x_2 = 0.3273
new: DataOwner3的最优x_3 = 0.3319
new: DataOwner4的最优x_4 = 0.2769
new: DataOwner5的最优x_5 = 0.1587
new: DataOwner6的最优x_6 = 0.1565
new: DataOwner7的最优x_7 = 0.2708
new: DataOwner8的最优x_8 = 0.3263
new: DataOwner9的最优x_9 = 0.1040
new: DataOwner10的最优x_10 = 0.3019
eta:2.87
new: DataOwner1的最优x_1 = 0.2889
new: DataOwner2的最优x_2 = 0.3284
new: DataOwner3的最优x_3 = 0.3331
new: DataOwner4的最优x_4 = 0.2779
new: DataOwner5的最优x_5 = 0.1593
new: DataOwner6的最优x_6 = 0.1570
new: DataOwner7的最优x_7 = 0.2717
new: DataOwner8的最优x_8 = 0.3274
new: DataOwner9的最优x_9 = 0.1044
new: DataOwner10的最优x_10 = 0.3029
eta:2.88
new: DataOwner1的最优x_1 = 0.2899
new: DataOwner2的最优x_2 = 0.3295
new: DataOwner3的最优x_3 = 0.3342
new: DataOwner4的最优x_4 = 0.2788
new: DataOwner5的最优x_5 = 0.1598
new: DataOwner6的最优x_6 = 0.1576
new: DataOwner7的最优x_7 = 0.2727
new: DataOwner8的最优x_8 = 0.3286
new: DataOwner9的最优x_9 = 0.1047
new: DataOwner10的最优x_10 = 0.3040
eta:2.89
new: DataOwner1的最优x_1 = 0.2909
new: DataOwner2的最优x_2 = 0.3307
new: DataOwner3的最优x_3 = 0.3354
new: DataOwner4的最优x_4 = 0.2798
new: DataOwner5的最优x_5 = 0.1604
new: DataOwner6的最优x_6 = 0.1581
new: DataOwner7的最优x_7 = 0.2736
new: DataOwner8的最优x_8 = 0.3297
new: DataOwner9的最优x_9 = 0.1051
new: DataOwner10的最优x_10 = 0.3050
eta:2.9
new: DataOwner1的最优x_1 = 0.2919
new: DataOwner2的最优x_2 = 0.3318
new: DataOwner3的最优x_3 = 0.3366
new: DataOwner4的最优x_4 = 0.2808
new: DataOwner5的最优x_5 = 0.1610
new: DataOwner6的最优x_6 = 0.1587
new: DataOwner7的最优x_7 = 0.2746
new: DataOwner8的最优x_8 = 0.3309
new: DataOwner9的最优x_9 = 0.1055
new: DataOwner10的最优x_10 = 0.3061
eta:2.91
new: DataOwner1的最优x_1 = 0.2929
new: DataOwner2的最优x_2 = 0.3330
new: DataOwner3的最优x_3 = 0.3377
new: DataOwner4的最优x_4 = 0.2817
new: DataOwner5的最优x_5 = 0.1615
new: DataOwner6的最优x_6 = 0.1592
new: DataOwner7的最优x_7 = 0.2755
new: DataOwner8的最优x_8 = 0.3320
new: DataOwner9的最优x_9 = 0.1058
new: DataOwner10的最优x_10 = 0.3072
eta:2.92
new: DataOwner1的最优x_1 = 0.2939
new: DataOwner2的最优x_2 = 0.3341
new: DataOwner3的最优x_3 = 0.3389
new: DataOwner4的最优x_4 = 0.2827
new: DataOwner5的最优x_5 = 0.1621
new: DataOwner6的最优x_6 = 0.1598
new: DataOwner7的最优x_7 = 0.2765
new: DataOwner8的最优x_8 = 0.3331
new: DataOwner9的最优x_9 = 0.1062
new: DataOwner10的最优x_10 = 0.3082
eta:2.93
new: DataOwner1的最优x_1 = 0.2949
new: DataOwner2的最优x_2 = 0.3353
new: DataOwner3的最优x_3 = 0.3400
new: DataOwner4的最优x_4 = 0.2837
new: DataOwner5的最优x_5 = 0.1626
new: DataOwner6的最优x_6 = 0.1603
new: DataOwner7的最优x_7 = 0.2774
new: DataOwner8的最优x_8 = 0.3343
new: DataOwner9的最优x_9 = 0.1066
new: DataOwner10的最优x_10 = 0.3093
eta:2.94
new: DataOwner1的最优x_1 = 0.2959
new: DataOwner2的最优x_2 = 0.3364
new: DataOwner3的最优x_3 = 0.3412
new: DataOwner4的最优x_4 = 0.2846
new: DataOwner5的最优x_5 = 0.1632
new: DataOwner6的最优x_6 = 0.1609
new: DataOwner7的最优x_7 = 0.2784
new: DataOwner8的最优x_8 = 0.3354
new: DataOwner9的最优x_9 = 0.1069
new: DataOwner10的最优x_10 = 0.3103
eta:2.95
new: DataOwner1的最优x_1 = 0.2969
new: DataOwner2的最优x_2 = 0.3376
new: DataOwner3的最优x_3 = 0.3424
new: DataOwner4的最优x_4 = 0.2856
new: DataOwner5的最优x_5 = 0.1637
new: DataOwner6的最优x_6 = 0.1614
new: DataOwner7的最优x_7 = 0.2793
new: DataOwner8的最优x_8 = 0.3366
new: DataOwner9的最优x_9 = 0.1073
new: DataOwner10的最优x_10 = 0.3114
eta:2.96
new: DataOwner1的最优x_1 = 0.2979
new: DataOwner2的最优x_2 = 0.3387
new: DataOwner3的最优x_3 = 0.3435
new: DataOwner4的最优x_4 = 0.2866
new: DataOwner5的最优x_5 = 0.1643
new: DataOwner6的最优x_6 = 0.1620
new: DataOwner7的最优x_7 = 0.2803
new: DataOwner8的最优x_8 = 0.3377
new: DataOwner9的最优x_9 = 0.1076
new: DataOwner10的最优x_10 = 0.3124
eta:2.97
new: DataOwner1的最优x_1 = 0.2989
new: DataOwner2的最优x_2 = 0.3398
new: DataOwner3的最优x_3 = 0.3447
new: DataOwner4的最优x_4 = 0.2875
new: DataOwner5的最优x_5 = 0.1648
new: DataOwner6的最优x_6 = 0.1625
new: DataOwner7的最优x_7 = 0.2812
new: DataOwner8的最优x_8 = 0.3388
new: DataOwner9的最优x_9 = 0.1080
new: DataOwner10的最优x_10 = 0.3135
eta:2.98
new: DataOwner1的最优x_1 = 0.2999
new: DataOwner2的最优x_2 = 0.3410
new: DataOwner3的最优x_3 = 0.3458
new: DataOwner4的最优x_4 = 0.2885
new: DataOwner5的最优x_5 = 0.1654
new: DataOwner6的最优x_6 = 0.1630
new: DataOwner7的最优x_7 = 0.2822
new: DataOwner8的最优x_8 = 0.3400
new: DataOwner9的最优x_9 = 0.1084
new: DataOwner10的最优x_10 = 0.3145
eta:2.99
new: DataOwner1的最优x_1 = 0.3009
new: DataOwner2的最优x_2 = 0.3421
new: DataOwner3的最优x_3 = 0.3470
new: DataOwner4的最优x_4 = 0.2895
new: DataOwner5的最优x_5 = 0.1660
new: DataOwner6的最优x_6 = 0.1636
new: DataOwner7的最优x_7 = 0.2831
new: DataOwner8的最优x_8 = 0.3411
new: DataOwner9的最优x_9 = 0.1087
new: DataOwner10的最优x_10 = 0.3156
eta:3.0
new: DataOwner1的最优x_1 = 0.3019
new: DataOwner2的最优x_2 = 0.3433
new: DataOwner3的最优x_3 = 0.3482
new: DataOwner4的最优x_4 = 0.2904
new: DataOwner5的最优x_5 = 0.1665
new: DataOwner6的最优x_6 = 0.1641
new: DataOwner7的最优x_7 = 0.2841
new: DataOwner8的最优x_8 = 0.3423
new: DataOwner9的最优x_9 = 0.1091
new: DataOwner10的最优x_10 = 0.3167
DONE
最终的列表：
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.11747561557959967, 0.11747561557959967, 0.11747561557959967, 0.11747561557959967, 0.11747561557959967, 0.11747561557959967, 0.11747561557959967, 0.11747561557959967, 0.11747561557959967, 0.19933114524258522, 0.20771689657748146, 0.21596842012313203, 0.22408766158032897, 0.23207652464985562, 0.23207652464985562, 0.23207652464985562, 0.2552792754140847, 0.2627648630267059, 0.27012900128321565, 0.27012900128321565, 0.27012900128321565, 0.27012900128321565, 0.2984040591743265, 0.3051854078494283, 0.31185486174006044, 0.31841390315344176, 0.3248639851372941, 0.33120653223643426, 0.33120653223643426, 0.34357458193405865, 0.34960279768928704, 0.35552890625788675, 0.36135420034724647, 0.3670799482714396, 0.3727073945613851, 0.3782377605531159, 0.3836722449644006, 0.38901202443626637, 0.3942582540984657, 0.39941206806563945, 0.40447457995333, 0.40944688336539914, 0.41433005239381004, 0.4191251420306803, 0.4238331886679598, 0.42845521050618696, 0.43299220798580196, 0.4374451641951568, 0.4418150452694859, 0.4461028007798612, 0.450309364109421, 0.450309364109421, 0.4584825689990516, 0.4584825689990516, 0.4584825689990516, 0.4584825689990516, 0.4738940280734937, 0.4775570953507954, 0.4811458965621399, 0.4846612346963072, 0.4881038997832263, 0.49147466920091, 0.49477430787951615, 0.4980035686397204, 0.5011631924022879, 0.5011631924022879, 0.5072764346539109, 0.5102314777601566, 0.5131197335488871, 0.5159418871281203, 0.5186986130655193, 0.5213905757158652, 0.5213905757158652, 0.5213905757158652, 0.5213905757158652, 0.5213905757158652, 0.5339014974840403, 0.5362182813660133, 0.5384746813536564, 0.5406712866345705, 0.5428086778101192, 0.5448874270691774, 0.5469080983449487, 0.5488712474781544, 0.550777422366275, 0.5526271631126671, 0.5544210021808191, 0.5561594645322945, 0.5578430677676849, 0.5594723222645985, 0.5610477313207827, 0.5625697912550085, 0.5640389915711352, 0.5654558150601234, 0.5668207379344408, 0.5681342299251662, 0.5693967544542093, 0.5706087686556047, 0.571770723574544, 0.5728830642332987, 0.5739462297456852, 0.5749606534223612, 0.5759267628708413, 0.5768449800187687, 0.5777157216008331, 0.5785393984736356, 0.5793164164896072, 0.5800471761979353, 0.5807320730122054, 0.5813714972969402, 0.581965834455108, 0.5825154650092774, 0.5830207646857879, 0.5834821044939598, 0.5838998509493747, 0.5842743654295179, 0.5846060056910982, 0.58489512450316, 0.5851420704380896, 0.5853471878003462, 0.5855108166949301, 0.5856332930956494, 0.5857149489116122, 0.5857561120648718, 0.5857571064939513, 0.5857182523353379, 0.5856398658752511, 0.5855222596349439, 0.5853657424811161, 0.5851706196216309, 0.5849371926950482, 0.5846657598128031, 0.5843566156401179, 0.584010051371791, 0.5836263549362324, 0.5832058108548908, 0.5827487004600356, 0.5822553018424292, 0.5817258899392297, 0.5811607365747613, 0.5805601104651379, 0.5799242774771716, 0.5792535002451948, 0.5785480386463426, 0.5778081496245362, 0.577034087281258, 0.5762261029144782, 0.5753844449615664, 0.574509359524823, 0.5736010894660559, 0.5726598753491385, 0.571685955068439, 0.5706795639475304, 0.5696409347474334, 0.5685702977929343, 0.5674678808992883, 0.5663339094847384, 0.5651686065697887, 0.5639721928094303, 0.5627448865538984, 0.5614869038494055, 0.5601984584922601, 0.558879762041077, 0.5575310238716125, 0.5561524511833755, 0.5547442490364769, 0.553306620396218, 0.5518397661450869, 0.5503438851099811, 0.5488191741007762, 0.5472658278659073, 0.5456840394475349, 0.5440739995366513, 0.5424358972403229, 0.5407699196002134, 0.5390762518726786, 0.53735507744973, 0.5356065779084538, 0.5338309330272097, 0.5320283208456791, 0.5301989176124287, 0.5283428978829909, 0.5264604345086876, 0.5245516986570742, 0.5226168598435212, 0.5206560859435139, 0.5186695432273905, 0.5166573963594643, 0.5146198084386813, 0.5125569410084725, 0.5104689540786049, 0.5083560061443415, 0.5062182542056499, 0.5040558537861561, 0.5018689589501548, 0.49965772233701555, 0.497422295109863, 0.4951628271070936, 0.49287946672680505, 0.4905723610106265, 0.48824165564681055, 0.485887494986609, 0.48351002206163374, 0.4811093785981888, 0.4786857050348674, 0.4762391405366384, 0.47376982301164405, 0.47127788912262014, 0.46876347431379406, 0.4662267128030271, 0.4636677376186076, 0.4610866806022331, 0.458483672425785, 0.45585884260405907, 0.4532123195092157, 0.45054423038394864, 0.4478547013547587, 0.4451438574448563, 0.4424118225868785, 0.4396587196361357, 0.4368846703823124, 0.4340897955620755, 0.4312742148714399, 0.42843804697721355, 0.4255814095289847, 0.422704419170834, 0.4198071915508823, 0.41688984134108065, 0.41395248223183234, 0.41099522695785984, 0.4080181873050335, 0.4050214741152631, 0.40200519730844997, 0.398969465879504, 0.395914387914428, 0.3928400706090276, 0.38974662026095874, 0.3866341422960744, 0.3835027412695835, 0.3803525208751046, 0.3771835839583124, 0.37399603252262104, 0.3707899677437574, 0.3675654899684573, 0.36432269873576884, 0.3610616927757193, 0.357782570023788, 0.35448542762808666, 0.3511703619482942, 0.347837468610829, 0.3444868423758378, 0.3411185774522534, 0.3377327670620538, 0.3343295038342906, 0.33090887963133175, 0.3274709855960385, 0.3240159121237305, 0.3205437489962555, 0.3170545851830986, 0.31354850900477915, 0.3100256080966419, 0.3064859694357791, 0.3029296792618936, 0.29935682323292934, 0.2957674862881472, 0.2921617527421523, 0.2885397062538311, 0.28490142984433753, 0.2812470059085457, 0.2775765162015009, 0.2738900418841079, 0.27018766347679657, 0.26646946090770207, 0.2627355135101652, 0.2589858999863508, 0.25522069850323836, 0.2514399866087378, 0.2476438412829962, 0.24383233893272305, 0.24000555540046653, 0.23616356596766863, 0.23230644536071976, 0.22843426775684605, 0.22454710678870615, 0.220645035549611, 0.21672812660171825, 0.21279645197625685, 0.20885008318208165, 0.20488909120951782, 0.20091354653628146]
**** log-parameter_analysis 运行时间： 2025-02-11 11:16:48 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
**** log-parameter_analysis 运行时间： 2025-02-11 11:17:10 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
**** log-parameter_analysis 运行时间： 2025-02-11 11:18:01 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/initial/mnist_cnn_initial_model
Model saved to D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/mnist_cnn_model
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.005696709511827891
DataOwner2: noise random: 0.07691725166994018
DataOwner3: noise random: 0.06583276200029649
DataOwner4: noise random: 0.006740243061767937
DataOwner5: noise random: 0.07047448816938248
DataOwner6: noise random: 0.002681414110164671
DataOwner7: noise random: 0.046215723680669854
DataOwner8: noise random: 0.026648853343431325
DataOwner9: noise random: 0.06048439902164551
DataOwner10: noise random: 0.0868516939381472
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9999868353733449, 0.9976063015150004, 0.9982521308039283, 0.9999816503908247, 0.9979903030710292, 0.9999971009846416, 0.9991343844515999, 0.9997121512568425, 0.998515906646465, 0.9969496699033826]
归一化后的数据质量列表avg_f_list: [0.9996631388529237, 0.9215470537022452, 0.9427396343285828, 0.9994929961201771, 0.9341478819339428, 1.0, 0.971690367721612, 0.990649510351476, 0.951395313013454, 0.9]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3424
DataOwner1的最优x_1 = 0.1576
DataOwner2的最优x_2 = 0.0787
DataOwner3的最优x_3 = 0.1029
DataOwner4的最优x_4 = 0.1574
DataOwner5的最优x_5 = 0.0934
DataOwner6的最优x_6 = 0.1578
DataOwner7的最优x_7 = 0.1324
DataOwner8的最优x_8 = 0.1498
DataOwner9的最优x_9 = 0.1121
DataOwner10的最优x_10 = 0.0517
每个DataOwner应该贡献数据比例 xn_list = [0.15756464665682948, 0.07873453219946867, 0.10288831688227972, 0.15742077611254768, 0.09337121420898313, 0.1578491764791946, 0.13240850269253282, 0.14979335212706485, 0.11211478207353978, 0.051699423082414736]
ModelOwner的最大效用 U(Eta) = 0.5825
xn开始变化：
new_xn_list: [0.15756464665682948, 0.07873453219946867, 0.10288831688227972, 0.15742077611254768, 0.09337121420898313, 0.1578491764791946, 0.13240850269253282, 0.14979335212706485, 0.11211478207353978, 0.051699423082414736]
avg_f_list: [0.9996631388529237, 0.9215470537022452, 0.9427396343285828, 0.9994929961201771, 0.9341478819339428, 1.0, 0.971690367721612, 0.990649510351476, 0.951395313013454, 0.9]
============= xn0: 0.0 =============
new_qn: 0.0
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0022148686095613
============= xn0: 0.001 =============
new_qn: 0.0009996631388529238
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0032145317484142
============= xn0: 0.002 =============
new_qn: 0.0019993262777058475
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0042141948872672
============= xn0: 0.003 =============
new_qn: 0.002998989416558771
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.00521385802612
============= xn0: 0.004 =============
new_qn: 0.003998652555411695
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.006213521164973
============= xn0: 0.005 =============
new_qn: 0.004998315694264619
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.007213184303826
============= xn0: 0.006 =============
new_qn: 0.005997978833117542
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0082128474426788
============= xn0: 0.007 =============
new_qn: 0.006997641971970466
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0092125105815317
============= xn0: 0.008 =============
new_qn: 0.00799730511082339
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0102121737203846
============= xn0: 0.009000000000000001 =============
new_qn: 0.008996968249676314
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0112118368592375
============= xn0: 0.01 =============
new_qn: 0.009996631388529237
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0122114999980905
============= xn0: 0.011 =============
new_qn: 0.01099629452738216
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0132111631369434
============= xn0: 0.012 =============
new_qn: 0.011995957666235084
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0142108262757963
============= xn0: 0.013000000000000001 =============
new_qn: 0.01299562080508801
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0152104894146492
============= xn0: 0.014 =============
new_qn: 0.013995283943940931
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.016210152553502
============= xn0: 0.015 =============
new_qn: 0.014994947082793855
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.017209815692355
============= xn0: 0.016 =============
new_qn: 0.01599461022164678
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.018209478831208
============= xn0: 0.017 =============
new_qn: 0.016994273360499704
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0192091419700609
============= xn0: 0.018000000000000002 =============
new_qn: 0.017993936499352627
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.020208805108914
============= xn0: 0.019 =============
new_qn: 0.01899359963820555
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.021208468247767
============= xn0: 0.02 =============
new_qn: 0.019993262777058474
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0222081313866198
============= xn0: 0.021 =============
new_qn: 0.020992925915911398
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0232077945254727
============= xn0: 0.022 =============
new_qn: 0.02199258905476432
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0242074576643256
============= xn0: 0.023 =============
new_qn: 0.022992252193617245
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0252071208031786
============= xn0: 0.024 =============
new_qn: 0.02399191533247017
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0262067839420315
============= xn0: 0.025 =============
new_qn: 0.024991578471323092
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0272064470808844
============= xn0: 0.026000000000000002 =============
new_qn: 0.02599124161017602
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0282061102197373
============= xn0: 0.027 =============
new_qn: 0.02699090474902894
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0292057733585902
============= xn0: 0.028 =============
new_qn: 0.027990567887881863
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0302054364974431
============= xn0: 0.029 =============
new_qn: 0.02899023102673479
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.031205099636296
============= xn0: 0.03 =============
new_qn: 0.02998989416558771
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.032204762775149
============= xn0: 0.031 =============
new_qn: 0.030989557304440633
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0332044259140019
============= xn0: 0.032 =============
new_qn: 0.03198922044329356
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0342040890528548
============= xn0: 0.033 =============
new_qn: 0.03298888358214648
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0352037521917077
============= xn0: 0.034 =============
new_qn: 0.03398854672099941
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0362034153305606
============= xn0: 0.035 =============
new_qn: 0.034988209859852334
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0372030784694135
============= xn0: 0.036000000000000004 =============
new_qn: 0.035987872998705255
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0382027416082664
============= xn0: 0.037 =============
new_qn: 0.036987536137558175
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0392024047471193
============= xn0: 0.038 =============
new_qn: 0.0379871992764111
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0402020678859722
============= xn0: 0.039 =============
new_qn: 0.03898686241526402
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0412017310248252
============= xn0: 0.04 =============
new_qn: 0.03998652555411695
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0422013941636783
============= xn0: 0.041 =============
new_qn: 0.040986188692969876
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0432010573025312
============= xn0: 0.042 =============
new_qn: 0.041985851831822796
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0442007204413841
============= xn0: 0.043000000000000003 =============
new_qn: 0.04298551497067572
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.045200383580237
============= xn0: 0.044 =============
new_qn: 0.04398517810952864
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.04620004671909
============= xn0: 0.045 =============
new_qn: 0.04498484124838156
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0471997098579429
============= xn0: 0.046 =============
new_qn: 0.04598450438723449
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0481993729967958
============= xn0: 0.047 =============
new_qn: 0.04698416752608741
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0491990361356487
============= xn0: 0.048 =============
new_qn: 0.04798383066494034
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0501986992745016
============= xn0: 0.049 =============
new_qn: 0.048983493803793264
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0511983624133545
============= xn0: 0.05 =============
new_qn: 0.049983156942646184
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0521980255522074
============= xn0: 0.051000000000000004 =============
new_qn: 0.05098282008149911
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0531976886910603
============= xn0: 0.052000000000000005 =============
new_qn: 0.05198248322035204
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0541973518299133
============= xn0: 0.053 =============
new_qn: 0.05298214635920495
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0551970149687662
============= xn0: 0.054 =============
new_qn: 0.05398180949805788
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.056196678107619
============= xn0: 0.055 =============
new_qn: 0.054981472636910805
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.057196341246472
============= xn0: 0.056 =============
new_qn: 0.055981135775763725
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.058196004385325
============= xn0: 0.057 =============
new_qn: 0.05698079891461665
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0591956675241778
============= xn0: 0.058 =============
new_qn: 0.05798046205346958
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0601953306630307
============= xn0: 0.059000000000000004 =============
new_qn: 0.0589801251923225
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0611949938018836
============= xn0: 0.06 =============
new_qn: 0.05997978833117542
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0621946569407366
============= xn0: 0.061 =============
new_qn: 0.06097945147002835
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0631943200795897
============= xn0: 0.062 =============
new_qn: 0.06197911460888127
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0641939832184426
============= xn0: 0.063 =============
new_qn: 0.0629787777477342
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0651936463572955
============= xn0: 0.064 =============
new_qn: 0.06397844088658712
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0661933094961484
============= xn0: 0.065 =============
new_qn: 0.06497810402544005
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0671929726350013
============= xn0: 0.066 =============
new_qn: 0.06597776716429296
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0681926357738543
============= xn0: 0.067 =============
new_qn: 0.06697743030314589
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0691922989127072
============= xn0: 0.068 =============
new_qn: 0.06797709344199881
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.07019196205156
============= xn0: 0.069 =============
new_qn: 0.06897675658085174
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.071191625190413
============= xn0: 0.07 =============
new_qn: 0.06997641971970467
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.072191288329266
============= xn0: 0.07100000000000001 =============
new_qn: 0.0709760828585576
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0731909514681188
============= xn0: 0.07200000000000001 =============
new_qn: 0.07197574599741051
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0741906146069717
============= xn0: 0.073 =============
new_qn: 0.07297540913626342
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0751902777458247
============= xn0: 0.074 =============
new_qn: 0.07397507227511635
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0761899408846776
============= xn0: 0.075 =============
new_qn: 0.07497473541396928
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0771896040235305
============= xn0: 0.076 =============
new_qn: 0.0759743985528222
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0781892671623834
============= xn0: 0.077 =============
new_qn: 0.07697406169167512
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0791889303012363
============= xn0: 0.078 =============
new_qn: 0.07797372483052804
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0801885934400892
============= xn0: 0.079 =============
new_qn: 0.07897338796938097
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0811882565789421
============= xn0: 0.08 =============
new_qn: 0.0799730511082339
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.082187919717795
============= xn0: 0.081 =============
new_qn: 0.08097271424708682
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0831875828566482
============= xn0: 0.082 =============
new_qn: 0.08197237738593975
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.084187245995501
============= xn0: 0.083 =============
new_qn: 0.08297204052479266
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.085186909134354
============= xn0: 0.084 =============
new_qn: 0.08397170366364559
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.086186572273207
============= xn0: 0.085 =============
new_qn: 0.08497136680249852
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0871862354120598
============= xn0: 0.08600000000000001 =============
new_qn: 0.08597102994135145
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0881858985509127
============= xn0: 0.08700000000000001 =============
new_qn: 0.08697069308020437
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0891855616897657
============= xn0: 0.088 =============
new_qn: 0.08797035621905729
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0901852248286186
============= xn0: 0.089 =============
new_qn: 0.0889700193579102
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0911848879674715
============= xn0: 0.09 =============
new_qn: 0.08996968249676313
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0921845511063244
============= xn0: 0.091 =============
new_qn: 0.09096934563561605
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0931842142451773
============= xn0: 0.092 =============
new_qn: 0.09196900877446898
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0941838773840302
============= xn0: 0.093 =============
new_qn: 0.09296867191332191
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0951835405228831
============= xn0: 0.094 =============
new_qn: 0.09396833505217482
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.096183203661736
============= xn0: 0.095 =============
new_qn: 0.09496799819102775
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.097182866800589
============= xn0: 0.096 =============
new_qn: 0.09596766132988067
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0981825299394419
============= xn0: 0.097 =============
new_qn: 0.0969673244687336
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.0991821930782948
============= xn0: 0.098 =============
new_qn: 0.09796698760758653
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1001818562171477
============= xn0: 0.099 =============
new_qn: 0.09896665074643946
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1011815193560008
============= xn0: 0.1 =============
new_qn: 0.09996631388529237
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1021811824948538
============= xn0: 0.101 =============
new_qn: 0.1009659770241453
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1031808456337067
============= xn0: 0.10200000000000001 =============
new_qn: 0.10196564016299822
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1041805087725596
============= xn0: 0.10300000000000001 =============
new_qn: 0.10296530330185115
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1051801719114125
============= xn0: 0.10400000000000001 =============
new_qn: 0.10396496644070408
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1061798350502654
============= xn0: 0.105 =============
new_qn: 0.10496462957955699
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1071794981891183
============= xn0: 0.106 =============
new_qn: 0.1059642927184099
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1081791613279712
============= xn0: 0.107 =============
new_qn: 0.10696395585726283
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1091788244668241
============= xn0: 0.108 =============
new_qn: 0.10796361899611576
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.110178487605677
============= xn0: 0.109 =============
new_qn: 0.10896328213496868
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.11117815074453
============= xn0: 0.11 =============
new_qn: 0.10996294527382161
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1121778138833829
============= xn0: 0.111 =============
new_qn: 0.11096260841267452
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1131774770222358
============= xn0: 0.112 =============
new_qn: 0.11196227155152745
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1141771401610887
============= xn0: 0.113 =============
new_qn: 0.11296193469038038
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1151768032999416
============= xn0: 0.114 =============
new_qn: 0.1139615978292333
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1161764664387945
============= xn0: 0.115 =============
new_qn: 0.11496126096808623
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1171761295776474
============= xn0: 0.116 =============
new_qn: 0.11596092410693916
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1181757927165004
============= xn0: 0.117 =============
new_qn: 0.11696058724579207
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1191754558553533
============= xn0: 0.11800000000000001 =============
new_qn: 0.117960250384645
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1201751189942062
============= xn0: 0.11900000000000001 =============
new_qn: 0.11895991352349793
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.121174782133059
============= xn0: 0.12 =============
new_qn: 0.11995957666235084
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.122174445271912
============= xn0: 0.121 =============
new_qn: 0.12095923980120377
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1231741084107651
============= xn0: 0.122 =============
new_qn: 0.1219589029400567
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.124173771549618
============= xn0: 0.123 =============
new_qn: 0.1229585660789096
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.125173434688471
============= xn0: 0.124 =============
new_qn: 0.12395822921776253
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1261730978273239
============= xn0: 0.125 =============
new_qn: 0.12495789235661546
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1271727609661768
============= xn0: 0.126 =============
new_qn: 0.1259575554954684
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1281724241050297
============= xn0: 0.127 =============
new_qn: 0.1269572186343213
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1291720872438826
============= xn0: 0.128 =============
new_qn: 0.12795688177317424
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1301717503827355
============= xn0: 0.129 =============
new_qn: 0.12895654491202715
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1311714135215885
============= xn0: 0.13 =============
new_qn: 0.1299562080508801
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1321710766604414
============= xn0: 0.131 =============
new_qn: 0.130955871189733
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1331707397992943
============= xn0: 0.132 =============
new_qn: 0.13195553432858592
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1341704029381472
============= xn0: 0.133 =============
new_qn: 0.13295519746743886
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.135170066077
============= xn0: 0.134 =============
new_qn: 0.13395486060629178
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.136169729215853
============= xn0: 0.135 =============
new_qn: 0.13495452374514472
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.137169392354706
============= xn0: 0.136 =============
new_qn: 0.13595418688399763
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1381690554935588
============= xn0: 0.137 =============
new_qn: 0.13695385002285054
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1391687186324118
============= xn0: 0.138 =============
new_qn: 0.13795351316170348
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1401683817712647
============= xn0: 0.139 =============
new_qn: 0.1389531763005564
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1411680449101176
============= xn0: 0.14 =============
new_qn: 0.13995283943940934
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1421677080489707
============= xn0: 0.14100000000000001 =============
new_qn: 0.14095250257826225
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1431673711878236
============= xn0: 0.14200000000000002 =============
new_qn: 0.1419521657171152
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1441670343266765
============= xn0: 0.14300000000000002 =============
new_qn: 0.1429518288559681
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1451666974655295
============= xn0: 0.14400000000000002 =============
new_qn: 0.14395149199482102
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1461663606043824
============= xn0: 0.145 =============
new_qn: 0.14495115513367393
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1471660237432353
============= xn0: 0.146 =============
new_qn: 0.14595081827252684
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1481656868820882
============= xn0: 0.147 =============
new_qn: 0.14695048141137979
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1491653500209411
============= xn0: 0.148 =============
new_qn: 0.1479501445502327
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.150165013159794
============= xn0: 0.149 =============
new_qn: 0.1489498076890856
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.151164676298647
============= xn0: 0.15 =============
new_qn: 0.14994947082793855
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1521643394374999
============= xn0: 0.151 =============
new_qn: 0.15094913396679147
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1531640025763528
============= xn0: 0.152 =============
new_qn: 0.1519487971056444
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1541636657152057
============= xn0: 0.153 =============
new_qn: 0.15294846024449732
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1551633288540586
============= xn0: 0.154 =============
new_qn: 0.15394812338335023
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1561629919929115
============= xn0: 0.155 =============
new_qn: 0.15494778652220317
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1571626551317644
============= xn0: 0.156 =============
new_qn: 0.1559474496610561
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1581623182706173
============= xn0: 0.157 =============
new_qn: 0.15694711279990903
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1591619814094702
============= xn0: 0.158 =============
new_qn: 0.15794677593876194
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1601616445483232
============= xn0: 0.159 =============
new_qn: 0.15894643907761488
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.161161307687176
============= xn0: 0.16 =============
new_qn: 0.1599461022164678
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.162160970826029
============= xn0: 0.161 =============
new_qn: 0.1609457653553207
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.163160633964882
============= xn0: 0.162 =============
new_qn: 0.16194542849417365
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.164160297103735
============= xn0: 0.163 =============
new_qn: 0.16294509163302656
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.165159960242588
============= xn0: 0.164 =============
new_qn: 0.1639447547718795
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1661596233814409
============= xn0: 0.165 =============
new_qn: 0.16494441791073242
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1671592865202938
============= xn0: 0.166 =============
new_qn: 0.16594408104958533
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1681589496591467
============= xn0: 0.167 =============
new_qn: 0.16694374418843827
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1691586127979996
============= xn0: 0.168 =============
new_qn: 0.16794340732729118
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1701582759368525
============= xn0: 0.169 =============
new_qn: 0.16894307046614412
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1711579390757054
============= xn0: 0.17 =============
new_qn: 0.16994273360499704
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1721576022145583
============= xn0: 0.171 =============
new_qn: 0.17094239674384995
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1731572653534112
============= xn0: 0.17200000000000001 =============
new_qn: 0.1719420598827029
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1741569284922642
============= xn0: 0.17300000000000001 =============
new_qn: 0.1729417230215558
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.175156591631117
============= xn0: 0.17400000000000002 =============
new_qn: 0.17394138616040875
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.17615625476997
============= xn0: 0.17500000000000002 =============
new_qn: 0.17494104929926166
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.177155917908823
============= xn0: 0.176 =============
new_qn: 0.17594071243811457
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1781555810476758
============= xn0: 0.177 =============
new_qn: 0.17694037557696748
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1791552441865287
============= xn0: 0.178 =============
new_qn: 0.1779400387158204
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1801549073253816
============= xn0: 0.179 =============
new_qn: 0.17893970185467334
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1811545704642346
============= xn0: 0.18 =============
new_qn: 0.17993936499352625
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1821542336030875
============= xn0: 0.181 =============
new_qn: 0.1809390281323792
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1831538967419404
============= xn0: 0.182 =============
new_qn: 0.1819386912712321
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1841535598807933
============= xn0: 0.183 =============
new_qn: 0.18293835441008502
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1851532230196462
============= xn0: 0.184 =============
new_qn: 0.18393801754893796
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1861528861584991
============= xn0: 0.185 =============
new_qn: 0.18493768068779087
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.187152549297352
============= xn0: 0.186 =============
new_qn: 0.18593734382664381
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.188152212436205
============= xn0: 0.187 =============
new_qn: 0.18693700696549673
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1891518755750579
============= xn0: 0.188 =============
new_qn: 0.18793667010434964
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1901515387139108
============= xn0: 0.189 =============
new_qn: 0.18893633324320258
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1911512018527637
============= xn0: 0.19 =============
new_qn: 0.1899359963820555
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1921508649916166
============= xn0: 0.191 =============
new_qn: 0.19093565952090844
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1931505281304695
============= xn0: 0.192 =============
new_qn: 0.19193532265976135
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1941501912693224
============= xn0: 0.193 =============
new_qn: 0.1929349857986143
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1951498544081756
============= xn0: 0.194 =============
new_qn: 0.1939346489374672
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1961495175470285
============= xn0: 0.195 =============
new_qn: 0.19493431207632012
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1971491806858814
============= xn0: 0.196 =============
new_qn: 0.19593397521517306
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1981488438247343
============= xn0: 0.197 =============
new_qn: 0.19693363835402597
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.1991485069635872
============= xn0: 0.198 =============
new_qn: 0.1979333014928789
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2001481701024401
============= xn0: 0.199 =============
new_qn: 0.19893296463173182
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.201147833241293
============= xn0: 0.2 =============
new_qn: 0.19993262777058474
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.202147496380146
============= xn0: 0.201 =============
new_qn: 0.20093229090943768
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2031471595189989
============= xn0: 0.202 =============
new_qn: 0.2019319540482906
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2041468226578518
============= xn0: 0.203 =============
new_qn: 0.20293161718714353
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2051464857967047
============= xn0: 0.20400000000000001 =============
new_qn: 0.20393128032599644
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2061461489355576
============= xn0: 0.20500000000000002 =============
new_qn: 0.20493094346484936
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2071458120744105
============= xn0: 0.20600000000000002 =============
new_qn: 0.2059306066037023
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2081454752132634
============= xn0: 0.20700000000000002 =============
new_qn: 0.2069302697425552
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2091451383521163
============= xn0: 0.20800000000000002 =============
new_qn: 0.20792993288140815
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2101448014909693
============= xn0: 0.209 =============
new_qn: 0.20892959602026104
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2111444646298222
============= xn0: 0.21 =============
new_qn: 0.20992925915911398
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.212144127768675
============= xn0: 0.211 =============
new_qn: 0.2109289222979669
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.213143790907528
============= xn0: 0.212 =============
new_qn: 0.2119285854368198
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.214143454046381
============= xn0: 0.213 =============
new_qn: 0.21292824857567275
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2151431171852338
============= xn0: 0.214 =============
new_qn: 0.21392791171452566
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2161427803240867
============= xn0: 0.215 =============
new_qn: 0.2149275748533786
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2171424434629399
============= xn0: 0.216 =============
new_qn: 0.2159272379922315
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2181421066017928
============= xn0: 0.217 =============
new_qn: 0.21692690113108443
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2191417697406457
============= xn0: 0.218 =============
new_qn: 0.21792656426993737
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2201414328794986
============= xn0: 0.219 =============
new_qn: 0.21892622740879028
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2211410960183515
============= xn0: 0.22 =============
new_qn: 0.21992589054764322
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2221407591572044
============= xn0: 0.221 =============
new_qn: 0.22092555368649613
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2231404222960574
============= xn0: 0.222 =============
new_qn: 0.22192521682534905
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2241400854349103
============= xn0: 0.223 =============
new_qn: 0.222924879964202
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2251397485737632
============= xn0: 0.224 =============
new_qn: 0.2239245431030549
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.226139411712616
============= xn0: 0.225 =============
new_qn: 0.22492420624190784
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.227139074851469
============= xn0: 0.226 =============
new_qn: 0.22592386938076076
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.228138737990322
============= xn0: 0.227 =============
new_qn: 0.2269235325196137
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2291384011291748
============= xn0: 0.228 =============
new_qn: 0.2279231956584666
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2301380642680277
============= xn0: 0.229 =============
new_qn: 0.22892285879731952
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2311377274068807
============= xn0: 0.23 =============
new_qn: 0.22992252193617246
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2321373905457336
============= xn0: 0.231 =============
new_qn: 0.23092218507502538
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2331370536845865
============= xn0: 0.232 =============
new_qn: 0.23192184821387832
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2341367168234394
============= xn0: 0.233 =============
new_qn: 0.23292151135273123
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2351363799622923
============= xn0: 0.234 =============
new_qn: 0.23392117449158414
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2361360431011452
============= xn0: 0.23500000000000001 =============
new_qn: 0.23492083763043708
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2371357062399984
============= xn0: 0.23600000000000002 =============
new_qn: 0.23592050076929
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2381353693788513
============= xn0: 0.23700000000000002 =============
new_qn: 0.23692016390814294
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2391350325177042
============= xn0: 0.23800000000000002 =============
new_qn: 0.23791982704699585
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.240134695656557
============= xn0: 0.23900000000000002 =============
new_qn: 0.23891949018584877
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.24113435879541
============= xn0: 0.24 =============
new_qn: 0.23991915332470168
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.242134021934263
============= xn0: 0.241 =============
new_qn: 0.2409188164635546
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2431336850731158
============= xn0: 0.242 =============
new_qn: 0.24191847960240753
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2441333482119687
============= xn0: 0.243 =============
new_qn: 0.24291814274126045
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2451330113508217
============= xn0: 0.244 =============
new_qn: 0.2439178058801134
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2461326744896746
============= xn0: 0.245 =============
new_qn: 0.2449174690189663
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2471323376285275
============= xn0: 0.246 =============
new_qn: 0.2459171321578192
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2481320007673804
============= xn0: 0.247 =============
new_qn: 0.24691679529667215
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2491316639062333
============= xn0: 0.248 =============
new_qn: 0.24791645843552507
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2501313270450862
============= xn0: 0.249 =============
new_qn: 0.248916121574378
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2511309901839391
============= xn0: 0.25 =============
new_qn: 0.24991578471323092
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.252130653322792
============= xn0: 0.251 =============
new_qn: 0.25091544785208386
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.253130316461645
============= xn0: 0.252 =============
new_qn: 0.2519151109909368
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2541299796004979
============= xn0: 0.253 =============
new_qn: 0.2529147741297897
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2551296427393508
============= xn0: 0.254 =============
new_qn: 0.2539144372686426
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2561293058782037
============= xn0: 0.255 =============
new_qn: 0.25491410040749557
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2571289690170568
============= xn0: 0.256 =============
new_qn: 0.2559137635463485
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2581286321559098
============= xn0: 0.257 =============
new_qn: 0.2569134266852014
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2591282952947627
============= xn0: 0.258 =============
new_qn: 0.2579130898240543
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2601279584336156
============= xn0: 0.259 =============
new_qn: 0.2589127529629072
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2611276215724685
============= xn0: 0.26 =============
new_qn: 0.2599124161017602
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2621272847113214
============= xn0: 0.261 =============
new_qn: 0.2609120792406131
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2631269478501743
============= xn0: 0.262 =============
new_qn: 0.261911742379466
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2641266109890272
============= xn0: 0.263 =============
new_qn: 0.26291140551831893
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2651262741278801
============= xn0: 0.264 =============
new_qn: 0.26391106865717184
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.266125937266733
============= xn0: 0.265 =============
new_qn: 0.2649107317960248
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.267125600405586
============= xn0: 0.266 =============
new_qn: 0.2659103949348777
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2681252635444389
============= xn0: 0.267 =============
new_qn: 0.26691005807373064
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2691249266832918
============= xn0: 0.268 =============
new_qn: 0.26790972121258355
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2701245898221447
============= xn0: 0.269 =============
new_qn: 0.26890938435143646
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2711242529609976
============= xn0: 0.27 =============
new_qn: 0.26990904749028943
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2721239160998505
============= xn0: 0.271 =============
new_qn: 0.27090871062914235
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2731235792387035
============= xn0: 0.272 =============
new_qn: 0.27190837376799526
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2741232423775564
============= xn0: 0.273 =============
new_qn: 0.2729080369068482
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2751229055164093
============= xn0: 0.274 =============
new_qn: 0.2739077000457011
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2761225686552622
============= xn0: 0.275 =============
new_qn: 0.27490736318455405
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2771222317941153
============= xn0: 0.276 =============
new_qn: 0.27590702632340697
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2781218949329682
============= xn0: 0.277 =============
new_qn: 0.2769066894622599
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2791215580718212
============= xn0: 0.278 =============
new_qn: 0.2779063526011128
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.280121221210674
============= xn0: 0.279 =============
new_qn: 0.2789060157399657
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.281120884349527
============= xn0: 0.28 =============
new_qn: 0.2799056788788187
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.28212054748838
============= xn0: 0.281 =============
new_qn: 0.2809053420176716
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2831202106272328
============= xn0: 0.28200000000000003 =============
new_qn: 0.2819050051565245
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2841198737660857
============= xn0: 0.28300000000000003 =============
new_qn: 0.2829046682953774
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2851195369049386
============= xn0: 0.28400000000000003 =============
new_qn: 0.2839043314342304
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2861192000437915
============= xn0: 0.28500000000000003 =============
new_qn: 0.2849039945730833
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2871188631826445
============= xn0: 0.28600000000000003 =============
new_qn: 0.2859036577119362
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2881185263214974
============= xn0: 0.28700000000000003 =============
new_qn: 0.2869033208507891
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2891181894603503
============= xn0: 0.28800000000000003 =============
new_qn: 0.28790298398964204
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2901178525992032
============= xn0: 0.289 =============
new_qn: 0.28890264712849495
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.291117515738056
============= xn0: 0.29 =============
new_qn: 0.28990231026734786
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.292117178876909
============= xn0: 0.291 =============
new_qn: 0.2909019734062008
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.293116842015762
============= xn0: 0.292 =============
new_qn: 0.2919016365450537
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2941165051546148
============= xn0: 0.293 =============
new_qn: 0.2929012996839066
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2951161682934678
============= xn0: 0.294 =============
new_qn: 0.29390096282275957
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2961158314323207
============= xn0: 0.295 =============
new_qn: 0.2949006259616125
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2971154945711736
============= xn0: 0.296 =============
new_qn: 0.2959002891004654
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2981151577100265
============= xn0: 0.297 =============
new_qn: 0.2968999522393183
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.2991148208488794
============= xn0: 0.298 =============
new_qn: 0.2978996153781712
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3001144839877323
============= xn0: 0.299 =============
new_qn: 0.2988992785170242
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3011141471265855
============= xn0: 0.3 =============
new_qn: 0.2998989416558771
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3021138102654384
============= xn0: 0.301 =============
new_qn: 0.30089860479473
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3031134734042913
============= xn0: 0.302 =============
new_qn: 0.30189826793358293
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3041131365431442
============= xn0: 0.303 =============
new_qn: 0.30289793107243584
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3051127996819971
============= xn0: 0.304 =============
new_qn: 0.3038975942112888
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.30611246282085
============= xn0: 0.305 =============
new_qn: 0.3048972573501417
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.307112125959703
============= xn0: 0.306 =============
new_qn: 0.30589692048899464
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3081117890985559
============= xn0: 0.307 =============
new_qn: 0.30689658362784755
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3091114522374088
============= xn0: 0.308 =============
new_qn: 0.30789624676670047
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3101111153762617
============= xn0: 0.309 =============
new_qn: 0.30889590990555343
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3111107785151146
============= xn0: 0.31 =============
new_qn: 0.30989557304440635
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3121104416539675
============= xn0: 0.311 =============
new_qn: 0.31089523618325926
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3131101047928204
============= xn0: 0.312 =============
new_qn: 0.3118948993221122
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3141097679316733
============= xn0: 0.313 =============
new_qn: 0.3128945624609651
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3151094310705262
============= xn0: 0.314 =============
new_qn: 0.31389422559981806
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3161090942093792
============= xn0: 0.315 =============
new_qn: 0.31489388873867097
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.317108757348232
============= xn0: 0.316 =============
new_qn: 0.3158935518775239
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.318108420487085
============= xn0: 0.317 =============
new_qn: 0.3168932150163768
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.319108083625938
============= xn0: 0.318 =============
new_qn: 0.31789287815522976
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.320107746764791
============= xn0: 0.319 =============
new_qn: 0.3188925412940827
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.321107409903644
============= xn0: 0.32 =============
new_qn: 0.3198922044329356
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3221070730424969
============= xn0: 0.321 =============
new_qn: 0.3208918675717885
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3231067361813498
============= xn0: 0.322 =============
new_qn: 0.3218915307106414
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3241063993202027
============= xn0: 0.323 =============
new_qn: 0.3228911938494944
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3251060624590556
============= xn0: 0.324 =============
new_qn: 0.3238908569883473
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3261057255979085
============= xn0: 0.325 =============
new_qn: 0.3248905201272002
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3271053887367614
============= xn0: 0.326 =============
new_qn: 0.3258901832660531
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3281050518756143
============= xn0: 0.327 =============
new_qn: 0.32688984640490604
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3291047150144673
============= xn0: 0.328 =============
new_qn: 0.327889509543759
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3301043781533202
============= xn0: 0.329 =============
new_qn: 0.3288891726826119
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.331104041292173
============= xn0: 0.33 =============
new_qn: 0.32988883582146483
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.332103704431026
============= xn0: 0.331 =============
new_qn: 0.33088849896031775
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.333103367569879
============= xn0: 0.332 =============
new_qn: 0.33188816209917066
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3341030307087318
============= xn0: 0.333 =============
new_qn: 0.3328878252380236
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3351026938475847
============= xn0: 0.334 =============
new_qn: 0.33388748837687654
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3361023569864376
============= xn0: 0.335 =============
new_qn: 0.33488715151572945
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3371020201252906
============= xn0: 0.336 =============
new_qn: 0.33588681465458237
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3381016832641435
============= xn0: 0.337 =============
new_qn: 0.3368864777934353
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3391013464029964
============= xn0: 0.338 =============
new_qn: 0.33788614093228825
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3401010095418495
============= xn0: 0.339 =============
new_qn: 0.33888580407114116
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3411006726807024
============= xn0: 0.34 =============
new_qn: 0.3398854672099941
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3421003358195553
============= xn0: 0.341 =============
new_qn: 0.340885130348847
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3430999989584083
============= xn0: 0.342 =============
new_qn: 0.3418847934876999
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3440996620972612
============= xn0: 0.343 =============
new_qn: 0.34288445662655287
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.345099325236114
============= xn0: 0.34400000000000003 =============
new_qn: 0.3438841197654058
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.346098988374967
============= xn0: 0.34500000000000003 =============
new_qn: 0.3448837829042587
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.34709865151382
============= xn0: 0.34600000000000003 =============
new_qn: 0.3458834460431116
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3480983146526728
============= xn0: 0.34700000000000003 =============
new_qn: 0.3468831091819645
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3490979777915257
============= xn0: 0.34800000000000003 =============
new_qn: 0.3478827723208175
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3500976409303787
============= xn0: 0.34900000000000003 =============
new_qn: 0.3488824354596704
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3510973040692316
============= xn0: 0.35000000000000003 =============
new_qn: 0.3498820985985233
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3520969672080845
============= xn0: 0.35100000000000003 =============
new_qn: 0.35088176173737623
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3530966303469374
============= xn0: 0.352 =============
new_qn: 0.35188142487622914
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3540962934857903
============= xn0: 0.353 =============
new_qn: 0.35288108801508206
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3550959566246432
============= xn0: 0.354 =============
new_qn: 0.35388075115393497
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3560956197634961
============= xn0: 0.355 =============
new_qn: 0.3548804142927879
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.357095282902349
============= xn0: 0.356 =============
new_qn: 0.3558800774316408
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.358094946041202
============= xn0: 0.357 =============
new_qn: 0.35687974057049376
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3590946091800549
============= xn0: 0.358 =============
new_qn: 0.3578794037093467
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3600942723189078
============= xn0: 0.359 =============
new_qn: 0.3588790668481996
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3610939354577607
============= xn0: 0.36 =============
new_qn: 0.3598787299870525
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3620935985966136
============= xn0: 0.361 =============
new_qn: 0.3608783931259054
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3630932617354665
============= xn0: 0.362 =============
new_qn: 0.3618780562647584
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3640929248743197
============= xn0: 0.363 =============
new_qn: 0.3628777194036113
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3650925880131726
============= xn0: 0.364 =============
new_qn: 0.3638773825424642
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3660922511520255
============= xn0: 0.365 =============
new_qn: 0.3648770456813171
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3670919142908784
============= xn0: 0.366 =============
new_qn: 0.36587670882017004
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3680915774297313
============= xn0: 0.367 =============
new_qn: 0.366876371959023
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3690912405685842
============= xn0: 0.368 =============
new_qn: 0.3678760350978759
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3700909037074371
============= xn0: 0.369 =============
new_qn: 0.36887569823672883
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.37109056684629
============= xn0: 0.37 =============
new_qn: 0.36987536137558175
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.372090229985143
============= xn0: 0.371 =============
new_qn: 0.37087502451443466
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3730898931239959
============= xn0: 0.372 =============
new_qn: 0.37187468765328763
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3740895562628488
============= xn0: 0.373 =============
new_qn: 0.37287435079214054
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3750892194017017
============= xn0: 0.374 =============
new_qn: 0.37387401393099345
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3760888825405546
============= xn0: 0.375 =============
new_qn: 0.37487367706984637
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3770885456794075
============= xn0: 0.376 =============
new_qn: 0.3758733402086993
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3780882088182604
============= xn0: 0.377 =============
new_qn: 0.37687300334755225
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3790878719571134
============= xn0: 0.378 =============
new_qn: 0.37787266648640516
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3800875350959663
============= xn0: 0.379 =============
new_qn: 0.3788723296252581
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3810871982348192
============= xn0: 0.38 =============
new_qn: 0.379871992764111
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.382086861373672
============= xn0: 0.381 =============
new_qn: 0.3808716559029639
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.383086524512525
============= xn0: 0.382 =============
new_qn: 0.38187131904181687
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3840861876513781
============= xn0: 0.383 =============
new_qn: 0.3828709821806698
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.385085850790231
============= xn0: 0.384 =============
new_qn: 0.3838706453195227
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.386085513929084
============= xn0: 0.385 =============
new_qn: 0.3848703084583756
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3870851770679369
============= xn0: 0.386 =============
new_qn: 0.3858699715972286
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3880848402067898
============= xn0: 0.387 =============
new_qn: 0.3868696347360815
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3890845033456427
============= xn0: 0.388 =============
new_qn: 0.3878692978749344
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3900841664844956
============= xn0: 0.389 =============
new_qn: 0.3888689610137873
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3910838296233485
============= xn0: 0.39 =============
new_qn: 0.38986862415264023
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3920834927622014
============= xn0: 0.391 =============
new_qn: 0.3908682872914932
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3930831559010544
============= xn0: 0.392 =============
new_qn: 0.3918679504303461
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3940828190399073
============= xn0: 0.393 =============
new_qn: 0.392867613569199
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3950824821787602
============= xn0: 0.394 =============
new_qn: 0.39386727670805194
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.396082145317613
============= xn0: 0.395 =============
new_qn: 0.39486693984690485
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.397081808456466
============= xn0: 0.396 =============
new_qn: 0.3958666029857578
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.398081471595319
============= xn0: 0.397 =============
new_qn: 0.39686626612461073
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.3990811347341718
============= xn0: 0.398 =============
new_qn: 0.39786592926346365
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4000807978730248
============= xn0: 0.399 =============
new_qn: 0.39886559240231656
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4010804610118777
============= xn0: 0.4 =============
new_qn: 0.3998652555411695
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4020801241507306
============= xn0: 0.401 =============
new_qn: 0.40086491868002244
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4030797872895837
============= xn0: 0.402 =============
new_qn: 0.40186458181887535
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4040794504284366
============= xn0: 0.403 =============
new_qn: 0.40286424495772827
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4050791135672895
============= xn0: 0.404 =============
new_qn: 0.4038639080965812
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4060787767061425
============= xn0: 0.405 =============
new_qn: 0.4048635712354341
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4070784398449954
============= xn0: 0.406 =============
new_qn: 0.40586323437428706
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4080781029838483
============= xn0: 0.40700000000000003 =============
new_qn: 0.40686289751314
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4090777661227012
============= xn0: 0.40800000000000003 =============
new_qn: 0.4078625606519929
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.410077429261554
============= xn0: 0.40900000000000003 =============
new_qn: 0.4088622237908458
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.411077092400407
============= xn0: 0.41000000000000003 =============
new_qn: 0.4098618869296987
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.41207675553926
============= xn0: 0.41100000000000003 =============
new_qn: 0.4108615500685517
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4130764186781128
============= xn0: 0.41200000000000003 =============
new_qn: 0.4118612132074046
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4140760818169658
============= xn0: 0.41300000000000003 =============
new_qn: 0.4128608763462575
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4150757449558187
============= xn0: 0.41400000000000003 =============
new_qn: 0.4138605394851104
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4160754080946716
============= xn0: 0.41500000000000004 =============
new_qn: 0.41486020262396334
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4170750712335245
============= xn0: 0.41600000000000004 =============
new_qn: 0.4158598657628163
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4180747343723774
============= xn0: 0.417 =============
new_qn: 0.41685952890166916
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4190743975112303
============= xn0: 0.418 =============
new_qn: 0.4178591920405221
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4200740606500832
============= xn0: 0.419 =============
new_qn: 0.418858855179375
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4210737237889361
============= xn0: 0.42 =============
new_qn: 0.41985851831822796
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.422073386927789
============= xn0: 0.421 =============
new_qn: 0.42085818145708087
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.423073050066642
============= xn0: 0.422 =============
new_qn: 0.4218578445959338
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4240727132054949
============= xn0: 0.423 =============
new_qn: 0.4228575077347867
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4250723763443478
============= xn0: 0.424 =============
new_qn: 0.4238571708736396
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4260720394832007
============= xn0: 0.425 =============
new_qn: 0.4248568340124926
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4270717026220539
============= xn0: 0.426 =============
new_qn: 0.4258564971513455
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4280713657609068
============= xn0: 0.427 =============
new_qn: 0.4268561602901984
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4290710288997597
============= xn0: 0.428 =============
new_qn: 0.4278558234290513
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4300706920386124
============= xn0: 0.429 =============
new_qn: 0.42885548656790423
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4310703551774655
============= xn0: 0.43 =============
new_qn: 0.4298551497067572
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4320700183163184
============= xn0: 0.431 =============
new_qn: 0.4308548128456101
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4330696814551713
============= xn0: 0.432 =============
new_qn: 0.431854475984463
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4340693445940242
============= xn0: 0.433 =============
new_qn: 0.43285413912331594
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4350690077328772
============= xn0: 0.434 =============
new_qn: 0.43385380226216885
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.43606867087173
============= xn0: 0.435 =============
new_qn: 0.4348534654010218
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.437068334010583
============= xn0: 0.436 =============
new_qn: 0.43585312853987473
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.438067997149436
============= xn0: 0.437 =============
new_qn: 0.43685279167872765
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4390676602882888
============= xn0: 0.438 =============
new_qn: 0.43785245481758056
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4400673234271417
============= xn0: 0.439 =============
new_qn: 0.4388521179564335
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4410669865659946
============= xn0: 0.44 =============
new_qn: 0.43985178109528644
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4420666497048475
============= xn0: 0.441 =============
new_qn: 0.44085144423413936
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4430663128437005
============= xn0: 0.442 =============
new_qn: 0.44185110737299227
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4440659759825534
============= xn0: 0.443 =============
new_qn: 0.4428507705118452
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4450656391214063
============= xn0: 0.444 =============
new_qn: 0.4438504336506981
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4460653022602592
============= xn0: 0.445 =============
new_qn: 0.44485009678955106
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4470649653991121
============= xn0: 0.446 =============
new_qn: 0.445849759928404
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.448064628537965
============= xn0: 0.447 =============
new_qn: 0.4468494230672569
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.449064291676818
============= xn0: 0.448 =============
new_qn: 0.4478490862061098
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4500639548156709
============= xn0: 0.449 =============
new_qn: 0.4488487493449627
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4510636179545238
============= xn0: 0.45 =============
new_qn: 0.4498484124838157
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4520632810933771
============= xn0: 0.451 =============
new_qn: 0.4508480756226686
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.45306294423223
============= xn0: 0.452 =============
new_qn: 0.4518477387615215
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.454062607371083
============= xn0: 0.453 =============
new_qn: 0.4528474019003744
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4550622705099359
============= xn0: 0.454 =============
new_qn: 0.4538470650392274
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4560619336487888
============= xn0: 0.455 =============
new_qn: 0.4548467281780803
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4570615967876417
============= xn0: 0.456 =============
new_qn: 0.4558463913169332
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4580612599264946
============= xn0: 0.457 =============
new_qn: 0.45684605445578613
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4590609230653475
============= xn0: 0.458 =============
new_qn: 0.45784571759463905
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4600605862042004
============= xn0: 0.459 =============
new_qn: 0.458845380733492
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4610602493430533
============= xn0: 0.46 =============
new_qn: 0.4598450438723449
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4620599124819063
============= xn0: 0.461 =============
new_qn: 0.46084470701119784
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4630595756207592
============= xn0: 0.462 =============
new_qn: 0.46184437015005075
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.464059238759612
============= xn0: 0.463 =============
new_qn: 0.46284403328890367
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.465058901898465
============= xn0: 0.464 =============
new_qn: 0.46384369642775664
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.466058565037318
============= xn0: 0.465 =============
new_qn: 0.46484335956660955
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4670582281761708
============= xn0: 0.466 =============
new_qn: 0.46584302270546246
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4680578913150237
============= xn0: 0.467 =============
new_qn: 0.4668426858443154
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4690575544538766
============= xn0: 0.468 =============
new_qn: 0.4678423489831683
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4700572175927296
============= xn0: 0.46900000000000003 =============
new_qn: 0.46884201212202126
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4710568807315825
============= xn0: 0.47000000000000003 =============
new_qn: 0.46984167526087417
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4720565438704354
============= xn0: 0.47100000000000003 =============
new_qn: 0.4708413383997271
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4730562070092883
============= xn0: 0.47200000000000003 =============
new_qn: 0.47184100153858
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4740558701481412
============= xn0: 0.47300000000000003 =============
new_qn: 0.4728406646774329
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4750555332869941
============= xn0: 0.47400000000000003 =============
new_qn: 0.4738403278162859
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.476055196425847
============= xn0: 0.47500000000000003 =============
new_qn: 0.4748399909551388
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4770548595647
============= xn0: 0.47600000000000003 =============
new_qn: 0.4758396540939917
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4780545227035529
============= xn0: 0.47700000000000004 =============
new_qn: 0.4768393172328446
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4790541858424058
============= xn0: 0.47800000000000004 =============
new_qn: 0.47783898037169753
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4800538489812587
============= xn0: 0.47900000000000004 =============
new_qn: 0.4788386435105505
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4810535121201116
============= xn0: 0.48 =============
new_qn: 0.47983830664940336
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4820531752589645
============= xn0: 0.481 =============
new_qn: 0.48083796978825627
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4830528383978174
============= xn0: 0.482 =============
new_qn: 0.4818376329271092
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4840525015366703
============= xn0: 0.483 =============
new_qn: 0.4828372960659621
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4850521646755233
============= xn0: 0.484 =============
new_qn: 0.48383695920481506
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4860518278143762
============= xn0: 0.485 =============
new_qn: 0.484836622343668
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.487051490953229
============= xn0: 0.486 =============
new_qn: 0.4858362854825209
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.488051154092082
============= xn0: 0.487 =============
new_qn: 0.4868359486213738
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.489050817230935
============= xn0: 0.488 =============
new_qn: 0.4878356117602268
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4900504803697878
============= xn0: 0.489 =============
new_qn: 0.4888352748990797
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4910501435086407
============= xn0: 0.49 =============
new_qn: 0.4898349380379326
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4920498066474936
============= xn0: 0.491 =============
new_qn: 0.4908346011767855
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4930494697863466
============= xn0: 0.492 =============
new_qn: 0.4918342643156384
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4940491329251995
============= xn0: 0.493 =============
new_qn: 0.4928339274544914
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4950487960640528
============= xn0: 0.494 =============
new_qn: 0.4938335905933443
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4960484592029057
============= xn0: 0.495 =============
new_qn: 0.4948332537321972
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4970481223417587
============= xn0: 0.496 =============
new_qn: 0.49583291687105013
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4980477854806116
============= xn0: 0.497 =============
new_qn: 0.49683258000990305
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.4990474486194645
============= xn0: 0.498 =============
new_qn: 0.497832243148756
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5000471117583174
============= xn0: 0.499 =============
new_qn: 0.49883190628760893
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5010467748971703
============= xn0: 0.5 =============
new_qn: 0.49983156942646184
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5020464380360232
============= xn0: 0.501 =============
new_qn: 0.5008312325653148
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5030461011748761
============= xn0: 0.502 =============
new_qn: 0.5018308957041677
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.504045764313729
============= xn0: 0.503 =============
new_qn: 0.5028305588430206
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.505045427452582
============= xn0: 0.504 =============
new_qn: 0.5038302219818735
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5060450905914349
============= xn0: 0.505 =============
new_qn: 0.5048298851207265
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5070447537302878
============= xn0: 0.506 =============
new_qn: 0.5058295482595794
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5080444168691407
============= xn0: 0.507 =============
new_qn: 0.5068292113984323
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5090440800079936
============= xn0: 0.508 =============
new_qn: 0.5078288745372852
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5100437431468465
============= xn0: 0.509 =============
new_qn: 0.5088285376761381
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5110434062856994
============= xn0: 0.51 =============
new_qn: 0.5098282008149911
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5120430694245524
============= xn0: 0.511 =============
new_qn: 0.510827863953844
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5130427325634053
============= xn0: 0.512 =============
new_qn: 0.511827527092697
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5140423957022582
============= xn0: 0.513 =============
new_qn: 0.5128271902315499
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.515042058841111
============= xn0: 0.514 =============
new_qn: 0.5138268533704028
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.516041721979964
============= xn0: 0.515 =============
new_qn: 0.5148265165092557
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.517041385118817
============= xn0: 0.516 =============
new_qn: 0.5158261796481086
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5180410482576698
============= xn0: 0.517 =============
new_qn: 0.5168258427869615
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5190407113965227
============= xn0: 0.518 =============
new_qn: 0.5178255059258144
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5200403745353757
============= xn0: 0.519 =============
new_qn: 0.5188251690646674
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5210400376742286
============= xn0: 0.52 =============
new_qn: 0.5198248322035204
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5220397008130815
============= xn0: 0.521 =============
new_qn: 0.5208244953423733
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5230393639519344
============= xn0: 0.522 =============
new_qn: 0.5218241584812262
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5240390270907873
============= xn0: 0.523 =============
new_qn: 0.5228238216200791
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5250386902296402
============= xn0: 0.524 =============
new_qn: 0.523823484758932
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5260383533684931
============= xn0: 0.525 =============
new_qn: 0.524823147897785
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.527038016507346
============= xn0: 0.526 =============
new_qn: 0.5258228110366379
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.528037679646199
============= xn0: 0.527 =============
new_qn: 0.5268224741754908
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5290373427850519
============= xn0: 0.528 =============
new_qn: 0.5278221373143437
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5300370059239048
============= xn0: 0.529 =============
new_qn: 0.5288218004531966
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5310366690627577
============= xn0: 0.53 =============
new_qn: 0.5298214635920496
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.532036332201611
============= xn0: 0.531 =============
new_qn: 0.5308211267309025
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.533035995340464
============= xn0: 0.532 =============
new_qn: 0.5318207898697555
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5340356584793169
============= xn0: 0.533 =============
new_qn: 0.5328204530086084
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5350353216181698
============= xn0: 0.534 =============
new_qn: 0.5338201161474613
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5360349847570227
============= xn0: 0.535 =============
new_qn: 0.5348197792863142
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5370346478958756
============= xn0: 0.536 =============
new_qn: 0.5358194424251671
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5380343110347285
============= xn0: 0.537 =============
new_qn: 0.53681910556402
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5390339741735815
============= xn0: 0.538 =============
new_qn: 0.5378187687028729
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5400336373124344
============= xn0: 0.539 =============
new_qn: 0.538818431841726
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5410333004512873
============= xn0: 0.54 =============
new_qn: 0.5398180949805789
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5420329635901402
============= xn0: 0.541 =============
new_qn: 0.5408177581194318
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.543032626728993
============= xn0: 0.542 =============
new_qn: 0.5418174212582847
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.544032289867846
============= xn0: 0.543 =============
new_qn: 0.5428170843971376
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.545031953006699
============= xn0: 0.544 =============
new_qn: 0.5438167475359905
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5460316161455518
============= xn0: 0.545 =============
new_qn: 0.5448164106748434
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5470312792844048
============= xn0: 0.546 =============
new_qn: 0.5458160738136963
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5480309424232577
============= xn0: 0.547 =============
new_qn: 0.5468157369525493
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5490306055621106
============= xn0: 0.548 =============
new_qn: 0.5478154000914022
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5500302687009635
============= xn0: 0.549 =============
new_qn: 0.5488150632302552
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5510299318398164
============= xn0: 0.55 =============
new_qn: 0.5498147263691081
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5520295949786693
============= xn0: 0.551 =============
new_qn: 0.550814389507961
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5530292581175222
============= xn0: 0.552 =============
new_qn: 0.5518140526468139
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5540289212563752
============= xn0: 0.553 =============
new_qn: 0.5528137157856668
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.555028584395228
============= xn0: 0.554 =============
new_qn: 0.5538133789245198
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.556028247534081
============= xn0: 0.555 =============
new_qn: 0.5548130420633727
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.557027910672934
============= xn0: 0.556 =============
new_qn: 0.5558127052022256
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5580275738117868
============= xn0: 0.557 =============
new_qn: 0.5568123683410785
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5590272369506397
============= xn0: 0.558 =============
new_qn: 0.5578120314799314
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5600269000894926
============= xn0: 0.559 =============
new_qn: 0.5588116946187844
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5610265632283455
============= xn0: 0.56 =============
new_qn: 0.5598113577576374
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5620262263671985
============= xn0: 0.561 =============
new_qn: 0.5608110208964903
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5630258895060514
============= xn0: 0.562 =============
new_qn: 0.5618106840353432
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5640255526449043
============= xn0: 0.5630000000000001 =============
new_qn: 0.5628103471741961
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5650252157837572
============= xn0: 0.5640000000000001 =============
new_qn: 0.563810010313049
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.56602487892261
============= xn0: 0.5650000000000001 =============
new_qn: 0.5648096734519019
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.567024542061463
============= xn0: 0.5660000000000001 =============
new_qn: 0.5658093365907548
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.568024205200316
============= xn0: 0.5670000000000001 =============
new_qn: 0.5668089997296077
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5690238683391688
============= xn0: 0.5680000000000001 =============
new_qn: 0.5678086628684608
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5700235314780222
============= xn0: 0.5690000000000001 =============
new_qn: 0.5688083260073137
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5710231946168751
============= xn0: 0.5700000000000001 =============
new_qn: 0.5698079891461666
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.572022857755728
============= xn0: 0.5710000000000001 =============
new_qn: 0.5708076522850195
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.573022520894581
============= xn0: 0.5720000000000001 =============
new_qn: 0.5718073154238724
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5740221840334339
============= xn0: 0.5730000000000001 =============
new_qn: 0.5728069785627253
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5750218471722868
============= xn0: 0.5740000000000001 =============
new_qn: 0.5738066417015782
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5760215103111397
============= xn0: 0.5750000000000001 =============
new_qn: 0.5748063048404312
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5770211734499926
============= xn0: 0.5760000000000001 =============
new_qn: 0.5758059679792841
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5780208365888455
============= xn0: 0.577 =============
new_qn: 0.5768056311181369
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.579020499727698
============= xn0: 0.578 =============
new_qn: 0.5778052942569899
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5800201628665513
============= xn0: 0.579 =============
new_qn: 0.5788049573958428
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5810198260054042
============= xn0: 0.58 =============
new_qn: 0.5798046205346957
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5820194891442572
============= xn0: 0.581 =============
new_qn: 0.5808042836735486
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.58301915228311
============= xn0: 0.582 =============
new_qn: 0.5818039468124016
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.584018815421963
============= xn0: 0.583 =============
new_qn: 0.5828036099512545
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.585018478560816
============= xn0: 0.584 =============
new_qn: 0.5838032730901074
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5860181416996688
============= xn0: 0.585 =============
new_qn: 0.5848029362289603
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5870178048385217
============= xn0: 0.586 =============
new_qn: 0.5858025993678132
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5880174679773746
============= xn0: 0.587 =============
new_qn: 0.5868022625066661
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5890171311162276
============= xn0: 0.588 =============
new_qn: 0.5878019256455191
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5900167942550805
============= xn0: 0.589 =============
new_qn: 0.588801588784372
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5910164573939334
============= xn0: 0.59 =============
new_qn: 0.589801251923225
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5920161205327863
============= xn0: 0.591 =============
new_qn: 0.5908009150620779
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5930157836716392
============= xn0: 0.592 =============
new_qn: 0.5918005782009308
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5940154468104921
============= xn0: 0.593 =============
new_qn: 0.5928002413397837
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.595015109949345
============= xn0: 0.594 =============
new_qn: 0.5937999044786366
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.596014773088198
============= xn0: 0.595 =============
new_qn: 0.5947995676174895
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5970144362270509
============= xn0: 0.596 =============
new_qn: 0.5957992307563424
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5980140993659038
============= xn0: 0.597 =============
new_qn: 0.5967988938951954
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.5990137625047567
============= xn0: 0.598 =============
new_qn: 0.5977985570340484
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6000134256436096
============= xn0: 0.599 =============
new_qn: 0.5987982201729013
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6010130887824625
============= xn0: 0.6 =============
new_qn: 0.5997978833117542
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6020127519213154
============= xn0: 0.601 =============
new_qn: 0.6007975464506071
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6030124150601683
============= xn0: 0.602 =============
new_qn: 0.60179720958946
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6040120781990213
============= xn0: 0.603 =============
new_qn: 0.602796872728313
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6050117413378742
============= xn0: 0.604 =============
new_qn: 0.6037965358671659
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.606011404476727
============= xn0: 0.605 =============
new_qn: 0.6047961990060188
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.60701106761558
============= xn0: 0.606 =============
new_qn: 0.6057958621448717
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.608010730754433
============= xn0: 0.607 =============
new_qn: 0.6067955252837247
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6090103938932858
============= xn0: 0.608 =============
new_qn: 0.6077951884225776
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6100100570321387
============= xn0: 0.609 =============
new_qn: 0.6087948515614305
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6110097201709916
============= xn0: 0.61 =============
new_qn: 0.6097945147002835
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6120093833098446
============= xn0: 0.611 =============
new_qn: 0.6107941778391364
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6130090464486975
============= xn0: 0.612 =============
new_qn: 0.6117938409779893
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6140087095875504
============= xn0: 0.613 =============
new_qn: 0.6127935041168422
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6150083727264033
============= xn0: 0.614 =============
new_qn: 0.6137931672556951
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6160080358652562
============= xn0: 0.615 =============
new_qn: 0.614792830394548
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6170076990041091
============= xn0: 0.616 =============
new_qn: 0.6157924935334009
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.618007362142962
============= xn0: 0.617 =============
new_qn: 0.616792156672254
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6190070252818154
============= xn0: 0.618 =============
new_qn: 0.6177918198111069
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6200066884206683
============= xn0: 0.619 =============
new_qn: 0.6187914829499598
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6210063515595212
============= xn0: 0.62 =============
new_qn: 0.6197911460888127
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6220060146983741
============= xn0: 0.621 =============
new_qn: 0.6207908092276656
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.623005677837227
============= xn0: 0.622 =============
new_qn: 0.6217904723665185
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.62400534097608
============= xn0: 0.623 =============
new_qn: 0.6227901355053714
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6250050041149329
============= xn0: 0.624 =============
new_qn: 0.6237897986442243
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6260046672537858
============= xn0: 0.625 =============
new_qn: 0.6247894617830773
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6270043303926387
============= xn0: 0.626 =============
new_qn: 0.6257891249219302
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6280039935314916
============= xn0: 0.627 =============
new_qn: 0.6267887880607832
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6290036566703445
============= xn0: 0.628 =============
new_qn: 0.6277884511996361
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6300033198091974
============= xn0: 0.629 =============
new_qn: 0.628788114338489
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6310029829480504
============= xn0: 0.63 =============
new_qn: 0.6297877774773419
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6320026460869033
============= xn0: 0.631 =============
new_qn: 0.6307874406161948
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6330023092257562
============= xn0: 0.632 =============
new_qn: 0.6317871037550478
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.634001972364609
============= xn0: 0.633 =============
new_qn: 0.6327867668939007
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.635001635503462
============= xn0: 0.634 =============
new_qn: 0.6337864300327536
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.636001298642315
============= xn0: 0.635 =============
new_qn: 0.6347860931716065
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6370009617811678
============= xn0: 0.636 =============
new_qn: 0.6357857563104595
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6380006249200207
============= xn0: 0.637 =============
new_qn: 0.6367854194493124
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6390002880588737
============= xn0: 0.638 =============
new_qn: 0.6377850825881654
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6399999511977266
============= xn0: 0.639 =============
new_qn: 0.6387847457270183
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6409996143365795
============= xn0: 0.64 =============
new_qn: 0.6397844088658712
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6419992774754324
============= xn0: 0.641 =============
new_qn: 0.6407840720047241
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6429989406142853
============= xn0: 0.642 =============
new_qn: 0.641783735143577
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6439986037531382
============= xn0: 0.643 =============
new_qn: 0.6427833982824299
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6449982668919911
============= xn0: 0.644 =============
new_qn: 0.6437830614212828
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.645997930030844
============= xn0: 0.645 =============
new_qn: 0.6447827245601357
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.646997593169697
============= xn0: 0.646 =============
new_qn: 0.6457823876989888
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6479972563085499
============= xn0: 0.647 =============
new_qn: 0.6467820508378417
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6489969194474028
============= xn0: 0.648 =============
new_qn: 0.6477817139766946
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6499965825862557
============= xn0: 0.649 =============
new_qn: 0.6487813771155475
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6509962457251086
============= xn0: 0.65 =============
new_qn: 0.6497810402544004
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6519959088639615
============= xn0: 0.651 =============
new_qn: 0.6507807033932533
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6529955720028144
============= xn0: 0.652 =============
new_qn: 0.6517803665321062
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6539952351416674
============= xn0: 0.653 =============
new_qn: 0.6527800296709592
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6549948982805203
============= xn0: 0.654 =============
new_qn: 0.6537796928098121
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6559945614193732
============= xn0: 0.655 =============
new_qn: 0.654779355948665
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.656994224558226
============= xn0: 0.656 =============
new_qn: 0.655779019087518
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6579938876970794
============= xn0: 0.657 =============
new_qn: 0.6567786822263709
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6589935508359324
============= xn0: 0.658 =============
new_qn: 0.6577783453652238
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6599932139747853
============= xn0: 0.659 =============
new_qn: 0.6587780085040768
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6609928771136382
============= xn0: 0.66 =============
new_qn: 0.6597776716429297
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.661992540252491
============= xn0: 0.661 =============
new_qn: 0.6607773347817826
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.662992203391344
============= xn0: 0.662 =============
new_qn: 0.6617769979206355
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.663991866530197
============= xn0: 0.663 =============
new_qn: 0.6627766610594884
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6649915296690498
============= xn0: 0.664 =============
new_qn: 0.6637763241983413
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6659911928079028
============= xn0: 0.665 =============
new_qn: 0.6647759873371942
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6669908559467557
============= xn0: 0.666 =============
new_qn: 0.6657756504760473
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6679905190856086
============= xn0: 0.667 =============
new_qn: 0.6667753136149002
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6689901822244615
============= xn0: 0.668 =============
new_qn: 0.6677749767537531
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6699898453633144
============= xn0: 0.669 =============
new_qn: 0.668774639892606
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6709895085021673
============= xn0: 0.67 =============
new_qn: 0.6697743030314589
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6719891716410202
============= xn0: 0.671 =============
new_qn: 0.6707739661703118
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6729888347798731
============= xn0: 0.672 =============
new_qn: 0.6717736293091647
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.673988497918726
============= xn0: 0.673 =============
new_qn: 0.6727732924480176
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.674988161057579
============= xn0: 0.674 =============
new_qn: 0.6737729555868706
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6759878241964319
============= xn0: 0.675 =============
new_qn: 0.6747726187257236
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6769874873352848
============= xn0: 0.676 =============
new_qn: 0.6757722818645765
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6779871504741377
============= xn0: 0.677 =============
new_qn: 0.6767719450034294
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6789868136129906
============= xn0: 0.678 =============
new_qn: 0.6777716081422823
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6799864767518435
============= xn0: 0.679 =============
new_qn: 0.6787712712811352
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6809861398906965
============= xn0: 0.68 =============
new_qn: 0.6797709344199881
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6819858030295494
============= xn0: 0.681 =============
new_qn: 0.6807705975588411
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6829854661684023
============= xn0: 0.682 =============
new_qn: 0.681770260697694
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6839851293072552
============= xn0: 0.683 =============
new_qn: 0.6827699238365469
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.684984792446108
============= xn0: 0.684 =============
new_qn: 0.6837695869753998
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.685984455584961
============= xn0: 0.685 =============
new_qn: 0.6847692501142528
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.686984118723814
============= xn0: 0.686 =============
new_qn: 0.6857689132531057
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6879837818626668
============= xn0: 0.687 =============
new_qn: 0.6867685763919587
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6889834450015198
============= xn0: 0.6880000000000001 =============
new_qn: 0.6877682395308116
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6899831081403727
============= xn0: 0.6890000000000001 =============
new_qn: 0.6887679026696645
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6909827712792256
============= xn0: 0.6900000000000001 =============
new_qn: 0.6897675658085174
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6919824344180785
============= xn0: 0.6910000000000001 =============
new_qn: 0.6907672289473703
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6929820975569314
============= xn0: 0.6920000000000001 =============
new_qn: 0.6917668920862232
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6939817606957843
============= xn0: 0.6930000000000001 =============
new_qn: 0.6927665552250761
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6949814238346372
============= xn0: 0.6940000000000001 =============
new_qn: 0.693766218363929
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6959810869734901
============= xn0: 0.6950000000000001 =============
new_qn: 0.6947658815027821
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6969807501123435
============= xn0: 0.6960000000000001 =============
new_qn: 0.695765544641635
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6979804132511964
============= xn0: 0.6970000000000001 =============
new_qn: 0.6967652077804879
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6989800763900493
============= xn0: 0.6980000000000001 =============
new_qn: 0.6977648709193408
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.6999797395289022
============= xn0: 0.6990000000000001 =============
new_qn: 0.6987645340581937
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7009794026677552
============= xn0: 0.7000000000000001 =============
new_qn: 0.6997641971970466
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.701979065806608
============= xn0: 0.7010000000000001 =============
new_qn: 0.7007638603358995
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.702978728945461
============= xn0: 0.7020000000000001 =============
new_qn: 0.7017635234747525
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.703978392084314
============= xn0: 0.7030000000000001 =============
new_qn: 0.7027631866136054
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7049780552231668
============= xn0: 0.704 =============
new_qn: 0.7037628497524583
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7059777183620197
============= xn0: 0.705 =============
new_qn: 0.7047625128913112
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7069773815008726
============= xn0: 0.706 =============
new_qn: 0.7057621760301641
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7079770446397256
============= xn0: 0.707 =============
new_qn: 0.706761839169017
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7089767077785785
============= xn0: 0.708 =============
new_qn: 0.7077615023078699
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7099763709174314
============= xn0: 0.709 =============
new_qn: 0.7087611654467229
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7109760340562843
============= xn0: 0.71 =============
new_qn: 0.7097608285855758
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7119756971951372
============= xn0: 0.711 =============
new_qn: 0.7107604917244287
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7129753603339901
============= xn0: 0.712 =============
new_qn: 0.7117601548632816
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.713975023472843
============= xn0: 0.713 =============
new_qn: 0.7127598180021345
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.714974686611696
============= xn0: 0.714 =============
new_qn: 0.7137594811409875
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7159743497505489
============= xn0: 0.715 =============
new_qn: 0.7147591442798404
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7169740128894018
============= xn0: 0.716 =============
new_qn: 0.7157588074186934
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7179736760282547
============= xn0: 0.717 =============
new_qn: 0.7167584705575463
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7189733391671076
============= xn0: 0.718 =============
new_qn: 0.7177581336963992
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7199730023059605
============= xn0: 0.719 =============
new_qn: 0.7187577968352521
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7209726654448134
============= xn0: 0.72 =============
new_qn: 0.719757459974105
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7219723285836663
============= xn0: 0.721 =============
new_qn: 0.7207571231129579
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7229719917225192
============= xn0: 0.722 =============
new_qn: 0.7217567862518108
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7239716548613722
============= xn0: 0.723 =============
new_qn: 0.7227564493906637
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.724971318000225
============= xn0: 0.724 =============
new_qn: 0.7237561125295168
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.725970981139078
============= xn0: 0.725 =============
new_qn: 0.7247557756683697
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.726970644277931
============= xn0: 0.726 =============
new_qn: 0.7257554388072226
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7279703074167838
============= xn0: 0.727 =============
new_qn: 0.7267551019460755
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7289699705556367
============= xn0: 0.728 =============
new_qn: 0.7277547650849284
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7299696336944896
============= xn0: 0.729 =============
new_qn: 0.7287544282237813
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7309692968333426
============= xn0: 0.73 =============
new_qn: 0.7297540913626342
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7319689599721955
============= xn0: 0.731 =============
new_qn: 0.7307537545014872
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7329686231110484
============= xn0: 0.732 =============
new_qn: 0.7317534176403401
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7339682862499013
============= xn0: 0.733 =============
new_qn: 0.732753080779193
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7349679493887542
============= xn0: 0.734 =============
new_qn: 0.733752743918046
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7359676125276071
============= xn0: 0.735 =============
new_qn: 0.7347524070568989
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.73696727566646
============= xn0: 0.736 =============
new_qn: 0.7357520701957518
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.737966938805313
============= xn0: 0.737 =============
new_qn: 0.7367517333346048
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7389666019441659
============= xn0: 0.738 =============
new_qn: 0.7377513964734577
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7399662650830188
============= xn0: 0.739 =============
new_qn: 0.7387510596123106
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7409659282218717
============= xn0: 0.74 =============
new_qn: 0.7397507227511635
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7419655913607246
============= xn0: 0.741 =============
new_qn: 0.7407503858900164
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7429652544995775
============= xn0: 0.742 =============
new_qn: 0.7417500490288693
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7439649176384304
============= xn0: 0.743 =============
new_qn: 0.7427497121677223
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7449645807772838
============= xn0: 0.744 =============
new_qn: 0.7437493753065753
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7459642439161367
============= xn0: 0.745 =============
new_qn: 0.7447490384454282
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7469639070549896
============= xn0: 0.746 =============
new_qn: 0.7457487015842811
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7479635701938425
============= xn0: 0.747 =============
new_qn: 0.746748364723134
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7489632333326954
============= xn0: 0.748 =============
new_qn: 0.7477480278619869
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7499628964715483
============= xn0: 0.749 =============
new_qn: 0.7487476910008398
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7509625596104013
============= xn0: 0.75 =============
new_qn: 0.7497473541396927
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7519622227492542
============= xn0: 0.751 =============
new_qn: 0.7507470172785456
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.752961885888107
============= xn0: 0.752 =============
new_qn: 0.7517466804173986
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.75396154902696
============= xn0: 0.753 =============
new_qn: 0.7527463435562516
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.754961212165813
============= xn0: 0.754 =============
new_qn: 0.7537460066951045
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7559608753046658
============= xn0: 0.755 =============
new_qn: 0.7547456698339574
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7569605384435187
============= xn0: 0.756 =============
new_qn: 0.7557453329728103
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7579602015823717
============= xn0: 0.757 =============
new_qn: 0.7567449961116632
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7589598647212246
============= xn0: 0.758 =============
new_qn: 0.7577446592505162
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7599595278600775
============= xn0: 0.759 =============
new_qn: 0.7587443223893691
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7609591909989304
============= xn0: 0.76 =============
new_qn: 0.759743985528222
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7619588541377833
============= xn0: 0.761 =============
new_qn: 0.7607436486670749
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7629585172766362
============= xn0: 0.762 =============
new_qn: 0.7617433118059278
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7639581804154891
============= xn0: 0.763 =============
new_qn: 0.7627429749447808
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.764957843554342
============= xn0: 0.764 =============
new_qn: 0.7637426380836337
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.765957506693195
============= xn0: 0.765 =============
new_qn: 0.7647423012224867
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7669571698320479
============= xn0: 0.766 =============
new_qn: 0.7657419643613396
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7679568329709008
============= xn0: 0.767 =============
new_qn: 0.7667416275001925
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7689564961097537
============= xn0: 0.768 =============
new_qn: 0.7677412906390454
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7699561592486066
============= xn0: 0.769 =============
new_qn: 0.7687409537778983
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7709558223874595
============= xn0: 0.77 =============
new_qn: 0.7697406169167512
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7719554855263124
============= xn0: 0.771 =============
new_qn: 0.7707402800556041
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7729551486651653
============= xn0: 0.772 =============
new_qn: 0.7717399431944572
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7739548118040183
============= xn0: 0.773 =============
new_qn: 0.7727396063333101
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7749544749428712
============= xn0: 0.774 =============
new_qn: 0.773739269472163
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.775954138081724
============= xn0: 0.775 =============
new_qn: 0.7747389326110159
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.776953801220577
============= xn0: 0.776 =============
new_qn: 0.7757385957498688
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.77795346435943
============= xn0: 0.777 =============
new_qn: 0.7767382588887217
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7789531274982828
============= xn0: 0.778 =============
new_qn: 0.7777379220275746
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7799527906371357
============= xn0: 0.779 =============
new_qn: 0.7787375851664275
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7809524537759887
============= xn0: 0.78 =============
new_qn: 0.7797372483052805
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7819521169148416
============= xn0: 0.781 =============
new_qn: 0.7807369114441334
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7829517800536945
============= xn0: 0.782 =============
new_qn: 0.7817365745829864
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7839514431925478
============= xn0: 0.783 =============
new_qn: 0.7827362377218393
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7849511063314007
============= xn0: 0.784 =============
new_qn: 0.7837359008606922
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7859507694702537
============= xn0: 0.785 =============
new_qn: 0.7847355639995451
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7869504326091066
============= xn0: 0.786 =============
new_qn: 0.785735227138398
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7879500957479595
============= xn0: 0.787 =============
new_qn: 0.786734890277251
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7889497588868124
============= xn0: 0.788 =============
new_qn: 0.7877345534161039
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7899494220256653
============= xn0: 0.789 =============
new_qn: 0.7887342165549568
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7909490851645182
============= xn0: 0.79 =============
new_qn: 0.7897338796938097
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7919487483033711
============= xn0: 0.791 =============
new_qn: 0.7907335428326626
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.792948411442224
============= xn0: 0.792 =============
new_qn: 0.7917332059715156
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.793948074581077
============= xn0: 0.793 =============
new_qn: 0.7927328691103686
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7949477377199299
============= xn0: 0.794 =============
new_qn: 0.7937325322492215
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7959474008587828
============= xn0: 0.795 =============
new_qn: 0.7947321953880744
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7969470639976357
============= xn0: 0.796 =============
new_qn: 0.7957318585269273
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7979467271364886
============= xn0: 0.797 =============
new_qn: 0.7967315216657802
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7989463902753415
============= xn0: 0.798 =============
new_qn: 0.7977311848046331
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.7999460534141944
============= xn0: 0.799 =============
new_qn: 0.798730847943486
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8009457165530474
============= xn0: 0.8 =============
new_qn: 0.799730511082339
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8019453796919003
============= xn0: 0.801 =============
new_qn: 0.800730174221192
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8029450428307532
============= xn0: 0.802 =============
new_qn: 0.8017298373600449
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.803944705969606
============= xn0: 0.803 =============
new_qn: 0.8027295004988978
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.804944369108459
============= xn0: 0.804 =============
new_qn: 0.8037291636377507
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.805944032247312
============= xn0: 0.805 =============
new_qn: 0.8047288267766036
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8069436953861648
============= xn0: 0.806 =============
new_qn: 0.8057284899154565
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8079433585250178
============= xn0: 0.807 =============
new_qn: 0.8067281530543094
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8089430216638707
============= xn0: 0.808 =============
new_qn: 0.8077278161931624
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8099426848027236
============= xn0: 0.809 =============
new_qn: 0.8087274793320153
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8109423479415765
============= xn0: 0.81 =============
new_qn: 0.8097271424708682
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8119420110804294
============= xn0: 0.811 =============
new_qn: 0.8107268056097212
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8129416742192823
============= xn0: 0.812 =============
new_qn: 0.8117264687485741
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8139413373581352
============= xn0: 0.8130000000000001 =============
new_qn: 0.812726131887427
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8149410004969881
============= xn0: 0.8140000000000001 =============
new_qn: 0.81372579502628
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.815940663635841
============= xn0: 0.8150000000000001 =============
new_qn: 0.8147254581651329
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.816940326774694
============= xn0: 0.8160000000000001 =============
new_qn: 0.8157251213039858
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8179399899135469
============= xn0: 0.8170000000000001 =============
new_qn: 0.8167247844428387
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8189396530523998
============= xn0: 0.8180000000000001 =============
new_qn: 0.8177244475816916
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8199393161912527
============= xn0: 0.8190000000000001 =============
new_qn: 0.8187241107205445
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8209389793301056
============= xn0: 0.8200000000000001 =============
new_qn: 0.8197237738593974
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8219386424689585
============= xn0: 0.8210000000000001 =============
new_qn: 0.8207234369982505
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.822938305607812
============= xn0: 0.8220000000000001 =============
new_qn: 0.8217231001371034
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8239379687466648
============= xn0: 0.8230000000000001 =============
new_qn: 0.8227227632759563
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8249376318855177
============= xn0: 0.8240000000000001 =============
new_qn: 0.8237224264148092
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8259372950243706
============= xn0: 0.8250000000000001 =============
new_qn: 0.8247220895536621
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8269369581632235
============= xn0: 0.8260000000000001 =============
new_qn: 0.825721752692515
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8279366213020765
============= xn0: 0.8270000000000001 =============
new_qn: 0.8267214158313679
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8289362844409294
============= xn0: 0.8280000000000001 =============
new_qn: 0.8277210789702208
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8299359475797823
============= xn0: 0.8290000000000001 =============
new_qn: 0.8287207421090738
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8309356107186352
============= xn0: 0.8300000000000001 =============
new_qn: 0.8297204052479267
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8319352738574881
============= xn0: 0.8310000000000001 =============
new_qn: 0.8307200683867797
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.832934936996341
============= xn0: 0.8320000000000001 =============
new_qn: 0.8317197315256326
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.833934600135194
============= xn0: 0.833 =============
new_qn: 0.8327193946644854
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8349342632740469
============= xn0: 0.834 =============
new_qn: 0.8337190578033383
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8359339264128998
============= xn0: 0.835 =============
new_qn: 0.8347187209421912
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8369335895517527
============= xn0: 0.836 =============
new_qn: 0.8357183840810442
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8379332526906056
============= xn0: 0.837 =============
new_qn: 0.8367180472198971
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8389329158294585
============= xn0: 0.838 =============
new_qn: 0.83771771035875
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8399325789683114
============= xn0: 0.839 =============
new_qn: 0.8387173734976029
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8409322421071643
============= xn0: 0.84 =============
new_qn: 0.8397170366364559
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8419319052460172
============= xn0: 0.841 =============
new_qn: 0.8407166997753088
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8429315683848702
============= xn0: 0.842 =============
new_qn: 0.8417163629141617
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.843931231523723
============= xn0: 0.843 =============
new_qn: 0.8427160260530147
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.844930894662576
============= xn0: 0.844 =============
new_qn: 0.8437156891918676
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.845930557801429
============= xn0: 0.845 =============
new_qn: 0.8447153523307205
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8469302209402818
============= xn0: 0.846 =============
new_qn: 0.8457150154695734
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8479298840791347
============= xn0: 0.847 =============
new_qn: 0.8467146786084263
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8489295472179876
============= xn0: 0.848 =============
new_qn: 0.8477143417472792
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8499292103568405
============= xn0: 0.849 =============
new_qn: 0.8487140048861321
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8509288734956935
============= xn0: 0.85 =============
new_qn: 0.8497136680249852
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8519285366345464
============= xn0: 0.851 =============
new_qn: 0.8507133311638381
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8529281997733993
============= xn0: 0.852 =============
new_qn: 0.851712994302691
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8539278629122522
============= xn0: 0.853 =============
new_qn: 0.8527126574415439
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8549275260511051
============= xn0: 0.854 =============
new_qn: 0.8537123205803968
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.855927189189958
============= xn0: 0.855 =============
new_qn: 0.8547119837192497
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.856926852328811
============= xn0: 0.856 =============
new_qn: 0.8557116468581026
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8579265154676639
============= xn0: 0.857 =============
new_qn: 0.8567113099969556
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8589261786065168
============= xn0: 0.858 =============
new_qn: 0.8577109731358085
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8599258417453697
============= xn0: 0.859 =============
new_qn: 0.8587106362746614
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8609255048842226
============= xn0: 0.86 =============
new_qn: 0.8597102994135144
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8619251680230755
============= xn0: 0.861 =============
new_qn: 0.8607099625523673
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8629248311619284
============= xn0: 0.862 =============
new_qn: 0.8617096256912202
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8639244943007813
============= xn0: 0.863 =============
new_qn: 0.8627092888300731
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8649241574396342
============= xn0: 0.864 =============
new_qn: 0.863708951968926
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8659238205784872
============= xn0: 0.865 =============
new_qn: 0.864708615107779
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.86692348371734
============= xn0: 0.866 =============
new_qn: 0.8657082782466319
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.867923146856193
============= xn0: 0.867 =============
new_qn: 0.8667079413854848
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.868922809995046
============= xn0: 0.868 =============
new_qn: 0.8677076045243377
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8699224731338988
============= xn0: 0.869 =============
new_qn: 0.8687072676631907
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8709221362727522
============= xn0: 0.87 =============
new_qn: 0.8697069308020436
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.871921799411605
============= xn0: 0.871 =============
new_qn: 0.8707065939408966
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.872921462550458
============= xn0: 0.872 =============
new_qn: 0.8717062570797495
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.873921125689311
============= xn0: 0.873 =============
new_qn: 0.8727059202186024
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8749207888281638
============= xn0: 0.874 =============
new_qn: 0.8737055833574553
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8759204519670167
============= xn0: 0.875 =============
new_qn: 0.8747052464963082
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8769201151058696
============= xn0: 0.876 =============
new_qn: 0.8757049096351611
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8779197782447226
============= xn0: 0.877 =============
new_qn: 0.876704572774014
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8789194413835755
============= xn0: 0.878 =============
new_qn: 0.877704235912867
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8799191045224284
============= xn0: 0.879 =============
new_qn: 0.87870389905172
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8809187676612813
============= xn0: 0.88 =============
new_qn: 0.8797035621905729
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8819184308001342
============= xn0: 0.881 =============
new_qn: 0.8807032253294258
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8829180939389871
============= xn0: 0.882 =============
new_qn: 0.8817028884682787
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.88391775707784
============= xn0: 0.883 =============
new_qn: 0.8827025516071316
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.884917420216693
============= xn0: 0.884 =============
new_qn: 0.8837022147459845
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8859170833555459
============= xn0: 0.885 =============
new_qn: 0.8847018778848375
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8869167464943988
============= xn0: 0.886 =============
new_qn: 0.8857015410236904
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8879164096332517
============= xn0: 0.887 =============
new_qn: 0.8867012041625433
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8889160727721046
============= xn0: 0.888 =============
new_qn: 0.8877008673013962
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8899157359109575
============= xn0: 0.889 =============
new_qn: 0.8887005304402492
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8909153990498104
============= xn0: 0.89 =============
new_qn: 0.8897001935791021
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8919150621886633
============= xn0: 0.891 =============
new_qn: 0.890699856717955
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8929147253275163
============= xn0: 0.892 =============
new_qn: 0.891699519856808
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8939143884663692
============= xn0: 0.893 =============
new_qn: 0.8926991829956609
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.894914051605222
============= xn0: 0.894 =============
new_qn: 0.8936988461345138
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.895913714744075
============= xn0: 0.895 =============
new_qn: 0.8946985092733667
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.896913377882928
============= xn0: 0.896 =============
new_qn: 0.8956981724122196
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8979130410217808
============= xn0: 0.897 =============
new_qn: 0.8966978355510725
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8989127041606337
============= xn0: 0.898 =============
new_qn: 0.8976974986899254
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.8999123672994866
============= xn0: 0.899 =============
new_qn: 0.8986971618287785
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9009120304383396
============= xn0: 0.9 =============
new_qn: 0.8996968249676314
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9019116935771925
============= xn0: 0.901 =============
new_qn: 0.9006964881064843
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9029113567160454
============= xn0: 0.902 =============
new_qn: 0.9016961512453372
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9039110198548983
============= xn0: 0.903 =============
new_qn: 0.9026958143841901
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9049106829937512
============= xn0: 0.904 =============
new_qn: 0.903695477523043
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9059103461326041
============= xn0: 0.905 =============
new_qn: 0.9046951406618959
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.906910009271457
============= xn0: 0.906 =============
new_qn: 0.9056948038007488
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.90790967241031
============= xn0: 0.907 =============
new_qn: 0.9066944669396018
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9089093355491629
============= xn0: 0.908 =============
new_qn: 0.9076941300784548
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9099089986880162
============= xn0: 0.909 =============
new_qn: 0.9086937932173077
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9109086618268691
============= xn0: 0.91 =============
new_qn: 0.9096934563561606
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.911908324965722
============= xn0: 0.911 =============
new_qn: 0.9106931194950135
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.912907988104575
============= xn0: 0.912 =============
new_qn: 0.9116927826338664
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9139076512434279
============= xn0: 0.913 =============
new_qn: 0.9126924457727194
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9149073143822808
============= xn0: 0.914 =============
new_qn: 0.9136921089115723
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9159069775211337
============= xn0: 0.915 =============
new_qn: 0.9146917720504252
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9169066406599866
============= xn0: 0.916 =============
new_qn: 0.9156914351892781
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9179063037988395
============= xn0: 0.917 =============
new_qn: 0.916691098328131
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9189059669376924
============= xn0: 0.918 =============
new_qn: 0.917690761466984
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9199056300765454
============= xn0: 0.919 =============
new_qn: 0.9186904246058369
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9209052932153983
============= xn0: 0.92 =============
new_qn: 0.9196900877446899
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9219049563542512
============= xn0: 0.921 =============
new_qn: 0.9206897508835428
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.922904619493104
============= xn0: 0.922 =============
new_qn: 0.9216894140223957
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.923904282631957
============= xn0: 0.923 =============
new_qn: 0.9226890771612486
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.92490394577081
============= xn0: 0.924 =============
new_qn: 0.9236887403001015
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9259036089096628
============= xn0: 0.925 =============
new_qn: 0.9246884034389544
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9269032720485157
============= xn0: 0.926 =============
new_qn: 0.9256880665778073
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9279029351873687
============= xn0: 0.927 =============
new_qn: 0.9266877297166602
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9289025983262216
============= xn0: 0.928 =============
new_qn: 0.9276873928555133
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9299022614650745
============= xn0: 0.929 =============
new_qn: 0.9286870559943662
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9309019246039274
============= xn0: 0.93 =============
new_qn: 0.9296867191332191
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9319015877427803
============= xn0: 0.931 =============
new_qn: 0.930686382272072
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9329012508816332
============= xn0: 0.932 =============
new_qn: 0.9316860454109249
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9339009140204861
============= xn0: 0.933 =============
new_qn: 0.9326857085497778
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.934900577159339
============= xn0: 0.934 =============
new_qn: 0.9336853716886307
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.935900240298192
============= xn0: 0.935 =============
new_qn: 0.9346850348274837
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9368999034370449
============= xn0: 0.936 =============
new_qn: 0.9356846979663366
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9378995665758978
============= xn0: 0.937 =============
new_qn: 0.9366843611051896
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9388992297147507
============= xn0: 0.9380000000000001 =============
new_qn: 0.9376840242440425
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9398988928536036
============= xn0: 0.9390000000000001 =============
new_qn: 0.9386836873828954
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9408985559924565
============= xn0: 0.9400000000000001 =============
new_qn: 0.9396833505217483
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9418982191313094
============= xn0: 0.9410000000000001 =============
new_qn: 0.9406830136606013
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9428978822701624
============= xn0: 0.9420000000000001 =============
new_qn: 0.9416826767994542
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9438975454090153
============= xn0: 0.9430000000000001 =============
new_qn: 0.9426823399383071
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9448972085478682
============= xn0: 0.9440000000000001 =============
new_qn: 0.94368200307716
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.945896871686721
============= xn0: 0.9450000000000001 =============
new_qn: 0.9446816662160129
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.946896534825574
============= xn0: 0.9460000000000001 =============
new_qn: 0.9456813293548658
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.947896197964427
============= xn0: 0.9470000000000001 =============
new_qn: 0.9466809924937188
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9488958611032803
============= xn0: 0.9480000000000001 =============
new_qn: 0.9476806556325718
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9498955242421332
============= xn0: 0.9490000000000001 =============
new_qn: 0.9486803187714247
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.950895187380986
============= xn0: 0.9500000000000001 =============
new_qn: 0.9496799819102776
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.951894850519839
============= xn0: 0.9510000000000001 =============
new_qn: 0.9506796450491305
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.952894513658692
============= xn0: 0.9520000000000001 =============
new_qn: 0.9516793081879834
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9538941767975448
============= xn0: 0.9530000000000001 =============
new_qn: 0.9526789713268363
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9548938399363978
============= xn0: 0.9540000000000001 =============
new_qn: 0.9536786344656892
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9558935030752507
============= xn0: 0.9550000000000001 =============
new_qn: 0.9546782976045421
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9568931662141036
============= xn0: 0.9560000000000001 =============
new_qn: 0.9556779607433951
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9578928293529565
============= xn0: 0.9570000000000001 =============
new_qn: 0.9566776238822481
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9588924924918094
============= xn0: 0.9580000000000001 =============
new_qn: 0.957677287021101
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9598921556306623
============= xn0: 0.9590000000000001 =============
new_qn: 0.9586769501599539
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9608918187695152
============= xn0: 0.96 =============
new_qn: 0.9596766132988067
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9618914819083682
============= xn0: 0.961 =============
new_qn: 0.9606762764376596
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.962891145047221
============= xn0: 0.962 =============
new_qn: 0.9616759395765125
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.963890808186074
============= xn0: 0.963 =============
new_qn: 0.9626756027153655
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.964890471324927
============= xn0: 0.964 =============
new_qn: 0.9636752658542184
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9658901344637798
============= xn0: 0.965 =============
new_qn: 0.9646749289930713
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9668897976026327
============= xn0: 0.966 =============
new_qn: 0.9656745921319242
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9678894607414856
============= xn0: 0.967 =============
new_qn: 0.9666742552707772
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9688891238803385
============= xn0: 0.968 =============
new_qn: 0.9676739184096301
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9698887870191915
============= xn0: 0.969 =============
new_qn: 0.968673581548483
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9708884501580444
============= xn0: 0.97 =============
new_qn: 0.969673244687336
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9718881132968973
============= xn0: 0.971 =============
new_qn: 0.9706729078261889
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9728877764357502
============= xn0: 0.972 =============
new_qn: 0.9716725709650418
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.973887439574603
============= xn0: 0.973 =============
new_qn: 0.9726722341038947
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.974887102713456
============= xn0: 0.974 =============
new_qn: 0.9736718972427476
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.975886765852309
============= xn0: 0.975 =============
new_qn: 0.9746715603816005
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9768864289911618
============= xn0: 0.976 =============
new_qn: 0.9756712235204535
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9778860921300148
============= xn0: 0.977 =============
new_qn: 0.9766708866593065
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9788857552688677
============= xn0: 0.978 =============
new_qn: 0.9776705497981594
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9798854184077206
============= xn0: 0.979 =============
new_qn: 0.9786702129370123
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9808850815465735
============= xn0: 0.98 =============
new_qn: 0.9796698760758652
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9818847446854264
============= xn0: 0.981 =============
new_qn: 0.9806695392147181
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9828844078242793
============= xn0: 0.982 =============
new_qn: 0.981669202353571
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9838840709631322
============= xn0: 0.983 =============
new_qn: 0.9826688654924239
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9848837341019852
============= xn0: 0.984 =============
new_qn: 0.9836685286312769
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.985883397240838
============= xn0: 0.985 =============
new_qn: 0.9846681917701298
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.986883060379691
============= xn0: 0.986 =============
new_qn: 0.9856678549089828
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.987882723518544
============= xn0: 0.987 =============
new_qn: 0.9866675180478357
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9888823866573968
============= xn0: 0.988 =============
new_qn: 0.9876671811866886
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9898820497962497
============= xn0: 0.989 =============
new_qn: 0.9886668443255415
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9908817129351026
============= xn0: 0.99 =============
new_qn: 0.9896665074643944
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9918813760739555
============= xn0: 0.991 =============
new_qn: 0.9906661706032474
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9928810392128085
============= xn0: 0.992 =============
new_qn: 0.9916658337421003
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9938807023516614
============= xn0: 0.993 =============
new_qn: 0.9926654968809532
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9948803654905143
============= xn0: 0.994 =============
new_qn: 0.9936651600198061
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9958800286293672
============= xn0: 0.995 =============
new_qn: 0.994664823158659
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.99687969176822
============= xn0: 0.996 =============
new_qn: 0.995664486297512
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9978793549070735
============= xn0: 0.997 =============
new_qn: 0.9966641494363649
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9988790180459264
============= xn0: 0.998 =============
new_qn: 0.9976638125752179
best_Eta: 1.3424452899901669
sum(new_qn_list): 1.9998786811847793
============= xn0: 0.999 =============
new_qn: 0.9986634757140708
best_Eta: 1.3424452899901669
sum(new_qn_list): 2.0008783443236324
============= xn0: 1.0 =============
new_qn: 0.9996631388529237
best_Eta: 1.3424452899901669
sum(new_qn_list): 2.0018780074624853
DONE
最终的列表：
[0.0, 0.00033769301566141815, 0.0006727227700272561, 0.0010050972087623832, 0.0013348242459559163, 0.001661911764277906, 0.0019863676151351053, 0.0023081996188258113, 0.002627415564693777, 0.00294402321128121, 0.0032580302864808914, 0.003569444487687352, 0.0038782734819471885, 0.004184524906108482, 0.004488206366969305, 0.004789325441425442, 0.0050878896766170904, 0.005383906590074865, 0.005677383669864804, 0.005968328374732643, 0.006256748134247127, 0.006542650348942563, 0.006826042390460533, 0.007106931601690701, 0.0073853252969109245, 0.007661230761926437, 0.00793465525420825, 0.008205606003030785, 0.00847409020960867, 0.008740115047232742, 0.009003687661405246, 0.009264815169974308, 0.009523504663267561, 0.009779763204225016, 0.0100335978285312, 0.010285015544746465, 0.010534023334437606, 0.010780628152307695, 0.011024836926325117, 0.011266656557851966, 0.011506093921771626, 0.011743155866615645, 0.011977849214689834, 0.01221018076219977, 0.012440157279375391, 0.012667785510595046, 0.01289307217450874, 0.013116023964160728, 0.01333664754711135, 0.013554949565558222, 0.013770936636456718, 0.013984615351639737, 0.014195992277936914, 0.014405073957292881, 0.014611866906885214, 0.014816377619241476, 0.015018612562355503, 0.015218578179803453, 0.015416280890858648, 0.01561172709060623, 0.015804923150056924, 0.015995875416260158, 0.016184590212416708, 0.016371073837990552, 0.016555332568820122, 0.016737372657229008, 0.016917200332135923, 0.017094821799164245, 0.0172702432407507, 0.01744347081625354, 0.017614510662060226, 0.017783368891694365, 0.017950051595922056, 0.018114564842857797, 0.018276914678069578, 0.018437107124683508, 0.018595148183487925, 0.01875104383303683, 0.018904800029752714, 0.019056422708028944, 0.019205917780331427, 0.019353291137299902, 0.01949854864784853, 0.01964169615926581, 0.019782739497314364, 0.019921684466329678, 0.020058536849318603, 0.02019330240805725, 0.02032598688318832, 0.020456595994317858, 0.020585135440111646, 0.020711610898390784, 0.02083602802622704, 0.020958392460037514, 0.021078709815678734, 0.021196985688540518, 0.02131322565363887, 0.02142743526570888, 0.02153962005929666, 0.021649785548851067, 0.021757937228814875, 0.021864080573715416, 0.021968221038254646, 0.0220703640573989, 0.022170515046467942, 0.022268679401223887, 0.022364862497959145, 0.022459069693584396, 0.022551306325715684, 0.022641577712761277, 0.022729889154007962, 0.02281624592970692, 0.022900653301159188, 0.02298311651080047, 0.023063640782285663, 0.023142231320572984, 0.023218893312007313, 0.023293631924403604, 0.0233664523071294, 0.02343735959118705, 0.023506358889295614, 0.023573455295972157, 0.023638653887612715, 0.023701959722572746, 0.02376337784124727, 0.02382291326615049, 0.023880571001994894, 0.02393635603577024, 0.023990273336821855, 0.024042327856928558, 0.024092524530380355, 0.02414086827405526, 0.02418736398749649, 0.024232016552988428, 0.0242748308356327, 0.024315811683423644, 0.02435496392732356, 0.024392292381337166, 0.02442780184258636, 0.024461497091383705, 0.024493382891306315, 0.024523463989268968, 0.024551745115596796, 0.02457823098409767, 0.024602926292134453, 0.024625835720696432, 0.02464696393447066, 0.024666315581912984, 0.02468389529531831, 0.024699707690891, 0.02471375736881473, 0.02472604891332164, 0.024736586892761703, 0.024745375859671292, 0.024752420350841575, 0.02475772488738659, 0.02476129397481075, 0.024763132103076374, 0.02476324374667041, 0.02476163336467127, 0.02475830540081489, 0.02475326428356081, 0.024746514426157773, 0.024738060226708825, 0.024727906068236466, 0.02471605631874685, 0.024702515331294528, 0.024687287444045997, 0.02467037698034344, 0.024651788248767986, 0.02463152554320258, 0.024609593142894737, 0.024585995312518788, 0.024560736302237707, 0.024533820347765134, 0.024505251670426276, 0.02447503447721941, 0.024443172960876253, 0.024409671299922525, 0.024374533658738207, 0.024337764187617045, 0.024299367022826313, 0.024259346286665845, 0.024217706087527102, 0.024174450519951735, 0.02412958366468973, 0.024083109588757756, 0.02403503234549642, 0.02398535597462828, 0.023934084502314518, 0.023881221941211977, 0.023826772290529763, 0.02377073953608541, 0.023713127650361154, 0.023653940592559264, 0.023593182308657867, 0.02353085673146582, 0.02346696778067775, 0.023401519362928808, 0.023334515371848524, 0.023265959688115567, 0.023195856179510976, 0.023124208700971954, 0.023051021094645213, 0.0229762971899396, 0.022900040803579436, 0.022822255739656405, 0.022742945789682112, 0.02266211473264007, 0.022579766335037027, 0.022495904350955026, 0.0224105325221019, 0.02232365457786284, 0.022235274235350683, 0.022145395199456436, 0.022054021162899673, 0.021961155806278082, 0.02186680279811759, 0.02177096579492155, 0.021673648441219884, 0.02157485436961848, 0.021474587200847245, 0.021372850543809302, 0.021269647995628777, 0.0211649831416989, 0.02105885955573003, 0.020951280799796718, 0.02084225042438559, 0.02073177196844203, 0.020619848959417125, 0.020506484913314554, 0.020391683334736427, 0.020275447716930045, 0.020157781541833647, 0.020038688280121814, 0.0199181713912516, 0.019796234323507272, 0.01967288051404567, 0.019548113388940885, 0.01942193636322906, 0.01929435284095271, 0.01916536621520487, 0.019034979868173263, 0.018903197171183828, 0.018770021484744603, 0.018635456158588748, 0.018499504531717836, 0.01836216993244505, 0.01822345567843736, 0.018083365076758617, 0.017941901423911444, 0.01779906800587977, 0.01765486809817024, 0.01750930496585451, 0.017362381863610388, 0.017214102035763235, 0.017064468716327053, 0.016913485129045724, 0.01676115448743326, 0.016607479994814767, 0.016452464844366577, 0.016296112219156222, 0.016138425292183067, 0.015979407226417386, 0.01581906117484022, 0.015657390280482908, 0.015494397676466154, 0.015330086486038952, 0.01516445982261766, 0.014997520789824748, 0.014829272481526823, 0.014659717981873321, 0.014488860365334588, 0.01431670269673957, 0.01414324803131406, 0.013968499414717672, 0.013792459883081754, 0.013615132463046353, 0.013436520171797306, 0.013256626017103257, 0.013075452997352244, 0.012893004101588224, 0.012709282309547598, 0.012524290591695186, 0.012338031909260583, 0.01215050921427352, 0.011961725449600169, 0.011771683548978118, 0.011580386437052281, 0.011387837029409542, 0.011194038232614056, 0.010998992944242392, 0.010802704052917667, 0.010605174438344633, 0.010406406971343873, 0.01020640451388577, 0.01000516991912509, 0.00980270603143446, 0.00959901568643845, 0.009394101711046876, 0.00918796692348861, 0.008980614133344555, 0.008772046141580947, 0.008562265740582331, 0.008351275714184314, 0.008139078837706482, 0.00792567787798465, 0.007711075593403449, 0.007495274733928525, 0.007278278041138619, 0.00706008824825749, 0.00684070808018572, 0.006620140253532469, 0.0063983874766469495, 0.0061754524496496765, 0.005951337864463779, 0.005726046404846197, 0.0054995807464182644, 0.0052719435566971895, 0.0050431374951258046, 0.0048131652131040426, 0.004582029354018524, 0.0043497325532732, 0.004116277438319604, 0.00388166662868622, 0.003645902736009232, 0.003408988364061283, 0.0031709261087818352, 0.0029317185583059846, 0.0026913682929941007, 0.002449877885461027, 0.0022072499006050017, 0.0019634868956366347, 0.0017185914201074959, 0.0014725660159390364, 0.0012254132174510657, 0.0009771355513898405, 0.0007277355369569305, 0.00047721568583669605, 0.0002255785022246548, -2.717351714448535e-05, -0.0002810378829694593, -0.0005360121133544671, -0.0007920937337817513, -0.0010492802770845078, -0.0013075692834189634, -0.0015669583002378418, -0.0018274448822629408, -0.002089026591458376, -0.002351700997003936, -0.0026154656752681027, -0.0028803182097816293, -0.003146256191211061, -0.0034132772173327552, -0.003681378893006071, -0.003950558830147721, -0.0042208146477057396, -0.004492143971633333, -0.004764544434863738, -0.005038013677283848, -0.005312549345709239, -0.00558814909385863, -0.005864810582328406, -0.006142531478567637, -0.0064213094568529305, -0.0067011421982635655, -0.006982027390656342, -0.007263962728641271, -0.007546945913556702, -0.007830974653444622, -0.008116046663026566, -0.008402159663679187, -0.008689311383409781, -0.008977499556832857, -0.009266721925145327, -0.009556976236102965, -0.009848260243997042, -0.010140571709629786, -0.01043390840029157, -0.010728268089736981, -0.011023648558161514, -0.011320047592178528, -0.011617462984795823, -0.011915892535392603, -0.012215334049696602, -0.012515785339760999, -0.012817244223941926, -0.013119708526875828, -0.013423176079456256, -0.013727644718812215, -0.014033112288285077, -0.01433957663740687, -0.014647035621877802, -0.014955487103543996, -0.015264928950375789, -0.015575359036445524, -0.015886775241906237, -0.01619917545296945, -0.01651255756188358, -0.01682691946691267, -0.017142259072314536, -0.01745857428832015, -0.01777586303111156, -0.018094123222801017, -0.018413352791410154, -0.018733549670848504, -0.01905471180089313, -0.019376837127167534, -0.019699923601120828, -0.020023969180007317, -0.020348971826865625, -0.020674929510498763, -0.02100184020545348, -0.02132970189199973, -0.021658512556110787, -0.021988270189442938, -0.022318972789315716, -0.02265061835869181, -0.022983204906157018, -0.023316730445900602, -0.023651192997695747, -0.023986590586879963, -0.02432292124433505, -0.024660183006468217, -0.024998373915192496, -0.02533749201790758, -0.025677535367480298, -0.026018502022225887, -0.026360390045888638, -0.026703197507623178, -0.027046922481975433, -0.02739156304886392, -0.027737117293560876, -0.028083583306673765, -0.02843095918412658, -0.02877924302714141, -0.02912843294222034, -0.029478527041126357, -0.029829523440865757, -0.03018142026366971, -0.030534215636976225, -0.030887907693412153, -0.03124249457077488, -0.03159797441201495, -0.03195434536521774, -0.03231160558358637, -0.03266975322542348, -0.03302878645411372, -0.033388703438106515, -0.03374950235089813, -0.03411118137101515, -0.03447373868199605, -0.034837172472374944, -0.03520148093566383, -0.035566662270335725, -0.03593271467980769, -0.03629963637242395, -0.03666742556143854, -0.03703608046499918, -0.0374055993061298, -0.03777598031271462, -0.038147221717481195, -0.03851932175798356, -0.03889227867658629, -0.039266090720447844, -0.03964075614150425, -0.040016273196452956, -0.040392640146736436, -0.040769855258526166, -0.04114791680270652, -0.04152682305485883, -0.041906572295245526, -0.042287162808793965, -0.0426685928850809, -0.043050860818316494, -0.04343396490732904, -0.04381790345554898, -0.04420267477099343, -0.044588277166250745, -0.044974708958465004, -0.04536196846932089, -0.045750054025028075, -0.04613896395630618, -0.04652869659836939, -0.04691925029091171, -0.0473106233780915, -0.0477028142085168, -0.04809582113523003, -0.04848964251569349, -0.048884276711774366, -0.04927972208973008, -0.049675977020193385, -0.050073039878157755, -0.05047090904296325, -0.050869582898281085, -0.051269059832100394, -0.051669338236712714, -0.05207041650869815, -0.052472293048911045, -0.05287496626246585, -0.05327843455872272, -0.053682696351273373, -0.05408775005792743, -0.05449359410069782, -0.054900226905787275, -0.055307646903574426, -0.05571585252859962, -0.05612484221955144, -0.05653461441925273, -0.05694516757464746, -0.057356500136786115, -0.05776861056081345, -0.05818149730595373, -0.058595158835497874, -0.05900959361678998, -0.059424800121214094, -0.05984077682418071, -0.06025752220511349, -0.06067503474743613, -0.06109331293855946, -0.06151235526986781, -0.06193216023670678, -0.0623527263383693, -0.0627740520780834, -0.06319613596299883, -0.0636189765041748, -0.06404257221656667, -0.0644669216190134, -0.06489202323422488, -0.06531787558876945, -0.06574447721306093, -0.06617182664134619, -0.06659992241169321, -0.06702876306597771, -0.06745834714987153, -0.06788867321283004, -0.0683197398080796, -0.06875154549260554, -0.06918408882714, -0.06961736837614979, -0.07005138270782413, -0.07048613039406265, -0.07092161001046343, -0.07135782013631098, -0.07179475935456431, -0.07223242625184528, -0.07267081941842629, -0.07310993744821903, -0.07354977893876241, -0.07399034249121095, -0.07443162671032333, -0.07487363020445043, -0.07531635158552402, -0.07575978946904527, -0.07620394247407303, -0.07664880922321282, -0.07709438834260507, -0.07754067846191398, -0.07798767821431601, -0.07843538623648921, -0.07888380116860133, -0.07933292165429895, -0.07978274634069649, -0.08023327387836487, -0.08068450292132062, -0.08113643212701505, -0.081589060156323, -0.082042385673532, -0.08249640734633157, -0.08295112384580233, -0.08340653384640512, -0.08386263602597027, -0.08431942906568712, -0.08477691165009305, -0.08523508246706313, -0.08569394020779919, -0.08615348356681984, -0.08661371124194922, -0.0870746219343076, -0.08753621434829972, -0.08799848719160525, -0.0884614391751683, -0.0889250690131867, -0.0893893754231026, -0.08985435712559131, -0.09032001284455171, -0.09078634130709573, -0.09125334124353879, -0.09172101138738886, -0.0921893504753376, -0.09265835724724919, -0.09312803044615109, -0.09359836881822381, -0.0940693711127914, -0.09454103608231107, -0.0950133624823638, -0.09548634907164433, -0.09595999461195182, -0.09643429786817947, -0.0969092576083056, -0.09738487260338347, -0.0978611416275319, -0.09833806345792584, -0.09881563687478656, -0.09929386066137258, -0.09977273360396977, -0.10025225449188213, -0.10073242211742273, -0.10121323527590376, -0.10169469276562759, -0.10217679338787766, -0.1026595359469088, -0.10314291924993835, -0.10362694210713697, -0.10411160333161928, -0.10459690173943492, -0.10508283614955971, -0.10556940538388626, -0.10605660826721486, -0.10654444362724491, -0.10703291029456619, -0.10752200710264903, -0.10801173288783628, -0.10850208648933424, -0.10899306674920384, -0.1094846725123515, -0.10997690262652116, -0.11046975594228492, -0.11096323131303487, -0.11145732759497395, -0.11195204364710765, -0.11244737833123519, -0.11294333051194139, -0.11343989905658791, -0.11393708283530446, -0.11443488072098107, -0.11493329158925869, -0.11543231431852219, -0.11593194778989024, -0.11643219088720858, -0.11693304249704073, -0.11743450150866008, -0.11793656681404152, -0.11843923730785377, -0.1189425118874502, -0.11944638945286157, -0.11995086890678719, -0.1204559491545879, -0.12096162910427666, -0.12146790766651139, -0.12197478375458704, -0.12248225628442688, -0.12299032417457534, -0.1234989863461895, -0.12400824172303182, -0.12451808923146168, -0.12502852780042795, -0.12553955636146075, -0.12605117384866438, -0.12656337919870875, -0.1270761713508225, -0.12758954924678467, -0.12810351183091706, -0.1286180580500772, -0.12913318685365005, -0.12964889719354078, -0.130165188024167, -0.13068205830245183, -0.13119950698781557, -0.1317175330421685, -0.13223613542990398, -0.13275531311789024, -0.1332750650754635, -0.13379539027442022, -0.1343162876890106, -0.13483775629592987, -0.13535979507431262, -0.13588240300572396, -0.13640557907415352, -0.1369293222660075, -0.13745363157010204, -0.1379785059776556, -0.1385039444822821, -0.13902994607998342, -0.13955650976914324, -0.14008363455051853, -0.14061131942723426, -0.14113956340477496, -0.14166836549097817, -0.14219772469602798, -0.14272764003244742, -0.14325811051509207, -0.14378913516114267, -0.14432071299009863, -0.14485284302377133, -0.14538552428627693, -0.1459187558040297, -0.1464525366057352, -0.14698686572238395, -0.14752174218724423, -0.14805716503585553, -0.14859313330602197, -0.14912964603780576, -0.1496667022735202, -0.1502043010577233, -0.1507424414372116, -0.151281122461013, -0.15182034318038018, -0.15236010264878497, -0.15290039992191096, -0.1534412340576473, -0.15398260411608256, -0.154524509159498, -0.15506694825236111, -0.15560992046131972, -0.15615342485519523, -0.15669746050497635, -0.1572420264838128, -0.15778712186700905, -0.15833274573201828, -0.15887889715843573, -0.15942557522799283, -0.15997277902455087, -0.16052050763409487, -0.16106876014472704, -0.16161753564666181, -0.16216683323221814, -0.16271665199581453, -0.1632669910339627, -0.16381784944526134, -0.1643692263303903, -0.16492112079210441, -0.16547353193522807, -0.16602645886664813, -0.16657990069530915, -0.1671338565322068, -0.16768832549038226, -0.16824330668491605, -0.1687987992329224, -0.1693548022535435, -0.1699113148679432, -0.1704683361993019, -0.17102586537281028, -0.17158390151566372, -0.17214244375705623, -0.17270149122817546, -0.17326104306219636, -0.17382109839427562, -0.17438165636154634, -0.17494271610311174, -0.1755042767600402, -0.1760663374753597, -0.1766288973940512, -0.17719195566304435, -0.17775551143121127, -0.1783195638493611, -0.1788841120702348, -0.1794491552484988, -0.18001469254074098, -0.18058072310546358, -0.18114724610307908, -0.18171426069590413, -0.18228176604815416, -0.18284976132593833, -0.18341824569725385, -0.18398721833198084, -0.18455667840187706, -0.18512662508057198, -0.1856970575435627, -0.18626797496820724, -0.18683937653372062, -0.18741126142116893, -0.18798362881346353, -0.18855647789535768, -0.18912980785343925, -0.18970361787612688, -0.19027790715366477, -0.19085267487811652, -0.19142792024336164, -0.1920036424450886, -0.1925798406807916, -0.1931565141497642, -0.1937336620530946, -0.19431128359366123, -0.1948893779761265, -0.19546794440693316, -0.19604698209429872, -0.19662649024820977, -0.19720646808041864, -0.197786914804437, -0.19836782963553168, -0.19894921179071978, -0.19953106048876312, -0.200113374950165, -0.20069615439716304, -0.2012793980537263, -0.2018631051455495, -0.20244727490004855, -0.2030319065463555, -0.20361699931531418, -0.20420255243947527, -0.20478856515309118, -0.2053750366921121, -0.20596196629418095, -0.20654935319862788, -0.20713719664646724, -0.2077254958803917, -0.20831425014476768, -0.20890345868563154, -0.20949312075068383, -0.21008323558928577, -0.21067380245245393, -0.21126482059285623, -0.21185628926480682, -0.21244820772426187, -0.21304057522881514, -0.2136333910376934, -0.21422665441175148, -0.21482036461346876, -0.21541452090694369, -0.21600912255789018, -0.21660416883363276, -0.2171996590031019, -0.21779559233683032, -0.21839196810694794, -0.21898878558717794, -0.21958604405283222, -0.2201837427808071, -0.22078188104957897, -0.2213804581391997, -0.2219794733312932, -0.22257892590904993, -0.22317881515722393, -0.22377914036212732, -0.22437990081162684, -0.22498109579513947, -0.22558272460362816, -0.22618478652959728, -0.2267872808670891, -0.22739020691167955, -0.227993563960473, -0.2285973513120998, -0.22920156826671056, -0.22980621412597313, -0.230411288193068, -0.23101678977268403, -0.2316227181710151, -0.2322290726957551, -0.23283585265609474, -0.23344305736271698, -0.23405068612779267, -0.23465873826497818, -0.235267213089409, -0.23587610991769759, -0.23648542806792883, -0.23709516685965548, -0.23770532561389557, -0.23831590365312694, -0.23892690030128438, -0.23953831488375554, -0.24015014672737622, -0.24076239516042808, -0.24137505951263283, -0.24198813911514994, -0.24260163330057227, -0.24321554140292145, -0.24382986275764584, -0.24444459670161456, -0.24505974257311536, -0.24567529971185054, -0.24629126745893193, -0.24690764515687913, -0.24752443214961384, -0.24814162778245752, -0.24875923140212697, -0.24937724235673053, -0.24999565999576512, -0.2506144836701115, -0.25123371273203143, -0.25185334653516367, -0.2524733844345204, -0.25309382578648343, -0.25371466994880054, -0.25433591628058216, -0.25495756414229775, -0.25557961289577147, -0.25620206190417993, -0.2568249105320469, -0.2574481581452416, -0.2580718041109735, -0.2586958477977901, -0.2593202885755723, -0.25994512581553175, -0.26057035889020663, -0.26119598717345893, -0.2618220100404699, -0.26244842686773806, -0.2630752370330739, -0.26370243991559805, -0.26433003489573714, -0.2649580213552203, -0.2655863986770758, -0.2662151662456277, -0.26684432344649245, -0.2674738696665757, -0.26810380429406844, -0.2687341267184441, -0.2693648363304547, -0.2699959325221284, -0.27062741468676466, -0.2712592822189327, -0.271891534514467, -0.2725241709704641, -0.2731571909852796, -0.273790593958525, -0.2744243792910638, -0.27505854638500915, -0.2756930946437197, -0.27632802347179675, -0.2769633322750813, -0.2775990204606502, -0.2782350874368138, -0.2788715326131114, -0.2795083554003096, -0.2801455552103981, -0.28078313145658673, -0.28142108355330275, -0.2820594109161868, -0.2826981129620907, -0.2833371891090737, -0.28397663877639967, -0.2846164613845339, -0.28525665635513964, -0.2858972231110757, -0.28653816107639274, -0.28717946967633057, -0.2878211483373151, -0.2884631964869546, -0.28910561355403797, -0.2897483989685302, -0.29039155216157064, -0.2910350725654691, -0.29167895961370305, -0.292323212740915, -0.29296783138290894, -0.2936128149766478, -0.29425816296025054, -0.2949038747729884, -0.29554994985528293, -0.2961963876487026, -0.2968431875959596, -0.29749034914090766, -0.2981378717285381, -0.29878575480497804, -0.2994339978174866, -0.3000826002144523, -0.3007315614453906, -0.3013808809609402, -0.30203055821286107, -0.3026805926540308, -0.30333098373844225, -0.3039817309212005, -0.30463283365852045, -0.30528429140772295, -0.3059361036272332, -0.30658826977657716, -0.3072407893163792, -0.30789366170835897, -0.3085468864153288, -0.30920046290119096, -0.3098543906309347, -0.3105086690706337, -0.31116329768744344, -0.3118182759495979, -0.3124736033264075, -0.31312927928825607, -0.3137853033065978, -0.3144416748539556, -0.315098393403917, -0.3157554584311325, -0.3164128694113124, -0.31707062582122447, -0.31772872713869094, -0.3183871728425859, -0.319045962412833, -0.31970509533040226, -0.320364571077308, -0.3210243891366059, -0.3216845489923903, -0.322345050129792, -0.3230058920349749, -0.3236670741951344, -0.32432859609849407, -0.3249904572343033, -0.325652657092835, -0.3263151951653822, -0.3269780709442568, -0.3276412839227858, -0.3283048335953094, -0.32896871945717865, -0.32963294100475193]
**** log-parameter_analysis 运行时间： 2025-02-11 14:12:35 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/initial/mnist_cnn_initial_model
Model saved to D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/mnist_cnn_model
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.011209721920961625
DataOwner2: noise random: 0.04278864599040694
DataOwner3: noise random: 0.005542228696414697
DataOwner4: noise random: 0.026681868994756542
DataOwner5: noise random: 0.05610869477202225
DataOwner6: noise random: 0.06976278428087025
DataOwner7: noise random: 0.02248377023178254
DataOwner8: noise random: 0.01861638877834857
DataOwner9: noise random: 0.017609966294196467
DataOwner10: noise random: 0.031811350445270135
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9999492320877329, 0.9992557435629547, 0.9999875684465726, 0.9997118502536205, 0.9987308596376201, 0.9980331397636572, 0.9997959904287937, 0.9998601141210748, 0.9998745454298535, 0.9995922783353818]
归一化后的数据质量列表avg_f_list: [0.9980384877087218, 0.9625555595855133, 1.0, 0.9858926449779309, 0.9356994286904428, 0.9, 0.9901977483520643, 0.993478691414432, 0.994217081559077, 0.9797746464403541]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3580
DataOwner1的最优x_1 = 0.1461
DataOwner2的最优x_2 = 0.1115
DataOwner3的最优x_3 = 0.1479
DataOwner4的最优x_4 = 0.1349
DataOwner5的最优x_5 = 0.0815
DataOwner6的最优x_6 = 0.0357
DataOwner7的最优x_7 = 0.1389
DataOwner8的最优x_8 = 0.1419
DataOwner9的最优x_9 = 0.1426
DataOwner10的最优x_10 = 0.1290
每个DataOwner应该贡献数据比例 xn_list = [0.14609633040677505, 0.11152460939821499, 0.1478562967347818, 0.13485861110973227, 0.08154217516124777, 0.03572928019529818, 0.13890980301911401, 0.14194676550632604, 0.14262430553321678, 0.1289693446395754]
ModelOwner的最大效用 U(Eta) = 0.6008
xn开始变化：
new_xn_list: [0.14609633040677505, 0.11152460939821499, 0.1478562967347818, 0.13485861110973227, 0.08154217516124777, 0.03572928019529818, 0.13890980301911401, 0.14194676550632604, 0.14262430553321678, 0.1289693446395754]
avg_f_list: [0.9980384877087218, 0.9625555595855133, 1.0, 0.9858926449779309, 0.9356994286904428, 0.9, 0.9901977483520643, 0.993478691414432, 0.994217081559077, 0.9797746464403541]
============= xn0: 0.0 =============
new_qn: 0.0
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.043346037106785
============= xn0: 0.001 =============
new_qn: 0.0009980384877087219
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.0443440755944937
============= xn0: 0.002 =============
new_qn: 0.0019960769754174437
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.0453421140822026
============= xn0: 0.003 =============
new_qn: 0.0029941154631261658
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.0463401525699112
============= xn0: 0.004 =============
new_qn: 0.003992153950834887
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.0473381910576198
============= xn0: 0.005 =============
new_qn: 0.0049901924385436095
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.0483362295453287
============= xn0: 0.006 =============
new_qn: 0.0059882309262523315
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.0493342680330373
============= xn0: 0.007 =============
new_qn: 0.006986269413961053
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.0503323065207462
============= xn0: 0.008 =============
new_qn: 0.007984307901669775
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.0513303450084548
============= xn0: 0.009000000000000001 =============
new_qn: 0.008982346389378497
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.0523283834961634
============= xn0: 0.01 =============
new_qn: 0.009980384877087219
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.0533264219838723
============= xn0: 0.011 =============
new_qn: 0.01097842336479594
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.054324460471581
============= xn0: 0.012 =============
new_qn: 0.011976461852504663
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.0553224989592898
============= xn0: 0.013000000000000001 =============
new_qn: 0.012974500340213385
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.0563205374469984
============= xn0: 0.014 =============
new_qn: 0.013972538827922105
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.0573185759347072
============= xn0: 0.015 =============
new_qn: 0.014970577315630828
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.0583166144224159
============= xn0: 0.016 =============
new_qn: 0.01596861580333955
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.0593146529101245
============= xn0: 0.017 =============
new_qn: 0.016966654291048273
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.0603126913978334
============= xn0: 0.018000000000000002 =============
new_qn: 0.017964692778756994
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.061310729885542
============= xn0: 0.019 =============
new_qn: 0.018962731266465714
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.0623087683732508
============= xn0: 0.02 =============
new_qn: 0.019960769754174438
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.0633068068609595
============= xn0: 0.021 =============
new_qn: 0.020958808241883158
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.064304845348668
============= xn0: 0.022 =============
new_qn: 0.02195684672959188
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.065302883836377
============= xn0: 0.023 =============
new_qn: 0.022954885217300602
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.0663009223240856
============= xn0: 0.024 =============
new_qn: 0.023952923705009326
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.0672989608117944
============= xn0: 0.025 =============
new_qn: 0.024950962192718047
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.068296999299503
============= xn0: 0.026000000000000002 =============
new_qn: 0.02594900068042677
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.069295037787212
============= xn0: 0.027 =============
new_qn: 0.02694703916813549
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.0702930762749205
============= xn0: 0.028 =============
new_qn: 0.02794507765584421
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.0712911147626292
============= xn0: 0.029 =============
new_qn: 0.028943116143552935
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.072289153250338
============= xn0: 0.03 =============
new_qn: 0.029941154631261655
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.0732871917380467
============= xn0: 0.031 =============
new_qn: 0.030939193118970375
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.0742852302257555
============= xn0: 0.032 =============
new_qn: 0.0319372316066791
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.0752832687134641
============= xn0: 0.033 =============
new_qn: 0.03293527009438782
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.0762813072011728
============= xn0: 0.034 =============
new_qn: 0.03393330858209655
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.0772793456888816
============= xn0: 0.035 =============
new_qn: 0.03493134706980527
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.0782773841765902
============= xn0: 0.036000000000000004 =============
new_qn: 0.03592938555751399
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.079275422664299
============= xn0: 0.037 =============
new_qn: 0.03692742404522271
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.0802734611520077
============= xn0: 0.038 =============
new_qn: 0.03792546253293143
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.0812714996397166
============= xn0: 0.039 =============
new_qn: 0.03892350102064015
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.0822695381274252
============= xn0: 0.04 =============
new_qn: 0.039921539508348876
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.083267576615134
============= xn0: 0.041 =============
new_qn: 0.040919577996057596
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.0842656151028427
============= xn0: 0.042 =============
new_qn: 0.041917616483766316
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.0852636535905513
============= xn0: 0.043000000000000003 =============
new_qn: 0.042915654971475044
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.0862616920782602
============= xn0: 0.044 =============
new_qn: 0.04391369345918376
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.0872597305659688
============= xn0: 0.045 =============
new_qn: 0.04491173194689248
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.0882577690536777
============= xn0: 0.046 =============
new_qn: 0.045909770434601205
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.0892558075413863
============= xn0: 0.047 =============
new_qn: 0.046907808922309925
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.0902538460290951
============= xn0: 0.048 =============
new_qn: 0.04790584741001865
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.0912518845168038
============= xn0: 0.049 =============
new_qn: 0.04890388589772737
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.0922499230045124
============= xn0: 0.05 =============
new_qn: 0.04990192438543609
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.0932479614922213
============= xn0: 0.051000000000000004 =============
new_qn: 0.05089996287314482
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.0942459999799299
============= xn0: 0.052000000000000005 =============
new_qn: 0.05189800136085354
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.0952440384676387
============= xn0: 0.053 =============
new_qn: 0.052896039848562254
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.0962420769553474
============= xn0: 0.054 =============
new_qn: 0.05389407833627098
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.097240115443056
============= xn0: 0.055 =============
new_qn: 0.0548921168239797
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.0982381539307648
============= xn0: 0.056 =============
new_qn: 0.05589015531168842
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.0992361924184735
============= xn0: 0.057 =============
new_qn: 0.05688819379939715
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1002342309061823
============= xn0: 0.058 =============
new_qn: 0.05788623228710587
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.101232269393891
============= xn0: 0.059000000000000004 =============
new_qn: 0.05888427077481459
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1022303078815996
============= xn0: 0.06 =============
new_qn: 0.05988230926252331
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1032283463693084
============= xn0: 0.061 =============
new_qn: 0.06088034775023203
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.104226384857017
============= xn0: 0.062 =============
new_qn: 0.06187838623794075
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.105224423344726
============= xn0: 0.063 =============
new_qn: 0.06287642472564947
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1062224618324346
============= xn0: 0.064 =============
new_qn: 0.0638744632133582
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1072205003201434
============= xn0: 0.065 =============
new_qn: 0.06487250170106693
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.108218538807852
============= xn0: 0.066 =============
new_qn: 0.06587054018877564
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1092165772955607
============= xn0: 0.067 =============
new_qn: 0.06686857867648437
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1102146157832695
============= xn0: 0.068 =============
new_qn: 0.0678666171641931
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1112126542709782
============= xn0: 0.069 =============
new_qn: 0.06886465565190181
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.112210692758687
============= xn0: 0.07 =============
new_qn: 0.06986269413961053
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1132087312463956
============= xn0: 0.07100000000000001 =============
new_qn: 0.07086073262731926
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1142067697341043
============= xn0: 0.07200000000000001 =============
new_qn: 0.07185877111502798
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1152048082218131
============= xn0: 0.073 =============
new_qn: 0.07285680960273669
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1162028467095217
============= xn0: 0.074 =============
new_qn: 0.07385484809044542
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1172008851972306
============= xn0: 0.075 =============
new_qn: 0.07485288657815413
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1181989236849392
============= xn0: 0.076 =============
new_qn: 0.07585092506586286
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1191969621726479
============= xn0: 0.077 =============
new_qn: 0.07684896355357158
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1201950006603567
============= xn0: 0.078 =============
new_qn: 0.0778470020412803
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1211930391480653
============= xn0: 0.079 =============
new_qn: 0.07884504052898902
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1221910776357742
============= xn0: 0.08 =============
new_qn: 0.07984307901669775
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1231891161234828
============= xn0: 0.081 =============
new_qn: 0.08084111750440647
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1241871546111917
============= xn0: 0.082 =============
new_qn: 0.08183915599211519
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1251851930989003
============= xn0: 0.083 =============
new_qn: 0.08283719447982392
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.126183231586609
============= xn0: 0.084 =============
new_qn: 0.08383523296753263
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1271812700743178
============= xn0: 0.085 =============
new_qn: 0.08483327145524136
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1281793085620264
============= xn0: 0.08600000000000001 =============
new_qn: 0.08583130994295009
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1291773470497353
============= xn0: 0.08700000000000001 =============
new_qn: 0.0868293484306588
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.130175385537444
============= xn0: 0.088 =============
new_qn: 0.08782738691836751
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1311734240251525
============= xn0: 0.089 =============
new_qn: 0.08882542540607624
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1321714625128614
============= xn0: 0.09 =============
new_qn: 0.08982346389378495
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.13316950100057
============= xn0: 0.091 =============
new_qn: 0.09082150238149368
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1341675394882789
============= xn0: 0.092 =============
new_qn: 0.09181954086920241
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1351655779759875
============= xn0: 0.093 =============
new_qn: 0.09281757935691112
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1361636164636961
============= xn0: 0.094 =============
new_qn: 0.09381561784461985
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.137161654951405
============= xn0: 0.095 =============
new_qn: 0.09481365633232858
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1381596934391136
============= xn0: 0.096 =============
new_qn: 0.0958116948200373
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1391577319268225
============= xn0: 0.097 =============
new_qn: 0.09680973330774602
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.140155770414531
============= xn0: 0.098 =============
new_qn: 0.09780777179545475
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.14115380890224
============= xn0: 0.099 =============
new_qn: 0.09880581028316347
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1421518473899486
============= xn0: 0.1 =============
new_qn: 0.09980384877087219
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1431498858776572
============= xn0: 0.101 =============
new_qn: 0.10080188725858091
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.144147924365366
============= xn0: 0.10200000000000001 =============
new_qn: 0.10179992574628964
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1451459628530747
============= xn0: 0.10300000000000001 =============
new_qn: 0.10279796423399835
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1461440013407835
============= xn0: 0.10400000000000001 =============
new_qn: 0.10379600272170708
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1471420398284922
============= xn0: 0.105 =============
new_qn: 0.1047940412094158
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1481400783162008
============= xn0: 0.106 =============
new_qn: 0.10579207969712451
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1491381168039096
============= xn0: 0.107 =============
new_qn: 0.10679011818483324
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1501361552916183
============= xn0: 0.108 =============
new_qn: 0.10778815667254196
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1511341937793271
============= xn0: 0.109 =============
new_qn: 0.10878619516025068
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1521322322670358
============= xn0: 0.11 =============
new_qn: 0.1097842336479594
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1531302707547446
============= xn0: 0.111 =============
new_qn: 0.11078227213566813
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1541283092424532
============= xn0: 0.112 =============
new_qn: 0.11178031062337684
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1551263477301619
============= xn0: 0.113 =============
new_qn: 0.11277834911108557
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1561243862178705
============= xn0: 0.114 =============
new_qn: 0.1137763875987943
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1571224247055794
============= xn0: 0.115 =============
new_qn: 0.11477442608650301
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.158120463193288
============= xn0: 0.116 =============
new_qn: 0.11577246457421174
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1591185016809968
============= xn0: 0.117 =============
new_qn: 0.11677050306192047
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1601165401687055
============= xn0: 0.11800000000000001 =============
new_qn: 0.11776854154962918
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1611145786564143
============= xn0: 0.11900000000000001 =============
new_qn: 0.11876658003733791
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.162112617144123
============= xn0: 0.12 =============
new_qn: 0.11976461852504662
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1631106556318316
============= xn0: 0.121 =============
new_qn: 0.12076265701275533
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1641086941195404
============= xn0: 0.122 =============
new_qn: 0.12176069550046406
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.165106732607249
============= xn0: 0.123 =============
new_qn: 0.12275873398817279
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.166104771094958
============= xn0: 0.124 =============
new_qn: 0.1237567724758815
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1671028095826665
============= xn0: 0.125 =============
new_qn: 0.12475481096359023
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1681008480703752
============= xn0: 0.126 =============
new_qn: 0.12575284945129894
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.169098886558084
============= xn0: 0.127 =============
new_qn: 0.12675088793900768
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1700969250457927
============= xn0: 0.128 =============
new_qn: 0.1277489264267164
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1710949635335015
============= xn0: 0.129 =============
new_qn: 0.1287469649144251
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1720930020212101
============= xn0: 0.13 =============
new_qn: 0.12974500340213385
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.173091040508919
============= xn0: 0.131 =============
new_qn: 0.13074304188984257
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1740890789966276
============= xn0: 0.132 =============
new_qn: 0.13174108037755128
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1750871174843363
============= xn0: 0.133 =============
new_qn: 0.13273911886526002
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.176085155972045
============= xn0: 0.134 =============
new_qn: 0.13373715735296873
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1770831944597537
============= xn0: 0.135 =============
new_qn: 0.13473519584067745
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1780812329474626
============= xn0: 0.136 =============
new_qn: 0.1357332343283862
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1790792714351712
============= xn0: 0.137 =============
new_qn: 0.1367312728160949
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1800773099228798
============= xn0: 0.138 =============
new_qn: 0.13772931130380361
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1810753484105887
============= xn0: 0.139 =============
new_qn: 0.13872734979151236
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1820733868982973
============= xn0: 0.14 =============
new_qn: 0.13972538827922107
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1830714253860062
============= xn0: 0.14100000000000001 =============
new_qn: 0.14072342676692978
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1840694638737148
============= xn0: 0.14200000000000002 =============
new_qn: 0.14172146525463852
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1850675023614237
============= xn0: 0.14300000000000002 =============
new_qn: 0.14271950374234724
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1860655408491323
============= xn0: 0.14400000000000002 =============
new_qn: 0.14371754223005595
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.187063579336841
============= xn0: 0.145 =============
new_qn: 0.14471558071776466
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1880616178245498
============= xn0: 0.146 =============
new_qn: 0.14571361920547338
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1890596563122584
============= xn0: 0.147 =============
new_qn: 0.1467116576931821
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.190057694799967
============= xn0: 0.148 =============
new_qn: 0.14770969618089083
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1910557332876759
============= xn0: 0.149 =============
new_qn: 0.14870773466859954
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1920537717753845
============= xn0: 0.15 =============
new_qn: 0.14970577315630826
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1930518102630934
============= xn0: 0.151 =============
new_qn: 0.150703811644017
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.194049848750802
============= xn0: 0.152 =============
new_qn: 0.1517018501317257
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1950478872385109
============= xn0: 0.153 =============
new_qn: 0.15269988861943443
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1960459257262195
============= xn0: 0.154 =============
new_qn: 0.15369792710714317
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1970439642139281
============= xn0: 0.155 =============
new_qn: 0.15469596559485188
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.198042002701637
============= xn0: 0.156 =============
new_qn: 0.1556940040825606
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.1990400411893456
============= xn0: 0.157 =============
new_qn: 0.15669204257026934
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2000380796770544
============= xn0: 0.158 =============
new_qn: 0.15769008105797805
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.201036118164763
============= xn0: 0.159 =============
new_qn: 0.15868811954568676
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2020341566524717
============= xn0: 0.16 =============
new_qn: 0.1596861580333955
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2030321951401806
============= xn0: 0.161 =============
new_qn: 0.16068419652110422
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2040302336278892
============= xn0: 0.162 =============
new_qn: 0.16168223500881293
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.205028272115598
============= xn0: 0.163 =============
new_qn: 0.16268027349652167
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2060263106033067
============= xn0: 0.164 =============
new_qn: 0.16367831198423038
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2070243490910155
============= xn0: 0.165 =============
new_qn: 0.1646763504719391
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2080223875787242
============= xn0: 0.166 =============
new_qn: 0.16567438895964784
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2090204260664328
============= xn0: 0.167 =============
new_qn: 0.16667242744735655
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2100184645541416
============= xn0: 0.168 =============
new_qn: 0.16767046593506527
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2110165030418503
============= xn0: 0.169 =============
new_qn: 0.168668504422774
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2120145415295591
============= xn0: 0.17 =============
new_qn: 0.16966654291048272
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2130125800172677
============= xn0: 0.171 =============
new_qn: 0.17066458139819143
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2140106185049764
============= xn0: 0.17200000000000001 =============
new_qn: 0.17166261988590018
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2150086569926852
============= xn0: 0.17300000000000001 =============
new_qn: 0.1726606583736089
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2160066954803939
============= xn0: 0.17400000000000002 =============
new_qn: 0.1736586968613176
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2170047339681027
============= xn0: 0.17500000000000002 =============
new_qn: 0.17465673534902634
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2180027724558113
============= xn0: 0.176 =============
new_qn: 0.17565477383673503
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.21900081094352
============= xn0: 0.177 =============
new_qn: 0.17665281232444374
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2199988494312288
============= xn0: 0.178 =============
new_qn: 0.17765085081215248
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2209968879189375
============= xn0: 0.179 =============
new_qn: 0.1786488892998612
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2219949264066463
============= xn0: 0.18 =============
new_qn: 0.1796469277875699
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.222992964894355
============= xn0: 0.181 =============
new_qn: 0.18064496627527865
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2239910033820638
============= xn0: 0.182 =============
new_qn: 0.18164300476298736
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2249890418697724
============= xn0: 0.183 =============
new_qn: 0.18264104325069608
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.225987080357481
============= xn0: 0.184 =============
new_qn: 0.18363908173840482
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.22698511884519
============= xn0: 0.185 =============
new_qn: 0.18463712022611353
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2279831573328985
============= xn0: 0.186 =============
new_qn: 0.18563515871382225
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2289811958206074
============= xn0: 0.187 =============
new_qn: 0.186633197201531
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.229979234308316
============= xn0: 0.188 =============
new_qn: 0.1876312356892397
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2309772727960246
============= xn0: 0.189 =============
new_qn: 0.18862927417694844
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2319753112837335
============= xn0: 0.19 =============
new_qn: 0.18962731266465715
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2329733497714421
============= xn0: 0.191 =============
new_qn: 0.19062535115236587
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.233971388259151
============= xn0: 0.192 =============
new_qn: 0.1916233896400746
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2349694267468596
============= xn0: 0.193 =============
new_qn: 0.19262142812778332
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2359674652345685
============= xn0: 0.194 =============
new_qn: 0.19361946661549204
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.236965503722277
============= xn0: 0.195 =============
new_qn: 0.19461750510320078
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2379635422099857
============= xn0: 0.196 =============
new_qn: 0.1956155435909095
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2389615806976946
============= xn0: 0.197 =============
new_qn: 0.1966135820786182
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2399596191854032
============= xn0: 0.198 =============
new_qn: 0.19761162056632695
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.240957657673112
============= xn0: 0.199 =============
new_qn: 0.19860965905403566
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2419556961608207
============= xn0: 0.2 =============
new_qn: 0.19960769754174437
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2429537346485293
============= xn0: 0.201 =============
new_qn: 0.2006057360294531
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2439517731362382
============= xn0: 0.202 =============
new_qn: 0.20160377451716183
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2449498116239468
============= xn0: 0.203 =============
new_qn: 0.20260181300487054
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2459478501116557
============= xn0: 0.20400000000000001 =============
new_qn: 0.20359985149257928
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2469458885993643
============= xn0: 0.20500000000000002 =============
new_qn: 0.204597889980288
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2479439270870731
============= xn0: 0.20600000000000002 =============
new_qn: 0.2055959284679967
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2489419655747818
============= xn0: 0.20700000000000002 =============
new_qn: 0.20659396695570545
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2499400040624904
============= xn0: 0.20800000000000002 =============
new_qn: 0.20759200544341416
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2509380425501992
============= xn0: 0.209 =============
new_qn: 0.20859004393112285
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2519360810379079
============= xn0: 0.21 =============
new_qn: 0.2095880824188316
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2529341195256167
============= xn0: 0.211 =============
new_qn: 0.2105861209065403
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2539321580133254
============= xn0: 0.212 =============
new_qn: 0.21158415939424902
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.254930196501034
============= xn0: 0.213 =============
new_qn: 0.21258219788195776
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2559282349887428
============= xn0: 0.214 =============
new_qn: 0.21358023636966647
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2569262734764515
============= xn0: 0.215 =============
new_qn: 0.21457827485737518
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2579243119641603
============= xn0: 0.216 =============
new_qn: 0.21557631334508393
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.258922350451869
============= xn0: 0.217 =============
new_qn: 0.21657435183279264
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2599203889395776
============= xn0: 0.218 =============
new_qn: 0.21757239032050135
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2609184274272864
============= xn0: 0.219 =============
new_qn: 0.2185704288082101
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.261916465914995
============= xn0: 0.22 =============
new_qn: 0.2195684672959188
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.262914504402704
============= xn0: 0.221 =============
new_qn: 0.22056650578362752
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2639125428904125
============= xn0: 0.222 =============
new_qn: 0.22156454427133626
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2649105813781214
============= xn0: 0.223 =============
new_qn: 0.22256258275904497
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.26590861986583
============= xn0: 0.224 =============
new_qn: 0.2235606212467537
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2669066583535387
============= xn0: 0.225 =============
new_qn: 0.22455865973446243
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2679046968412475
============= xn0: 0.226 =============
new_qn: 0.22555669822217114
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2689027353289564
============= xn0: 0.227 =============
new_qn: 0.22655473670987986
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.269900773816665
============= xn0: 0.228 =============
new_qn: 0.2275527751975886
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2708988123043738
============= xn0: 0.229 =============
new_qn: 0.2285508136852973
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2718968507920825
============= xn0: 0.23 =============
new_qn: 0.22954885217300602
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2728948892797913
============= xn0: 0.231 =============
new_qn: 0.23054689066071476
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2738929277675
============= xn0: 0.232 =============
new_qn: 0.23154492914842348
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2748909662552086
============= xn0: 0.233 =============
new_qn: 0.2325429676361322
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2758890047429174
============= xn0: 0.234 =============
new_qn: 0.23354100612384093
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.276887043230626
============= xn0: 0.23500000000000001 =============
new_qn: 0.23453904461154965
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.277885081718335
============= xn0: 0.23600000000000002 =============
new_qn: 0.23553708309925836
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2788831202060436
============= xn0: 0.23700000000000002 =============
new_qn: 0.2365351215869671
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2798811586937524
============= xn0: 0.23800000000000002 =============
new_qn: 0.23753316007467581
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.280879197181461
============= xn0: 0.23900000000000002 =============
new_qn: 0.23853119856238453
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2818772356691697
============= xn0: 0.24 =============
new_qn: 0.23952923705009324
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2828752741568785
============= xn0: 0.241 =============
new_qn: 0.24052727553780195
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2838733126445872
============= xn0: 0.242 =============
new_qn: 0.24152531402551067
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2848713511322958
============= xn0: 0.243 =============
new_qn: 0.2425233525132194
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2858693896200046
============= xn0: 0.244 =============
new_qn: 0.24352139100092812
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2868674281077133
============= xn0: 0.245 =============
new_qn: 0.24451942948863684
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2878654665954221
============= xn0: 0.246 =============
new_qn: 0.24551746797634558
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2888635050831307
============= xn0: 0.247 =============
new_qn: 0.2465155064640543
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2898615435708396
============= xn0: 0.248 =============
new_qn: 0.247513544951763
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2908595820585482
============= xn0: 0.249 =============
new_qn: 0.24851158343947174
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2918576205462569
============= xn0: 0.25 =============
new_qn: 0.24950962192718046
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2928556590339657
============= xn0: 0.251 =============
new_qn: 0.2505076604148892
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2938536975216743
============= xn0: 0.252 =============
new_qn: 0.2515056989025979
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2948517360093832
============= xn0: 0.253 =============
new_qn: 0.2525037373903066
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2958497744970918
============= xn0: 0.254 =============
new_qn: 0.25350177587801537
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2968478129848007
============= xn0: 0.255 =============
new_qn: 0.25449981436572405
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2978458514725093
============= xn0: 0.256 =============
new_qn: 0.2554978528534328
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.298843889960218
============= xn0: 0.257 =============
new_qn: 0.25649589134114154
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.2998419284479268
============= xn0: 0.258 =============
new_qn: 0.2574939298288502
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3008399669356354
============= xn0: 0.259 =============
new_qn: 0.25849196831655896
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3018380054233443
============= xn0: 0.26 =============
new_qn: 0.2594900068042677
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.302836043911053
============= xn0: 0.261 =============
new_qn: 0.2604880452919764
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3038340823987615
============= xn0: 0.262 =============
new_qn: 0.26148608377968513
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3048321208864704
============= xn0: 0.263 =============
new_qn: 0.26248412226739387
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.305830159374179
============= xn0: 0.264 =============
new_qn: 0.26348216075510256
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3068281978618879
============= xn0: 0.265 =============
new_qn: 0.2644801992428113
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3078262363495965
============= xn0: 0.266 =============
new_qn: 0.26547823773052004
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3088242748373053
============= xn0: 0.267 =============
new_qn: 0.2664762762182287
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.309822313325014
============= xn0: 0.268 =============
new_qn: 0.26747431470593747
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3108203518127226
============= xn0: 0.269 =============
new_qn: 0.2684723531936462
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3118183903004315
============= xn0: 0.27 =============
new_qn: 0.2694703916813549
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.31281642878814
============= xn0: 0.271 =============
new_qn: 0.27046843016906363
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.313814467275849
============= xn0: 0.272 =============
new_qn: 0.2714664686567724
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3148125057635576
============= xn0: 0.273 =============
new_qn: 0.27246450714448106
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3158105442512662
============= xn0: 0.274 =============
new_qn: 0.2734625456321898
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.316808582738975
============= xn0: 0.275 =============
new_qn: 0.27446058411989854
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3178066212266837
============= xn0: 0.276 =============
new_qn: 0.27545862260760723
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3188046597143925
============= xn0: 0.277 =============
new_qn: 0.27645666109531597
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3198026982021012
============= xn0: 0.278 =============
new_qn: 0.2774546995830247
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.32080073668981
============= xn0: 0.279 =============
new_qn: 0.2784527380707334
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3217987751775186
============= xn0: 0.28 =============
new_qn: 0.27945077655844214
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3227968136652273
============= xn0: 0.281 =============
new_qn: 0.2804488150461509
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3237948521529361
============= xn0: 0.28200000000000003 =============
new_qn: 0.28144685353385956
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3247928906406448
============= xn0: 0.28300000000000003 =============
new_qn: 0.2824448920215683
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3257909291283536
============= xn0: 0.28400000000000003 =============
new_qn: 0.28344293050927705
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3267889676160622
============= xn0: 0.28500000000000003 =============
new_qn: 0.28444096899698573
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3277870061037709
============= xn0: 0.28600000000000003 =============
new_qn: 0.2854390074846945
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3287850445914797
============= xn0: 0.28700000000000003 =============
new_qn: 0.2864370459724032
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3297830830791884
============= xn0: 0.28800000000000003 =============
new_qn: 0.2874350844601119
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3307811215668972
============= xn0: 0.289 =============
new_qn: 0.2884331229478206
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3317791600546058
============= xn0: 0.29 =============
new_qn: 0.2894311614355293
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3327771985423145
============= xn0: 0.291 =============
new_qn: 0.290429199923238
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3337752370300233
============= xn0: 0.292 =============
new_qn: 0.29142723841094675
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.334773275517732
============= xn0: 0.293 =============
new_qn: 0.2924252768986555
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3357713140054408
============= xn0: 0.294 =============
new_qn: 0.2934233153863642
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3367693524931494
============= xn0: 0.295 =============
new_qn: 0.2944213538740729
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.337767390980858
============= xn0: 0.296 =============
new_qn: 0.29541939236178166
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.338765429468567
============= xn0: 0.297 =============
new_qn: 0.29641743084949035
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3397634679562755
============= xn0: 0.298 =============
new_qn: 0.2974154693371991
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3407615064439844
============= xn0: 0.299 =============
new_qn: 0.29841350782490783
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.341759544931693
============= xn0: 0.3 =============
new_qn: 0.2994115463126165
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3427575834194017
============= xn0: 0.301 =============
new_qn: 0.30040958480032526
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3437556219071105
============= xn0: 0.302 =============
new_qn: 0.301407623288034
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3447536603948191
============= xn0: 0.303 =============
new_qn: 0.3024056617757427
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.345751698882528
============= xn0: 0.304 =============
new_qn: 0.3034037002634514
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3467497373702366
============= xn0: 0.305 =============
new_qn: 0.30440173875116017
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3477477758579455
============= xn0: 0.306 =============
new_qn: 0.30539977723886885
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.348745814345654
============= xn0: 0.307 =============
new_qn: 0.3063978157265776
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3497438528333627
============= xn0: 0.308 =============
new_qn: 0.30739585421428633
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3507418913210716
============= xn0: 0.309 =============
new_qn: 0.308393892701995
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3517399298087802
============= xn0: 0.31 =============
new_qn: 0.30939193118970376
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.352737968296489
============= xn0: 0.311 =============
new_qn: 0.3103899696774125
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3537360067841977
============= xn0: 0.312 =============
new_qn: 0.3113880081651212
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3547340452719063
============= xn0: 0.313 =============
new_qn: 0.31238604665282993
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3557320837596152
============= xn0: 0.314 =============
new_qn: 0.31338408514053867
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3567301222473238
============= xn0: 0.315 =============
new_qn: 0.31438212362824736
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3577281607350327
============= xn0: 0.316 =============
new_qn: 0.3153801621159561
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3587261992227413
============= xn0: 0.317 =============
new_qn: 0.31637820060366484
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3597242377104501
============= xn0: 0.318 =============
new_qn: 0.3173762390913735
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3607222761981588
============= xn0: 0.319 =============
new_qn: 0.31837427757908227
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3617203146858674
============= xn0: 0.32 =============
new_qn: 0.319372316066791
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3627183531735763
============= xn0: 0.321 =============
new_qn: 0.3203703545544997
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3637163916612849
============= xn0: 0.322 =============
new_qn: 0.32136839304220843
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3647144301489937
============= xn0: 0.323 =============
new_qn: 0.3223664315299172
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3657124686367024
============= xn0: 0.324 =============
new_qn: 0.32336447001762586
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.366710507124411
============= xn0: 0.325 =============
new_qn: 0.3243625085053346
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3677085456121199
============= xn0: 0.326 =============
new_qn: 0.32536054699304334
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3687065840998285
============= xn0: 0.327 =============
new_qn: 0.32635858548075203
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3697046225875373
============= xn0: 0.328 =============
new_qn: 0.32735662396846077
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.370702661075246
============= xn0: 0.329 =============
new_qn: 0.3283546624561695
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3717006995629548
============= xn0: 0.33 =============
new_qn: 0.3293527009438782
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3726987380506634
============= xn0: 0.331 =============
new_qn: 0.33035073943158694
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.373696776538372
============= xn0: 0.332 =============
new_qn: 0.3313487779192957
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.374694815026081
============= xn0: 0.333 =============
new_qn: 0.33234681640700436
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3756928535137896
============= xn0: 0.334 =============
new_qn: 0.3333448548947131
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3766908920014984
============= xn0: 0.335 =============
new_qn: 0.33434289338242185
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.377688930489207
============= xn0: 0.336 =============
new_qn: 0.33534093187013053
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3786869689769157
============= xn0: 0.337 =============
new_qn: 0.3363389703578393
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3796850074646245
============= xn0: 0.338 =============
new_qn: 0.337337008845548
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3806830459523332
============= xn0: 0.339 =============
new_qn: 0.3383350473332567
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.381681084440042
============= xn0: 0.34 =============
new_qn: 0.33933308582096544
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3826791229277506
============= xn0: 0.341 =============
new_qn: 0.3403311243086742
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3836771614154595
============= xn0: 0.342 =============
new_qn: 0.34132916279638287
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3846751999031681
============= xn0: 0.343 =============
new_qn: 0.3423272012840916
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3856732383908767
============= xn0: 0.34400000000000003 =============
new_qn: 0.34332523977180035
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3866712768785856
============= xn0: 0.34500000000000003 =============
new_qn: 0.34432327825950904
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3876693153662942
============= xn0: 0.34600000000000003 =============
new_qn: 0.3453213167472178
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.388667353854003
============= xn0: 0.34700000000000003 =============
new_qn: 0.3463193552349265
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3896653923417117
============= xn0: 0.34800000000000003 =============
new_qn: 0.3473173937226352
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3906634308294203
============= xn0: 0.34900000000000003 =============
new_qn: 0.34831543221034394
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3916614693171292
============= xn0: 0.35000000000000003 =============
new_qn: 0.3493134706980527
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3926595078048378
============= xn0: 0.35100000000000003 =============
new_qn: 0.35031150918576137
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3936575462925467
============= xn0: 0.352 =============
new_qn: 0.35130954767347006
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3946555847802553
============= xn0: 0.353 =============
new_qn: 0.3523075861611788
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.395653623267964
============= xn0: 0.354 =============
new_qn: 0.3533056246488875
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3966516617556728
============= xn0: 0.355 =============
new_qn: 0.3543036631365962
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3976497002433814
============= xn0: 0.356 =============
new_qn: 0.35530170162430497
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.3986477387310903
============= xn0: 0.357 =============
new_qn: 0.35629974011201365
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.399645777218799
============= xn0: 0.358 =============
new_qn: 0.3572977785997224
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4006438157065075
============= xn0: 0.359 =============
new_qn: 0.35829581708743113
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4016418541942164
============= xn0: 0.36 =============
new_qn: 0.3592938555751398
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.402639892681925
============= xn0: 0.361 =============
new_qn: 0.36029189406284856
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4036379311696339
============= xn0: 0.362 =============
new_qn: 0.3612899325505573
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4046359696573425
============= xn0: 0.363 =============
new_qn: 0.362287971038266
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4056340081450511
============= xn0: 0.364 =============
new_qn: 0.36328600952597473
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.40663204663276
============= xn0: 0.365 =============
new_qn: 0.36428404801368347
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4076300851204686
============= xn0: 0.366 =============
new_qn: 0.36528208650139216
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4086281236081775
============= xn0: 0.367 =============
new_qn: 0.3662801249891009
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.409626162095886
============= xn0: 0.368 =============
new_qn: 0.36727816347680964
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4106242005835947
============= xn0: 0.369 =============
new_qn: 0.3682762019645183
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4116222390713034
============= xn0: 0.37 =============
new_qn: 0.36927424045222706
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.412620277559012
============= xn0: 0.371 =============
new_qn: 0.3702722789399358
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.413618316046721
============= xn0: 0.372 =============
new_qn: 0.3712703174276445
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4146163545344297
============= xn0: 0.373 =============
new_qn: 0.37226835591535323
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4156143930221383
============= xn0: 0.374 =============
new_qn: 0.373266394403062
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.416612431509847
============= xn0: 0.375 =============
new_qn: 0.3742644328907707
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.417610469997556
============= xn0: 0.376 =============
new_qn: 0.3752624713784794
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4186085084852647
============= xn0: 0.377 =============
new_qn: 0.37626050986618814
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4196065469729733
============= xn0: 0.378 =============
new_qn: 0.3772585483538969
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.420604585460682
============= xn0: 0.379 =============
new_qn: 0.37825658684160557
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4216026239483905
============= xn0: 0.38 =============
new_qn: 0.3792546253293143
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4226006624360996
============= xn0: 0.381 =============
new_qn: 0.38025266381702305
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4235987009238082
============= xn0: 0.382 =============
new_qn: 0.38125070230473174
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4245967394115169
============= xn0: 0.383 =============
new_qn: 0.3822487407924405
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4255947778992255
============= xn0: 0.384 =============
new_qn: 0.3832467792801492
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4265928163869341
============= xn0: 0.385 =============
new_qn: 0.3842448177678579
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4275908548746432
============= xn0: 0.386 =============
new_qn: 0.38524285625556665
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4285888933623518
============= xn0: 0.387 =============
new_qn: 0.3862408947432754
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4295869318500605
============= xn0: 0.388 =============
new_qn: 0.3872389332309841
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.430584970337769
============= xn0: 0.389 =============
new_qn: 0.3882369717186928
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4315830088254777
============= xn0: 0.39 =============
new_qn: 0.38923501020640155
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4325810473131868
============= xn0: 0.391 =============
new_qn: 0.39023304869411024
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4335790858008954
============= xn0: 0.392 =============
new_qn: 0.391231087181819
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.434577124288604
============= xn0: 0.393 =============
new_qn: 0.3922291256695277
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4355751627763127
============= xn0: 0.394 =============
new_qn: 0.3932271641572364
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4365732012640213
============= xn0: 0.395 =============
new_qn: 0.39422520264494515
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4375712397517304
============= xn0: 0.396 =============
new_qn: 0.3952232411326539
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.438569278239439
============= xn0: 0.397 =============
new_qn: 0.3962212796203626
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4395673167271477
============= xn0: 0.398 =============
new_qn: 0.3972193181080713
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4405653552148563
============= xn0: 0.399 =============
new_qn: 0.39821735659578006
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.441563393702565
============= xn0: 0.4 =============
new_qn: 0.39921539508348874
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.442561432190274
============= xn0: 0.401 =============
new_qn: 0.4002134335711975
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4435594706779826
============= xn0: 0.402 =============
new_qn: 0.4012114720589062
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4445575091656913
============= xn0: 0.403 =============
new_qn: 0.4022095105466149
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4455555476533999
============= xn0: 0.404 =============
new_qn: 0.40320754903432365
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4465535861411085
============= xn0: 0.405 =============
new_qn: 0.4042055875220324
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4475516246288176
============= xn0: 0.406 =============
new_qn: 0.4052036260097411
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4485496631165262
============= xn0: 0.40700000000000003 =============
new_qn: 0.4062016644974498
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4495477016042349
============= xn0: 0.40800000000000003 =============
new_qn: 0.40719970298515856
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4505457400919435
============= xn0: 0.40900000000000003 =============
new_qn: 0.40819774147286725
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.451543778579652
============= xn0: 0.41000000000000003 =============
new_qn: 0.409195779960576
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4525418170673612
============= xn0: 0.41100000000000003 =============
new_qn: 0.41019381844828473
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4535398555550698
============= xn0: 0.41200000000000003 =============
new_qn: 0.4111918569359934
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4545378940427784
============= xn0: 0.41300000000000003 =============
new_qn: 0.41218989542370216
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.455535932530487
============= xn0: 0.41400000000000003 =============
new_qn: 0.4131879339114109
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4565339710181961
============= xn0: 0.41500000000000004 =============
new_qn: 0.4141859723991196
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4575320095059048
============= xn0: 0.41600000000000004 =============
new_qn: 0.4151840108868283
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4585300479936134
============= xn0: 0.417 =============
new_qn: 0.416182049374537
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.459528086481322
============= xn0: 0.418 =============
new_qn: 0.4171800878622457
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4605261249690307
============= xn0: 0.419 =============
new_qn: 0.41817812634995444
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4615241634567393
============= xn0: 0.42 =============
new_qn: 0.4191761648376632
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4625222019444484
============= xn0: 0.421 =============
new_qn: 0.42017420332537186
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.463520240432157
============= xn0: 0.422 =============
new_qn: 0.4211722418130806
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4645182789198656
============= xn0: 0.423 =============
new_qn: 0.42217028030078935
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4655163174075743
============= xn0: 0.424 =============
new_qn: 0.42316831878849803
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.466514355895283
============= xn0: 0.425 =============
new_qn: 0.4241663572762068
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.467512394382992
============= xn0: 0.426 =============
new_qn: 0.4251643957639155
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4685104328707006
============= xn0: 0.427 =============
new_qn: 0.4261624342516242
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4695084713584092
============= xn0: 0.428 =============
new_qn: 0.42716047273933294
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4705065098461179
============= xn0: 0.429 =============
new_qn: 0.4281585112270417
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.471504548333827
============= xn0: 0.43 =============
new_qn: 0.42915654971475037
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4725025868215356
============= xn0: 0.431 =============
new_qn: 0.4301545882024591
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4735006253092442
============= xn0: 0.432 =============
new_qn: 0.43115262669016785
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4744986637969528
============= xn0: 0.433 =============
new_qn: 0.43215066517787654
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4754967022846615
============= xn0: 0.434 =============
new_qn: 0.4331487036655853
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4764947407723705
============= xn0: 0.435 =============
new_qn: 0.434146742153294
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4774927792600792
============= xn0: 0.436 =============
new_qn: 0.4351447806410027
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4784908177477878
============= xn0: 0.437 =============
new_qn: 0.43614281912871145
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4794888562354964
============= xn0: 0.438 =============
new_qn: 0.4371408576164202
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.480486894723205
============= xn0: 0.439 =============
new_qn: 0.43813889610412887
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4814849332109141
============= xn0: 0.44 =============
new_qn: 0.4391369345918376
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4824829716986228
============= xn0: 0.441 =============
new_qn: 0.44013497307954635
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4834810101863314
============= xn0: 0.442 =============
new_qn: 0.44113301156725504
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.48447904867404
============= xn0: 0.443 =============
new_qn: 0.4421310500549638
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4854770871617486
============= xn0: 0.444 =============
new_qn: 0.4431290885426725
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4864751256494577
============= xn0: 0.445 =============
new_qn: 0.4441271270303812
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4874731641371663
============= xn0: 0.446 =============
new_qn: 0.44512516551808995
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.488471202624875
============= xn0: 0.447 =============
new_qn: 0.4461232040057987
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4894692411125836
============= xn0: 0.448 =============
new_qn: 0.4471212424935074
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4904672796002922
============= xn0: 0.449 =============
new_qn: 0.4481192809812161
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4914653180880013
============= xn0: 0.45 =============
new_qn: 0.44911731946892486
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.49246335657571
============= xn0: 0.451 =============
new_qn: 0.45011535795663354
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4934613950634186
============= xn0: 0.452 =============
new_qn: 0.4511133964443423
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4944594335511272
============= xn0: 0.453 =============
new_qn: 0.452111434932051
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4954574720388363
============= xn0: 0.454 =============
new_qn: 0.4531094734197597
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.496455510526545
============= xn0: 0.455 =============
new_qn: 0.45410751190746845
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4974535490142535
============= xn0: 0.456 =============
new_qn: 0.4551055503951772
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4984515875019622
============= xn0: 0.457 =============
new_qn: 0.4561035888828859
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.4994496259896708
============= xn0: 0.458 =============
new_qn: 0.4571016273705946
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5004476644773799
============= xn0: 0.459 =============
new_qn: 0.45809966585830336
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5014457029650885
============= xn0: 0.46 =============
new_qn: 0.45909770434601205
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5024437414527971
============= xn0: 0.461 =============
new_qn: 0.4600957428337208
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5034417799405058
============= xn0: 0.462 =============
new_qn: 0.46109378132142953
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5044398184282144
============= xn0: 0.463 =============
new_qn: 0.4620918198091382
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5054378569159235
============= xn0: 0.464 =============
new_qn: 0.46308985829684696
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.506435895403632
============= xn0: 0.465 =============
new_qn: 0.4640878967845557
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5074339338913407
============= xn0: 0.466 =============
new_qn: 0.4650859352722644
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5084319723790494
============= xn0: 0.467 =============
new_qn: 0.4660839737599731
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.509430010866758
============= xn0: 0.468 =============
new_qn: 0.46708201224768187
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.510428049354467
============= xn0: 0.46900000000000003 =============
new_qn: 0.46808005073539055
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5114260878421757
============= xn0: 0.47000000000000003 =============
new_qn: 0.4690780892230993
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5124241263298843
============= xn0: 0.47100000000000003 =============
new_qn: 0.47007612771080803
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.513422164817593
============= xn0: 0.47200000000000003 =============
new_qn: 0.4710741661985167
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5144202033053016
============= xn0: 0.47300000000000003 =============
new_qn: 0.47207220468622546
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5154182417930107
============= xn0: 0.47400000000000003 =============
new_qn: 0.4730702431739342
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5164162802807193
============= xn0: 0.47500000000000003 =============
new_qn: 0.4740682816616429
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.517414318768428
============= xn0: 0.47600000000000003 =============
new_qn: 0.47506632014935163
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5184123572561365
============= xn0: 0.47700000000000004 =============
new_qn: 0.47606435863706037
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5194103957438456
============= xn0: 0.47800000000000004 =============
new_qn: 0.47706239712476906
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5204084342315543
============= xn0: 0.47900000000000004 =============
new_qn: 0.4780604356124778
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5214064727192629
============= xn0: 0.48 =============
new_qn: 0.4790584741001865
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5224045112069715
============= xn0: 0.481 =============
new_qn: 0.48005651258789517
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5234025496946801
============= xn0: 0.482 =============
new_qn: 0.4810545510756039
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5244005881823888
============= xn0: 0.483 =============
new_qn: 0.48205258956331265
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5253986266700978
============= xn0: 0.484 =============
new_qn: 0.48305062805102134
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5263966651578065
============= xn0: 0.485 =============
new_qn: 0.4840486665387301
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.527394703645515
============= xn0: 0.486 =============
new_qn: 0.4850467050264388
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5283927421332237
============= xn0: 0.487 =============
new_qn: 0.4860447435141475
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5293907806209324
============= xn0: 0.488 =============
new_qn: 0.48704278200185624
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5303888191086414
============= xn0: 0.489 =============
new_qn: 0.488040820489565
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.53138685759635
============= xn0: 0.49 =============
new_qn: 0.48903885897727367
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5323848960840587
============= xn0: 0.491 =============
new_qn: 0.4900368974649824
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5333829345717673
============= xn0: 0.492 =============
new_qn: 0.49103493595269115
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5343809730594764
============= xn0: 0.493 =============
new_qn: 0.49203297444039984
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.535379011547185
============= xn0: 0.494 =============
new_qn: 0.4930310129281086
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5363770500348937
============= xn0: 0.495 =============
new_qn: 0.4940290514158173
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5373750885226023
============= xn0: 0.496 =============
new_qn: 0.495027089903526
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.538373127010311
============= xn0: 0.497 =============
new_qn: 0.49602512839123475
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.53937116549802
============= xn0: 0.498 =============
new_qn: 0.4970231668789435
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5403692039857286
============= xn0: 0.499 =============
new_qn: 0.4980212053666522
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5413672424734373
============= xn0: 0.5 =============
new_qn: 0.4990192438543609
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.542365280961146
============= xn0: 0.501 =============
new_qn: 0.5000172823420697
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5433633194488545
============= xn0: 0.502 =============
new_qn: 0.5010153208297784
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5443613579365636
============= xn0: 0.503 =============
new_qn: 0.5020133593174871
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5453593964242722
============= xn0: 0.504 =============
new_qn: 0.5030113978051958
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5463574349119809
============= xn0: 0.505 =============
new_qn: 0.5040094362929045
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5473554733996895
============= xn0: 0.506 =============
new_qn: 0.5050074747806133
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5483535118873981
============= xn0: 0.507 =============
new_qn: 0.506005513268322
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5493515503751072
============= xn0: 0.508 =============
new_qn: 0.5070035517560307
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5503495888628158
============= xn0: 0.509 =============
new_qn: 0.5080015902437395
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5513476273505244
============= xn0: 0.51 =============
new_qn: 0.5089996287314481
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.552345665838233
============= xn0: 0.511 =============
new_qn: 0.5099976672191568
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5533437043259417
============= xn0: 0.512 =============
new_qn: 0.5109957057068656
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5543417428136508
============= xn0: 0.513 =============
new_qn: 0.5119937441945743
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5553397813013594
============= xn0: 0.514 =============
new_qn: 0.5129917826822831
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.556337819789068
============= xn0: 0.515 =============
new_qn: 0.5139898211699918
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5573358582767767
============= xn0: 0.516 =============
new_qn: 0.5149878596577004
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5583338967644853
============= xn0: 0.517 =============
new_qn: 0.5159858981454092
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5593319352521944
============= xn0: 0.518 =============
new_qn: 0.5169839366331179
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.560329973739903
============= xn0: 0.519 =============
new_qn: 0.5179819751208267
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5613280122276116
============= xn0: 0.52 =============
new_qn: 0.5189800136085354
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5623260507153203
============= xn0: 0.521 =============
new_qn: 0.5199780520962441
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5633240892030293
============= xn0: 0.522 =============
new_qn: 0.5209760905839528
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.564322127690738
============= xn0: 0.523 =============
new_qn: 0.5219741290716615
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5653201661784466
============= xn0: 0.524 =============
new_qn: 0.5229721675593703
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5663182046661552
============= xn0: 0.525 =============
new_qn: 0.523970206047079
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5673162431538639
============= xn0: 0.526 =============
new_qn: 0.5249682445347877
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.568314281641573
============= xn0: 0.527 =============
new_qn: 0.5259662830224965
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5693123201292816
============= xn0: 0.528 =============
new_qn: 0.5269643215102051
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5703103586169902
============= xn0: 0.529 =============
new_qn: 0.5279623599979139
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5713083971046988
============= xn0: 0.53 =============
new_qn: 0.5289603984856226
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5723064355924075
============= xn0: 0.531 =============
new_qn: 0.5299584369733313
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5733044740801165
============= xn0: 0.532 =============
new_qn: 0.5309564754610401
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5743025125678252
============= xn0: 0.533 =============
new_qn: 0.5319545139487488
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5753005510555338
============= xn0: 0.534 =============
new_qn: 0.5329525524364574
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5762985895432424
============= xn0: 0.535 =============
new_qn: 0.5339505909241662
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.577296628030951
============= xn0: 0.536 =============
new_qn: 0.5349486294118749
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5782946665186601
============= xn0: 0.537 =============
new_qn: 0.5359466678995837
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5792927050063688
============= xn0: 0.538 =============
new_qn: 0.5369447063872924
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5802907434940774
============= xn0: 0.539 =============
new_qn: 0.5379427448750012
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.581288781981786
============= xn0: 0.54 =============
new_qn: 0.5389407833627098
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5822868204694946
============= xn0: 0.541 =============
new_qn: 0.5399388218504185
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5832848589572037
============= xn0: 0.542 =============
new_qn: 0.5409368603381273
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5842828974449124
============= xn0: 0.543 =============
new_qn: 0.541934898825836
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.585280935932621
============= xn0: 0.544 =============
new_qn: 0.5429329373135448
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5862789744203296
============= xn0: 0.545 =============
new_qn: 0.5439309758012535
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5872770129080387
============= xn0: 0.546 =============
new_qn: 0.5449290142889621
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5882750513957473
============= xn0: 0.547 =============
new_qn: 0.5459270527766709
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.589273089883456
============= xn0: 0.548 =============
new_qn: 0.5469250912643796
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5902711283711646
============= xn0: 0.549 =============
new_qn: 0.5479231297520883
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5912691668588732
============= xn0: 0.55 =============
new_qn: 0.5489211682397971
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5922672053465823
============= xn0: 0.551 =============
new_qn: 0.5499192067275058
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.593265243834291
============= xn0: 0.552 =============
new_qn: 0.5509172452152145
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5942632823219995
============= xn0: 0.553 =============
new_qn: 0.5519152837029232
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5952613208097082
============= xn0: 0.554 =============
new_qn: 0.5529133221906319
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5962593592974168
============= xn0: 0.555 =============
new_qn: 0.5539113606783407
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5972573977851259
============= xn0: 0.556 =============
new_qn: 0.5549093991660494
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5982554362728345
============= xn0: 0.557 =============
new_qn: 0.5559074376537582
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.5992534747605431
============= xn0: 0.558 =============
new_qn: 0.5569054761414668
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6002515132482518
============= xn0: 0.559 =============
new_qn: 0.5579035146291755
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6012495517359604
============= xn0: 0.56 =============
new_qn: 0.5589015531168843
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6022475902236695
============= xn0: 0.561 =============
new_qn: 0.559899591604593
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.603245628711378
============= xn0: 0.562 =============
new_qn: 0.5608976300923018
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6042436671990867
============= xn0: 0.5630000000000001 =============
new_qn: 0.5618956685800105
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6052417056867954
============= xn0: 0.5640000000000001 =============
new_qn: 0.5628937070677191
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.606239744174504
============= xn0: 0.5650000000000001 =============
new_qn: 0.5638917455554279
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.607237782662213
============= xn0: 0.5660000000000001 =============
new_qn: 0.5648897840431366
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6082358211499217
============= xn0: 0.5670000000000001 =============
new_qn: 0.5658878225308454
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6092338596376303
============= xn0: 0.5680000000000001 =============
new_qn: 0.5668858610185541
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.610231898125339
============= xn0: 0.5690000000000001 =============
new_qn: 0.5678838995062628
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.611229936613048
============= xn0: 0.5700000000000001 =============
new_qn: 0.5688819379939715
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6122279751007567
============= xn0: 0.5710000000000001 =============
new_qn: 0.5698799764816802
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6132260135884653
============= xn0: 0.5720000000000001 =============
new_qn: 0.570878014969389
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.614224052076174
============= xn0: 0.5730000000000001 =============
new_qn: 0.5718760534570977
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6152220905638826
============= xn0: 0.5740000000000001 =============
new_qn: 0.5728740919448064
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6162201290515916
============= xn0: 0.5750000000000001 =============
new_qn: 0.5738721304325152
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6172181675393003
============= xn0: 0.5760000000000001 =============
new_qn: 0.5748701689202238
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6182162060270089
============= xn0: 0.577 =============
new_qn: 0.5758682074079324
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6192142445147175
============= xn0: 0.578 =============
new_qn: 0.5768662458956412
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6202122830024261
============= xn0: 0.579 =============
new_qn: 0.5778642843833499
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6212103214901348
============= xn0: 0.58 =============
new_qn: 0.5788623228710587
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6222083599778439
============= xn0: 0.581 =============
new_qn: 0.5798603613587674
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6232063984655525
============= xn0: 0.582 =============
new_qn: 0.580858399846476
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.624204436953261
============= xn0: 0.583 =============
new_qn: 0.5818564383341848
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6252024754409697
============= xn0: 0.584 =============
new_qn: 0.5828544768218935
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6262005139286784
============= xn0: 0.585 =============
new_qn: 0.5838525153096022
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6271985524163874
============= xn0: 0.586 =============
new_qn: 0.584850553797311
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.628196590904096
============= xn0: 0.587 =============
new_qn: 0.5858485922850197
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6291946293918047
============= xn0: 0.588 =============
new_qn: 0.5868466307727284
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6301926678795133
============= xn0: 0.589 =============
new_qn: 0.5878446692604371
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.631190706367222
============= xn0: 0.59 =============
new_qn: 0.5888427077481458
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.632188744854931
============= xn0: 0.591 =============
new_qn: 0.5898407462358546
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6331867833426397
============= xn0: 0.592 =============
new_qn: 0.5908387847235633
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6341848218303483
============= xn0: 0.593 =============
new_qn: 0.5918368232112721
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.635182860318057
============= xn0: 0.594 =============
new_qn: 0.5928348616989807
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6361808988057656
============= xn0: 0.595 =============
new_qn: 0.5938329001866894
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6371789372934746
============= xn0: 0.596 =============
new_qn: 0.5948309386743982
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6381769757811833
============= xn0: 0.597 =============
new_qn: 0.5958289771621069
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.639175014268892
============= xn0: 0.598 =============
new_qn: 0.5968270156498157
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6401730527566005
============= xn0: 0.599 =============
new_qn: 0.5978250541375244
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6411710912443096
============= xn0: 0.6 =============
new_qn: 0.598823092625233
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6421691297320182
============= xn0: 0.601 =============
new_qn: 0.5998211311129418
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6431671682197269
============= xn0: 0.602 =============
new_qn: 0.6008191696006505
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6441652067074355
============= xn0: 0.603 =============
new_qn: 0.6018172080883593
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6451632451951441
============= xn0: 0.604 =============
new_qn: 0.602815246576068
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6461612836828532
============= xn0: 0.605 =============
new_qn: 0.6038132850637767
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6471593221705618
============= xn0: 0.606 =============
new_qn: 0.6048113235514854
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6481573606582705
============= xn0: 0.607 =============
new_qn: 0.6058093620391941
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.649155399145979
============= xn0: 0.608 =============
new_qn: 0.6068074005269029
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6501534376336877
============= xn0: 0.609 =============
new_qn: 0.6078054390146116
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6511514761213968
============= xn0: 0.61 =============
new_qn: 0.6088034775023203
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6521495146091054
============= xn0: 0.611 =============
new_qn: 0.6098015159900291
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.653147553096814
============= xn0: 0.612 =============
new_qn: 0.6107995544777377
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6541455915845227
============= xn0: 0.613 =============
new_qn: 0.6117975929654464
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6551436300722313
============= xn0: 0.614 =============
new_qn: 0.6127956314531552
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6561416685599404
============= xn0: 0.615 =============
new_qn: 0.6137936699408639
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.657139707047649
============= xn0: 0.616 =============
new_qn: 0.6147917084285727
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6581377455353576
============= xn0: 0.617 =============
new_qn: 0.6157897469162814
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6591357840230663
============= xn0: 0.618 =============
new_qn: 0.61678778540399
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.660133822510775
============= xn0: 0.619 =============
new_qn: 0.6177858238916988
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.661131860998484
============= xn0: 0.62 =============
new_qn: 0.6187838623794075
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6621298994861926
============= xn0: 0.621 =============
new_qn: 0.6197819008671163
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6631279379739012
============= xn0: 0.622 =============
new_qn: 0.620779939354825
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6641259764616099
============= xn0: 0.623 =============
new_qn: 0.6217779778425337
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.665124014949319
============= xn0: 0.624 =============
new_qn: 0.6227760163302424
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6661220534370276
============= xn0: 0.625 =============
new_qn: 0.6237740548179511
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6671200919247362
============= xn0: 0.626 =============
new_qn: 0.6247720933056599
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6681181304124448
============= xn0: 0.627 =============
new_qn: 0.6257701317933686
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6691161689001535
============= xn0: 0.628 =============
new_qn: 0.6267681702810773
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6701142073878625
============= xn0: 0.629 =============
new_qn: 0.6277662087687861
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6711122458755712
============= xn0: 0.63 =============
new_qn: 0.6287642472564947
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6721102843632798
============= xn0: 0.631 =============
new_qn: 0.6297622857442035
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6731083228509884
============= xn0: 0.632 =============
new_qn: 0.6307603242319122
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.674106361338697
============= xn0: 0.633 =============
new_qn: 0.6317583627196209
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6751043998264061
============= xn0: 0.634 =============
new_qn: 0.6327564012073297
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6761024383141148
============= xn0: 0.635 =============
new_qn: 0.6337544396950384
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6771004768018234
============= xn0: 0.636 =============
new_qn: 0.634752478182747
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.678098515289532
============= xn0: 0.637 =============
new_qn: 0.6357505166704558
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6790965537772407
============= xn0: 0.638 =============
new_qn: 0.6367485551581645
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6800945922649497
============= xn0: 0.639 =============
new_qn: 0.6377465936458733
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6810926307526584
============= xn0: 0.64 =============
new_qn: 0.638744632133582
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.682090669240367
============= xn0: 0.641 =============
new_qn: 0.6397426706212908
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6830887077280756
============= xn0: 0.642 =============
new_qn: 0.6407407091089994
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6840867462157842
============= xn0: 0.643 =============
new_qn: 0.6417387475967081
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6850847847034933
============= xn0: 0.644 =============
new_qn: 0.6427367860844169
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.686082823191202
============= xn0: 0.645 =============
new_qn: 0.6437348245721256
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6870808616789106
============= xn0: 0.646 =============
new_qn: 0.6447328630598343
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6880789001666192
============= xn0: 0.647 =============
new_qn: 0.6457309015475431
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6890769386543283
============= xn0: 0.648 =============
new_qn: 0.6467289400352517
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.690074977142037
============= xn0: 0.649 =============
new_qn: 0.6477269785229605
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6910730156297455
============= xn0: 0.65 =============
new_qn: 0.6487250170106692
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6920710541174542
============= xn0: 0.651 =============
new_qn: 0.6497230554983779
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6930690926051628
============= xn0: 0.652 =============
new_qn: 0.6507210939860867
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6940671310928719
============= xn0: 0.653 =============
new_qn: 0.6517191324737954
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6950651695805805
============= xn0: 0.654 =============
new_qn: 0.6527171709615041
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6960632080682891
============= xn0: 0.655 =============
new_qn: 0.6537152094492128
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6970612465559978
============= xn0: 0.656 =============
new_qn: 0.6547132479369215
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6980592850437064
============= xn0: 0.657 =============
new_qn: 0.6557112864246303
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.6990573235314155
============= xn0: 0.658 =============
new_qn: 0.656709324912339
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.700055362019124
============= xn0: 0.659 =============
new_qn: 0.6577073634000478
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7010534005068327
============= xn0: 0.66 =============
new_qn: 0.6587054018877564
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7020514389945414
============= xn0: 0.661 =============
new_qn: 0.6597034403754651
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.70304947748225
============= xn0: 0.662 =============
new_qn: 0.6607014788631739
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.704047515969959
============= xn0: 0.663 =============
new_qn: 0.6616995173508826
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7050455544576677
============= xn0: 0.664 =============
new_qn: 0.6626975558385914
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7060435929453763
============= xn0: 0.665 =============
new_qn: 0.6636955943263001
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.707041631433085
============= xn0: 0.666 =============
new_qn: 0.6646936328140087
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7080396699207936
============= xn0: 0.667 =============
new_qn: 0.6656916713017175
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7090377084085027
============= xn0: 0.668 =============
new_qn: 0.6666897097894262
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7100357468962113
============= xn0: 0.669 =============
new_qn: 0.667687748277135
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.71103378538392
============= xn0: 0.67 =============
new_qn: 0.6686857867648437
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7120318238716286
============= xn0: 0.671 =============
new_qn: 0.6696838252525524
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7130298623593376
============= xn0: 0.672 =============
new_qn: 0.6706818637402611
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7140279008470463
============= xn0: 0.673 =============
new_qn: 0.6716799022279698
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.715025939334755
============= xn0: 0.674 =============
new_qn: 0.6726779407156785
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7160239778224635
============= xn0: 0.675 =============
new_qn: 0.6736759792033873
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7170220163101721
============= xn0: 0.676 =============
new_qn: 0.674674017691096
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7180200547978812
============= xn0: 0.677 =============
new_qn: 0.6756720561788048
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7190180932855899
============= xn0: 0.678 =============
new_qn: 0.6766700946665134
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7200161317732985
============= xn0: 0.679 =============
new_qn: 0.6776681331542221
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7210141702610071
============= xn0: 0.68 =============
new_qn: 0.6786661716419309
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7220122087487157
============= xn0: 0.681 =============
new_qn: 0.6796642101296396
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7230102472364248
============= xn0: 0.682 =============
new_qn: 0.6806622486173484
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7240082857241334
============= xn0: 0.683 =============
new_qn: 0.6816602871050571
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.725006324211842
============= xn0: 0.684 =============
new_qn: 0.6826583255927657
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7260043626995507
============= xn0: 0.685 =============
new_qn: 0.6836563640804745
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7270024011872593
============= xn0: 0.686 =============
new_qn: 0.6846544025681832
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7280004396749684
============= xn0: 0.687 =============
new_qn: 0.685652441055892
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.728998478162677
============= xn0: 0.6880000000000001 =============
new_qn: 0.6866504795436007
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7299965166503857
============= xn0: 0.6890000000000001 =============
new_qn: 0.6876485180313094
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7309945551380943
============= xn0: 0.6900000000000001 =============
new_qn: 0.6886465565190181
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.731992593625803
============= xn0: 0.6910000000000001 =============
new_qn: 0.6896445950067268
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.732990632113512
============= xn0: 0.6920000000000001 =============
new_qn: 0.6906426334944356
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7339886706012206
============= xn0: 0.6930000000000001 =============
new_qn: 0.6916406719821443
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7349867090889293
============= xn0: 0.6940000000000001 =============
new_qn: 0.692638710469853
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.735984747576638
============= xn0: 0.6950000000000001 =============
new_qn: 0.6936367489575618
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.736982786064347
============= xn0: 0.6960000000000001 =============
new_qn: 0.6946347874452704
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7379808245520556
============= xn0: 0.6970000000000001 =============
new_qn: 0.6956328259329791
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7389788630397642
============= xn0: 0.6980000000000001 =============
new_qn: 0.6966308644206879
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7399769015274729
============= xn0: 0.6990000000000001 =============
new_qn: 0.6976289029083966
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7409749400151815
============= xn0: 0.7000000000000001 =============
new_qn: 0.6986269413961054
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7419729785028906
============= xn0: 0.7010000000000001 =============
new_qn: 0.6996249798838141
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7429710169905992
============= xn0: 0.7020000000000001 =============
new_qn: 0.7006230183715227
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7439690554783078
============= xn0: 0.7030000000000001 =============
new_qn: 0.7016210568592315
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7449670939660165
============= xn0: 0.704 =============
new_qn: 0.7026190953469401
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.745965132453725
============= xn0: 0.705 =============
new_qn: 0.7036171338346489
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7469631709414337
============= xn0: 0.706 =============
new_qn: 0.7046151723223576
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7479612094291428
============= xn0: 0.707 =============
new_qn: 0.7056132108100663
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7489592479168514
============= xn0: 0.708 =============
new_qn: 0.706611249297775
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.74995728640456
============= xn0: 0.709 =============
new_qn: 0.7076092877854837
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7509553248922687
============= xn0: 0.71 =============
new_qn: 0.7086073262731925
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7519533633799773
============= xn0: 0.711 =============
new_qn: 0.7096053647609012
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7529514018676864
============= xn0: 0.712 =============
new_qn: 0.7106034032486099
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.753949440355395
============= xn0: 0.713 =============
new_qn: 0.7116014417363187
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7549474788431036
============= xn0: 0.714 =============
new_qn: 0.7125994802240273
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7559455173308123
============= xn0: 0.715 =============
new_qn: 0.713597518711736
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.756943555818521
============= xn0: 0.716 =============
new_qn: 0.7145955571994448
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.75794159430623
============= xn0: 0.717 =============
new_qn: 0.7155935956871535
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7589396327939386
============= xn0: 0.718 =============
new_qn: 0.7165916341748623
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7599376712816472
============= xn0: 0.719 =============
new_qn: 0.717589672662571
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7609357097693559
============= xn0: 0.72 =============
new_qn: 0.7185877111502796
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7619337482570645
============= xn0: 0.721 =============
new_qn: 0.7195857496379884
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7629317867447736
============= xn0: 0.722 =============
new_qn: 0.7205837881256971
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7639298252324822
============= xn0: 0.723 =============
new_qn: 0.7215818266134059
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7649278637201908
============= xn0: 0.724 =============
new_qn: 0.7225798651011146
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7659259022078995
============= xn0: 0.725 =============
new_qn: 0.7235779035888233
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7669239406956085
============= xn0: 0.726 =============
new_qn: 0.724575942076532
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7679219791833172
============= xn0: 0.727 =============
new_qn: 0.7255739805642407
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7689200176710258
============= xn0: 0.728 =============
new_qn: 0.7265720190519495
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7699180561587344
============= xn0: 0.729 =============
new_qn: 0.7275700575396582
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.770916094646443
============= xn0: 0.73 =============
new_qn: 0.7285680960273669
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7719141331341521
============= xn0: 0.731 =============
new_qn: 0.7295661345150757
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7729121716218608
============= xn0: 0.732 =============
new_qn: 0.7305641730027843
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7739102101095694
============= xn0: 0.733 =============
new_qn: 0.731562211490493
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.774908248597278
============= xn0: 0.734 =============
new_qn: 0.7325602499782018
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7759062870849867
============= xn0: 0.735 =============
new_qn: 0.7335582884659105
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7769043255726957
============= xn0: 0.736 =============
new_qn: 0.7345563269536193
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7779023640604044
============= xn0: 0.737 =============
new_qn: 0.735554365441328
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.778900402548113
============= xn0: 0.738 =============
new_qn: 0.7365524039290366
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7798984410358216
============= xn0: 0.739 =============
new_qn: 0.7375504424167454
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7808964795235303
============= xn0: 0.74 =============
new_qn: 0.7385484809044541
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7818945180112393
============= xn0: 0.741 =============
new_qn: 0.7395465193921629
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.782892556498948
============= xn0: 0.742 =============
new_qn: 0.7405445578798716
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7838905949866566
============= xn0: 0.743 =============
new_qn: 0.7415425963675804
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7848886334743652
============= xn0: 0.744 =============
new_qn: 0.742540634855289
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7858866719620738
============= xn0: 0.745 =============
new_qn: 0.7435386733429977
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.786884710449783
============= xn0: 0.746 =============
new_qn: 0.7445367118307065
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7878827489374916
============= xn0: 0.747 =============
new_qn: 0.7455347503184152
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7888807874252002
============= xn0: 0.748 =============
new_qn: 0.746532788806124
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7898788259129088
============= xn0: 0.749 =============
new_qn: 0.7475308272938327
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7908768644006179
============= xn0: 0.75 =============
new_qn: 0.7485288657815414
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7918749028883265
============= xn0: 0.751 =============
new_qn: 0.7495269042692501
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7928729413760351
============= xn0: 0.752 =============
new_qn: 0.7505249427569588
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7938709798637438
============= xn0: 0.753 =============
new_qn: 0.7515229812446675
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7948690183514524
============= xn0: 0.754 =============
new_qn: 0.7525210197323763
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7958670568391615
============= xn0: 0.755 =============
new_qn: 0.753519058220085
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.79686509532687
============= xn0: 0.756 =============
new_qn: 0.7545170967077938
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7978631338145787
============= xn0: 0.757 =============
new_qn: 0.7555151351955024
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.7988611723022874
============= xn0: 0.758 =============
new_qn: 0.7565131736832111
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.799859210789996
============= xn0: 0.759 =============
new_qn: 0.7575112121709199
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.800857249277705
============= xn0: 0.76 =============
new_qn: 0.7585092506586286
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8018552877654137
============= xn0: 0.761 =============
new_qn: 0.7595072891463374
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8028533262531223
============= xn0: 0.762 =============
new_qn: 0.7605053276340461
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.803851364740831
============= xn0: 0.763 =============
new_qn: 0.7615033661217547
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8048494032285396
============= xn0: 0.764 =============
new_qn: 0.7625014046094635
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8058474417162487
============= xn0: 0.765 =============
new_qn: 0.7634994430971722
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8068454802039573
============= xn0: 0.766 =============
new_qn: 0.764497481584881
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.807843518691666
============= xn0: 0.767 =============
new_qn: 0.7654955200725897
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8088415571793746
============= xn0: 0.768 =============
new_qn: 0.7664935585602984
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8098395956670836
============= xn0: 0.769 =============
new_qn: 0.7674915970480071
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8108376341547923
============= xn0: 0.77 =============
new_qn: 0.7684896355357158
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.811835672642501
============= xn0: 0.771 =============
new_qn: 0.7694876740234246
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8128337111302095
============= xn0: 0.772 =============
new_qn: 0.7704857125111333
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8138317496179182
============= xn0: 0.773 =============
new_qn: 0.771483750998842
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8148297881056272
============= xn0: 0.774 =============
new_qn: 0.7724817894865508
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8158278265933359
============= xn0: 0.775 =============
new_qn: 0.7734798279742594
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8168258650810445
============= xn0: 0.776 =============
new_qn: 0.7744778664619681
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8178239035687531
============= xn0: 0.777 =============
new_qn: 0.7754759049496769
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8188219420564617
============= xn0: 0.778 =============
new_qn: 0.7764739434373856
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8198199805441708
============= xn0: 0.779 =============
new_qn: 0.7774719819250944
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8208180190318795
============= xn0: 0.78 =============
new_qn: 0.7784700204128031
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.821816057519588
============= xn0: 0.781 =============
new_qn: 0.7794680589005117
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8228140960072967
============= xn0: 0.782 =============
new_qn: 0.7804660973882205
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8238121344950053
============= xn0: 0.783 =============
new_qn: 0.7814641358759292
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8248101729827144
============= xn0: 0.784 =============
new_qn: 0.782462174363638
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.825808211470423
============= xn0: 0.785 =============
new_qn: 0.7834602128513467
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8268062499581317
============= xn0: 0.786 =============
new_qn: 0.7844582513390554
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8278042884458403
============= xn0: 0.787 =============
new_qn: 0.7854562898267641
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.828802326933549
============= xn0: 0.788 =============
new_qn: 0.7864543283144728
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.829800365421258
============= xn0: 0.789 =============
new_qn: 0.7874523668021816
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8307984039089666
============= xn0: 0.79 =============
new_qn: 0.7884504052898903
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8317964423966753
============= xn0: 0.791 =============
new_qn: 0.789448443777599
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.832794480884384
============= xn0: 0.792 =============
new_qn: 0.7904464822653078
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.833792519372093
============= xn0: 0.793 =============
new_qn: 0.7914445207530164
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8347905578598016
============= xn0: 0.794 =============
new_qn: 0.7924425592407252
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8357885963475102
============= xn0: 0.795 =============
new_qn: 0.7934405977284339
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8367866348352189
============= xn0: 0.796 =============
new_qn: 0.7944386362161426
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8377846733229275
============= xn0: 0.797 =============
new_qn: 0.7954366747038514
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8387827118106366
============= xn0: 0.798 =============
new_qn: 0.7964347131915601
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8397807502983452
============= xn0: 0.799 =============
new_qn: 0.7974327516792687
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8407787887860538
============= xn0: 0.8 =============
new_qn: 0.7984307901669775
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8417768272737625
============= xn0: 0.801 =============
new_qn: 0.7994288286546862
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.842774865761471
============= xn0: 0.802 =============
new_qn: 0.800426867142395
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8437729042491802
============= xn0: 0.803 =============
new_qn: 0.8014249056301037
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8447709427368888
============= xn0: 0.804 =============
new_qn: 0.8024229441178125
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8457689812245974
============= xn0: 0.805 =============
new_qn: 0.8034209826055211
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.846767019712306
============= xn0: 0.806 =============
new_qn: 0.8044190210932298
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8477650582000147
============= xn0: 0.807 =============
new_qn: 0.8054170595809386
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8487630966877238
============= xn0: 0.808 =============
new_qn: 0.8064150980686473
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8497611351754324
============= xn0: 0.809 =============
new_qn: 0.807413136556356
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.850759173663141
============= xn0: 0.81 =============
new_qn: 0.8084111750440648
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8517572121508497
============= xn0: 0.811 =============
new_qn: 0.8094092135317734
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8527552506385583
============= xn0: 0.812 =============
new_qn: 0.8104072520194822
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8537532891262674
============= xn0: 0.8130000000000001 =============
new_qn: 0.8114052905071909
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.854751327613976
============= xn0: 0.8140000000000001 =============
new_qn: 0.8124033289948996
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8557493661016846
============= xn0: 0.8150000000000001 =============
new_qn: 0.8134013674826084
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8567474045893932
============= xn0: 0.8160000000000001 =============
new_qn: 0.8143994059703171
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8577454430771023
============= xn0: 0.8170000000000001 =============
new_qn: 0.8153974444580258
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.858743481564811
============= xn0: 0.8180000000000001 =============
new_qn: 0.8163954829457345
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8597415200525196
============= xn0: 0.8190000000000001 =============
new_qn: 0.8173935214334432
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8607395585402282
============= xn0: 0.8200000000000001 =============
new_qn: 0.818391559921152
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8617375970279368
============= xn0: 0.8210000000000001 =============
new_qn: 0.8193895984088607
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.862735635515646
============= xn0: 0.8220000000000001 =============
new_qn: 0.8203876368965695
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8637336740033545
============= xn0: 0.8230000000000001 =============
new_qn: 0.8213856753842781
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8647317124910632
============= xn0: 0.8240000000000001 =============
new_qn: 0.8223837138719868
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8657297509787718
============= xn0: 0.8250000000000001 =============
new_qn: 0.8233817523596956
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8667277894664804
============= xn0: 0.8260000000000001 =============
new_qn: 0.8243797908474043
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8677258279541895
============= xn0: 0.8270000000000001 =============
new_qn: 0.8253778293351131
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8687238664418981
============= xn0: 0.8280000000000001 =============
new_qn: 0.8263758678228218
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8697219049296068
============= xn0: 0.8290000000000001 =============
new_qn: 0.8273739063105304
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8707199434173154
============= xn0: 0.8300000000000001 =============
new_qn: 0.8283719447982392
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.871717981905024
============= xn0: 0.8310000000000001 =============
new_qn: 0.8293699832859479
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.872716020392733
============= xn0: 0.8320000000000001 =============
new_qn: 0.8303680217736567
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8737140588804417
============= xn0: 0.833 =============
new_qn: 0.8313660602613653
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8747120973681504
============= xn0: 0.834 =============
new_qn: 0.832364098749074
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.875710135855859
============= xn0: 0.835 =============
new_qn: 0.8333621372367827
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8767081743435676
============= xn0: 0.836 =============
new_qn: 0.8343601757244914
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8777062128312763
============= xn0: 0.837 =============
new_qn: 0.8353582142122001
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8787042513189853
============= xn0: 0.838 =============
new_qn: 0.8363562526999089
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.879702289806694
============= xn0: 0.839 =============
new_qn: 0.8373542911876176
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8807003282944026
============= xn0: 0.84 =============
new_qn: 0.8383523296753264
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8816983667821112
============= xn0: 0.841 =============
new_qn: 0.839350368163035
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8826964052698199
============= xn0: 0.842 =============
new_qn: 0.8403484066507437
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.883694443757529
============= xn0: 0.843 =============
new_qn: 0.8413464451384525
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8846924822452376
============= xn0: 0.844 =============
new_qn: 0.8423444836261612
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8856905207329462
============= xn0: 0.845 =============
new_qn: 0.84334252211387
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8866885592206548
============= xn0: 0.846 =============
new_qn: 0.8443405606015787
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.887686597708364
============= xn0: 0.847 =============
new_qn: 0.8453385990892873
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8886846361960725
============= xn0: 0.848 =============
new_qn: 0.8463366375769961
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8896826746837811
============= xn0: 0.849 =============
new_qn: 0.8473346760647048
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8906807131714898
============= xn0: 0.85 =============
new_qn: 0.8483327145524135
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8916787516591984
============= xn0: 0.851 =============
new_qn: 0.8493307530401223
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8926767901469075
============= xn0: 0.852 =============
new_qn: 0.850328791527831
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8936748286346161
============= xn0: 0.853 =============
new_qn: 0.8513268300155397
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8946728671223247
============= xn0: 0.854 =============
new_qn: 0.8523248685032484
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8956709056100334
============= xn0: 0.855 =============
new_qn: 0.8533229069909571
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.896668944097742
============= xn0: 0.856 =============
new_qn: 0.8543209454786659
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.897666982585451
============= xn0: 0.857 =============
new_qn: 0.8553189839663746
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8986650210731597
============= xn0: 0.858 =============
new_qn: 0.8563170224540834
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.8996630595608683
============= xn0: 0.859 =============
new_qn: 0.857315060941792
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.900661098048577
============= xn0: 0.86 =============
new_qn: 0.8583130994295007
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9016591365362856
============= xn0: 0.861 =============
new_qn: 0.8593111379172095
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9026571750239947
============= xn0: 0.862 =============
new_qn: 0.8603091764049182
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9036552135117033
============= xn0: 0.863 =============
new_qn: 0.861307214892627
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.904653251999412
============= xn0: 0.864 =============
new_qn: 0.8623052533803357
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9056512904871206
============= xn0: 0.865 =============
new_qn: 0.8633032918680443
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9066493289748292
============= xn0: 0.866 =============
new_qn: 0.8643013303557531
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9076473674625383
============= xn0: 0.867 =============
new_qn: 0.8652993688434618
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.908645405950247
============= xn0: 0.868 =============
new_qn: 0.8662974073311706
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9096434444379555
============= xn0: 0.869 =============
new_qn: 0.8672954458188793
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9106414829256642
============= xn0: 0.87 =============
new_qn: 0.868293484306588
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9116395214133732
============= xn0: 0.871 =============
new_qn: 0.8692915227942967
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9126375599010819
============= xn0: 0.872 =============
new_qn: 0.8702895612820054
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9136355983887905
============= xn0: 0.873 =============
new_qn: 0.8712875997697141
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9146336368764991
============= xn0: 0.874 =============
new_qn: 0.8722856382574229
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9156316753642078
============= xn0: 0.875 =============
new_qn: 0.8732836767451316
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9166297138519168
============= xn0: 0.876 =============
new_qn: 0.8742817152328404
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9176277523396255
============= xn0: 0.877 =============
new_qn: 0.875279753720549
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.918625790827334
============= xn0: 0.878 =============
new_qn: 0.8762777922082577
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9196238293150427
============= xn0: 0.879 =============
new_qn: 0.8772758306959665
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9206218678027513
============= xn0: 0.88 =============
new_qn: 0.8782738691836752
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9216199062904604
============= xn0: 0.881 =============
new_qn: 0.879271907671384
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.922617944778169
============= xn0: 0.882 =============
new_qn: 0.8802699461590927
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9236159832658777
============= xn0: 0.883 =============
new_qn: 0.8812679846468013
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9246140217535863
============= xn0: 0.884 =============
new_qn: 0.8822660231345101
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.925612060241295
============= xn0: 0.885 =============
new_qn: 0.8832640616222188
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.926610098729004
============= xn0: 0.886 =============
new_qn: 0.8842621001099276
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9276081372167126
============= xn0: 0.887 =============
new_qn: 0.8852601385976363
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9286061757044213
============= xn0: 0.888 =============
new_qn: 0.886258177085345
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.92960421419213
============= xn0: 0.889 =============
new_qn: 0.8872562155730537
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9306022526798385
============= xn0: 0.89 =============
new_qn: 0.8882542540607624
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9316002911675476
============= xn0: 0.891 =============
new_qn: 0.8892522925484712
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9325983296552562
============= xn0: 0.892 =============
new_qn: 0.8902503310361799
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9335963681429649
============= xn0: 0.893 =============
new_qn: 0.8912483695238886
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9345944066306735
============= xn0: 0.894 =============
new_qn: 0.8922464080115974
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9355924451183826
============= xn0: 0.895 =============
new_qn: 0.893244446499306
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9365904836060912
============= xn0: 0.896 =============
new_qn: 0.8942424849870148
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9375885220937998
============= xn0: 0.897 =============
new_qn: 0.8952405234747235
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9385865605815085
============= xn0: 0.898 =============
new_qn: 0.8962385619624322
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9395845990692175
============= xn0: 0.899 =============
new_qn: 0.897236600450141
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9405826375569262
============= xn0: 0.9 =============
new_qn: 0.8982346389378497
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9415806760446348
============= xn0: 0.901 =============
new_qn: 0.8992326774255583
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9425787145323434
============= xn0: 0.902 =============
new_qn: 0.9002307159132671
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.943576753020052
============= xn0: 0.903 =============
new_qn: 0.9012287544009758
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9445747915077611
============= xn0: 0.904 =============
new_qn: 0.9022267928886846
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9455728299954698
============= xn0: 0.905 =============
new_qn: 0.9032248313763933
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9465708684831784
============= xn0: 0.906 =============
new_qn: 0.904222869864102
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.947568906970887
============= xn0: 0.907 =============
new_qn: 0.9052209083518107
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9485669454585957
============= xn0: 0.908 =============
new_qn: 0.9062189468395194
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9495649839463047
============= xn0: 0.909 =============
new_qn: 0.9072169853272282
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9505630224340134
============= xn0: 0.91 =============
new_qn: 0.9082150238149369
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.951561060921722
============= xn0: 0.911 =============
new_qn: 0.9092130623026456
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9525590994094306
============= xn0: 0.912 =============
new_qn: 0.9102111007903544
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9535571378971397
============= xn0: 0.913 =============
new_qn: 0.911209139278063
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9545551763848483
============= xn0: 0.914 =============
new_qn: 0.9122071777657718
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.955553214872557
============= xn0: 0.915 =============
new_qn: 0.9132052162534805
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9565512533602656
============= xn0: 0.916 =============
new_qn: 0.9142032547411892
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9575492918479742
============= xn0: 0.917 =============
new_qn: 0.915201293228898
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9585473303356833
============= xn0: 0.918 =============
new_qn: 0.9161993317166067
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.959545368823392
============= xn0: 0.919 =============
new_qn: 0.9171973702043154
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9605434073111005
============= xn0: 0.92 =============
new_qn: 0.9181954086920241
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9615414457988092
============= xn0: 0.921 =============
new_qn: 0.9191934471797328
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9625394842865178
============= xn0: 0.922 =============
new_qn: 0.9201914856674416
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9635375227742269
============= xn0: 0.923 =============
new_qn: 0.9211895241551503
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9645355612619355
============= xn0: 0.924 =============
new_qn: 0.9221875626428591
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9655335997496441
============= xn0: 0.925 =============
new_qn: 0.9231856011305677
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9665316382373528
============= xn0: 0.926 =============
new_qn: 0.9241836396182764
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9675296767250614
============= xn0: 0.927 =============
new_qn: 0.9251816781059852
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9685277152127705
============= xn0: 0.928 =============
new_qn: 0.9261797165936939
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.969525753700479
============= xn0: 0.929 =============
new_qn: 0.9271777550814027
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9705237921881877
============= xn0: 0.93 =============
new_qn: 0.9281757935691114
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9715218306758964
============= xn0: 0.931 =============
new_qn: 0.92917383205682
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.972519869163605
============= xn0: 0.932 =============
new_qn: 0.9301718705445288
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.973517907651314
============= xn0: 0.933 =============
new_qn: 0.9311699090322375
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9745159461390227
============= xn0: 0.934 =============
new_qn: 0.9321679475199462
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9755139846267313
============= xn0: 0.935 =============
new_qn: 0.933165986007655
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.97651202311444
============= xn0: 0.936 =============
new_qn: 0.9341640244953637
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.977510061602149
============= xn0: 0.937 =============
new_qn: 0.9351620629830724
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9785081000898577
============= xn0: 0.9380000000000001 =============
new_qn: 0.9361601014707811
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9795061385775663
============= xn0: 0.9390000000000001 =============
new_qn: 0.9371581399584898
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.980504177065275
============= xn0: 0.9400000000000001 =============
new_qn: 0.9381561784461986
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9815022155529836
============= xn0: 0.9410000000000001 =============
new_qn: 0.9391542169339073
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9825002540406926
============= xn0: 0.9420000000000001 =============
new_qn: 0.9401522554216161
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9834982925284013
============= xn0: 0.9430000000000001 =============
new_qn: 0.9411502939093247
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.98449633101611
============= xn0: 0.9440000000000001 =============
new_qn: 0.9421483323970334
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9854943695038185
============= xn0: 0.9450000000000001 =============
new_qn: 0.9431463708847422
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9864924079915272
============= xn0: 0.9460000000000001 =============
new_qn: 0.9441444093724509
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9874904464792362
============= xn0: 0.9470000000000001 =============
new_qn: 0.9451424478601597
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9884884849669449
============= xn0: 0.9480000000000001 =============
new_qn: 0.9461404863478684
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9894865234546535
============= xn0: 0.9490000000000001 =============
new_qn: 0.947138524835577
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9904845619423621
============= xn0: 0.9500000000000001 =============
new_qn: 0.9481365633232858
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9914826004300707
============= xn0: 0.9510000000000001 =============
new_qn: 0.9491346018109945
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9924806389177798
============= xn0: 0.9520000000000001 =============
new_qn: 0.9501326402987033
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9934786774054885
============= xn0: 0.9530000000000001 =============
new_qn: 0.951130678786412
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.994476715893197
============= xn0: 0.9540000000000001 =============
new_qn: 0.9521287172741207
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9954747543809057
============= xn0: 0.9550000000000001 =============
new_qn: 0.9531267557618294
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9964727928686143
============= xn0: 0.9560000000000001 =============
new_qn: 0.9541247942495381
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9974708313563234
============= xn0: 0.9570000000000001 =============
new_qn: 0.9551228327372469
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.998468869844032
============= xn0: 0.9580000000000001 =============
new_qn: 0.9561208712249556
best_Eta: 1.3580065196996762
sum(new_qn_list): 1.9994669083317407
============= xn0: 0.9590000000000001 =============
new_qn: 0.9571189097126643
best_Eta: 1.3580065196996762
sum(new_qn_list): 2.000464946819449
============= xn0: 0.96 =============
new_qn: 0.958116948200373
best_Eta: 1.3580065196996762
sum(new_qn_list): 2.0014629853071577
============= xn0: 0.961 =============
new_qn: 0.9591149866880816
best_Eta: 1.3580065196996762
sum(new_qn_list): 2.0024610237948663
============= xn0: 0.962 =============
new_qn: 0.9601130251757903
best_Eta: 1.3580065196996762
sum(new_qn_list): 2.0034590622825754
============= xn0: 0.963 =============
new_qn: 0.9611110636634991
best_Eta: 1.3580065196996762
sum(new_qn_list): 2.004457100770284
============= xn0: 0.964 =============
new_qn: 0.9621091021512078
best_Eta: 1.3580065196996762
sum(new_qn_list): 2.0054551392579927
============= xn0: 0.965 =============
new_qn: 0.9631071406389166
best_Eta: 1.3580065196996762
sum(new_qn_list): 2.0064531777457013
============= xn0: 0.966 =============
new_qn: 0.9641051791266253
best_Eta: 1.3580065196996762
sum(new_qn_list): 2.0074512162334104
============= xn0: 0.967 =============
new_qn: 0.9651032176143339
best_Eta: 1.3580065196996762
sum(new_qn_list): 2.008449254721119
============= xn0: 0.968 =============
new_qn: 0.9661012561020427
best_Eta: 1.3580065196996762
sum(new_qn_list): 2.0094472932088276
============= xn0: 0.969 =============
new_qn: 0.9670992945897514
best_Eta: 1.3580065196996762
sum(new_qn_list): 2.0104453316965363
============= xn0: 0.97 =============
new_qn: 0.9680973330774602
best_Eta: 1.3580065196996762
sum(new_qn_list): 2.011443370184245
============= xn0: 0.971 =============
new_qn: 0.9690953715651689
best_Eta: 1.3580065196996762
sum(new_qn_list): 2.012441408671954
============= xn0: 0.972 =============
new_qn: 0.9700934100528776
best_Eta: 1.3580065196996762
sum(new_qn_list): 2.0134394471596626
============= xn0: 0.973 =============
new_qn: 0.9710914485405863
best_Eta: 1.3580065196996762
sum(new_qn_list): 2.0144374856473712
============= xn0: 0.974 =============
new_qn: 0.972089487028295
best_Eta: 1.3580065196996762
sum(new_qn_list): 2.01543552413508
============= xn0: 0.975 =============
new_qn: 0.9730875255160037
best_Eta: 1.3580065196996762
sum(new_qn_list): 2.0164335626227885
============= xn0: 0.976 =============
new_qn: 0.9740855640037125
best_Eta: 1.3580065196996762
sum(new_qn_list): 2.0174316011104976
============= xn0: 0.977 =============
new_qn: 0.9750836024914212
best_Eta: 1.3580065196996762
sum(new_qn_list): 2.018429639598206
============= xn0: 0.978 =============
new_qn: 0.97608164097913
best_Eta: 1.3580065196996762
sum(new_qn_list): 2.019427678085915
============= xn0: 0.979 =============
new_qn: 0.9770796794668386
best_Eta: 1.3580065196996762
sum(new_qn_list): 2.0204257165736235
============= xn0: 0.98 =============
new_qn: 0.9780777179545473
best_Eta: 1.3580065196996762
sum(new_qn_list): 2.021423755061332
============= xn0: 0.981 =============
new_qn: 0.9790757564422561
best_Eta: 1.3580065196996762
sum(new_qn_list): 2.022421793549041
============= xn0: 0.982 =============
new_qn: 0.9800737949299648
best_Eta: 1.3580065196996762
sum(new_qn_list): 2.02341983203675
============= xn0: 0.983 =============
new_qn: 0.9810718334176736
best_Eta: 1.3580065196996762
sum(new_qn_list): 2.0244178705244584
============= xn0: 0.984 =============
new_qn: 0.9820698719053823
best_Eta: 1.3580065196996762
sum(new_qn_list): 2.025415909012167
============= xn0: 0.985 =============
new_qn: 0.9830679103930909
best_Eta: 1.3580065196996762
sum(new_qn_list): 2.0264139474998757
============= xn0: 0.986 =============
new_qn: 0.9840659488807997
best_Eta: 1.3580065196996762
sum(new_qn_list): 2.0274119859875848
============= xn0: 0.987 =============
new_qn: 0.9850639873685084
best_Eta: 1.3580065196996762
sum(new_qn_list): 2.0284100244752934
============= xn0: 0.988 =============
new_qn: 0.9860620258562172
best_Eta: 1.3580065196996762
sum(new_qn_list): 2.029408062963002
============= xn0: 0.989 =============
new_qn: 0.9870600643439259
best_Eta: 1.3580065196996762
sum(new_qn_list): 2.0304061014507107
============= xn0: 0.99 =============
new_qn: 0.9880581028316346
best_Eta: 1.3580065196996762
sum(new_qn_list): 2.0314041399384197
============= xn0: 0.991 =============
new_qn: 0.9890561413193433
best_Eta: 1.3580065196996762
sum(new_qn_list): 2.0324021784261284
============= xn0: 0.992 =============
new_qn: 0.990054179807052
best_Eta: 1.3580065196996762
sum(new_qn_list): 2.033400216913837
============= xn0: 0.993 =============
new_qn: 0.9910522182947608
best_Eta: 1.3580065196996762
sum(new_qn_list): 2.0343982554015456
============= xn0: 0.994 =============
new_qn: 0.9920502567824695
best_Eta: 1.3580065196996762
sum(new_qn_list): 2.0353962938892542
============= xn0: 0.995 =============
new_qn: 0.9930482952701782
best_Eta: 1.3580065196996762
sum(new_qn_list): 2.0363943323769633
============= xn0: 0.996 =============
new_qn: 0.994046333757887
best_Eta: 1.3580065196996762
sum(new_qn_list): 2.037392370864672
============= xn0: 0.997 =============
new_qn: 0.9950443722455956
best_Eta: 1.3580065196996762
sum(new_qn_list): 2.0383904093523806
============= xn0: 0.998 =============
new_qn: 0.9960424107333044
best_Eta: 1.3580065196996762
sum(new_qn_list): 2.039388447840089
============= xn0: 0.999 =============
new_qn: 0.9970404492210131
best_Eta: 1.3580065196996762
sum(new_qn_list): 2.040386486327798
============= xn0: 1.0 =============
new_qn: 0.9980384877087218
best_Eta: 1.3580065196996762
sum(new_qn_list): 2.041384524815507
DONE
最终的列表：
[0.0, 0.00029779332778626555, 0.0005931085239201783, 0.000885952679606527, 0.0011763328590204515, 0.0014642560994361127, 0.0017497294113546164, 0.0020327597786312128, 0.002313354158601783, 0.0025915194822085843, 0.002867262654125291, 0.0031405905528813314, 0.0034115100309855158, 0.003680027915048952, 0.003946151005907275, 0.0042098860787422, 0.0044712398832023464, 0.004730219143523423, 0.004986830558647716, 0.005241080802342899, 0.005492976523320186, 0.005742524345351798, 0.005989730867387806, 0.006234602663672293, 0.006477146283858846, 0.0067173682531254456, 0.006955275072288679, 0.007190873217917334, 0.007424169142445331, 0.007655169274284033, 0.007883880017934004, 0.008110307754095963, 0.008334458839781358, 0.008556339608422121, 0.008775956369979888, 0.008993315411054692, 0.009208422994992894, 0.009421285361994686, 0.009631908729220796, 0.009840299290898864, 0.010046463218428961, 0.010250406660488716, 0.010452135743137796, 0.010651656569921772, 0.010848975221975497, 0.011044097758125815, 0.011237030214993872, 0.011427778607096567, 0.011616348926947857, 0.011802747145159068, 0.011986979210539005, 0.012169051050193405, 0.01234896856962367, 0.012526737652825352, 0.012702364162385897, 0.012875853939581954, 0.013047212804476092, 0.013216446556013, 0.013383560972115259, 0.013548561809778419, 0.01371145480516571, 0.013872245673702197, 0.014030940110168394, 0.01418754378879343, 0.014342062363347563, 0.014494501467234452, 0.014644866713582672, 0.014793163695336883, 0.014939397985348546, 0.015083575136465868, 0.015225700681623752, 0.015365780133932777, 0.015503818986767906, 0.015639822713856902, 0.015773796769367868, 0.015905746587996664, 0.016035677585053698, 0.016163595156550248, 0.016289504679284444, 0.016413411510926595, 0.01653532099010431, 0.01665523843648682, 0.016773169150869335, 0.016889118415256407, 0.017003091492945252, 0.017115093628608566, 0.017225130048376655, 0.01733320595991944, 0.01743932655252789, 0.017543496997194888, 0.017645722446696058, 0.017746008035669697, 0.017844358880696604, 0.01794078008037936, 0.018035276715421253, 0.018127853848704786, 0.018218516525369627, 0.018307269772890378, 0.018394118601153753, 0.018479068002535473, 0.018562122951976706, 0.018643288407060032, 0.01872256930808519, 0.018799970578144184, 0.018875497123196416, 0.018949153832142754, 0.019020945576899914, 0.019090877212474086, 0.01915895357703419, 0.01922517949198478, 0.019289559762038708, 0.019352099175289197, 0.019412802503281604, 0.01947167450108496, 0.019528719907363004, 0.019583943444444793, 0.019637349818395086, 0.01968894371908432, 0.019738729820258052, 0.019786712779606544, 0.019832897238833302, 0.019877287823723666, 0.019919889144213193, 0.019960705794455275, 0.019999742352888705, 0.020037003382304852, 0.020072493429914268, 0.020106217027413442, 0.0201381786910505, 0.020168382921691458, 0.020196834204885272, 0.02022353701092902, 0.020248495794932847, 0.020271714996884183, 0.020293199041712034, 0.0203129523393506, 0.020330979284802986, 0.020347284258203924, 0.02036187162488301, 0.020374745735427108, 0.020385910925742268, 0.020395371517115984, 0.020403131816278508, 0.0204091961154641, 0.0204135686924721, 0.020416253810727425, 0.020417255719341, 0.020416578653169648, 0.020414226832875998, 0.020410204464987786, 0.02040451574195684, 0.02039716484221832, 0.02038815593024873, 0.0203774931566246, 0.020365180658080156, 0.020351222557565024, 0.020335622964301775, 0.020318385973842723, 0.02029951566812699, 0.02027901611553687, 0.020256891370954044, 0.020233145475815673, 0.02020778245816987, 0.020180806332731394, 0.02015222110093637, 0.02012203075099761, 0.02009023925795886, 0.02005685058374901, 0.020021868677236626, 0.019985297474283265, 0.019947140897797144, 0.01990740285778647, 0.01986608725141223, 0.01982319796304105, 0.01977873886429765, 0.019732713814117064, 0.01968512665879646, 0.019635981232046906, 0.019585281355045003, 0.01953303083648361, 0.019479233472623342, 0.019423893047342794, 0.019367013332189237, 0.019308598086428735, 0.019248651057096028, 0.01918717597904443, 0.019124176574995017, 0.01905965655558614, 0.018993619619422225, 0.018926069453122474, 0.018857009731369556, 0.01878644411695765, 0.018714376260840854, 0.018640809802180414, 0.018565748368392948, 0.01848919557519732, 0.01841115502666177, 0.01833163031525123, 0.018250625021873468, 0.0181681427159259, 0.018084186955341558, 0.017998761286635162, 0.01791186924494892, 0.01782351435409807, 0.017733700126616292, 0.017642430063800424, 0.01754970765575603, 0.01745553638144151, 0.017359919708712673, 0.01726286109436731, 0.017164363984188885, 0.01706443181299047, 0.016963068004658494, 0.016860275972195887, 0.016756059117765676, 0.016650420832733576, 0.01654336449771121, 0.016434893482598134, 0.016325011146624824, 0.016213720838394674, 0.01610102589592563, 0.01598692964669257, 0.0158714354076685, 0.015754546485366006, 0.0156362661758786, 0.015516597764921536, 0.01539554452787284, 0.015273109729813883, 0.015149296625569408, 0.015024108459748525, 0.014897548466783983, 0.014769619870972778, 0.01464032588651501, 0.014509669717554102, 0.014377654558215797, 0.014244283592647067, 0.01410955999505556, 0.013973486929747786, 0.013836067551168202, 0.013697305003937316, 0.013557202422889825, 0.013415762933112807, 0.013272989649983746, 0.013128885679207725, 0.01298345411685553, 0.012836698049400463, 0.012688620553755858, 0.012539224697311668, 0.01238851353797188, 0.012236490124190702, 0.012083157495008878, 0.01192851868009076, 0.011772576699759396, 0.011615334565033608, 0.011456795277662801, 0.011296961830163321, 0.011135837205853483, 0.010973424378889152, 0.010809726314298662, 0.010644745968018232, 0.010478486286926214, 0.010310950208877956, 0.010142140662740717, 0.009972060568427588, 0.009800712836931458, 0.009628100370359993, 0.009454226061968218, 0.009279092796193489, 0.009102703448688187, 0.00892506088635353, 0.008746167967372709, 0.008566027541244137, 0.008384642448814317, 0.008202015522310424, 0.008018149585373502, 0.007833047453090602, 0.007646711932027039, 0.007459145820259083, 0.007270351907405492, 0.007080332974659931, 0.006889091794822277, 0.00669663113233071, 0.006502953743292794, 0.006308062375517287, 0.006111959768545061, 0.00591464865367991, 0.005716131754020248, 0.005516411784489195, 0.0053154914518653285, 0.005113373454813885, 0.004910060483916179, 0.004705555221700297, 0.0044998603426714134, 0.004292978513341372, 0.004084912392258722, 0.0038756646300385222, 0.0036652378693915466, 0.003453634745154144, 0.0032408578843173275, 0.00302690990605603, 0.0028117934217579688, 0.0025955110350528443, 0.0023780653418410402, 0.002159458930321878, 0.0019396943810231493, 0.0017187742668283157, 0.0014967011530055419, 0.0012734775972357837, 0.0010491061496404885, 0.0008235893528099614, 0.0005969297418306208, 0.000369129844313254, 0.0001401921804199957, -8.988073710802791e-05, -0.00032108640292305424, -0.0005534223190443166, -0.0007868859948300666, -0.0010214749469513729, -0.0012571866993650316, -0.0014940187832865326, -0.0017319687371641912, -0.0019710341066521697, -0.0022112124445839987, -0.0024525013109469307, -0.002694898272855295, -0.002938400904524796, -0.0031830067872462564, -0.0034287135093604704, -0.0036755186662317807, -0.003923419860223265, -0.004172414700670646, -0.0044225008038573654, -0.004673675792989218, -0.004925937298169425, -0.005179282956373155, -0.005433710411423265, -0.005689217313965045, -0.005945801321441846, -0.006203460098070046, -0.006462191314815402, -0.006721992649367625, -0.006982861786117234, -0.007244796416130628, -0.007507794237126164, -0.00777185295345062, -0.008036970276054989, -0.008303143922470724, -0.00857037161678631, -0.008838651089623728, -0.009107980078114863, -0.00937835632587819, -0.009649777582995955, -0.0099222416059902, -0.010195746157800722, -0.010470289007761535, -0.010745867931577946, -0.011022480711304405, -0.011300125135321248, -0.0115787989983126, -0.011858500101243674, -0.012139226251338675, -0.012420975262058098, -0.012703744953076967, -0.012987533150262964, -0.013272337685653723, -0.013558156397435683, -0.013844987129921937, -0.014132827733530473, -0.014421676064763023, -0.014711529986183025, -0.015002387366394587, -0.015294246080021112, -0.015587104007683539, -0.015880959035980025, -0.016175809057464574, -0.016471651970625778, -0.016768485679866107, -0.01706630809548132, -0.017365117133639585, -0.01766491071636106, -0.017965686771496958, -0.018267443232709346, -0.018570178039451102, -0.018873889136944932, -0.019178574476163945, -0.01948423201381122, -0.019790859712299214, -0.020098455539731108, -0.020407017469879885, -0.020716543482168892, -0.02102703156165242, -0.02133847969899566, -0.021650885890455385, -0.021964248137860687, -0.02227856444859333, -0.022593832835568817, -0.022910051317216518, -0.023227217917461462, -0.023545330665704856, -0.02386438759680476, -0.024184386751058107, -0.02450532617418072, -0.024827203917289764, -0.02515001803688449, -0.02547376659482764, -0.025798447658327228, -0.026124059299917735, -0.026450599597442226, -0.026778066634033815, -0.02710645849809762, -0.02743577328329244, -0.027766009088512844, -0.02809716401787138, -0.028429236180680273, -0.028762223691434163, -0.02909612466979178, -0.02943093724055862, -0.02976665953366936, -0.030103289684170254, -0.030440825832201535, -0.03077926612298043, -0.03111860870678329, -0.03145885173892876, -0.031799993379760694, -0.03214203179463071, -0.032484965153881384, -0.032828791632829035, -0.03317350941174718, -0.03351911667584967, -0.03386561161527374, -0.03421299242506315, -0.03456125730515214, -0.03491040446034849, -0.03526043210031721, -0.03561133843956382, -0.03596312169741822, -0.03631578009801861, -0.03666931187029476, -0.03702371524795245, -0.03737898846945714, -0.037735129778017296, -0.03809213742156964, -0.03845000965276257, -0.03880874472894019, -0.03916834091212701, -0.03952879646901181, -0.03989010967093215, -0.04025227879385912, -0.040615302118381436, -0.040979177929690025, -0.0413439045175627, -0.04170948017634879, -0.042075903204954246, -0.04244317190682595, -0.04281128458993644, -0.04318023956676964, -0.043550035154305, -0.0439206696740031, -0.044292141451790334, -0.044664448818044156, -0.045037590107578684, -0.045411563659629395, -0.0457863678178389, -0.046162000930242364, -0.0465384613492521, -0.046915747431644506, -0.04729385753854448, -0.04767279003541136, -0.04805254329202513, -0.048433115682471106, -0.04881450558512662, -0.049196711382646585, -0.049579731461949494, -0.04996356421420306, -0.05034820803481049, -0.05073366132339646, -0.051119922483793545, -0.05150698992402797, -0.051894862056305835, -0.05228353729700008, -0.052673014066635715, -0.05306329078987759, -0.05345436589551539, -0.05384623781645148, -0.05423890498968609, -0.054632365856305376, -0.05502661886146687, -0.055421662454387066, -0.05581749508832762, -0.05621411522058223, -0.056611521312464, -0.0570097118292916, -0.05740868524037701, -0.05780844001901181, -0.058208974642454836, -0.058610287591919, -0.059012377352558654, -0.05941524241345664, -0.05981888126761142, -0.06022329241192487, -0.060628474347189265, -0.06103442557807487, -0.06144114461311745, -0.06184862996470536, -0.06225688014906777, -0.0626658936862618, -0.06307566910016038, -0.0634862049184402, -0.06389749967256853, -0.0643095518977922, -0.06472236013312488, -0.06513592292133497, -0.06555023880893351, -0.06596530634616227, -0.06638112408698199, -0.0667976905890601, -0.06721500441375916, -0.06763306412612469, -0.06805186829487353, -0.06847141549238245, -0.06889170429467595, -0.06931273328141513, -0.06973450103588547, -0.07015700614498549, -0.07058024719921574, -0.07100422279266683, -0.07142893152300817, -0.07185437199147626, -0.07228054280286395, -0.07270744256550854, -0.07313506989128105, -0.07356342339557498, -0.07399250169729432, -0.07442230341884348, -0.07485282718611597, -0.07528407162848277, -0.07571603537878197, -0.07614871707330739, -0.0765821153517981, -0.0770162288574272, -0.07745105623679127, -0.07788659613989918, -0.07832284722016142, -0.07875980813438, -0.07919747754273704, -0.07963585410878465, -0.08007493649943387, -0.08051472338494431, -0.08095521343891404, -0.08139640533826858, -0.08183829776325091, -0.0822808893974104, -0.08272417892759332, -0.08316816504393182, -0.08361284643983424, -0.08405822181197459, -0.08450428986028191, -0.08495104928793096, -0.08539849880133177, -0.08584663711011908, -0.08629546292714324, -0.08674497496845901, -0.08719517195331672, -0.0876460526041517, -0.08809761564657481, -0.08854985980936181, -0.08900278382444432, -0.08945638642689974, -0.08991066635494155, -0.09036562234990975, -0.0908212531562606, -0.09127755752155742, -0.09173453419646127, -0.09219218193472078, -0.09265049949316329, -0.09310948563168425, -0.09356913911323905, -0.09402945870383272, -0.09449044317251093, -0.09495209129135052, -0.09541440183545025, -0.09587737358292103, -0.09634100531487771, -0.09680529581542852, -0.0972702438716671, -0.09773584827366277, -0.09820210781445105, -0.09866902129002542, -0.0991365874993278, -0.09960480524423926, -0.10007367332957179, -0.10054319056305827, -0.10101335575534481, -0.10148416771998081, -0.10195562527341073, -0.10242772723496507, -0.10290047242685119, -0.10337385967414542, -0.10384788780478355, -0.10432255564955256, -0.10479786204208164, -0.10527380581883383, -0.10575038581909735, -0.10622760088497696, -0.10670544986138569, -0.10718393159603562, -0.10766304493943046, -0.10814278874485589, -0.10862316186837245, -0.10910416316880622, -0.1095857915077405, -0.11006804574950813, -0.11055092476118267, -0.11103442741256997, -0.1115185525762008, -0.11200329912732143, -0.11248866594388673, -0.11297465190655076, -0.11346125589865996, -0.11394847680624398, -0.11443631351800776, -0.11492476492532455, -0.11541382992222649, -0.11590350740539779, -0.11639379627416568, -0.11688469543049362, -0.11737620377897262, -0.11786832022681382, -0.1183610436838407, -0.11885437306248048, -0.11934830727775758, -0.11984284524728461, -0.120337985891256, -0.12083372813243942, -0.12133007089616765, -0.1218270131103324, -0.12232455370537565, -0.12282269161428194, -0.12332142577257199, -0.12382075511829349, -0.12432067859201501, -0.12482119513681811, -0.12532230369828945, -0.12582400322451393, -0.12632629266606676, -0.12682917097600688, -0.12733263710986875, -0.1278366900256559, -0.12834132868383263, -0.12884655204731765, -0.12935235908147635, -0.12985874875411374, -0.13036572003546754, -0.13087327189820008, -0.13138140331739245, -0.13189011327053635, -0.13239940073752765, -0.1329092647006591, -0.1334197041446129, -0.1339307180564544, -0.13444230542562507, -0.1349544652439344, -0.13546719650555483, -0.13598049820701286, -0.13649436934718373, -0.13700880892728395, -0.13752381595086416, -0.138039389423803, -0.1385555283542993, -0.13907223175286654, -0.13958949863232528, -0.1401073280077968, -0.14062571889669606, -0.14114467031872524, -0.14166418129586722, -0.14218425085237885, -0.14270487801478426, -0.14322606181186814, -0.14374780127466968, -0.1442700954364754, -0.14479294333281312, -0.14531634400144555, -0.14584029648236307, -0.14636479981777806, -0.14688985305211855, -0.14741545523202082, -0.14794160540632417, -0.1484683026260637, -0.14899554594446474, -0.14952333441693588, -0.1500516671010632, -0.1505805430566034, -0.15110996134547805, -0.15163992103176727, -0.15217042118170343, -0.1527014608636651, -0.15323303914817044, -0.15376515510787192, -0.15429780781754932, -0.15483099635410402, -0.1553647197965532, -0.15589897722602308, -0.1564337677257438, -0.15696909038104212, -0.15750494427933703, -0.15804132851013253, -0.15857824216501226, -0.159115684337633, -0.15965365412371968, -0.16019215062105863, -0.16073117292949235, -0.16127072015091304, -0.16181079138925702, -0.16235138575049923, -0.1628925023426473, -0.16343414027573522, -0.1639762986618185, -0.1645189766149674, -0.16506217325126227, -0.16560588768878726, -0.16615011904762478, -0.16669486644984943, -0.16724012901952312, -0.16778590588268905, -0.16833219616736605, -0.1688789990035432, -0.16942631352317394, -0.16997413886017076, -0.1705224741503999, -0.17107131853167556, -0.17162067114375446, -0.1721705311283298, -0.17272089762902743, -0.17327176979139847, -0.17382314676291521, -0.17437502769296553, -0.17492741173284654, -0.1754802980357607, -0.17603368575680955, -0.17658757405298853, -0.17714196208318178, -0.17769684900815674, -0.17825223399055923, -0.17880811619490744, -0.17936449478758776, -0.17992136893684874, -0.18047873781279578, -0.18103660058738702, -0.18159495643442702, -0.1821538045295621, -0.18271314405027528, -0.18327297417588084, -0.18383329408751992, -0.18439410296815417, -0.18495540000256272, -0.18551718437733433, -0.18607945528086545, -0.18664221190335273, -0.18720545343678918, -0.18776917907495938, -0.1883333880134337, -0.18889807944956383, -0.18946325258247831, -0.1900289066130766, -0.1905950407440251, -0.19116165417975128, -0.19172874612644009, -0.19229631579202833, -0.19286436238619964, -0.19343288512038015, -0.19400188320773348, -0.1945713558631561, -0.1951413023032723, -0.19571172174642992, -0.1962826134126946, -0.19685397652384662, -0.19742581030337458, -0.19799811397647193, -0.19857088677003154, -0.19914412791264158, -0.1997178366345802, -0.200292012167812, -0.2008666537459819, -0.2014417606044122, -0.2020173319800963, -0.20259336711169607, -0.20316986523953529, -0.20374682560559654, -0.20432424745351618, -0.2049021300285797, -0.2054804725777175, -0.2060592743495001, -0.2066385345941345, -0.20721825256345827, -0.20779842751093625, -0.20837905869165618, -0.20896014536232366, -0.20954168678125806, -0.21012368220838806, -0.21070613090524748, -0.21128903213497086, -0.21187238516228868, -0.21245618925352416, -0.21304044367658737, -0.21362514770097252, -0.21421030059775248, -0.2147959016395753, -0.21538195010065941, -0.21596844525678982, -0.21655538638531324, -0.21714277276513516, -0.21773060367671415, -0.21831887840205866, -0.2189075962247221, -0.2194967564297997, -0.22008635830392353, -0.22067640113525866, -0.22126688421349894, -0.2218578068298631, -0.22244916827709038, -0.22304096784943672, -0.2236332048426709, -0.22422587855406928, -0.22481898828241376, -0.22541253332798572, -0.22600651299256358, -0.226600926579418, -0.22719577339330788, -0.22779105274047662, -0.22838676392864854, -0.22898290626702378, -0.22957947906627574, -0.23017648163854576, -0.230773913297441, -0.2313717733580285, -0.23197006113683305, -0.232568775951832, -0.23316791712245244, -0.23376748396956626, -0.23436747581548756, -0.23496789198396784, -0.2355687318001929, -0.23616999459077814, -0.23677167968376578, -0.23737378640862028, -0.23797631409622544, -0.23857926207887958, -0.2391826296902927, -0.23978641626558206, -0.2403906211412693, -0.2409952436552758, -0.24160028314692006, -0.24220573895691244, -0.2428116104273531, -0.24341789690172766, -0.2440245977249036, -0.24463171224312652, -0.2452392398040163, -0.24584717975656434, -0.2464555314511292, -0.2470642942394331, -0.24767346747455876, -0.24828305051094524, -0.24889304270438473, -0.24950344341201935, -0.25011425199233694, -0.2507254678051678, -0.25133709021168116, -0.25194911857438196, -0.2525615522571071, -0.2531743906250221, -0.25378763304461716, -0.25440127888370434, -0.25501532751141376, -0.25562977829819067, -0.25624463061579106, -0.2568598838372792, -0.25747553733702344, -0.25809159049069375, -0.25870804267525727, -0.25932489326897623, -0.25994214165140284, -0.2605597872033777, -0.2611778293070256, -0.2617962673457521, -0.2624151007042408, -0.2630343287684488, -0.2636539509256052, -0.2642739665642063, -0.26489437507401337, -0.2655151758460482, -0.26613636827259113, -0.26675795174717676, -0.2673799256645918, -0.26800228942087057, -0.2686250424132929, -0.26924818404037976, -0.26987171370189167, -0.270495630798824, -0.27111993473340446, -0.2717446249090901, -0.27236970073056366, -0.2729951616037306, -0.27362100693571667, -0.27424723613486324, -0.2748738486107255, -0.27550084377406936, -0.2761282210368672, -0.27675597981229616, -0.27738411951473385, -0.2780126395597562, -0.27864153936413383, -0.2792708183458297, -0.2799004759239948, -0.28053051151896635, -0.28116092455226427, -0.28179171444658835, -0.2824228806258148, -0.2830544225149937, -0.28368633954034594, -0.28431863112926004, -0.2849512967102893, -0.28558433571314934, -0.28621774756871365, -0.2868515317090128, -0.2874856875672295, -0.2881202145776971, -0.28875511217589567, -0.28939037979844995, -0.2900260168831257, -0.29066202286882725, -0.2912983971955949, -0.29193513930460124, -0.29257224863814857, -0.29320972463966666, -0.29384756675370916, -0.29448577442595136, -0.2951243471031866, -0.2957632842333239, -0.29640258526538554, -0.29704224964950376, -0.29768227683691795, -0.29832266627997184, -0.2989634174321112, -0.2996045297478803, -0.3002460026829201, -0.3008878356939647, -0.3015300282388389, -0.3021725797764553, -0.30281548976681194, -0.3034587576709892, -0.30410238295114767, -0.3047463650705243, -0.3053907034934308, -0.3060353976852508, -0.3066804471124366, -0.307325851242507, -0.30797160954404457, -0.30861772148669264, -0.30926418654115306, -0.3099110041791836, -0.3105581738735952, -0.3112056950982489, -0.3118535673280537, -0.31250179003896417, -0.31315036270797725, -0.31379928481313046, -0.31444855583349796, -0.31509817524918937, -0.31574814254134687, -0.316398457192142, -0.3170491186847738, -0.3177001265034657, -0.3183514801334636, -0.3190031790610328, -0.3196552227734558, -0.3203076107590299, -0.32096034250706396, -0.32161341750787675, -0.3222668352527942, -0.3229205952341466, -0.3235746969452664, -0.32422913988048585, -0.32488392353513407, -0.3255390474055353, -0.32619451098900587, -0.3268503137838519, -0.3275064552893665, -0.3281629350058287, -0.32881975243449924, -0.3294769070776197, -0.3301343984384084, -0.33079222602105984, -0.3314503893307411, -0.33210888787359005, -0.3327677211567127, -0.33342688868818016, -0.3340863899770279, -0.3347462245332521, -0.33540639186780763, -0.3360668914926057]
**** log-parameter_analysis 运行时间： 2025-02-11 14:16:05 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
**** log-parameter_analysis 运行时间： 2025-02-11 14:17:39 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/initial/mnist_cnn_initial_model
Model saved to D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/mnist_cnn_model
初始化模型的准确率：
Accuracy: 26.82%
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.08431936488018583
DataOwner2: noise random: 0.08401740002843354
DataOwner3: noise random: 0.032250737156280664
DataOwner4: noise random: 0.01639410537183498
DataOwner5: noise random: 0.038273079025064605
DataOwner6: noise random: 0.048707733725744565
DataOwner7: noise random: 0.06329195783810276
DataOwner8: noise random: 0.0918426858298904
DataOwner9: noise random: 0.006170893430875935
DataOwner10: noise random: 0.07099972814992671
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9971281519595534, 0.9971296924107541, 0.9995796000308153, 0.9998912842522266, 0.9994081384876942, 0.9990414254847686, 0.9983807957566089, 0.9965809370990871, 0.9999845840892667, 0.9979582333652912]
归一化后的数据质量列表avg_f_list: [0.9160773094872979, 0.9161225683289215, 0.9881014670551926, 0.997258827448637, 0.9830638840268763, 0.9722897642669943, 0.9528802975959261, 0.9, 1.0, 0.9404653088342577]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3369
DataOwner1的最优x_1 = 0.0771
DataOwner2的最优x_2 = 0.0771
DataOwner3的最优x_3 = 0.1511
DataOwner4的最优x_4 = 0.1589
DataOwner5的最优x_5 = 0.1466
DataOwner6的最优x_6 = 0.1368
DataOwner7的最优x_7 = 0.1178
DataOwner8的最优x_8 = 0.0570
DataOwner9的最优x_9 = 0.1612
DataOwner10的最优x_10 = 0.1048
每个DataOwner应该贡献数据比例 xn_list = [0.07708637870410784, 0.07714074996661338, 0.1510556915601233, 0.15887860408076251, 0.1466158509773341, 0.136782374865988, 0.11783915812378791, 0.05704920697533572, 0.16115974515578374, 0.10483584610609639]
ModelOwner的最大效用 U(Eta) = 0.5761
DONE
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.336937519616539
DataOwner1的分配到的支付 ： 0.0821
DataOwner2的分配到的支付 ： 0.0822
DataOwner3的分配到的支付 ： 0.1736
DataOwner4的分配到的支付 ： 0.1843
DataOwner5的分配到的支付 ： 0.1676
DataOwner6的分配到的支付 ： 0.1547
DataOwner7的分配到的支付 ： 0.1306
DataOwner8的分配到的支付 ： 0.0597
DataOwner9的分配到的支付 ： 0.1874
DataOwner10的分配到的支付 ： 0.1147
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC1', 'DataOwner2': 'CPC2', 'DataOwner3': 'CPC3', 'DataOwner4': 'CPC4', 'DataOwner5': 'CPC5', 'DataOwner6': 'CPC6', 'DataOwner7': 'CPC7', 'DataOwner8': 'CPC8', 'DataOwner9': 'CPC9', 'DataOwner10': 'CPC10'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC2
DataOwner3 把数据交给 CPC3
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC8
DataOwner9 把数据交给 CPC9
DataOwner10 把数据交给 CPC10
DONE
----- literation 1: 模型训练 -----
CPC1调整模型中, 本轮训练的数据量为：92.00 :
**** log-parameter_analysis 运行时间： 2025-02-11 14:18:21 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/initial/mnist_cnn_initial_model
Model saved to D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/mnist_cnn_model
初始化模型的准确率：
Accuracy: 26.82%
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.06608683686781604
DataOwner2: noise random: 0.03666728255183157
**** log-parameter_analysis 运行时间： 2025-02-11 14:18:36 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/initial/mnist_cnn_initial_model
Model saved to D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/mnist_cnn_model
初始化模型的准确率：
Accuracy: 26.82%
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.023162026125638537
DataOwner2: noise random: 0.06535129569741827
DataOwner3: noise random: 0.021678735486713643
DataOwner4: noise random: 0.07461438876539689
DataOwner5: noise random: 0.05779531800843273
DataOwner6: noise random: 0.014846137244271607
DataOwner7: noise random: 0.0858389411607049
DataOwner8: noise random: 0.0794961371630708
DataOwner9: noise random: 0.020274322369488496
DataOwner10: noise random: 0.03698860313025797
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9997827086231089, 0.9982813258432192, 0.9998097879242844, 0.9977597736126356, 0.9986514423523972, 0.9999109225684436, 0.9970224503783762, 0.9974404449357952, 0.9998335316068853, 0.9994462334321582]
归一化后的数据质量列表avg_f_list: [0.9955611847060333, 0.9435827448563238, 0.9964986803574933, 0.9255264093175204, 0.9563963184282207, 1.0, 0.9, 0.9144711297154371, 0.9973206956319515, 0.9839122862985034]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3425
DataOwner1的最优x_1 = 0.1540
DataOwner2的最优x_2 = 0.1038
DataOwner3的最优x_3 = 0.1548
DataOwner4的最优x_4 = 0.0834
DataOwner5的最优x_5 = 0.1172
DataOwner6的最优x_6 = 0.1578
DataOwner7的最优x_7 = 0.0516
DataOwner8的最优x_8 = 0.0701
DataOwner9的最优x_9 = 0.1555
DataOwner10的最优x_10 = 0.1437
每个DataOwner应该贡献数据比例 xn_list = [0.15403248954433976, 0.10375948464742055, 0.1548377885337772, 0.08339941652249383, 0.11724535971933271, 0.15781633807784015, 0.051646485297269513, 0.07009522974909047, 0.15554117666180867, 0.1437452368503758]
ModelOwner的最大效用 U(Eta) = 0.5826
DONE
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3424992111851823
DataOwner1的分配到的支付 ： 0.1775
DataOwner2的分配到的支付 ： 0.1133
DataOwner3的分配到的支付 ： 0.1786
DataOwner4的分配到的支付 ： 0.0893
DataOwner5的分配到的支付 ： 0.1298
DataOwner6的分配到的支付 ： 0.1827
DataOwner7的分配到的支付 ： 0.0538
DataOwner8的分配到的支付 ： 0.0742
DataOwner9的分配到的支付 ： 0.1796
DataOwner10的分配到的支付 ： 0.1637
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC1', 'DataOwner2': 'CPC2', 'DataOwner3': 'CPC3', 'DataOwner4': 'CPC4', 'DataOwner5': 'CPC5', 'DataOwner6': 'CPC6', 'DataOwner7': 'CPC7', 'DataOwner8': 'CPC8', 'DataOwner9': 'CPC9', 'DataOwner10': 'CPC10'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC2
DataOwner3 把数据交给 CPC3
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC8
DataOwner9 把数据交给 CPC9
DataOwner10 把数据交给 CPC10
DONE
----- literation 1: 模型训练 -----
CPC1调整模型中, 本轮训练的数据量为：973.00 :
Model loaded from D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/mnist_cnn_model
原模型评估：
Accuracy: 26.82%
Epoch 1/5, Loss: 2.2992
Epoch 2/5, Loss: 2.2946
Epoch 3/5, Loss: 2.2887
Epoch 4/5, Loss: 2.2803
**** log-parameter_analysis 运行时间： 2025-02-11 14:19:19 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/initial/mnist_cnn_initial_model
Model saved to D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/mnist_cnn_model
初始化模型的准确率：
Accuracy: 26.82%
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.0872098507964944
DataOwner2: noise random: 0.07800871636031947
DataOwner3: noise random: 0.06790647817400938
DataOwner4: noise random: 0.004052143119414576
DataOwner5: noise random: 0.09145872541554612
DataOwner6: noise random: 0.03755068579944126
DataOwner7: noise random: 0.012018271636236977
DataOwner8: noise random: 0.06515631651991254
DataOwner9: noise random: 0.0353497996469891
DataOwner10: noise random: 0.05896958304376532
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9969194959874818, 0.9975368991414559, 0.9981360170111873, 0.9999933551439014, 0.996612190313136, 0.9994283870463954, 0.9999416031634942, 0.998285611490789, 0.9994926487079824, 0.9985959555599025]
归一化后的数据质量列表avg_f_list: [0.9090887516500109, 0.9273488242840449, 0.9450680985495261, 1.0, 0.9, 0.9832907259543997, 0.9984694038002438, 0.9494924459886261, 0.9851913035601495, 0.9586710600061897]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3358
DataOwner1的最优x_1 = 0.0696
DataOwner2的最优x_2 = 0.0912
DataOwner3的最优x_3 = 0.1106
DataOwner4的最优x_4 = 0.1618
DataOwner5的最优x_5 = 0.0581
DataOwner6的最优x_6 = 0.1475
DataOwner7的最优x_7 = 0.1605
DataOwner8的最优x_8 = 0.1152
DataOwner9的最优x_9 = 0.1492
DataOwner10的最优x_10 = 0.1245
每个DataOwner应该贡献数据比例 xn_list = [0.06956190606598912, 0.0912139316421253, 0.1105902588475689, 0.16180676814065403, 0.058098571084353116, 0.1475221787203789, 0.16054152844332123, 0.11519330754238094, 0.14920045342151944, 0.12445792405463915]
ModelOwner的最大效用 U(Eta) = 0.5749
DONE
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3358382904889337
DataOwner1的分配到的支付 ： 0.0736
DataOwner2的分配到的支付 ： 0.0985
DataOwner3的分配到的支付 ： 0.1217
DataOwner4的分配到的支付 ： 0.1884
DataOwner5的分配到的支付 ： 0.0609
DataOwner6的分配到的支付 ： 0.1689
DataOwner7的分配到的支付 ： 0.1866
DataOwner8的分配到的支付 ： 0.1273
DataOwner9的分配到的支付 ： 0.1711
DataOwner10的分配到的支付 ： 0.1389
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC1', 'DataOwner2': 'CPC2', 'DataOwner3': 'CPC3', 'DataOwner4': 'CPC4', 'DataOwner5': 'CPC5', 'DataOwner6': 'CPC6', 'DataOwner7': 'CPC7', 'DataOwner8': 'CPC8', 'DataOwner9': 'CPC9', 'DataOwner10': 'CPC10'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC2
DataOwner3 把数据交给 CPC3
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC8
DataOwner9 把数据交给 CPC9
DataOwner10 把数据交给 CPC10
DONE
----- literation 1: 模型训练 -----
CPC1调整模型中, 本轮训练的数据量为：286.00 :
Model loaded from D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/mnist_cnn_model
原模型评估：
Accuracy: 26.82%
Epoch 1/5, Loss: 2.2962
Epoch 2/5, Loss: 2.2935
Epoch 3/5, Loss: 2.2930
Epoch 4/5, Loss: 2.2917
Epoch 5/5, Loss: 2.2932
新模型评估：
Accuracy: 30.60%
Model saved to D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/mnist_cnn_model
CPC2调整模型中, 本轮训练的数据量为：824.00 :
Model loaded from D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/mnist_cnn_model
原模型评估：
Accuracy: 30.60%
Epoch 1/5, Loss: 2.2896
Epoch 2/5, Loss: 2.2852
Epoch 3/5, Loss: 2.2802
**** log-parameter_analysis 运行时间： 2025-02-11 14:29:37 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/initial/mnist_cnn_initial_model
Model saved to D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/mnist_cnn_model
初始化模型的准确率：
Accuracy: 26.82%
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.004730408262431596
DataOwner2: noise random: 0.0957942113101885
DataOwner3: noise random: 0.07534332853033898
DataOwner4: noise random: 0.01084443312664264
DataOwner5: noise random: 0.09026708612197688
DataOwner6: noise random: 0.07829379380519057
DataOwner7: noise random: 0.0008890356599701943
DataOwner8: noise random: 0.04100236887039954
DataOwner9: noise random: 0.06452790055648398
DataOwner10: noise random: 0.01490408360967479
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9999909476443113, 0.9962893945000794, 0.9977039026359197, 0.9999525323773029, 0.99670632969279, 0.9975235254752874, 0.999999680119707, 0.9993198697088743, 0.998314675886424, 0.999910026533689]
归一化后的数据质量列表avg_f_list: [0.9997646414241135, 0.9, 0.9381239689030273, 0.9987292691927896, 0.9112372802380787, 0.933262425099552, 1.0, 0.9816776798196771, 0.9545855924306951, 0.9975836473196644]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.3426
DataOwner1的最优x_1 = 0.1575
DataOwner2的最优x_2 = 0.0515
DataOwner3的最优x_3 = 0.0977
DataOwner4的最优x_4 = 0.1566
DataOwner5的最优x_5 = 0.0659
DataOwner6的最优x_6 = 0.0922
DataOwner7的最优x_7 = 0.1577
DataOwner8的最优x_8 = 0.1416
DataOwner9的最优x_9 = 0.1153
DataOwner10的最优x_10 = 0.1557
每个DataOwner应该贡献数据比例 xn_list = [0.15752624709284516, 0.051499605530497025, 0.09765340444966848, 0.1566488294031596, 0.06593710926671371, 0.09219808673108844, 0.15772514609208696, 0.1416110845116208, 0.11527195032604093, 0.15567333853522702]
ModelOwner的最大效用 U(Eta) = 0.5827
DONE
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.3426479688498996
DataOwner1的分配到的支付 ： 0.1823
DataOwner2的分配到的支付 ： 0.0536
DataOwner3的分配到的支付 ： 0.1060
DataOwner4的分配到的支付 ： 0.1811
DataOwner5的分配到的支付 ： 0.0695
DataOwner6的分配到的支付 ： 0.0996
DataOwner7的分配到的支付 ： 0.1825
DataOwner8的分配到的支付 ： 0.1609
DataOwner9的分配到的支付 ： 0.1274
DataOwner10的分配到的支付 ： 0.1797
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC1', 'DataOwner2': 'CPC2', 'DataOwner3': 'CPC3', 'DataOwner4': 'CPC4', 'DataOwner5': 'CPC5', 'DataOwner6': 'CPC6', 'DataOwner7': 'CPC7', 'DataOwner8': 'CPC8', 'DataOwner9': 'CPC9', 'DataOwner10': 'CPC10'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC2
DataOwner3 把数据交给 CPC3
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC8
DataOwner9 把数据交给 CPC9
DataOwner10 把数据交给 CPC10
DONE
----- literation 1: 模型训练 -----
CPC1调整模型中, 本轮训练的数据量为：472.00 :
Model loaded from D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/mnist_cnn_model
原模型评估：
Accuracy: 26.82%
Epoch 1/5, Loss: 2.2942
Epoch 2/5, Loss: 2.2951
Epoch 3/5, Loss: 2.2926
Epoch 4/5, Loss: 2.2894
Epoch 5/5, Loss: 2.2875
新模型评估：
Accuracy: 31.76%
Model saved to D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/mnist_cnn_model
CPC2调整模型中, 本轮训练的数据量为：154.00 :
Model loaded from D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/mnist_cnn_model
原模型评估：
Accuracy: 31.76%
Epoch 1/5, Loss: 2.2821
Epoch 2/5, Loss: 2.2759
Epoch 3/5, Loss: 2.2781
Epoch 4/5, Loss: 2.2718
Epoch 5/5, Loss: 2.2699
新模型评估：
Accuracy: 34.74%
Model saved to D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/mnist_cnn_model
CPC3调整模型中, 本轮训练的数据量为：878.00 :
Model loaded from D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/mnist_cnn_model
原模型评估：
Accuracy: 34.74%
Epoch 1/5, Loss: 2.2808
Epoch 2/5, Loss: 2.2747
Epoch 3/5, Loss: 2.2671
Epoch 4/5, Loss: 2.2593
**** log-parameter_analysis 运行时间： 2025-02-11 15:18:25 ****
========================= 客户端数量: 2 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
**** log-parameter_analysis 运行时间： 2025-02-11 15:20:24 ****
========================= 客户端数量: 2 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
**** log-parameter_analysis 运行时间： 2025-02-11 15:21:13 ****
========================= 客户端数量: 2 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/initial/mnist_cnn_initial_model
Model saved to D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/mnist_cnn_model
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.01129893460684871
DataOwner2: noise random: 0.07930574929122106
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9999483769451936, 0.9974548402352506]
归一化后的数据质量列表avg_f_list: [1.0, 0.9]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.3889
DataOwner1的最优x_1 = 0.0970
DataOwner2的最优x_2 = 0.0970
每个DataOwner应该贡献数据比例 xn_list = [0.09695284916255063, 0.09695284916257853]
ModelOwner的最大效用 U(Eta) = 0.0338
DONE
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：0.3888886505297596
DataOwner1的分配到的支付 ： 0.2047
DataOwner2的分配到的支付 ： 0.1842
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC1', 'DataOwner2': 'CPC2'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC2
DONE
----- literation 1: 模型训练 -----
CPC1调整模型中, 本轮训练的数据量为：2908.00 :
Model loaded from D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/mnist_cnn_model
原模型评估：
Accuracy: 26.82%
**** log-parameter_analysis-MIX 运行时间： 2025-02-11 15:27:05 ****
========================= 客户端数量: 2 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/initial/mnist_cnn_initial_model
Model saved to D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/mnist_cnn_model
DONE
----- 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.05602135809012457
DataOwner2: noise random: 0.05033405653223823
DONE
----- 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9987294340763224, 0.9989749170173005]
归一化后的数据质量列表avg_f_list: [0.9, 1.0]
DONE
========================= literation: 1 =========================
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.3889
DataOwner1的最优x_1 = 0.0970
DataOwner2的最优x_2 = 0.0970
每个DataOwner应该贡献数据比例 xn_list = [0.0969528491625785, 0.09695284916255063]
ModelOwner的最大效用 U(Eta) = 0.0338
DONE
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：0.3888886505297596
DataOwner1的分配到的支付 ： 0.1842
DataOwner2的分配到的支付 ： 0.2047
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC1', 'DataOwner2': 'CPC2'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC2
DONE
----- literation 1: 模型训练 -----
CPC1调整模型中, 本轮训练的数据量为：5494.00 :
Model loaded from D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/mnist_cnn_model
原模型评估：
Accuracy: 26.82%
Epoch 1/5, Loss: 2.2854
Epoch 2/5, Loss: 2.2160
Epoch 3/5, Loss: 2.0317
Epoch 4/5, Loss: 1.7248
Epoch 5/5, Loss: 1.3840
新模型评估：
Accuracy: 68.25%
Model saved to D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/mnist_cnn_model
CPC2调整模型中, 本轮训练的数据量为：323.00 :
Model loaded from D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/mnist_cnn_model
原模型评估：
Accuracy: 68.25%
Epoch 1/5, Loss: 1.1688
Epoch 2/5, Loss: 1.2952
Epoch 3/5, Loss: 1.2813
Epoch 4/5, Loss: 1.2529
Epoch 5/5, Loss: 1.1189
新模型评估：
Accuracy: 72.11%
Model saved to D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/mnist_cnn_model
DONE
========================= literation: 2 =========================
----- literation 2: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.3889
DataOwner1的最优x_1 = 0.0970
DataOwner2的最优x_2 = 0.0970
每个DataOwner应该贡献数据比例 xn_list = [0.0969528491625785, 0.09695284916255063]
ModelOwner的最大效用 U(Eta) = 0.0338
DONE
----- literation 2: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：0.3888886505297596
DataOwner1的分配到的支付 ： 0.1842
DataOwner2的分配到的支付 ： 0.2047
DONE
----- literation 2: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC2
DONE
----- literation 2: 模型训练 -----
CPC1调整模型中, 本轮训练的数据量为：5494.00 :
Model loaded from D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/mnist_cnn_model
原模型评估：
Accuracy: 72.11%
Epoch 1/5, Loss: 1.0867
**** log-parameter_analysis 运行时间： 2025-02-11 15:34:20 ****
========================= 客户端数量: 2 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/initial/mnist_cnn_initial_model
Model saved to D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/mnist_cnn_model
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.03227826227525093
DataOwner2: noise random: 0.06919097162901892
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9995784124532222, 0.998062256238304]
归一化后的数据质量列表avg_f_list: [1.0, 0.9]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 0.3889
DataOwner1的最优x_1 = 0.0970
DataOwner2的最优x_2 = 0.0970
每个DataOwner应该贡献数据比例 xn_list = [0.09695284916255063, 0.09695284916257853]
ModelOwner的最大效用 U(Eta) = 0.0338
DONE
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：0.3888886505297596
DataOwner1的分配到的支付 ： 0.2047
DataOwner2的分配到的支付 ： 0.1842
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC1', 'DataOwner2': 'CPC2'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC2
DONE
----- literation 1: 模型训练 -----
CPC1调整模型中, 本轮训练的数据量为：1163.00 :
Model loaded from D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/mnist_cnn_model
原模型评估：
Accuracy: 26.82%
Epoch 1/5, Loss: 2.2961
Epoch 2/5, Loss: 2.2894
Epoch 3/5, Loss: 2.2845
**** log-parameter_analysis 运行时间： 2025-02-11 15:46:54 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/initial/mnist_cnn_initial_model
Model saved to D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/mnist_cnn_model
初始化模型的准确率：
Accuracy: 26.82%
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.025
DataOwner2: noise random: 0.025
DataOwner3: noise random: 0.05
DataOwner4: noise random: 0.05
DataOwner5: noise random: 0.05
DataOwner6: noise random: 0.05
DataOwner7: noise random: 0.05
DataOwner8: noise random: 0.05
DataOwner9: noise random: 0.05
DataOwner10: noise random: 0.05
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
**** log-parameter_analysis 运行时间： 2025-02-11 15:48:27 ****
========================= 客户端数量: 10 =========================
---------------------------------- 定义参数值 ----------------------------------
DONE
---------------------------------- 准备工作 ----------------------------------
初始数据占MNIST的比例：0.1%
model initing...
Model loaded from D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/initial/mnist_cnn_initial_model
Model saved to D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/mnist_cnn_model
初始化模型的准确率：
Accuracy: 26.82%
DONE
========================= literation: 1 =========================
----- literation 1: 为 DataOwner 的数据添加噪声 -----
DataOwner1: noise random: 0.025
DataOwner2: noise random: 0.025
DataOwner3: noise random: 0.05
DataOwner4: noise random: 0.05
DataOwner5: noise random: 0.05
DataOwner6: noise random: 0.05
DataOwner7: noise random: 0.05
DataOwner8: noise random: 0.05
DataOwner9: noise random: 0.05
DataOwner10: noise random: 0.05
DONE
----- literation 1: 计算 DataOwner 的数据质量 -----
DataOwners自行评估数据质量：
数据质量列表avg_f_list: [0.9997471634361871, 0.9997473587820458, 0.9989895155061571, 0.9989876618960292, 0.9989899346819713, 0.9989885011899569, 0.9989902483516906, 0.9989872338120389, 0.9989869061773302, 0.9989859977367509]
归一化后的数据质量列表avg_f_list: [0.9999743425461655, 1.0, 0.9004620369570985, 0.9002185768878712, 0.9005170930722965, 0.9003288128833792, 0.9005582916233976, 0.9001623507395936, 0.900119317974689, 0.9]
DONE
----- literation 1: 计算 ModelOwner 总体支付和 DataOwners 最优数据量 -----
Stackelberg均衡结果：
ModelOwner的最优Eta = 1.2904
DataOwner1的最优x_1 = 0.1848
DataOwner2的最优x_2 = 0.1848
DataOwner3的最优x_3 = 0.0970
DataOwner4的最优x_4 = 0.0967
DataOwner5的最优x_5 = 0.0970
DataOwner6的最优x_6 = 0.0968
DataOwner7的最优x_7 = 0.0971
DataOwner8的最优x_8 = 0.0967
DataOwner9的最优x_9 = 0.0966
DataOwner10的最优x_10 = 0.0965
每个DataOwner应该贡献数据比例 xn_list = [0.1848166386844928, 0.18483452806597586, 0.09698758654651417, 0.0967195426360644, 0.09704815999954675, 0.09684094757412577, 0.09709347693798627, 0.09665759554674594, 0.09661017345973885, 0.09647863513225532]
ModelOwner的最大效用 U(Eta) = 0.5247
DONE
----- literation 1: DataOwner 分配 ModelOwner 的支付 -----
ModelOwner的最优总支付：1.290435848657937
DataOwner1的分配到的支付 ： 0.2235
DataOwner2的分配到的支付 ： 0.2236
DataOwner3的分配到的支付 ： 0.1056
DataOwner4的分配到的支付 ： 0.1053
DataOwner5的分配到的支付 ： 0.1057
DataOwner6的分配到的支付 ： 0.1055
DataOwner7的分配到的支付 ： 0.1058
DataOwner8的分配到的支付 ： 0.1052
DataOwner9的分配到的支付 ： 0.1052
DataOwner10的分配到的支付 ： 0.1050
DONE
----- literation 1: 匹配 DataOwner 和 CPC -----
{'DataOwner1': 'CPC1', 'DataOwner2': 'CPC2', 'DataOwner3': 'CPC3', 'DataOwner4': 'CPC4', 'DataOwner5': 'CPC5', 'DataOwner6': 'CPC6', 'DataOwner7': 'CPC7', 'DataOwner8': 'CPC8', 'DataOwner9': 'CPC9', 'DataOwner10': 'CPC10'}
DONE
----- literation 1: DataOwner 向 CPC 提交数据 -----
DataOwner1 把数据交给 CPC1
DataOwner2 把数据交给 CPC2
DataOwner3 把数据交给 CPC3
DataOwner4 把数据交给 CPC4
DataOwner5 把数据交给 CPC5
DataOwner6 把数据交给 CPC6
DataOwner7 把数据交给 CPC7
DataOwner8 把数据交给 CPC8
DataOwner9 把数据交给 CPC9
DataOwner10 把数据交给 CPC10
DONE
----- literation 1: 模型训练 -----
CPC1调整模型中, 本轮训练的数据量为：1093.00 :
Model loaded from D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/mnist_cnn_model
原模型评估：
Accuracy: 26.82%
Epoch 1/5, Loss: 2.3035
Epoch 2/5, Loss: 2.2957
Epoch 3/5, Loss: 2.2942
Epoch 4/5, Loss: 2.2848
Epoch 5/5, Loss: 2.2807
新模型评估：
Accuracy: 37.74%
Model saved to D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/mnist_cnn_model
CPC2调整模型中, 本轮训练的数据量为：1097.00 :
Model loaded from D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/mnist_cnn_model
原模型评估：
Accuracy: 37.74%
Epoch 1/5, Loss: 2.2605
Epoch 2/5, Loss: 2.2460
Epoch 3/5, Loss: 2.2336
Epoch 4/5, Loss: 2.2160
Epoch 5/5, Loss: 2.1959
新模型评估：
Accuracy: 41.33%
Model saved to D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/mnist_cnn_model
CPC3调整模型中, 本轮训练的数据量为：586.00 :
Model loaded from D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/mnist_cnn_model
原模型评估：
Accuracy: 41.33%
Epoch 1/5, Loss: 2.1982
Epoch 2/5, Loss: 2.1806
Epoch 3/5, Loss: 2.1642
Epoch 4/5, Loss: 2.1539
Epoch 5/5, Loss: 2.1399
新模型评估：
Accuracy: 45.64%
Model saved to D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/mnist_cnn_model
CPC4调整模型中, 本轮训练的数据量为：581.00 :
Model loaded from D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/mnist_cnn_model
原模型评估：
Accuracy: 45.64%
Epoch 1/5, Loss: 2.1005
Epoch 2/5, Loss: 2.1152
Epoch 3/5, Loss: 2.1204
Epoch 4/5, Loss: 2.0686
Epoch 5/5, Loss: 2.0819
新模型评估：
Accuracy: 46.16%
Model saved to D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/mnist_cnn_model
CPC5调整模型中, 本轮训练的数据量为：581.00 :
Model loaded from D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/mnist_cnn_model
原模型评估：
Accuracy: 46.16%
Epoch 1/5, Loss: 2.0862
Epoch 2/5, Loss: 2.0677
Epoch 3/5, Loss: 2.0576
Epoch 4/5, Loss: 2.0442
Epoch 5/5, Loss: 2.0174
新模型评估：
Accuracy: 47.42%
Model saved to D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/mnist_cnn_model
CPC6调整模型中, 本轮训练的数据量为：572.00 :
Model loaded from D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/mnist_cnn_model
原模型评估：
Accuracy: 47.42%
Epoch 1/5, Loss: 2.0026
Epoch 2/5, Loss: 1.9855
Epoch 3/5, Loss: 1.9674
Epoch 4/5, Loss: 1.9507
Epoch 5/5, Loss: 1.9333
新模型评估：
Accuracy: 51.23%
Model saved to D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/mnist_cnn_model
CPC7调整模型中, 本轮训练的数据量为：590.00 :
Model loaded from D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/mnist_cnn_model
原模型评估：
Accuracy: 51.23%
Epoch 1/5, Loss: 1.8907
Epoch 2/5, Loss: 1.8750
Epoch 3/5, Loss: 1.8550
Epoch 4/5, Loss: 1.8389
Epoch 5/5, Loss: 1.8006
新模型评估：
Accuracy: 54.57%
Model saved to D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/mnist_cnn_model
CPC8调整模型中, 本轮训练的数据量为：570.00 :
Model loaded from D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/mnist_cnn_model
原模型评估：
Accuracy: 54.57%
Epoch 1/5, Loss: 1.8269
Epoch 2/5, Loss: 1.8051
Epoch 3/5, Loss: 1.7842
Epoch 4/5, Loss: 1.7631
Epoch 5/5, Loss: 1.7422
新模型评估：
Accuracy: 56.76%
Model saved to D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/mnist_cnn_model
CPC9调整模型中, 本轮训练的数据量为：592.00 :
Model loaded from D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/mnist_cnn_model
原模型评估：
Accuracy: 56.76%
Epoch 1/5, Loss: 1.7320
Epoch 2/5, Loss: 1.6832
Epoch 3/5, Loss: 1.6604
Epoch 4/5, Loss: 1.6353
Epoch 5/5, Loss: 1.6262
新模型评估：
Accuracy: 61.70%
Model saved to D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/mnist_cnn_model
CPC10调整模型中, 本轮训练的数据量为：585.00 :
Model loaded from D:/project/academic-paper-writing/QD-RDFL/QD-RDFL/data/model/mnist_cnn_model
原模型评估：
Accuracy: 61.70%
Epoch 1/5, Loss: 1.5790
Epoch 2/5, Loss: 1.5663
Epoch 3/5, Loss: 1.5068
Epoch 4/5, Loss: 1.4834
Epoch 5/5, Loss: 1.4773
新模型评估：
